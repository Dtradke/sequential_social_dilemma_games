== Status ==
Memory usage on this node: 8.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+
| Trial name                           | status   | loc   |
|--------------------------------------+----------+-------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |
+--------------------------------------+----------+-------+


[2m[36m(pid=28570)[0m 2021-11-21 10:26:48,943	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=28570)[0m 2021-11-21 10:26:48,949	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=28570)[0m 2021-11-21 10:28:39,280	INFO trainable.py:180 -- _setup took 110.337 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=28570)[0m 2021-11-21 10:28:39,281	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=28570)[0m 2021-11-21 10:28:39,281	WARNING util.py:37 -- Install gputil for GPU system monitoring.
Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.8541666666666665
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.1979166666666665
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 3.9583333333333335
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.875
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 4.114583333333333
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 137
    cleaning_beam_agent-0_mean: 112.84375
    cleaning_beam_agent-0_min: 89
    cleaning_beam_agent-1_max: 129
    cleaning_beam_agent-1_mean: 108.13541666666667
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 134
    cleaning_beam_agent-2_mean: 110.71875
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 102.51041666666667
    cleaning_beam_agent-3_min: 64
    cleaning_beam_agent-4_max: 137
    cleaning_beam_agent-4_mean: 105.92708333333333
    cleaning_beam_agent-4_min: 83
    cleaning_beam_agent-5_max: 130
    cleaning_beam_agent-5_mean: 107.40625
    cleaning_beam_agent-5_min: 87
    fire_beam_agent-0_max: 143
    fire_beam_agent-0_mean: 121.78125
    fire_beam_agent-0_min: 99
    fire_beam_agent-1_max: 149
    fire_beam_agent-1_mean: 115.09375
    fire_beam_agent-1_min: 95
    fire_beam_agent-2_max: 134
    fire_beam_agent-2_mean: 111.6875
    fire_beam_agent-2_min: 88
    fire_beam_agent-3_max: 153
    fire_beam_agent-3_mean: 117.41666666666667
    fire_beam_agent-3_min: 92
    fire_beam_agent-4_max: 141
    fire_beam_agent-4_mean: 119.23958333333333
    fire_beam_agent-4_min: 87
    fire_beam_agent-5_max: 131
    fire_beam_agent-5_mean: 112.0625
    fire_beam_agent-5_min: 90
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-31-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -3084.0
  episode_reward_mean: -7038.895833333333
  episode_reward_min: -14723.0
  episodes_this_iter: 96
  episodes_total: 96
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 23489.663
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.18544340133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.011312104761600494
        model: {}
        policy_loss: -0.0015992950648069382
        total_loss: 1.4073692560195923
        vf_explained_var: -0.00037410855293273926
        vf_loss: 14105.525390625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1913046836853027
        entropy_coeff: 0.0017600000137463212
        kl: 0.00514399167150259
        model: {}
        policy_loss: -0.0010286136530339718
        total_loss: 1.3705286979675293
        vf_explained_var: -0.0006081163883209229
        vf_loss: 13743.8515625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1917786598205566
        entropy_coeff: 0.0017600000137463212
        kl: 0.006543819326907396
        model: {}
        policy_loss: -0.0024220943450927734
        total_loss: 1.4063785076141357
        vf_explained_var: -0.0005866885185241699
        vf_loss: 14113.4921875
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1901793479919434
        entropy_coeff: 0.0017600000137463212
        kl: 0.007735809776932001
        model: {}
        policy_loss: -0.0016578701324760914
        total_loss: 1.4139631986618042
        vf_explained_var: -0.0005681216716766357
        vf_loss: 14179.287109375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1900105476379395
        entropy_coeff: 0.0017600000137463212
        kl: 0.007361759431660175
        model: {}
        policy_loss: -0.0027634077705442905
        total_loss: 1.4202945232391357
        vf_explained_var: -0.00038000941276550293
        vf_loss: 14254.400390625
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.186877727508545
        entropy_coeff: 0.0017600000137463212
        kl: 0.008716455660760403
        model: {}
        policy_loss: -0.002312395954504609
        total_loss: 1.3923126459121704
        vf_explained_var: -0.0006793439388275146
        vf_loss: 13967.306640625
    load_time_ms: 18205.427
    num_steps_sampled: 96000
    num_steps_trained: 96000
    sample_time_ms: 121907.926
    update_time_ms: 3497.802
  iterations_since_restore: 1
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.12420634920635
    ram_util_percent: 10.136904761904763
  pid: 28570
  policy_reward_max:
    agent-0: -460.0
    agent-1: -460.0
    agent-2: -471.0
    agent-3: -471.0
    agent-4: -309.5
    agent-5: -309.5
  policy_reward_mean:
    agent-0: -1168.171875
    agent-1: -1168.171875
    agent-2: -1179.1770833333333
    agent-3: -1179.1770833333333
    agent-4: -1172.0989583333333
    agent-5: -1172.0989583333333
  policy_reward_min:
    agent-0: -2678.5
    agent-1: -2678.5
    agent-2: -2438.0
    agent-3: -2438.0
    agent-4: -2466.5
    agent-5: -2466.5
  sampler_perf:
    mean_env_wait_ms: 29.95783553058371
    mean_inference_ms: 15.500168343047639
    mean_processing_ms: 66.48525098463396
  time_since_restore: 170.2771759033203
  time_this_iter_s: 170.2771759033203
  time_total_s: 170.2771759033203
  timestamp: 1637508695
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 96000
  training_iteration: 1
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+-------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |    ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+-------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      1 |          170.277 | 96000 |  -7038.9 |
+--------------------------------------+----------+-------------------+--------+------------------+-------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 5.02
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.79
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 3.7
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.94
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 4.48
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 117
    cleaning_beam_agent-0_mean: 94.83
    cleaning_beam_agent-0_min: 75
    cleaning_beam_agent-1_max: 129
    cleaning_beam_agent-1_mean: 104.19
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 141
    cleaning_beam_agent-2_mean: 110.92
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 118.19
    cleaning_beam_agent-3_min: 90
    cleaning_beam_agent-4_max: 131
    cleaning_beam_agent-4_mean: 106.12
    cleaning_beam_agent-4_min: 83
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 111.93
    cleaning_beam_agent-5_min: 86
    fire_beam_agent-0_max: 169
    fire_beam_agent-0_mean: 135.4
    fire_beam_agent-0_min: 104
    fire_beam_agent-1_max: 137
    fire_beam_agent-1_mean: 106.99
    fire_beam_agent-1_min: 84
    fire_beam_agent-2_max: 125
    fire_beam_agent-2_mean: 95.84
    fire_beam_agent-2_min: 76
    fire_beam_agent-3_max: 133
    fire_beam_agent-3_mean: 109.88
    fire_beam_agent-3_min: 82
    fire_beam_agent-4_max: 128
    fire_beam_agent-4_mean: 101.4
    fire_beam_agent-4_min: 78
    fire_beam_agent-5_max: 138
    fire_beam_agent-5_mean: 110.37
    fire_beam_agent-5_min: 82
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-34-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -3619.0
  episode_reward_mean: -6611.88
  episode_reward_min: -9576.0
  episodes_this_iter: 96
  episodes_total: 192
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 18173.868
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.185791254043579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021091329399496317
        model: {}
        policy_loss: -7.38147646188736e-05
        total_loss: 0.9780064821243286
        vf_explained_var: -0.000917583703994751
        vf_loss: 9815.0546875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1863465309143066
        entropy_coeff: 0.0017600000137463212
        kl: 0.005223788321018219
        model: {}
        policy_loss: -0.001149836927652359
        total_loss: 0.9683672785758972
        vf_explained_var: -0.000669330358505249
        vf_loss: 9723.2041015625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.188887357711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.004636695608496666
        model: {}
        policy_loss: -0.0008427267894148827
        total_loss: 1.0798561573028564
        vf_explained_var: -0.0006222724914550781
        vf_loss: 10836.240234375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.193450689315796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017062562983483076
        model: {}
        policy_loss: -0.0008113905787467957
        total_loss: 1.0787047147750854
        vf_explained_var: -0.0005608797073364258
        vf_loss: 10830.353515625
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1862082481384277
        entropy_coeff: 0.0017600000137463212
        kl: 0.002037659753113985
        model: {}
        policy_loss: 1.634994987398386e-05
        total_loss: 1.0449507236480713
        vf_explained_var: -0.0005683302879333496
        vf_loss: 10483.7451171875
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.177963972091675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045626722276210785
        model: {}
        policy_loss: -0.000934236217290163
        total_loss: 1.0306015014648438
        vf_explained_var: -0.000618666410446167
        vf_loss: 10344.564453125
    load_time_ms: 15932.038
    num_steps_sampled: 192000
    num_steps_trained: 192000
    sample_time_ms: 123059.231
    update_time_ms: 1760.026
  iterations_since_restore: 2
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.566355140186918
    ram_util_percent: 11.32523364485981
  pid: 28570
  policy_reward_max:
    agent-0: -437.0
    agent-1: -437.0
    agent-2: -435.5
    agent-3: -435.5
    agent-4: -489.0
    agent-5: -489.0
  policy_reward_mean:
    agent-0: -1079.2
    agent-1: -1079.2
    agent-2: -1119.74
    agent-3: -1119.74
    agent-4: -1107.0
    agent-5: -1107.0
  policy_reward_min:
    agent-0: -1983.5
    agent-1: -1983.5
    agent-2: -1913.0
    agent-3: -1913.0
    agent-4: -1848.5
    agent-5: -1848.5
  sampler_perf:
    mean_env_wait_ms: 30.415869815981114
    mean_inference_ms: 15.245912265592997
    mean_processing_ms: 67.51468912625388
  time_since_restore: 321.13848757743835
  time_this_iter_s: 150.86131167411804
  time_total_s: 321.13848757743835
  timestamp: 1637508846
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 192000
  training_iteration: 2
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 19.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      2 |          321.138 | 192000 | -6611.88 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 4.73
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.85
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 4.23
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.71
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 4.79
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 103
    cleaning_beam_agent-0_mean: 80.53
    cleaning_beam_agent-0_min: 63
    cleaning_beam_agent-1_max: 124
    cleaning_beam_agent-1_mean: 96.6
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 136
    cleaning_beam_agent-2_mean: 111.07
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 130.4
    cleaning_beam_agent-3_min: 105
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 102.04
    cleaning_beam_agent-4_min: 73
    cleaning_beam_agent-5_max: 147
    cleaning_beam_agent-5_mean: 123.96
    cleaning_beam_agent-5_min: 101
    fire_beam_agent-0_max: 163
    fire_beam_agent-0_mean: 129.46
    fire_beam_agent-0_min: 103
    fire_beam_agent-1_max: 120
    fire_beam_agent-1_mean: 92.65
    fire_beam_agent-1_min: 67
    fire_beam_agent-2_max: 110
    fire_beam_agent-2_mean: 84.77
    fire_beam_agent-2_min: 65
    fire_beam_agent-3_max: 125
    fire_beam_agent-3_mean: 98.77
    fire_beam_agent-3_min: 71
    fire_beam_agent-4_max: 120
    fire_beam_agent-4_mean: 96.61
    fire_beam_agent-4_min: 74
    fire_beam_agent-5_max: 121
    fire_beam_agent-5_mean: 100.05
    fire_beam_agent-5_min: 80
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-36-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2475.0
  episode_reward_mean: -5925.73
  episode_reward_min: -10218.0
  episodes_this_iter: 96
  episodes_total: 288
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 16391.525
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.180128335952759
        entropy_coeff: 0.0017600000137463212
        kl: 0.00311154592782259
        model: {}
        policy_loss: -0.000516233965754509
        total_loss: 0.7243826389312744
        vf_explained_var: -0.001124948263168335
        vf_loss: 7284.24853515625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012480191653594375
        entropy: 2.1852405071258545
        entropy_coeff: 0.0017600000137463212
        kl: 0.003925447352230549
        model: {}
        policy_loss: -0.0009755464270710945
        total_loss: 0.7273887395858765
        vf_explained_var: -0.0008118748664855957
        vf_loss: 7314.25244140625
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1792712211608887
        entropy_coeff: 0.0017600000137463212
        kl: 0.002747512422502041
        model: {}
        policy_loss: -0.00047937314957380295
        total_loss: 0.8006654977798462
        vf_explained_var: -0.0007124543190002441
        vf_loss: 8047.05712890625
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1809446811676025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045157549902796745
        model: {}
        policy_loss: -0.00084987364243716
        total_loss: 0.7998254299163818
        vf_explained_var: -0.0006195008754730225
        vf_loss: 8040.6220703125
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.181584119796753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021663315128535032
        model: {}
        policy_loss: -0.0006137634627521038
        total_loss: 0.6913986206054688
        vf_explained_var: -0.0009826719760894775
        vf_loss: 6956.35302734375
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.172593593597412
        entropy_coeff: 0.0017600000137463212
        kl: 0.004423532634973526
        model: {}
        policy_loss: -0.0010856724111363292
        total_loss: 0.6850574016571045
        vf_explained_var: -0.0010984539985656738
        vf_loss: 6895.244140625
    load_time_ms: 15154.374
    num_steps_sampled: 288000
    num_steps_trained: 288000
    sample_time_ms: 118076.69
    update_time_ms: 1179.239
  iterations_since_restore: 3
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.15833333333333
    ram_util_percent: 10.539583333333333
  pid: 28570
  policy_reward_max:
    agent-0: -437.0
    agent-1: -437.0
    agent-2: -321.5
    agent-3: -321.5
    agent-4: -441.0
    agent-5: -441.0
  policy_reward_mean:
    agent-0: -993.225
    agent-1: -993.225
    agent-2: -1006.91
    agent-3: -1006.91
    agent-4: -962.73
    agent-5: -962.73
  policy_reward_min:
    agent-0: -1793.0
    agent-1: -1793.0
    agent-2: -1944.5
    agent-3: -1944.5
    agent-4: -2316.0
    agent-5: -2316.0
  sampler_perf:
    mean_env_wait_ms: 29.513487450475864
    mean_inference_ms: 14.689717400264964
    mean_processing_ms: 65.44816862091255
  time_since_restore: 455.8176074028015
  time_this_iter_s: 134.67911982536316
  time_total_s: 455.8176074028015
  timestamp: 1637508981
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 288000
  training_iteration: 3
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 19.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      3 |          455.818 | 288000 | -5925.73 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.67
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 5.12
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.7
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 3.72
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 2.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 106
    cleaning_beam_agent-0_mean: 84.67
    cleaning_beam_agent-0_min: 66
    cleaning_beam_agent-1_max: 141
    cleaning_beam_agent-1_mean: 113.36
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 118.77
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 143.65
    cleaning_beam_agent-3_min: 105
    cleaning_beam_agent-4_max: 127
    cleaning_beam_agent-4_mean: 102.3
    cleaning_beam_agent-4_min: 84
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 137.94
    cleaning_beam_agent-5_min: 112
    fire_beam_agent-0_max: 135
    fire_beam_agent-0_mean: 100.22
    fire_beam_agent-0_min: 75
    fire_beam_agent-1_max: 112
    fire_beam_agent-1_mean: 84.9
    fire_beam_agent-1_min: 68
    fire_beam_agent-2_max: 96
    fire_beam_agent-2_mean: 71.62
    fire_beam_agent-2_min: 55
    fire_beam_agent-3_max: 97
    fire_beam_agent-3_mean: 74.99
    fire_beam_agent-3_min: 59
    fire_beam_agent-4_max: 116
    fire_beam_agent-4_mean: 92.94
    fire_beam_agent-4_min: 70
    fire_beam_agent-5_max: 121
    fire_beam_agent-5_mean: 77.78
    fire_beam_agent-5_min: 52
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-38-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2948.0
  episode_reward_mean: -5104.31
  episode_reward_min: -7457.0
  episodes_this_iter: 96
  episodes_total: 384
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 15534.36
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1543190479278564
        entropy_coeff: 0.0017600000137463212
        kl: 0.00996621698141098
        model: {}
        policy_loss: -0.0008613057434558868
        total_loss: 0.38789355754852295
        vf_explained_var: -0.0018131732940673828
        vf_loss: 3920.4814453125
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.178865909576416
        entropy_coeff: 0.0017600000137463212
        kl: 0.003149721771478653
        model: {}
        policy_loss: -0.0005832531023770571
        total_loss: 0.3947267532348633
        vf_explained_var: -0.0012893974781036377
        vf_loss: 3988.29833984375
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1674365997314453
        entropy_coeff: 0.0017600000137463212
        kl: 0.007155544590204954
        model: {}
        policy_loss: -0.0014824504032731056
        total_loss: 0.4992678761482239
        vf_explained_var: -0.0011385083198547363
        vf_loss: 5042.07275390625
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1430859565734863
        entropy_coeff: 0.0017600000137463212
        kl: 0.016308490186929703
        model: {}
        policy_loss: -0.0014490396715700626
        total_loss: 0.4989219307899475
        vf_explained_var: -0.0010116398334503174
        vf_loss: 5033.2734375
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1499135494232178
        entropy_coeff: 0.0017600000137463212
        kl: 0.021687684580683708
        model: {}
        policy_loss: -0.002060151193290949
        total_loss: 0.44238731265068054
        vf_explained_var: -0.0011529922485351562
        vf_loss: 4471.46923828125
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1487808227539062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0069228531792759895
        model: {}
        policy_loss: -0.00018954160623252392
        total_loss: 0.44298475980758667
        vf_explained_var: -0.0013387501239776611
        vf_loss: 4466.1005859375
    load_time_ms: 14774.662
    num_steps_sampled: 384000
    num_steps_trained: 384000
    sample_time_ms: 114765.003
    update_time_ms: 888.45
  iterations_since_restore: 4
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.903723404255317
    ram_util_percent: 11.080319148936173
  pid: 28570
  policy_reward_max:
    agent-0: -401.0
    agent-1: -401.0
    agent-2: -435.0
    agent-3: -435.0
    agent-4: -454.0
    agent-5: -454.0
  policy_reward_mean:
    agent-0: -822.43
    agent-1: -822.43
    agent-2: -879.98
    agent-3: -879.98
    agent-4: -849.745
    agent-5: -849.745
  policy_reward_min:
    agent-0: -1519.5
    agent-1: -1519.5
    agent-2: -1721.0
    agent-3: -1721.0
    agent-4: -1456.0
    agent-5: -1456.0
  sampler_perf:
    mean_env_wait_ms: 28.700490976318676
    mean_inference_ms: 14.31692423409915
    mean_processing_ms: 63.78173630333582
  time_since_restore: 587.3735547065735
  time_this_iter_s: 131.55594730377197
  time_total_s: 587.3735547065735
  timestamp: 1637509113
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 384000
  training_iteration: 4
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      4 |          587.374 | 384000 | -5104.31 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 5.3
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 4.38
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 3.6
    apples_agent-2_min: 0
    apples_agent-3_max: 39
    apples_agent-3_mean: 4.53
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 4.85
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 86.36
    cleaning_beam_agent-0_min: 67
    cleaning_beam_agent-1_max: 140
    cleaning_beam_agent-1_mean: 115.75
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 161
    cleaning_beam_agent-2_mean: 132.72
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 216
    cleaning_beam_agent-3_mean: 182.65
    cleaning_beam_agent-3_min: 132
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 108.95
    cleaning_beam_agent-4_min: 88
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 148.05
    cleaning_beam_agent-5_min: 119
    fire_beam_agent-0_max: 106
    fire_beam_agent-0_mean: 72.75
    fire_beam_agent-0_min: 56
    fire_beam_agent-1_max: 96
    fire_beam_agent-1_mean: 63.99
    fire_beam_agent-1_min: 49
    fire_beam_agent-2_max: 78
    fire_beam_agent-2_mean: 59.9
    fire_beam_agent-2_min: 43
    fire_beam_agent-3_max: 76
    fire_beam_agent-3_mean: 53.52
    fire_beam_agent-3_min: 36
    fire_beam_agent-4_max: 105
    fire_beam_agent-4_mean: 79.21
    fire_beam_agent-4_min: 58
    fire_beam_agent-5_max: 82
    fire_beam_agent-5_mean: 61.57
    fire_beam_agent-5_min: 44
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-40-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2194.0
  episode_reward_mean: -3952.28
  episode_reward_min: -7457.0
  episodes_this_iter: 96
  episodes_total: 480
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 15001.273
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.1121277809143066
        entropy_coeff: 0.0017600000137463212
        kl: 0.013315027579665184
        model: {}
        policy_loss: -0.0003619017079472542
        total_loss: 0.21696487069129944
        vf_explained_var: -0.0018298327922821045
        vf_loss: 2203.78369140625
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.158231496810913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0053035300225019455
        model: {}
        policy_loss: -0.0008631478995084763
        total_loss: 0.22154293954372406
        vf_explained_var: -0.0012502968311309814
        vf_loss: 2259.39404296875
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.151855707168579
        entropy_coeff: 0.0017600000137463212
        kl: 0.005215308628976345
        model: {}
        policy_loss: -0.000537017360329628
        total_loss: 0.244344562292099
        vf_explained_var: -0.001480698585510254
        vf_loss: 2484.080810546875
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.1387922763824463
        entropy_coeff: 0.0017600000137463212
        kl: 0.006707625463604927
        model: {}
        policy_loss: -0.0004946572007611394
        total_loss: 0.2438916712999344
        vf_explained_var: -0.0013176798820495605
        vf_loss: 2478.15234375
      agent-4:
        cur_kl_coeff: 0.07500000298023224
        cur_lr: 0.0012360383989289403
        entropy: 2.1456398963928223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049033863469958305
        model: {}
        policy_loss: -0.0004847487434744835
        total_loss: 0.21052470803260803
        vf_explained_var: -0.0015864074230194092
        vf_loss: 2144.18017578125
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.1280710697174072
        entropy_coeff: 0.0017600000137463212
        kl: 0.011088884435594082
        model: {}
        policy_loss: -0.0011689169332385063
        total_loss: 0.21086396276950836
        vf_explained_var: -0.0016942322254180908
        vf_loss: 2152.23876953125
    load_time_ms: 14566.143
    num_steps_sampled: 480000
    num_steps_trained: 480000
    sample_time_ms: 113029.616
    update_time_ms: 714.063
  iterations_since_restore: 5
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.2026455026455
    ram_util_percent: 11.639682539682543
  pid: 28570
  policy_reward_max:
    agent-0: -145.0
    agent-1: -145.0
    agent-2: -260.5
    agent-3: -260.5
    agent-4: -178.0
    agent-5: -178.0
  policy_reward_mean:
    agent-0: -653.225
    agent-1: -653.225
    agent-2: -676.415
    agent-3: -676.415
    agent-4: -646.5
    agent-5: -646.5
  policy_reward_min:
    agent-0: -1232.0
    agent-1: -1232.0
    agent-2: -1721.0
    agent-3: -1721.0
    agent-4: -1145.5
    agent-5: -1145.5
  sampler_perf:
    mean_env_wait_ms: 28.253580945594337
    mean_inference_ms: 14.082302534024631
    mean_processing_ms: 62.83614326462268
  time_since_restore: 720.1948573589325
  time_this_iter_s: 132.821302652359
  time_total_s: 720.1948573589325
  timestamp: 1637509245
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 480000
  training_iteration: 5
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      5 |          720.195 | 480000 | -3952.28 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 36
    apples_agent-1_mean: 4.89
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 4.68
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 4.31
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 4.05
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 99
    cleaning_beam_agent-0_mean: 70.02
    cleaning_beam_agent-0_min: 51
    cleaning_beam_agent-1_max: 136
    cleaning_beam_agent-1_mean: 114.79
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 169
    cleaning_beam_agent-2_mean: 145.87
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 216
    cleaning_beam_agent-3_mean: 170.25
    cleaning_beam_agent-3_min: 143
    cleaning_beam_agent-4_max: 141
    cleaning_beam_agent-4_mean: 110.92
    cleaning_beam_agent-4_min: 81
    cleaning_beam_agent-5_max: 200
    cleaning_beam_agent-5_mean: 169.45
    cleaning_beam_agent-5_min: 138
    fire_beam_agent-0_max: 81
    fire_beam_agent-0_mean: 55.81
    fire_beam_agent-0_min: 41
    fire_beam_agent-1_max: 76
    fire_beam_agent-1_mean: 52.58
    fire_beam_agent-1_min: 34
    fire_beam_agent-2_max: 71
    fire_beam_agent-2_mean: 45.87
    fire_beam_agent-2_min: 27
    fire_beam_agent-3_max: 71
    fire_beam_agent-3_mean: 42.82
    fire_beam_agent-3_min: 26
    fire_beam_agent-4_max: 88
    fire_beam_agent-4_mean: 62.24
    fire_beam_agent-4_min: 46
    fire_beam_agent-5_max: 65
    fire_beam_agent-5_mean: 42.59
    fire_beam_agent-5_min: 28
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-42-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -1544.0
  episode_reward_mean: -3027.08
  episode_reward_min: -6092.0
  episodes_this_iter: 96
  episodes_total: 576
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 14641.468
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.0973076820373535
        entropy_coeff: 0.0017600000137463212
        kl: 0.006704899948090315
        model: {}
        policy_loss: -0.0006869311910122633
        total_loss: 0.1130269467830658
        vf_explained_var: -0.0023843348026275635
        vf_loss: 1170.698974609375
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1309590339660645
        entropy_coeff: 0.0017600000137463212
        kl: 0.013991281390190125
        model: {}
        policy_loss: 0.0002359524369239807
        total_loss: 0.11632794141769409
        vf_explained_var: -0.0016758441925048828
        vf_loss: 1191.42919921875
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1296753883361816
        entropy_coeff: 0.0017600000137463212
        kl: 0.008151048794388771
        model: {}
        policy_loss: -0.0013461913913488388
        total_loss: 0.11094559729099274
        vf_explained_var: -0.0020467936992645264
        vf_loss: 1156.3245849609375
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1241204738616943
        entropy_coeff: 0.0017600000137463212
        kl: 0.008590172976255417
        model: {}
        policy_loss: -0.0010761748999357224
        total_loss: 0.1109994500875473
        vf_explained_var: -0.0019392967224121094
        vf_loss: 1153.845703125
      agent-4:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0012300480157136917
        entropy: 2.1296379566192627
        entropy_coeff: 0.0017600000137463212
        kl: 0.020355191081762314
        model: {}
        policy_loss: -0.0007614577189087868
        total_loss: 0.12033003568649292
        vf_explained_var: -0.00212743878364563
        vf_loss: 1240.7633056640625
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.0971782207489014
        entropy_coeff: 0.0017600000137463212
        kl: 0.006138993427157402
        model: {}
        policy_loss: -0.00044652726501226425
        total_loss: 0.12080781906843185
        vf_explained_var: -0.00229722261428833
        vf_loss: 1246.38427734375
    load_time_ms: 14341.7
    num_steps_sampled: 576000
    num_steps_trained: 576000
    sample_time_ms: 111629.209
    update_time_ms: 597.841
  iterations_since_restore: 6
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.23333333333333
    ram_util_percent: 11.603225806451617
  pid: 28570
  policy_reward_max:
    agent-0: -116.5
    agent-1: -116.5
    agent-2: -88.5
    agent-3: -88.5
    agent-4: -187.5
    agent-5: -187.5
  policy_reward_mean:
    agent-0: -487.985
    agent-1: -487.985
    agent-2: -507.805
    agent-3: -507.805
    agent-4: -517.75
    agent-5: -517.75
  policy_reward_min:
    agent-0: -954.0
    agent-1: -954.0
    agent-2: -1132.0
    agent-3: -1132.0
    agent-4: -1145.5
    agent-5: -1145.5
  sampler_perf:
    mean_env_wait_ms: 27.877878124003622
    mean_inference_ms: 13.91665136139158
    mean_processing_ms: 62.13859796030569
  time_since_restore: 850.9993662834167
  time_this_iter_s: 130.80450892448425
  time_total_s: 850.9993662834167
  timestamp: 1637509376
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 576000
  training_iteration: 6
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      6 |          850.999 | 576000 | -3027.08 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 5.57
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 5.3
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 3.88
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 4.7
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 5.14
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 84.42
    cleaning_beam_agent-0_min: 56
    cleaning_beam_agent-1_max: 121
    cleaning_beam_agent-1_mean: 95.53
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 188
    cleaning_beam_agent-2_mean: 157.06
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 167.42
    cleaning_beam_agent-3_min: 134
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 85.97
    cleaning_beam_agent-4_min: 68
    cleaning_beam_agent-5_max: 219
    cleaning_beam_agent-5_mean: 191.63
    cleaning_beam_agent-5_min: 161
    fire_beam_agent-0_max: 61
    fire_beam_agent-0_mean: 46.16
    fire_beam_agent-0_min: 33
    fire_beam_agent-1_max: 65
    fire_beam_agent-1_mean: 36.12
    fire_beam_agent-1_min: 25
    fire_beam_agent-2_max: 50
    fire_beam_agent-2_mean: 33.41
    fire_beam_agent-2_min: 21
    fire_beam_agent-3_max: 53
    fire_beam_agent-3_mean: 31.49
    fire_beam_agent-3_min: 18
    fire_beam_agent-4_max: 66
    fire_beam_agent-4_mean: 44.59
    fire_beam_agent-4_min: 28
    fire_beam_agent-5_max: 45
    fire_beam_agent-5_mean: 30.65
    fire_beam_agent-5_min: 16
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-45-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -744.0
  episode_reward_mean: -2190.15
  episode_reward_min: -3868.0
  episodes_this_iter: 96
  episodes_total: 672
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 14397.26
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.058042526245117
        entropy_coeff: 0.0017600000137463212
        kl: 0.007148374803364277
        model: {}
        policy_loss: -0.00016309786587953568
        total_loss: 0.0580902062356472
        vf_explained_var: -0.0037641823291778564
        vf_loss: 615.1805419921875
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.1269402503967285
        entropy_coeff: 0.0017600000137463212
        kl: 0.006056495010852814
        model: {}
        policy_loss: -0.0009631611173972487
        total_loss: 0.05650677531957626
        vf_explained_var: -0.0027317702770233154
        vf_loss: 609.1052856445312
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.1108531951904297
        entropy_coeff: 0.0017600000137463212
        kl: 0.008950348943471909
        model: {}
        policy_loss: -0.0014510173350572586
        total_loss: 0.0667584240436554
        vf_explained_var: -0.0026968717575073242
        vf_loss: 714.7701416015625
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.088308095932007
        entropy_coeff: 0.0017600000137463212
        kl: 0.009709227830171585
        model: {}
        policy_loss: -0.0017604436725378036
        total_loss: 0.06657074391841888
        vf_explained_var: -0.002789616584777832
        vf_loss: 715.2115478515625
      agent-4:
        cur_kl_coeff: 0.05624999850988388
        cur_lr: 0.0012240576324984431
        entropy: 2.0949907302856445
        entropy_coeff: 0.0017600000137463212
        kl: 0.006327413953840733
        model: {}
        policy_loss: -0.001254718517884612
        total_loss: 0.0744982361793518
        vf_explained_var: -0.0025021135807037354
        vf_loss: 790.8422241210938
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012240576324984431
        entropy: 2.0594372749328613
        entropy_coeff: 0.0017600000137463212
        kl: 0.01132168434560299
        model: {}
        policy_loss: -0.0017009731382131577
        total_loss: 0.07427051663398743
        vf_explained_var: -0.0027107298374176025
        vf_loss: 790.3002319335938
    load_time_ms: 14191.82
    num_steps_sampled: 672000
    num_steps_trained: 672000
    sample_time_ms: 110715.296
    update_time_ms: 514.803
  iterations_since_restore: 7
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.196791443850262
    ram_util_percent: 11.607486631016044
  pid: 28570
  policy_reward_max:
    agent-0: -116.0
    agent-1: -116.0
    agent-2: -140.0
    agent-3: -140.0
    agent-4: -106.5
    agent-5: -106.5
  policy_reward_mean:
    agent-0: -355.68
    agent-1: -355.68
    agent-2: -367.055
    agent-3: -367.055
    agent-4: -372.34
    agent-5: -372.34
  policy_reward_min:
    agent-0: -786.0
    agent-1: -786.0
    agent-2: -833.0
    agent-3: -833.0
    agent-4: -788.0
    agent-5: -788.0
  sampler_perf:
    mean_env_wait_ms: 27.57738555208479
    mean_inference_ms: 13.809201945797797
    mean_processing_ms: 61.70278956771048
  time_since_restore: 982.5351107120514
  time_this_iter_s: 131.53574442863464
  time_total_s: 982.5351107120514
  timestamp: 1637509508
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 672000
  training_iteration: 7
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      7 |          982.535 | 672000 | -2190.15 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 5.68
    apples_agent-0_min: 0
    apples_agent-1_max: 31
    apples_agent-1_mean: 5.44
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 4.7
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 4.37
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 6.77
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 106
    cleaning_beam_agent-0_mean: 88.92
    cleaning_beam_agent-0_min: 62
    cleaning_beam_agent-1_max: 127
    cleaning_beam_agent-1_mean: 103.62
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 210
    cleaning_beam_agent-2_mean: 177.75
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 245
    cleaning_beam_agent-3_mean: 214.96
    cleaning_beam_agent-3_min: 164
    cleaning_beam_agent-4_max: 110
    cleaning_beam_agent-4_mean: 87.03
    cleaning_beam_agent-4_min: 68
    cleaning_beam_agent-5_max: 243
    cleaning_beam_agent-5_mean: 206.97
    cleaning_beam_agent-5_min: 176
    fire_beam_agent-0_max: 54
    fire_beam_agent-0_mean: 40.51
    fire_beam_agent-0_min: 28
    fire_beam_agent-1_max: 45
    fire_beam_agent-1_mean: 30.77
    fire_beam_agent-1_min: 16
    fire_beam_agent-2_max: 41
    fire_beam_agent-2_mean: 26.59
    fire_beam_agent-2_min: 15
    fire_beam_agent-3_max: 37
    fire_beam_agent-3_mean: 25.79
    fire_beam_agent-3_min: 14
    fire_beam_agent-4_max: 63
    fire_beam_agent-4_mean: 32.08
    fire_beam_agent-4_min: 16
    fire_beam_agent-5_max: 41
    fire_beam_agent-5_mean: 26.12
    fire_beam_agent-5_min: 16
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-47-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -944.0
  episode_reward_mean: -1751.82
  episode_reward_min: -3233.0
  episodes_this_iter: 96
  episodes_total: 768
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 14195.593
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.051604747772217
        entropy_coeff: 0.0017600000137463212
        kl: 0.008113931864500046
        model: {}
        policy_loss: -0.0017143283039331436
        total_loss: 0.05622600391507149
        vf_explained_var: -0.002814054489135742
        vf_loss: 611.45458984375
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.0866594314575195
        entropy_coeff: 0.0017600000137463212
        kl: 0.010834901593625546
        model: {}
        policy_loss: -0.0016158209182322025
        total_loss: 0.05480172485113144
        vf_explained_var: -0.0016975998878479004
        vf_loss: 595.4832153320312
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.072296142578125
        entropy_coeff: 0.0017600000137463212
        kl: 0.010102514177560806
        model: {}
        policy_loss: -0.0014453600160777569
        total_loss: 0.04831240698695183
        vf_explained_var: -0.003004610538482666
        vf_loss: 528.998779296875
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.092484712600708
        entropy_coeff: 0.0017600000137463212
        kl: 0.005118988454341888
        model: {}
        policy_loss: -0.0009736344218254089
        total_loss: 0.048334620893001556
        vf_explained_var: -0.0026202499866485596
        vf_loss: 527.3507690429688
      agent-4:
        cur_kl_coeff: 0.05624999850988388
        cur_lr: 0.0012180672492831945
        entropy: 2.026503324508667
        entropy_coeff: 0.0017600000137463212
        kl: 0.016167394816875458
        model: {}
        policy_loss: -0.002275907201692462
        total_loss: 0.05843338370323181
        vf_explained_var: -0.003100275993347168
        vf_loss: 633.6652221679688
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012180672492831945
        entropy: 2.025822877883911
        entropy_coeff: 0.0017600000137463212
        kl: 0.007542666047811508
        model: {}
        policy_loss: -0.0016772011294960976
        total_loss: 0.0583232119679451
        vf_explained_var: -0.0030518770217895508
        vf_loss: 631.88720703125
    load_time_ms: 14100.187
    num_steps_sampled: 768000
    num_steps_trained: 768000
    sample_time_ms: 110066.593
    update_time_ms: 452.203
  iterations_since_restore: 8
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.221276595744683
    ram_util_percent: 11.630319148936172
  pid: 28570
  policy_reward_max:
    agent-0: -55.0
    agent-1: -55.0
    agent-2: -85.0
    agent-3: -85.0
    agent-4: -34.0
    agent-5: -34.0
  policy_reward_mean:
    agent-0: -292.685
    agent-1: -292.685
    agent-2: -298.45
    agent-3: -298.45
    agent-4: -284.775
    agent-5: -284.775
  policy_reward_min:
    agent-0: -603.0
    agent-1: -603.0
    agent-2: -627.5
    agent-3: -627.5
    agent-4: -748.0
    agent-5: -748.0
  sampler_perf:
    mean_env_wait_ms: 27.401141794046563
    mean_inference_ms: 13.729681551731376
    mean_processing_ms: 61.387051705791265
  time_since_restore: 1114.3820140361786
  time_this_iter_s: 131.8469033241272
  time_total_s: 1114.3820140361786
  timestamp: 1637509640
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 768000
  training_iteration: 8
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      8 |          1114.38 | 768000 | -1751.82 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 6.61
    apples_agent-0_min: 0
    apples_agent-1_max: 42
    apples_agent-1_mean: 7.82
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 4.2
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 4.41
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 7.46
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 88.68
    cleaning_beam_agent-0_min: 69
    cleaning_beam_agent-1_max: 122
    cleaning_beam_agent-1_mean: 84.55
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 236
    cleaning_beam_agent-2_mean: 210.19
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 226
    cleaning_beam_agent-3_mean: 190.15
    cleaning_beam_agent-3_min: 164
    cleaning_beam_agent-4_max: 101
    cleaning_beam_agent-4_mean: 70.88
    cleaning_beam_agent-4_min: 44
    cleaning_beam_agent-5_max: 270
    cleaning_beam_agent-5_mean: 230.83
    cleaning_beam_agent-5_min: 196
    fire_beam_agent-0_max: 54
    fire_beam_agent-0_mean: 30.07
    fire_beam_agent-0_min: 18
    fire_beam_agent-1_max: 33
    fire_beam_agent-1_mean: 21.79
    fire_beam_agent-1_min: 10
    fire_beam_agent-2_max: 33
    fire_beam_agent-2_mean: 17.89
    fire_beam_agent-2_min: 8
    fire_beam_agent-3_max: 29
    fire_beam_agent-3_mean: 18.11
    fire_beam_agent-3_min: 6
    fire_beam_agent-4_max: 37
    fire_beam_agent-4_mean: 21.86
    fire_beam_agent-4_min: 11
    fire_beam_agent-5_max: 31
    fire_beam_agent-5_mean: 19.73
    fire_beam_agent-5_min: 8
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-49-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -600.0
  episode_reward_mean: -1349.85
  episode_reward_min: -2095.0
  episodes_this_iter: 96
  episodes_total: 864
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 14063.553
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.033175230026245
        entropy_coeff: 0.0017600000137463212
        kl: 0.006951755844056606
        model: {}
        policy_loss: -0.0013522659428417683
        total_loss: 0.043098557740449905
        vf_explained_var: -0.004475265741348267
        vf_loss: 476.8162536621094
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.0527005195617676
        entropy_coeff: 0.0017600000137463212
        kl: 0.006418406497687101
        model: {}
        policy_loss: -0.0012333219638094306
        total_loss: 0.041364144533872604
        vf_explained_var: -0.002718716859817505
        vf_loss: 458.8930358886719
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.0578272342681885
        entropy_coeff: 0.0017600000137463212
        kl: 0.010397246107459068
        model: {}
        policy_loss: -0.0003563209902495146
        total_loss: 0.04641829431056976
        vf_explained_var: -0.003963857889175415
        vf_loss: 498.7652893066406
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.087116241455078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026244467590004206
        model: {}
        policy_loss: -0.00037397444248199463
        total_loss: 0.045175909996032715
        vf_explained_var: -0.0037078261375427246
        vf_loss: 490.91986083984375
      agent-4:
        cur_kl_coeff: 0.05624999850988388
        cur_lr: 0.0012120767496526241
        entropy: 1.954829454421997
        entropy_coeff: 0.0017600000137463212
        kl: 0.010154908522963524
        model: {}
        policy_loss: -0.002145805163308978
        total_loss: 0.045331891626119614
        vf_explained_var: -0.004298150539398193
        vf_loss: 503.46978759765625
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012120767496526241
        entropy: 2.0035507678985596
        entropy_coeff: 0.0017600000137463212
        kl: 0.007669966667890549
        model: {}
        policy_loss: -0.001272047869861126
        total_loss: 0.046374231576919556
        vf_explained_var: -0.004529416561126709
        vf_loss: 507.89031982421875
    load_time_ms: 14016.699
    num_steps_sampled: 864000
    num_steps_trained: 864000
    sample_time_ms: 109411.35
    update_time_ms: 403.891
  iterations_since_restore: 9
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.303243243243244
    ram_util_percent: 11.564864864864864
  pid: 28570
  policy_reward_max:
    agent-0: -35.0
    agent-1: -35.0
    agent-2: -56.5
    agent-3: -56.5
    agent-4: -68.0
    agent-5: -68.0
  policy_reward_mean:
    agent-0: -211.37
    agent-1: -211.37
    agent-2: -239.63
    agent-3: -239.63
    agent-4: -223.925
    agent-5: -223.925
  policy_reward_min:
    agent-0: -444.0
    agent-1: -444.0
    agent-2: -471.0
    agent-3: -471.0
    agent-4: -516.0
    agent-5: -516.0
  sampler_perf:
    mean_env_wait_ms: 27.215343076012445
    mean_inference_ms: 13.661317900479634
    mean_processing_ms: 61.067396067198636
  time_since_restore: 1244.9884560108185
  time_this_iter_s: 130.6064419746399
  time_total_s: 1244.9884560108185
  timestamp: 1637509771
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 864000
  training_iteration: 9
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |      9 |          1244.99 | 864000 | -1349.85 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 6.75
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 7.98
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 3.74
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 4.06
    apples_agent-3_min: 0
    apples_agent-4_max: 50
    apples_agent-4_mean: 7.55
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 92.45
    cleaning_beam_agent-0_min: 70
    cleaning_beam_agent-1_max: 115
    cleaning_beam_agent-1_mean: 90.63
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 254
    cleaning_beam_agent-2_mean: 222.87
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 233
    cleaning_beam_agent-3_mean: 195.55
    cleaning_beam_agent-3_min: 168
    cleaning_beam_agent-4_max: 77
    cleaning_beam_agent-4_mean: 51.25
    cleaning_beam_agent-4_min: 30
    cleaning_beam_agent-5_max: 322
    cleaning_beam_agent-5_mean: 279.42
    cleaning_beam_agent-5_min: 220
    fire_beam_agent-0_max: 34
    fire_beam_agent-0_mean: 20.81
    fire_beam_agent-0_min: 11
    fire_beam_agent-1_max: 25
    fire_beam_agent-1_mean: 15.43
    fire_beam_agent-1_min: 8
    fire_beam_agent-2_max: 25
    fire_beam_agent-2_mean: 13.89
    fire_beam_agent-2_min: 4
    fire_beam_agent-3_max: 22
    fire_beam_agent-3_mean: 14.63
    fire_beam_agent-3_min: 7
    fire_beam_agent-4_max: 32
    fire_beam_agent-4_mean: 19.69
    fire_beam_agent-4_min: 8
    fire_beam_agent-5_max: 24
    fire_beam_agent-5_mean: 13.4
    fire_beam_agent-5_min: 6
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-51-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -410.0
  episode_reward_mean: -983.8
  episode_reward_min: -1776.0
  episodes_this_iter: 96
  episodes_total: 960
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 13945.733
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 2.0158610343933105
        entropy_coeff: 0.0017600000137463212
        kl: 0.00545809930190444
        model: {}
        policy_loss: -0.0011956992093473673
        total_loss: 0.039396192878484726
        vf_explained_var: -0.005049467086791992
        vf_loss: 438.66912841796875
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 2.016416549682617
        entropy_coeff: 0.0017600000137463212
        kl: 0.013538060709834099
        model: {}
        policy_loss: -0.0014949087053537369
        total_loss: 0.03830515593290329
        vf_explained_var: -0.0032065510749816895
        vf_loss: 426.7205810546875
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 2.0532541275024414
        entropy_coeff: 0.0017600000137463212
        kl: 0.008551260456442833
        model: {}
        policy_loss: -0.00135874655097723
        total_loss: 0.03849650174379349
        vf_explained_var: -0.003823697566986084
        vf_loss: 430.41412353515625
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0012060863664373755
        entropy: 2.058682680130005
        entropy_coeff: 0.0017600000137463212
        kl: 0.01003181654959917
        model: {}
        policy_loss: -0.0005488742608577013
        total_loss: 0.03869931399822235
        vf_explained_var: -0.00340995192527771
        vf_loss: 426.206787109375
      agent-4:
        cur_kl_coeff: 0.05624999850988388
        cur_lr: 0.0012060863664373755
        entropy: 1.9808039665222168
        entropy_coeff: 0.0017600000137463212
        kl: 0.008887765929102898
        model: {}
        policy_loss: -0.0016222009435296059
        total_loss: 0.04152611643075943
        vf_explained_var: -0.00456276535987854
        vf_loss: 461.345947265625
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012060863664373755
        entropy: 1.9733799695968628
        entropy_coeff: 0.0017600000137463212
        kl: 0.013363640755414963
        model: {}
        policy_loss: -0.0014286395162343979
        total_loss: 0.043129391968250275
        vf_explained_var: -0.004857063293457031
        vf_loss: 473.6300354003906
    load_time_ms: 13947.635
    num_steps_sampled: 960000
    num_steps_trained: 960000
    sample_time_ms: 108938.431
    update_time_ms: 365.286
  iterations_since_restore: 10
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.20909090909091
    ram_util_percent: 11.649732620320858
  pid: 28570
  policy_reward_max:
    agent-0: -9.5
    agent-1: -9.5
    agent-2: -27.0
    agent-3: -27.0
    agent-4: -7.5
    agent-5: -7.5
  policy_reward_mean:
    agent-0: -159.71
    agent-1: -159.71
    agent-2: -175.135
    agent-3: -175.135
    agent-4: -157.055
    agent-5: -157.055
  policy_reward_min:
    agent-0: -387.5
    agent-1: -387.5
    agent-2: -389.5
    agent-3: -389.5
    agent-4: -413.5
    agent-5: -413.5
  sampler_perf:
    mean_env_wait_ms: 27.07875410700516
    mean_inference_ms: 13.605633168895103
    mean_processing_ms: 60.81816617904528
  time_since_restore: 1375.9819416999817
  time_this_iter_s: 130.9934856891632
  time_total_s: 1375.9819416999817
  timestamp: 1637509902
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 960000
  training_iteration: 10
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     10 |          1375.98 | 960000 |   -983.8 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 6.09
    apples_agent-0_min: 0
    apples_agent-1_max: 40
    apples_agent-1_mean: 6.66
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 3.57
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 4.6
    apples_agent-3_min: 0
    apples_agent-4_max: 50
    apples_agent-4_mean: 8.36
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 3.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 126
    cleaning_beam_agent-0_mean: 99.4
    cleaning_beam_agent-0_min: 77
    cleaning_beam_agent-1_max: 131
    cleaning_beam_agent-1_mean: 108.78
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 246
    cleaning_beam_agent-2_mean: 207.36
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 206
    cleaning_beam_agent-3_mean: 155.52
    cleaning_beam_agent-3_min: 126
    cleaning_beam_agent-4_max: 65
    cleaning_beam_agent-4_mean: 36.77
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 300
    cleaning_beam_agent-5_mean: 263.26
    cleaning_beam_agent-5_min: 231
    fire_beam_agent-0_max: 28
    fire_beam_agent-0_mean: 14.45
    fire_beam_agent-0_min: 5
    fire_beam_agent-1_max: 19
    fire_beam_agent-1_mean: 10.98
    fire_beam_agent-1_min: 5
    fire_beam_agent-2_max: 16
    fire_beam_agent-2_mean: 9.59
    fire_beam_agent-2_min: 4
    fire_beam_agent-3_max: 17
    fire_beam_agent-3_mean: 8.85
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 37
    fire_beam_agent-4_mean: 20.16
    fire_beam_agent-4_min: 12
    fire_beam_agent-5_max: 21
    fire_beam_agent-5_mean: 9.47
    fire_beam_agent-5_min: 3
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-53-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -313.0
  episode_reward_mean: -737.86
  episode_reward_min: -1261.0
  episodes_this_iter: 96
  episodes_total: 1056
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12883.437
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001200095983222127
        entropy: 1.931078314781189
        entropy_coeff: 0.0017600000137463212
        kl: 0.012032889761030674
        model: {}
        policy_loss: -0.0017736908048391342
        total_loss: 0.03397608548402786
        vf_explained_var: -0.005096524953842163
        vf_loss: 385.46826171875
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001200095983222127
        entropy: 2.0206737518310547
        entropy_coeff: 0.0017600000137463212
        kl: 0.005657227709889412
        model: {}
        policy_loss: -0.0012422557920217514
        total_loss: 0.03394870087504387
        vf_explained_var: -0.0034491121768951416
        vf_loss: 384.6448059082031
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001200095983222127
        entropy: 2.0303456783294678
        entropy_coeff: 0.0017600000137463212
        kl: 0.009216368198394775
        model: {}
        policy_loss: -0.0015526660718023777
        total_loss: 0.03584393858909607
        vf_explained_var: -0.004167377948760986
        vf_loss: 405.09197998046875
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001200095983222127
        entropy: 2.067173480987549
        entropy_coeff: 0.0017600000137463212
        kl: 0.009880796074867249
        model: {}
        policy_loss: -0.0011754604056477547
        total_loss: 0.03601393476128578
        vf_explained_var: -0.0038077235221862793
        vf_loss: 405.8059997558594
      agent-4:
        cur_kl_coeff: 0.05624999850988388
        cur_lr: 0.001200095983222127
        entropy: 1.8180243968963623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073582506738603115
        model: {}
        policy_loss: -0.0012988569214940071
        total_loss: 0.03261948004364967
        vf_explained_var: -0.005035132169723511
        vf_loss: 367.04156494140625
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001200095983222127
        entropy: 1.9768297672271729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0053361146710813046
        model: {}
        policy_loss: -0.0008506779558956623
        total_loss: 0.03595878928899765
        vf_explained_var: -0.005785167217254639
        vf_loss: 400.21881103515625
    load_time_ms: 13458.644
    num_steps_sampled: 1056000
    num_steps_trained: 1056000
    sample_time_ms: 107228.263
    update_time_ms: 17.095
  iterations_since_restore: 11
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.186096256684497
    ram_util_percent: 11.626737967914442
  pid: 28570
  policy_reward_max:
    agent-0: 13.0
    agent-1: 13.0
    agent-2: -27.5
    agent-3: -27.5
    agent-4: -27.5
    agent-5: -27.5
  policy_reward_mean:
    agent-0: -117.425
    agent-1: -117.425
    agent-2: -139.615
    agent-3: -139.615
    agent-4: -111.89
    agent-5: -111.89
  policy_reward_min:
    agent-0: -334.5
    agent-1: -334.5
    agent-2: -426.5
    agent-3: -426.5
    agent-4: -315.0
    agent-5: -315.0
  sampler_perf:
    mean_env_wait_ms: 26.939378862017566
    mean_inference_ms: 13.567916620778242
    mean_processing_ms: 60.63463935296496
  time_since_restore: 1507.1023209095001
  time_this_iter_s: 131.12037920951843
  time_total_s: 1507.1023209095001
  timestamp: 1637510033
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 1056000
  training_iteration: 11
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     11 |           1507.1 | 1056000 |  -737.86 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.41
    apples_agent-0_min: 0
    apples_agent-1_max: 25
    apples_agent-1_mean: 6.86
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 3.6
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.98
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 9.21
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 123
    cleaning_beam_agent-0_mean: 92.98
    cleaning_beam_agent-0_min: 71
    cleaning_beam_agent-1_max: 132
    cleaning_beam_agent-1_mean: 106.44
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 227
    cleaning_beam_agent-2_mean: 184.58
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 188
    cleaning_beam_agent-3_mean: 152.55
    cleaning_beam_agent-3_min: 127
    cleaning_beam_agent-4_max: 52
    cleaning_beam_agent-4_mean: 35.18
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 280
    cleaning_beam_agent-5_mean: 247.69
    cleaning_beam_agent-5_min: 206
    fire_beam_agent-0_max: 22
    fire_beam_agent-0_mean: 10.52
    fire_beam_agent-0_min: 3
    fire_beam_agent-1_max: 19
    fire_beam_agent-1_mean: 10.63
    fire_beam_agent-1_min: 4
    fire_beam_agent-2_max: 14
    fire_beam_agent-2_mean: 6.81
    fire_beam_agent-2_min: 2
    fire_beam_agent-3_max: 14
    fire_beam_agent-3_mean: 6.5
    fire_beam_agent-3_min: 1
    fire_beam_agent-4_max: 28
    fire_beam_agent-4_mean: 17.15
    fire_beam_agent-4_min: 6
    fire_beam_agent-5_max: 18
    fire_beam_agent-5_mean: 6.79
    fire_beam_agent-5_min: 1
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-56-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -91.0
  episode_reward_mean: -550.18
  episode_reward_min: -1087.0
  episodes_this_iter: 96
  episodes_total: 1152
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12892.713
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011941056000068784
        entropy: 1.898314118385315
        entropy_coeff: 0.0017600000137463212
        kl: 0.006552192848175764
        model: {}
        policy_loss: -0.0010386733338236809
        total_loss: 0.027517158538103104
        vf_explained_var: -0.006608724594116211
        vf_loss: 315.69256591796875
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011941056000068784
        entropy: 1.9623055458068848
        entropy_coeff: 0.0017600000137463212
        kl: 0.00732584111392498
        model: {}
        policy_loss: -0.001261221244931221
        total_loss: 0.02879675291478634
        vf_explained_var: -0.003657311201095581
        vf_loss: 331.4533996582031
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011941056000068784
        entropy: 2.0374231338500977
        entropy_coeff: 0.0017600000137463212
        kl: 0.007578346412628889
        model: {}
        policy_loss: -0.001513949828222394
        total_loss: 0.02927369996905327
        vf_explained_var: -0.0050058066844940186
        vf_loss: 339.9459533691406
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011941056000068784
        entropy: 2.049391984939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073656062595546246
        model: {}
        policy_loss: -0.000706787221133709
        total_loss: 0.030971964821219444
        vf_explained_var: -0.004703164100646973
        vf_loss: 351.01544189453125
      agent-4:
        cur_kl_coeff: 0.05624999850988388
        cur_lr: 0.0011941056000068784
        entropy: 1.7916748523712158
        entropy_coeff: 0.0017600000137463212
        kl: 0.004623732529580593
        model: {}
        policy_loss: -0.0014188080094754696
        total_loss: 0.026325296610593796
        vf_explained_var: -0.005048155784606934
        vf_loss: 306.37371826171875
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011941056000068784
        entropy: 2.027541399002075
        entropy_coeff: 0.0017600000137463212
        kl: 0.012876037508249283
        model: {}
        policy_loss: -0.0020685649942606688
        total_loss: 0.028711456805467606
        vf_explained_var: -0.007494688034057617
        vf_loss: 337.0469055175781
    load_time_ms: 13438.061
    num_steps_sampled: 1152000
    num_steps_trained: 1152000
    sample_time_ms: 105279.557
    update_time_ms: 16.507
  iterations_since_restore: 12
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.148663101604278
    ram_util_percent: 11.618181818181817
  pid: 28570
  policy_reward_max:
    agent-0: 13.5
    agent-1: 13.5
    agent-2: 3.5
    agent-3: 3.5
    agent-4: 7.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: -88.205
    agent-1: -88.205
    agent-2: -95.305
    agent-3: -95.305
    agent-4: -91.58
    agent-5: -91.58
  policy_reward_min:
    agent-0: -251.0
    agent-1: -251.0
    agent-2: -252.0
    agent-3: -252.0
    agent-4: -233.5
    agent-5: -233.5
  sampler_perf:
    mean_env_wait_ms: 26.796611091278606
    mean_inference_ms: 13.534533971001235
    mean_processing_ms: 60.50058670561171
  time_since_restore: 1638.347613811493
  time_this_iter_s: 131.2452929019928
  time_total_s: 1638.347613811493
  timestamp: 1637510164
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 1152000
  training_iteration: 12
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     12 |          1638.35 | 1152000 |  -550.18 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.3
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 5.98
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 3.81
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 4.56
    apples_agent-3_min: 0
    apples_agent-4_max: 47
    apples_agent-4_mean: 9.93
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 88.22
    cleaning_beam_agent-0_min: 70
    cleaning_beam_agent-1_max: 124
    cleaning_beam_agent-1_mean: 96.78
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 192
    cleaning_beam_agent-2_mean: 153.37
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 164.33
    cleaning_beam_agent-3_min: 139
    cleaning_beam_agent-4_max: 69
    cleaning_beam_agent-4_mean: 43.2
    cleaning_beam_agent-4_min: 32
    cleaning_beam_agent-5_max: 269
    cleaning_beam_agent-5_mean: 193.35
    cleaning_beam_agent-5_min: 170
    fire_beam_agent-0_max: 27
    fire_beam_agent-0_mean: 14.87
    fire_beam_agent-0_min: 5
    fire_beam_agent-1_max: 15
    fire_beam_agent-1_mean: 6.77
    fire_beam_agent-1_min: 2
    fire_beam_agent-2_max: 14
    fire_beam_agent-2_mean: 6.51
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 13
    fire_beam_agent-3_mean: 5.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 26
    fire_beam_agent-4_mean: 12.24
    fire_beam_agent-4_min: 3
    fire_beam_agent-5_max: 12
    fire_beam_agent-5_mean: 3.77
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_10-58-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -18.0
  episode_reward_mean: -428.29
  episode_reward_min: -924.0
  episodes_this_iter: 96
  episodes_total: 1248
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12900.191
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011881152167916298
        entropy: 1.8846206665039062
        entropy_coeff: 0.0017600000137463212
        kl: 0.010755015537142754
        model: {}
        policy_loss: -0.0012553392443805933
        total_loss: 0.019713321700692177
        vf_explained_var: -0.005207359790802002
        vf_loss: 237.47845458984375
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011881152167916298
        entropy: 1.98060941696167
        entropy_coeff: 0.0017600000137463212
        kl: 0.009558015502989292
        model: {}
        policy_loss: -0.0015327336732298136
        total_loss: 0.02201566845178604
        vf_explained_var: -0.0029987692832946777
        vf_loss: 265.56378173828125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011881152167916298
        entropy: 2.0303544998168945
        entropy_coeff: 0.0017600000137463212
        kl: 0.009923122823238373
        model: {}
        policy_loss: -0.0013765795156359673
        total_loss: 0.02363385632634163
        vf_explained_var: -0.004816830158233643
        vf_loss: 280.8770446777344
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011881152167916298
        entropy: 1.9988245964050293
        entropy_coeff: 0.0017600000137463212
        kl: 0.008303701877593994
        model: {}
        policy_loss: -0.0015542109031230211
        total_loss: 0.02513124607503414
        vf_explained_var: -0.004444122314453125
        vf_loss: 299.9579772949219
      agent-4:
        cur_kl_coeff: 0.02812499925494194
        cur_lr: 0.0011881152167916298
        entropy: 1.7728732824325562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014244341291487217
        model: {}
        policy_loss: -0.0008064799476414919
        total_loss: 0.021058078855276108
        vf_explained_var: -0.004424691200256348
        vf_loss: 249.44757080078125
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011881152167916298
        entropy: 2.0512919425964355
        entropy_coeff: 0.0017600000137463212
        kl: 0.006355036981403828
        model: {}
        policy_loss: -0.0010152732720598578
        total_loss: 0.021646486595273018
        vf_explained_var: -0.005245625972747803
        vf_loss: 259.5428161621094
    load_time_ms: 13413.153
    num_steps_sampled: 1248000
    num_steps_trained: 1248000
    sample_time_ms: 104883.345
    update_time_ms: 16.31
  iterations_since_restore: 13
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.2752688172043
    ram_util_percent: 11.559139784946236
  pid: 28570
  policy_reward_max:
    agent-0: 6.0
    agent-1: 6.0
    agent-2: 9.5
    agent-3: 9.5
    agent-4: 6.5
    agent-5: 6.5
  policy_reward_mean:
    agent-0: -68.805
    agent-1: -68.805
    agent-2: -75.085
    agent-3: -75.085
    agent-4: -70.255
    agent-5: -70.255
  policy_reward_min:
    agent-0: -251.0
    agent-1: -251.0
    agent-2: -255.5
    agent-3: -255.5
    agent-4: -206.5
    agent-5: -206.5
  sampler_perf:
    mean_env_wait_ms: 26.650380303155107
    mean_inference_ms: 13.50285357438455
    mean_processing_ms: 60.390163222755945
  time_since_restore: 1768.828619003296
  time_this_iter_s: 130.48100519180298
  time_total_s: 1768.828619003296
  timestamp: 1637510295
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 1248000
  training_iteration: 13
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     13 |          1768.83 | 1248000 |  -428.29 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.59
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 5.7
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 3.14
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 5.77
    apples_agent-3_min: 0
    apples_agent-4_max: 41
    apples_agent-4_mean: 9.63
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 116
    cleaning_beam_agent-0_mean: 96.48
    cleaning_beam_agent-0_min: 74
    cleaning_beam_agent-1_max: 113
    cleaning_beam_agent-1_mean: 85.01
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 220
    cleaning_beam_agent-2_mean: 180.29
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 157.58
    cleaning_beam_agent-3_min: 126
    cleaning_beam_agent-4_max: 59
    cleaning_beam_agent-4_mean: 40.77
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 192
    cleaning_beam_agent-5_mean: 158.01
    cleaning_beam_agent-5_min: 124
    fire_beam_agent-0_max: 24
    fire_beam_agent-0_mean: 12.69
    fire_beam_agent-0_min: 6
    fire_beam_agent-1_max: 11
    fire_beam_agent-1_mean: 4.63
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 13
    fire_beam_agent-2_mean: 3.8
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 8
    fire_beam_agent-3_mean: 3.93
    fire_beam_agent-3_min: 1
    fire_beam_agent-4_max: 20
    fire_beam_agent-4_mean: 12.05
    fire_beam_agent-4_min: 2
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 2.91
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-00-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -29.0
  episode_reward_mean: -340.93
  episode_reward_min: -769.0
  episodes_this_iter: 96
  episodes_total: 1344
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12888.783
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011821248335763812
        entropy: 1.906026840209961
        entropy_coeff: 0.0017600000137463212
        kl: 0.00938950851559639
        model: {}
        policy_loss: -0.0016973672900348902
        total_loss: 0.014333073049783707
        vf_explained_var: -0.003215998411178589
        vf_loss: 189.1557159423828
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011821248335763812
        entropy: 1.9919594526290894
        entropy_coeff: 0.0017600000137463212
        kl: 0.006109719164669514
        model: {}
        policy_loss: -0.0010658018290996552
        total_loss: 0.01766614057123661
        vf_explained_var: -0.002150416374206543
        vf_loss: 219.32305908203125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011821248335763812
        entropy: 2.0185489654541016
        entropy_coeff: 0.0017600000137463212
        kl: 0.012967679649591446
        model: {}
        policy_loss: -0.0018942155875265598
        total_loss: 0.018633533269166946
        vf_explained_var: -0.004417538642883301
        vf_loss: 234.3201141357422
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011821248335763812
        entropy: 2.007765769958496
        entropy_coeff: 0.0017600000137463212
        kl: 0.00864382553845644
        model: {}
        policy_loss: -0.00040521426126360893
        total_loss: 0.021083645522594452
        vf_explained_var: -0.003938108682632446
        vf_loss: 248.06434631347656
      agent-4:
        cur_kl_coeff: 0.01406249962747097
        cur_lr: 0.0011821248335763812
        entropy: 1.8615303039550781
        entropy_coeff: 0.0017600000137463212
        kl: 0.009821892715990543
        model: {}
        policy_loss: -0.0012086937204003334
        total_loss: 0.015432203188538551
        vf_explained_var: -0.002787470817565918
        vf_loss: 197.7906951904297
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011821248335763812
        entropy: 2.0659091472625732
        entropy_coeff: 0.0017600000137463212
        kl: 0.00812213309109211
        model: {}
        policy_loss: -0.0012788556050509214
        total_loss: 0.01582188904285431
        vf_explained_var: -0.0036347806453704834
        vf_loss: 203.30636596679688
    load_time_ms: 13398.519
    num_steps_sampled: 1344000
    num_steps_trained: 1344000
    sample_time_ms: 104698.674
    update_time_ms: 16.223
  iterations_since_restore: 14
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.344565217391303
    ram_util_percent: 11.640217391304349
  pid: 28570
  policy_reward_max:
    agent-0: 3.5
    agent-1: 3.5
    agent-2: 8.5
    agent-3: 8.5
    agent-4: 15.5
    agent-5: 15.5
  policy_reward_mean:
    agent-0: -55.255
    agent-1: -55.255
    agent-2: -59.915
    agent-3: -59.915
    agent-4: -55.295
    agent-5: -55.295
  policy_reward_min:
    agent-0: -174.0
    agent-1: -174.0
    agent-2: -216.0
    agent-3: -216.0
    agent-4: -162.5
    agent-5: -162.5
  sampler_perf:
    mean_env_wait_ms: 26.525305053795115
    mean_inference_ms: 13.473504832177094
    mean_processing_ms: 60.274264942507635
  time_since_restore: 1898.2632117271423
  time_this_iter_s: 129.43459272384644
  time_total_s: 1898.2632117271423
  timestamp: 1637510424
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 1344000
  training_iteration: 14
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     14 |          1898.26 | 1344000 |  -340.93 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.61
    apples_agent-0_min: 0
    apples_agent-1_max: 33
    apples_agent-1_mean: 6.83
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.75
    apples_agent-2_min: 0
    apples_agent-3_max: 39
    apples_agent-3_mean: 5.38
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 7.56
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 4.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 110.49
    cleaning_beam_agent-0_min: 81
    cleaning_beam_agent-1_max: 121
    cleaning_beam_agent-1_mean: 101.51
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 212
    cleaning_beam_agent-2_mean: 176.62
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 156.84
    cleaning_beam_agent-3_min: 129
    cleaning_beam_agent-4_max: 63
    cleaning_beam_agent-4_mean: 49.51
    cleaning_beam_agent-4_min: 34
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 140.17
    cleaning_beam_agent-5_min: 106
    fire_beam_agent-0_max: 19
    fire_beam_agent-0_mean: 6.77
    fire_beam_agent-0_min: 2
    fire_beam_agent-1_max: 8
    fire_beam_agent-1_mean: 3.77
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 6
    fire_beam_agent-2_mean: 2.25
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 7
    fire_beam_agent-3_mean: 3.31
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 19
    fire_beam_agent-4_mean: 8.53
    fire_beam_agent-4_min: 3
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 1.91
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-02-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 22.0
  episode_reward_mean: -224.73
  episode_reward_min: -624.0
  episodes_this_iter: 96
  episodes_total: 1440
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12867.54
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011761344503611326
        entropy: 1.8088507652282715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061798542737960815
        model: {}
        policy_loss: -0.0016408683732151985
        total_loss: 0.010277261957526207
        vf_explained_var: -0.006563276052474976
        vf_loss: 147.92715454101562
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011761344503611326
        entropy: 1.9929349422454834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025530471466481686
        model: {}
        policy_loss: -0.0006453702226281166
        total_loss: 0.014206365682184696
        vf_explained_var: -0.003459155559539795
        vf_loss: 182.31649780273438
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011761344503611326
        entropy: 1.948695421218872
        entropy_coeff: 0.0017600000137463212
        kl: 0.007398234214633703
        model: {}
        policy_loss: -0.00044426496606320143
        total_loss: 0.01440782006829977
        vf_explained_var: -0.004200756549835205
        vf_loss: 179.1187744140625
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011761344503611326
        entropy: 1.969653606414795
        entropy_coeff: 0.0017600000137463212
        kl: 0.018873725086450577
        model: {}
        policy_loss: -0.001563317491672933
        total_loss: 0.014734907075762749
        vf_explained_var: -0.0035014450550079346
        vf_loss: 192.9296875
      agent-4:
        cur_kl_coeff: 0.01406249962747097
        cur_lr: 0.0011761344503611326
        entropy: 1.8179196119308472
        entropy_coeff: 0.0017600000137463212
        kl: 0.007937478832900524
        model: {}
        policy_loss: -0.0006469204090535641
        total_loss: 0.012633329257369041
        vf_explained_var: -0.004883259534835815
        vf_loss: 163.68167114257812
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011761344503611326
        entropy: 2.0570201873779297
        entropy_coeff: 0.0017600000137463212
        kl: 0.009650432504713535
        model: {}
        policy_loss: -0.0017794780433177948
        total_loss: 0.011612358503043652
        vf_explained_var: -0.006787121295928955
        vf_loss: 165.29673767089844
    load_time_ms: 13347.257
    num_steps_sampled: 1440000
    num_steps_trained: 1440000
    sample_time_ms: 104410.221
    update_time_ms: 16.569
  iterations_since_restore: 15
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.319565217391304
    ram_util_percent: 11.617391304347828
  pid: 28570
  policy_reward_max:
    agent-0: 17.0
    agent-1: 17.0
    agent-2: 15.5
    agent-3: 15.5
    agent-4: 16.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: -32.52
    agent-1: -32.52
    agent-2: -45.085
    agent-3: -45.085
    agent-4: -34.76
    agent-5: -34.76
  policy_reward_min:
    agent-0: -127.5
    agent-1: -127.5
    agent-2: -148.5
    agent-3: -148.5
    agent-4: -130.5
    agent-5: -130.5
  sampler_perf:
    mean_env_wait_ms: 26.40055847712836
    mean_inference_ms: 13.448224212956276
    mean_processing_ms: 60.17173498238063
  time_since_restore: 2027.423981666565
  time_this_iter_s: 129.1607699394226
  time_total_s: 2027.423981666565
  timestamp: 1637510554
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 1440000
  training_iteration: 15
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     15 |          2027.42 | 1440000 |  -224.73 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.19
    apples_agent-0_min: 0
    apples_agent-1_max: 37
    apples_agent-1_mean: 6.63
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 4.21
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 5.46
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 10.08
    apples_agent-4_min: 0
    apples_agent-5_max: 44
    apples_agent-5_mean: 5.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 173
    cleaning_beam_agent-0_mean: 140.96
    cleaning_beam_agent-0_min: 101
    cleaning_beam_agent-1_max: 116
    cleaning_beam_agent-1_mean: 96.52
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 254
    cleaning_beam_agent-2_mean: 217.36
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 238
    cleaning_beam_agent-3_mean: 201.25
    cleaning_beam_agent-3_min: 146
    cleaning_beam_agent-4_max: 71
    cleaning_beam_agent-4_mean: 51.5
    cleaning_beam_agent-4_min: 33
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 148.46
    cleaning_beam_agent-5_min: 111
    fire_beam_agent-0_max: 10
    fire_beam_agent-0_mean: 2.92
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 6
    fire_beam_agent-1_mean: 2.37
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 6
    fire_beam_agent-2_mean: 1.52
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 9
    fire_beam_agent-3_mean: 2.39
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 12
    fire_beam_agent-4_mean: 5.96
    fire_beam_agent-4_min: 1
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 1.77
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-04-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: -115.93
  episode_reward_min: -342.0
  episodes_this_iter: 96
  episodes_total: 1536
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12867.934
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011701439507305622
        entropy: 1.8745564222335815
        entropy_coeff: 0.0017600000137463212
        kl: 0.007278235163539648
        model: {}
        policy_loss: -0.0012476583942770958
        total_loss: 0.006416745483875275
        vf_explained_var: -0.004989057779312134
        vf_loss: 105.9970474243164
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011701439507305622
        entropy: 1.9243379831314087
        entropy_coeff: 0.0017600000137463212
        kl: 0.01309388130903244
        model: {}
        policy_loss: -0.0007159223314374685
        total_loss: 0.010397187434136868
        vf_explained_var: -0.0038430392742156982
        vf_loss: 141.72598266601562
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011701439507305622
        entropy: 1.9704957008361816
        entropy_coeff: 0.0017600000137463212
        kl: 0.005042185075581074
        model: {}
        policy_loss: -0.0011835573241114616
        total_loss: 0.009707149118185043
        vf_explained_var: -0.005605220794677734
        vf_loss: 141.06671142578125
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011701439507305622
        entropy: 1.9467860460281372
        entropy_coeff: 0.0017600000137463212
        kl: 0.008757577277719975
        model: {}
        policy_loss: -0.0012005376629531384
        total_loss: 0.010715274140238762
        vf_explained_var: -0.004555642604827881
        vf_loss: 151.232177734375
      agent-4:
        cur_kl_coeff: 0.01406249962747097
        cur_lr: 0.0011701439507305622
        entropy: 1.6765460968017578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0211039911955595
        model: {}
        policy_loss: -0.001758616417646408
        total_loss: 0.00811523012816906
        vf_explained_var: -0.0032354891300201416
        vf_loss: 125.2779312133789
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011701439507305622
        entropy: 2.0494308471679688
        entropy_coeff: 0.0017600000137463212
        kl: 0.014763642102479935
        model: {}
        policy_loss: -0.002054009586572647
        total_loss: 0.007535842712968588
        vf_explained_var: -0.00422590970993042
        vf_loss: 124.58662414550781
    load_time_ms: 13357.228
    num_steps_sampled: 1536000
    num_steps_trained: 1536000
    sample_time_ms: 104283.457
    update_time_ms: 16.257
  iterations_since_restore: 16
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.256216216216213
    ram_util_percent: 11.539459459459458
  pid: 28570
  policy_reward_max:
    agent-0: 20.0
    agent-1: 20.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 20.0
    agent-5: 20.0
  policy_reward_mean:
    agent-0: -18.765
    agent-1: -18.765
    agent-2: -21.975
    agent-3: -21.975
    agent-4: -17.225
    agent-5: -17.225
  policy_reward_min:
    agent-0: -96.5
    agent-1: -96.5
    agent-2: -119.5
    agent-3: -119.5
    agent-4: -93.5
    agent-5: -93.5
  sampler_perf:
    mean_env_wait_ms: 26.322360442829936
    mean_inference_ms: 13.423810059424614
    mean_processing_ms: 60.078073254958355
  time_since_restore: 2157.0757977962494
  time_this_iter_s: 129.65181612968445
  time_total_s: 2157.0757977962494
  timestamp: 1637510683
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 1536000
  training_iteration: 16
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     16 |          2157.08 | 1536000 |  -115.93 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 5.44
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 7.28
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 43
    apples_agent-3_mean: 6.42
    apples_agent-3_min: 0
    apples_agent-4_max: 46
    apples_agent-4_mean: 8.05
    apples_agent-4_min: 0
    apples_agent-5_max: 29
    apples_agent-5_mean: 4.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 109.84
    cleaning_beam_agent-0_min: 79
    cleaning_beam_agent-1_max: 106
    cleaning_beam_agent-1_mean: 82.29
    cleaning_beam_agent-1_min: 60
    cleaning_beam_agent-2_max: 286
    cleaning_beam_agent-2_mean: 248.92
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 189.62
    cleaning_beam_agent-3_min: 156
    cleaning_beam_agent-4_max: 73
    cleaning_beam_agent-4_mean: 51.76
    cleaning_beam_agent-4_min: 38
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 111.45
    cleaning_beam_agent-5_min: 88
    fire_beam_agent-0_max: 6
    fire_beam_agent-0_mean: 2.2
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 7
    fire_beam_agent-1_mean: 1.79
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 6
    fire_beam_agent-2_mean: 1.13
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 5
    fire_beam_agent-3_mean: 1.78
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 11
    fire_beam_agent-4_mean: 3.99
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 1.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-06-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 86.0
  episode_reward_mean: -77.74
  episode_reward_min: -306.0
  episodes_this_iter: 96
  episodes_total: 1632
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12855.841
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011641535675153136
        entropy: 1.9422117471694946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036644828505814075
        model: {}
        policy_loss: -0.0008361542131751776
        total_loss: 0.0034324107691645622
        vf_explained_var: -0.004044264554977417
        vf_loss: 75.03634643554688
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011641535675153136
        entropy: 1.8438836336135864
        entropy_coeff: 0.0017600000137463212
        kl: 0.02037903293967247
        model: {}
        policy_loss: -0.0017496589571237564
        total_loss: 0.006012218073010445
        vf_explained_var: -0.00391802191734314
        vf_loss: 104.97637939453125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011641535675153136
        entropy: 1.8959721326828003
        entropy_coeff: 0.0017600000137463212
        kl: 0.007135682739317417
        model: {}
        policy_loss: -0.0013759571593254805
        total_loss: 0.006207600235939026
        vf_explained_var: -0.003414630889892578
        vf_loss: 105.63685607910156
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011641535675153136
        entropy: 1.897681474685669
        entropy_coeff: 0.0017600000137463212
        kl: 0.014880425296723843
        model: {}
        policy_loss: 7.062219083309174e-05
        total_loss: 0.008506465703248978
        vf_explained_var: -0.0027830302715301514
        vf_loss: 114.03752136230469
      agent-4:
        cur_kl_coeff: 0.02109375037252903
        cur_lr: 0.0011641535675153136
        entropy: 1.599733591079712
        entropy_coeff: 0.0017600000137463212
        kl: 0.00798819214105606
        model: {}
        policy_loss: -0.0013000313192605972
        total_loss: 0.005188062787055969
        vf_explained_var: -0.00236666202545166
        vf_loss: 91.35123443603516
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011641535675153136
        entropy: 2.0207228660583496
        entropy_coeff: 0.0017600000137463212
        kl: 0.012021185830235481
        model: {}
        policy_loss: -0.001564151607453823
        total_loss: 0.004416549578309059
        vf_explained_var: -0.003094971179962158
        vf_loss: 89.36112976074219
    load_time_ms: 13371.132
    num_steps_sampled: 1632000
    num_steps_trained: 1632000
    sample_time_ms: 104189.768
    update_time_ms: 16.839
  iterations_since_restore: 17
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.16720430107527
    ram_util_percent: 11.620967741935486
  pid: 28570
  policy_reward_max:
    agent-0: 27.0
    agent-1: 27.0
    agent-2: 18.5
    agent-3: 18.5
    agent-4: 15.5
    agent-5: 15.5
  policy_reward_mean:
    agent-0: -10.58
    agent-1: -10.58
    agent-2: -18.71
    agent-3: -18.71
    agent-4: -9.58
    agent-5: -9.58
  policy_reward_min:
    agent-0: -77.0
    agent-1: -77.0
    agent-2: -101.5
    agent-3: -101.5
    agent-4: -91.0
    agent-5: -91.0
  sampler_perf:
    mean_env_wait_ms: 26.237463654687225
    mean_inference_ms: 13.405557937872395
    mean_processing_ms: 59.99540032950391
  time_since_restore: 2287.6992270946503
  time_this_iter_s: 130.62342929840088
  time_total_s: 2287.6992270946503
  timestamp: 1637510814
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 1632000
  training_iteration: 17
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     17 |           2287.7 | 1632000 |   -77.74 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 7.31
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 3.18
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 5.28
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 10.13
    apples_agent-4_min: 0
    apples_agent-5_max: 32
    apples_agent-5_mean: 4.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 96.97
    cleaning_beam_agent-0_min: 73
    cleaning_beam_agent-1_max: 85
    cleaning_beam_agent-1_mean: 63.09
    cleaning_beam_agent-1_min: 45
    cleaning_beam_agent-2_max: 323
    cleaning_beam_agent-2_mean: 273.26
    cleaning_beam_agent-2_min: 233
    cleaning_beam_agent-3_max: 208
    cleaning_beam_agent-3_mean: 160.71
    cleaning_beam_agent-3_min: 132
    cleaning_beam_agent-4_max: 72
    cleaning_beam_agent-4_mean: 49.81
    cleaning_beam_agent-4_min: 36
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 134.0
    cleaning_beam_agent-5_min: 107
    fire_beam_agent-0_max: 9
    fire_beam_agent-0_mean: 2.6
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 5
    fire_beam_agent-1_mean: 1.27
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 5
    fire_beam_agent-2_mean: 0.96
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 5
    fire_beam_agent-3_mean: 1.27
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 6
    fire_beam_agent-4_mean: 2.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.76
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-09-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 107.0
  episode_reward_mean: -40.66
  episode_reward_min: -240.0
  episodes_this_iter: 96
  episodes_total: 1728
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12856.548
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001158163184300065
        entropy: 1.9016022682189941
        entropy_coeff: 0.0017600000137463212
        kl: 0.00792994350194931
        model: {}
        policy_loss: -0.0010596427600830793
        total_loss: 0.0009529388626106083
        vf_explained_var: -0.00277864933013916
        vf_loss: 51.611488342285156
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001158163184300065
        entropy: 1.8155218362808228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061347754672169685
        model: {}
        policy_loss: -0.0008931849151849747
        total_loss: 0.0036572434473782778
        vf_explained_var: -0.0034046471118927
        vf_loss: 75.15694427490234
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001158163184300065
        entropy: 1.9232043027877808
        entropy_coeff: 0.0017600000137463212
        kl: 0.005245054140686989
        model: {}
        policy_loss: -0.00018198369070887566
        total_loss: 0.0045421067625284195
        vf_explained_var: -0.002275705337524414
        vf_loss: 78.46676635742188
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001158163184300065
        entropy: 1.8913475275039673
        entropy_coeff: 0.0017600000137463212
        kl: 0.025257857516407967
        model: {}
        policy_loss: -0.0016924317460507154
        total_loss: 0.003998021129518747
        vf_explained_var: -0.0021320581436157227
        vf_loss: 83.87775421142578
      agent-4:
        cur_kl_coeff: 0.02109375037252903
        cur_lr: 0.001158163184300065
        entropy: 1.6472458839416504
        entropy_coeff: 0.0017600000137463212
        kl: 0.009198790416121483
        model: {}
        policy_loss: -0.0013165352866053581
        total_loss: 0.002947575878351927
        vf_explained_var: -0.0016930997371673584
        vf_loss: 69.69226837158203
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001158163184300065
        entropy: 2.0116426944732666
        entropy_coeff: 0.0017600000137463212
        kl: 0.00940247904509306
        model: {}
        policy_loss: -0.0013849390670657158
        total_loss: 0.0024325521662831306
        vf_explained_var: -0.001832127571105957
        vf_loss: 68.87857055664062
    load_time_ms: 13358.488
    num_steps_sampled: 1728000
    num_steps_trained: 1728000
    sample_time_ms: 104145.516
    update_time_ms: 17.119
  iterations_since_restore: 18
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.158288770053478
    ram_util_percent: 11.653475935828878
  pid: 28570
  policy_reward_max:
    agent-0: 22.5
    agent-1: 22.5
    agent-2: 21.5
    agent-3: 21.5
    agent-4: 28.5
    agent-5: 28.5
  policy_reward_mean:
    agent-0: -1.945
    agent-1: -1.945
    agent-2: -12.015
    agent-3: -12.015
    agent-4: -6.37
    agent-5: -6.37
  policy_reward_min:
    agent-0: -52.0
    agent-1: -52.0
    agent-2: -73.5
    agent-3: -73.5
    agent-4: -116.0
    agent-5: -116.0
  sampler_perf:
    mean_env_wait_ms: 26.17250487385693
    mean_inference_ms: 13.389830838188937
    mean_processing_ms: 59.957750179742455
  time_since_restore: 2418.989078760147
  time_this_iter_s: 131.28985166549683
  time_total_s: 2418.989078760147
  timestamp: 1637510945
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 1728000
  training_iteration: 18
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     18 |          2418.99 | 1728000 |   -40.66 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 6.81
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 7.69
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.92
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 4.64
    apples_agent-3_min: 0
    apples_agent-4_max: 38
    apples_agent-4_mean: 9.41
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 111
    cleaning_beam_agent-0_mean: 88.22
    cleaning_beam_agent-0_min: 66
    cleaning_beam_agent-1_max: 74
    cleaning_beam_agent-1_mean: 60.79
    cleaning_beam_agent-1_min: 42
    cleaning_beam_agent-2_max: 283
    cleaning_beam_agent-2_mean: 248.4
    cleaning_beam_agent-2_min: 210
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 156.34
    cleaning_beam_agent-3_min: 135
    cleaning_beam_agent-4_max: 80
    cleaning_beam_agent-4_mean: 56.17
    cleaning_beam_agent-4_min: 42
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 123.77
    cleaning_beam_agent-5_min: 102
    fire_beam_agent-0_max: 6
    fire_beam_agent-0_mean: 2.18
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.85
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.78
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.79
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 5
    fire_beam_agent-4_mean: 1.63
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.45
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-11-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 90.0
  episode_reward_mean: -12.94
  episode_reward_min: -189.0
  episodes_this_iter: 96
  episodes_total: 1824
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12850.574
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011521728010848165
        entropy: 1.8732850551605225
        entropy_coeff: 0.0017600000137463212
        kl: 0.009379932656884193
        model: {}
        policy_loss: -0.0013292396906763315
        total_loss: -0.0006035903934389353
        vf_explained_var: -0.002595186233520508
        vf_loss: 37.88134765625
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011521728010848165
        entropy: 1.8432451486587524
        entropy_coeff: 0.0017600000137463212
        kl: 0.01025746762752533
        model: {}
        policy_loss: -0.0016385215567424893
        total_loss: 0.0008403256651945412
        vf_explained_var: -0.004084527492523193
        vf_loss: 53.38304901123047
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011521728010848165
        entropy: 1.8958078622817993
        entropy_coeff: 0.0017600000137463212
        kl: 0.007543839048594236
        model: {}
        policy_loss: -0.001400515902787447
        total_loss: 0.0012767165899276733
        vf_explained_var: -0.005315959453582764
        vf_loss: 56.366600036621094
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011521728010848165
        entropy: 1.8291081190109253
        entropy_coeff: 0.0017600000137463212
        kl: 0.010118978098034859
        model: {}
        policy_loss: -0.0018214033916592598
        total_loss: 0.0011371755972504616
        vf_explained_var: -0.004350751638412476
        vf_loss: 57.983482360839844
      agent-4:
        cur_kl_coeff: 0.02109375037252903
        cur_lr: 0.0011521728010848165
        entropy: 1.7664742469787598
        entropy_coeff: 0.0017600000137463212
        kl: 0.011431708931922913
        model: {}
        policy_loss: -0.0014590555801987648
        total_loss: 0.0005610622465610504
        vf_explained_var: -0.0014565587043762207
        vf_loss: 48.8797492980957
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011521728010848165
        entropy: 2.0125696659088135
        entropy_coeff: 0.0017600000137463212
        kl: 0.005414952524006367
        model: {}
        policy_loss: -0.0007945164106786251
        total_loss: 0.000759185291826725
        vf_explained_var: -0.0019554495811462402
        vf_loss: 48.25075912475586
    load_time_ms: 13361.978
    num_steps_sampled: 1824000
    num_steps_trained: 1824000
    sample_time_ms: 104269.374
    update_time_ms: 17.031
  iterations_since_restore: 19
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.183422459893045
    ram_util_percent: 11.650267379679141
  pid: 28570
  policy_reward_max:
    agent-0: 30.0
    agent-1: 30.0
    agent-2: 31.0
    agent-3: 31.0
    agent-4: 34.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: -1.15
    agent-1: -1.15
    agent-2: -3.46
    agent-3: -3.46
    agent-4: -1.86
    agent-5: -1.86
  policy_reward_min:
    agent-0: -72.0
    agent-1: -72.0
    agent-2: -72.0
    agent-3: -72.0
    agent-4: -93.5
    agent-5: -93.5
  sampler_perf:
    mean_env_wait_ms: 26.104994021960014
    mean_inference_ms: 13.377297709300313
    mean_processing_ms: 59.9277067141968
  time_since_restore: 2550.8521332740784
  time_this_iter_s: 131.86305451393127
  time_total_s: 2550.8521332740784
  timestamp: 1637511077
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 1824000
  training_iteration: 19
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     19 |          2550.85 | 1824000 |   -12.94 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 8.75
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 8.52
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 3.63
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 6.71
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 6.79
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 100
    cleaning_beam_agent-0_mean: 76.82
    cleaning_beam_agent-0_min: 59
    cleaning_beam_agent-1_max: 114
    cleaning_beam_agent-1_mean: 91.76
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 284
    cleaning_beam_agent-2_mean: 249.14
    cleaning_beam_agent-2_min: 215
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 109.69
    cleaning_beam_agent-3_min: 83
    cleaning_beam_agent-4_max: 102
    cleaning_beam_agent-4_mean: 73.09
    cleaning_beam_agent-4_min: 48
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 132.2
    cleaning_beam_agent-5_min: 100
    fire_beam_agent-0_max: 6
    fire_beam_agent-0_mean: 1.36
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.79
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.37
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.84
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 1.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.57
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-13-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 106.0
  episode_reward_mean: 18.39
  episode_reward_min: -125.0
  episodes_this_iter: 96
  episodes_total: 1920
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12839.855
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011461824178695679
        entropy: 1.739368200302124
        entropy_coeff: 0.0017600000137463212
        kl: 0.013689570128917694
        model: {}
        policy_loss: -0.0016595128690823913
        total_loss: -0.0016830208478495479
        vf_explained_var: -0.002090632915496826
        vf_loss: 26.95538902282715
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011461824178695679
        entropy: 1.7767446041107178
        entropy_coeff: 0.0017600000137463212
        kl: 0.005084964446723461
        model: {}
        policy_loss: -0.001146447379142046
        total_loss: -0.00034680109820328653
        vf_explained_var: -0.002699047327041626
        vf_loss: 37.36030578613281
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011461824178695679
        entropy: 1.8887218236923218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0074622659012675285
        model: {}
        policy_loss: -0.002517515327781439
        total_loss: -0.0013576094061136246
        vf_explained_var: -0.005726873874664307
        vf_loss: 41.10943603515625
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011461824178695679
        entropy: 1.8183321952819824
        entropy_coeff: 0.0017600000137463212
        kl: 0.01867067441344261
        model: {}
        policy_loss: -0.002798589877784252
        total_loss: -0.0012941556051373482
        vf_explained_var: -0.012800812721252441
        vf_loss: 40.04547119140625
      agent-4:
        cur_kl_coeff: 0.02109375037252903
        cur_lr: 0.0011461824178695679
        entropy: 1.8503930568695068
        entropy_coeff: 0.0017600000137463212
        kl: 0.007310457061976194
        model: {}
        policy_loss: -0.0006620385684072971
        total_loss: -0.0006387040484696627
        vf_explained_var: -0.0028088390827178955
        vf_loss: 31.25819206237793
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011461824178695679
        entropy: 2.023496627807617
        entropy_coeff: 0.0017600000137463212
        kl: 0.004926934372633696
        model: {}
        policy_loss: -0.0007589443121105433
        total_loss: -0.0009453549282625318
        vf_explained_var: -0.0031618475914001465
        vf_loss: 31.285980224609375
    load_time_ms: 13345.799
    num_steps_sampled: 1920000
    num_steps_trained: 1920000
    sample_time_ms: 104242.925
    update_time_ms: 17.044
  iterations_since_restore: 20
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.13978494623656
    ram_util_percent: 11.555376344086023
  pid: 28570
  policy_reward_max:
    agent-0: 39.5
    agent-1: 39.5
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 25.0
    agent-5: 25.0
  policy_reward_mean:
    agent-0: 4.79
    agent-1: 4.79
    agent-2: 3.255
    agent-3: 3.255
    agent-4: 1.15
    agent-5: 1.15
  policy_reward_min:
    agent-0: -47.0
    agent-1: -47.0
    agent-2: -43.0
    agent-3: -43.0
    agent-4: -46.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 26.03965368364191
    mean_inference_ms: 13.366426681272536
    mean_processing_ms: 59.88881398489801
  time_since_restore: 2681.3260967731476
  time_this_iter_s: 130.4739634990692
  time_total_s: 2681.3260967731476
  timestamp: 1637511208
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 1920000
  training_iteration: 20
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     20 |          2681.33 | 1920000 |    18.39 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 5.8
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 5.16
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 6.2
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 6.43
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 117
    cleaning_beam_agent-0_mean: 92.52
    cleaning_beam_agent-0_min: 73
    cleaning_beam_agent-1_max: 116
    cleaning_beam_agent-1_mean: 96.45
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 257
    cleaning_beam_agent-2_mean: 160.97
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 81.43
    cleaning_beam_agent-3_min: 60
    cleaning_beam_agent-4_max: 95
    cleaning_beam_agent-4_mean: 70.76
    cleaning_beam_agent-4_min: 56
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 126.94
    cleaning_beam_agent-5_min: 102
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.81
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.44
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.37
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.59
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.71
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.33
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-15-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 109.0
  episode_reward_mean: 21.44
  episode_reward_min: -146.0
  episodes_this_iter: 96
  episodes_total: 2016
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12845.381
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011401920346543193
        entropy: 1.7495648860931396
        entropy_coeff: 0.0017600000137463212
        kl: 0.006551308557391167
        model: {}
        policy_loss: -0.0008442238322459161
        total_loss: -0.002151958644390106
        vf_explained_var: -0.001100093126296997
        vf_loss: 16.077213287353516
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011401920346543193
        entropy: 1.6531014442443848
        entropy_coeff: 0.0017600000137463212
        kl: 0.019951757043600082
        model: {}
        policy_loss: -0.002368406392633915
        total_loss: -0.002228030003607273
        vf_explained_var: -0.0025433897972106934
        vf_loss: 23.016433715820312
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011401920346543193
        entropy: 1.8814043998718262
        entropy_coeff: 0.0017600000137463212
        kl: 0.015855373814702034
        model: {}
        policy_loss: -0.0033493780065327883
        total_loss: -0.0025848043151199818
        vf_explained_var: -0.00280115008354187
        vf_loss: 32.830745697021484
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011401920346543193
        entropy: 1.8660571575164795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0140609759837389
        model: {}
        policy_loss: -0.0018081353046000004
        total_loss: -0.00183382211253047
        vf_explained_var: 0.00014050304889678955
        vf_loss: 27.31285858154297
      agent-4:
        cur_kl_coeff: 0.02109375037252903
        cur_lr: 0.0011401920346543193
        entropy: 1.6700074672698975
        entropy_coeff: 0.0017600000137463212
        kl: 0.030880840495228767
        model: {}
        policy_loss: -0.0026455605402588844
        total_loss: -0.002884307876229286
        vf_explained_var: -0.0019473731517791748
        vf_loss: 20.490726470947266
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011401920346543193
        entropy: 2.0093603134155273
        entropy_coeff: 0.0017600000137463212
        kl: 0.010844569653272629
        model: {}
        policy_loss: -0.0011526411399245262
        total_loss: -0.0023585413582623005
        vf_explained_var: -0.003355175256729126
        vf_loss: 20.59457778930664
    load_time_ms: 13344.966
    num_steps_sampled: 2016000
    num_steps_trained: 2016000
    sample_time_ms: 104124.276
    update_time_ms: 16.98
  iterations_since_restore: 21
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.16290322580645
    ram_util_percent: 11.577956989247312
  pid: 28570
  policy_reward_max:
    agent-0: 28.0
    agent-1: 28.0
    agent-2: 30.0
    agent-3: 30.0
    agent-4: 26.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 4.9
    agent-1: 4.9
    agent-2: 1.925
    agent-3: 1.925
    agent-4: 3.895
    agent-5: 3.895
  policy_reward_min:
    agent-0: -45.5
    agent-1: -45.5
    agent-2: -69.5
    agent-3: -69.5
    agent-4: -46.5
    agent-5: -46.5
  sampler_perf:
    mean_env_wait_ms: 25.961834699825932
    mean_inference_ms: 13.352671507090282
    mean_processing_ms: 59.85461664682515
  time_since_restore: 2811.2903418540955
  time_this_iter_s: 129.96424508094788
  time_total_s: 2811.2903418540955
  timestamp: 1637511338
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 2016000
  training_iteration: 21
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     21 |          2811.29 | 2016000 |    21.44 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.32
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 5.6
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 4.76
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 4.38
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 6.83
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 3.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 102
    cleaning_beam_agent-0_mean: 69.55
    cleaning_beam_agent-0_min: 48
    cleaning_beam_agent-1_max: 109
    cleaning_beam_agent-1_mean: 81.22
    cleaning_beam_agent-1_min: 63
    cleaning_beam_agent-2_max: 185
    cleaning_beam_agent-2_mean: 144.22
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 103.54
    cleaning_beam_agent-3_min: 77
    cleaning_beam_agent-4_max: 82
    cleaning_beam_agent-4_mean: 62.34
    cleaning_beam_agent-4_min: 46
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 124.6
    cleaning_beam_agent-5_min: 99
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.45
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.25
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.31
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.5
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.34
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.35
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-17-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 82.0
  episode_reward_mean: 24.17
  episode_reward_min: -113.0
  episodes_this_iter: 96
  episodes_total: 2112
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12829.119
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011342016514390707
        entropy: 1.766769528388977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060753244906663895
        model: {}
        policy_loss: -0.001379379304125905
        total_loss: -0.003224148415029049
        vf_explained_var: 2.63899564743042e-05
        vf_loss: 11.128617286682129
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.5268856287002563
        entropy_coeff: 0.0017600000137463212
        kl: 0.01280337106436491
        model: {}
        policy_loss: -0.0015060510486364365
        total_loss: -0.0021845754235982895
        vf_explained_var: -0.003421783447265625
        vf_loss: 15.28671646118164
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011342016514390707
        entropy: 1.841110110282898
        entropy_coeff: 0.0017600000137463212
        kl: 0.008861975744366646
        model: {}
        policy_loss: -0.002665222156792879
        total_loss: -0.003501195926219225
        vf_explained_var: -0.0040052831172943115
        vf_loss: 19.612789154052734
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011342016514390707
        entropy: 1.8546732664108276
        entropy_coeff: 0.0017600000137463212
        kl: 0.012761181220412254
        model: {}
        policy_loss: -0.0016993966419249773
        total_loss: -0.0027662930078804493
        vf_explained_var: -0.005029052495956421
        vf_loss: 17.187864303588867
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0011342016514390707
        entropy: 1.769808053970337
        entropy_coeff: 0.0017600000137463212
        kl: 0.014216892421245575
        model: {}
        policy_loss: -0.002238047309219837
        total_loss: -0.003428901545703411
        vf_explained_var: -0.0037783384323120117
        vf_loss: 14.741756439208984
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011342016514390707
        entropy: 1.9762351512908936
        entropy_coeff: 0.0017600000137463212
        kl: 0.009472246281802654
        model: {}
        policy_loss: -0.0016647616866976023
        total_loss: -0.0034473626874387264
        vf_explained_var: -0.00047856569290161133
        vf_loss: 14.587674140930176
    load_time_ms: 13321.332
    num_steps_sampled: 2112000
    num_steps_trained: 2112000
    sample_time_ms: 103961.599
    update_time_ms: 17.246
  iterations_since_restore: 22
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.23206521739131
    ram_util_percent: 11.629891304347824
  pid: 28570
  policy_reward_max:
    agent-0: 28.5
    agent-1: 28.5
    agent-2: 18.5
    agent-3: 18.5
    agent-4: 22.5
    agent-5: 22.5
  policy_reward_mean:
    agent-0: 5.765
    agent-1: 5.765
    agent-2: 2.67
    agent-3: 2.67
    agent-4: 3.65
    agent-5: 3.65
  policy_reward_min:
    agent-0: -22.5
    agent-1: -22.5
    agent-2: -43.5
    agent-3: -43.5
    agent-4: -47.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.87544804305346
    mean_inference_ms: 13.342024744346947
    mean_processing_ms: 59.81004273679722
  time_since_restore: 2940.472564935684
  time_this_iter_s: 129.18222308158875
  time_total_s: 2940.472564935684
  timestamp: 1637511467
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 2112000
  training_iteration: 22
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     22 |          2940.47 | 2112000 |    24.17 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 8.04
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 6.18
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 6.78
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 6.53
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 7.35
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 126
    cleaning_beam_agent-0_mean: 105.52
    cleaning_beam_agent-0_min: 57
    cleaning_beam_agent-1_max: 115
    cleaning_beam_agent-1_mean: 96.66
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 198
    cleaning_beam_agent-2_mean: 164.74
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 82.67
    cleaning_beam_agent-3_min: 62
    cleaning_beam_agent-4_max: 115
    cleaning_beam_agent-4_mean: 89.07
    cleaning_beam_agent-4_min: 60
    cleaning_beam_agent-5_max: 205
    cleaning_beam_agent-5_mean: 171.22
    cleaning_beam_agent-5_min: 121
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.26
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.27
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.29
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.4
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.22
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.31
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-19-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 94.0
  episode_reward_mean: 40.04
  episode_reward_min: -57.0
  episodes_this_iter: 96
  episodes_total: 2208
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12824.523
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011282111518085003
        entropy: 1.568306565284729
        entropy_coeff: 0.0017600000137463212
        kl: 0.006023557856678963
        model: {}
        policy_loss: -0.00139702670276165
        total_loss: -0.003014378249645233
        vf_explained_var: -0.001354902982711792
        vf_loss: 9.922783851623535
      agent-1:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.8028013706207275
        entropy_coeff: 0.0017600000137463212
        kl: 0.002768112812191248
        model: {}
        policy_loss: -0.0002180812880396843
        total_loss: -0.002171069383621216
        vf_explained_var: -0.0012059807777404785
        vf_loss: 11.161412239074707
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011282111518085003
        entropy: 1.8694190979003906
        entropy_coeff: 0.0017600000137463212
        kl: 0.012492305599153042
        model: {}
        policy_loss: -0.003021078649908304
        total_loss: -0.004236248787492514
        vf_explained_var: -0.0062167346477508545
        vf_loss: 14.503946304321289
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011282111518085003
        entropy: 1.7768787145614624
        entropy_coeff: 0.0017600000137463212
        kl: 0.012518160976469517
        model: {}
        policy_loss: -0.0020568668842315674
        total_loss: -0.0034858183935284615
        vf_explained_var: -0.0002932250499725342
        vf_loss: 12.289239883422852
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0011282111518085003
        entropy: 1.7597719430923462
        entropy_coeff: 0.0017600000137463212
        kl: 0.005626447033137083
        model: {}
        policy_loss: -0.0005870494060218334
        total_loss: -0.0024037682451307774
        vf_explained_var: -0.0007471442222595215
        vf_loss: 11.024556159973145
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011282111518085003
        entropy: 1.9433953762054443
        entropy_coeff: 0.0017600000137463212
        kl: 0.021964937448501587
        model: {}
        policy_loss: -0.0023187133483588696
        total_loss: -0.0038770134560763836
        vf_explained_var: -0.026980459690093994
        vf_loss: 13.12952995300293
    load_time_ms: 13327.533
    num_steps_sampled: 2208000
    num_steps_trained: 2208000
    sample_time_ms: 104032.684
    update_time_ms: 17.3
  iterations_since_restore: 23
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.277956989247315
    ram_util_percent: 11.552688172043013
  pid: 28570
  policy_reward_max:
    agent-0: 27.5
    agent-1: 27.5
    agent-2: 24.5
    agent-3: 24.5
    agent-4: 24.0
    agent-5: 24.0
  policy_reward_mean:
    agent-0: 7.42
    agent-1: 7.42
    agent-2: 8.72
    agent-3: 8.72
    agent-4: 3.88
    agent-5: 3.88
  policy_reward_min:
    agent-0: -18.5
    agent-1: -18.5
    agent-2: -21.0
    agent-3: -21.0
    agent-4: -36.5
    agent-5: -36.5
  sampler_perf:
    mean_env_wait_ms: 25.832963743372066
    mean_inference_ms: 13.337873627884852
    mean_processing_ms: 59.80453102814626
  time_since_restore: 3071.7315380573273
  time_this_iter_s: 131.25897312164307
  time_total_s: 3071.7315380573273
  timestamp: 1637511599
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 2208000
  training_iteration: 23
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     23 |          3071.73 | 2208000 |    40.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 10.71
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 6.05
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 6.94
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 6.15
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 8.4
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 5.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 96.01
    cleaning_beam_agent-0_min: 75
    cleaning_beam_agent-1_max: 128
    cleaning_beam_agent-1_mean: 106.18
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 217
    cleaning_beam_agent-2_mean: 173.87
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 100.2
    cleaning_beam_agent-3_min: 76
    cleaning_beam_agent-4_max: 106
    cleaning_beam_agent-4_mean: 83.86
    cleaning_beam_agent-4_min: 66
    cleaning_beam_agent-5_max: 206
    cleaning_beam_agent-5_mean: 175.14
    cleaning_beam_agent-5_min: 152
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.2
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.18
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.13
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.35
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-22-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 110.0
  episode_reward_mean: 48.78
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 2304
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12822.669
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011222207685932517
        entropy: 1.5045506954193115
        entropy_coeff: 0.0017600000137463212
        kl: 0.007412548642605543
        model: {}
        policy_loss: -0.0017021243693307042
        total_loss: -0.0033638854511082172
        vf_explained_var: -0.0011836886405944824
        vf_loss: 8.009329795837402
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0011222207685932517
        entropy: 1.754232406616211
        entropy_coeff: 0.0017600000137463212
        kl: 0.009297246113419533
        model: {}
        policy_loss: -0.0006269426085054874
        total_loss: -0.0026474865153431892
        vf_explained_var: -0.0008420348167419434
        vf_loss: 8.92581558227539
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011222207685932517
        entropy: 1.8536056280136108
        entropy_coeff: 0.0017600000137463212
        kl: 0.011483673937618732
        model: {}
        policy_loss: -0.0034976075403392315
        total_loss: -0.005078325048089027
        vf_explained_var: -0.003711193799972534
        vf_loss: 11.074444770812988
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.855070948600769
        entropy_coeff: 0.0017600000137463212
        kl: 0.008105456829071045
        model: {}
        policy_loss: -0.0012676042970269918
        total_loss: -0.003227779408916831
        vf_explained_var: -0.0032618343830108643
        vf_loss: 10.0079984664917
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0011222207685932517
        entropy: 1.7571548223495483
        entropy_coeff: 0.0017600000137463212
        kl: 0.00857524573802948
        model: {}
        policy_loss: -0.0010492352303117514
        total_loss: -0.0031363985035568476
        vf_explained_var: -0.0008043348789215088
        vf_loss: 7.34102725982666
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011222207685932517
        entropy: 1.9692555665969849
        entropy_coeff: 0.0017600000137463212
        kl: 0.010571793653070927
        model: {}
        policy_loss: -0.001078344415873289
        total_loss: -0.003375624306499958
        vf_explained_var: -0.0011801421642303467
        vf_loss: 7.721708297729492
    load_time_ms: 13295.417
    num_steps_sampled: 2304000
    num_steps_trained: 2304000
    sample_time_ms: 104286.711
    update_time_ms: 17.725
  iterations_since_restore: 24
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.188297872340428
    ram_util_percent: 11.603191489361706
  pid: 28570
  policy_reward_max:
    agent-0: 33.0
    agent-1: 33.0
    agent-2: 22.0
    agent-3: 22.0
    agent-4: 23.0
    agent-5: 23.0
  policy_reward_mean:
    agent-0: 8.655
    agent-1: 8.655
    agent-2: 8.045
    agent-3: 8.045
    agent-4: 7.69
    agent-5: 7.69
  policy_reward_min:
    agent-0: -38.5
    agent-1: -38.5
    agent-2: -46.5
    agent-3: -46.5
    agent-4: -22.0
    agent-5: -22.0
  sampler_perf:
    mean_env_wait_ms: 25.79963846918671
    mean_inference_ms: 13.332692078414654
    mean_processing_ms: 59.806699640186324
  time_since_restore: 3203.3383944034576
  time_this_iter_s: 131.60685634613037
  time_total_s: 3203.3383944034576
  timestamp: 1637511731
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 2304000
  training_iteration: 24
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     24 |          3203.34 | 2304000 |    48.78 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 11.4
    apples_agent-0_min: 1
    apples_agent-1_max: 43
    apples_agent-1_mean: 6.58
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 8.09
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 7.02
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 7.32
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 133
    cleaning_beam_agent-0_mean: 109.58
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 116
    cleaning_beam_agent-1_mean: 94.29
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 193
    cleaning_beam_agent-2_mean: 162.41
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 119.93
    cleaning_beam_agent-3_min: 94
    cleaning_beam_agent-4_max: 134
    cleaning_beam_agent-4_mean: 106.56
    cleaning_beam_agent-4_min: 68
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 184.69
    cleaning_beam_agent-5_min: 153
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.17
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.15
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.12
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.23
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-24-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 133.0
  episode_reward_mean: 54.13
  episode_reward_min: -69.0
  episodes_this_iter: 96
  episodes_total: 2400
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12854.673
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011162303853780031
        entropy: 1.4600523710250854
        entropy_coeff: 0.0017600000137463212
        kl: 0.00553036667406559
        model: {}
        policy_loss: -0.0017104987055063248
        total_loss: -0.0035876124165952206
        vf_explained_var: -0.0015182793140411377
        vf_loss: 5.54322624206543
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0011162303853780031
        entropy: 1.7682826519012451
        entropy_coeff: 0.0017600000137463212
        kl: 0.013532388024032116
        model: {}
        policy_loss: -0.00118222925812006
        total_loss: -0.0034209778532385826
        vf_explained_var: -0.0002913922071456909
        vf_loss: 6.196983814239502
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011162303853780031
        entropy: 1.8392043113708496
        entropy_coeff: 0.0017600000137463212
        kl: 0.01449914462864399
        model: {}
        policy_loss: -0.004279184155166149
        total_loss: -0.0059428769163787365
        vf_explained_var: -0.0020935237407684326
        vf_loss: 8.483501434326172
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.8196532726287842
        entropy_coeff: 0.0017600000137463212
        kl: 0.00791874248534441
        model: {}
        policy_loss: -0.001443643239326775
        total_loss: -0.0035888312850147486
        vf_explained_var: -0.00279119610786438
        vf_loss: 7.6044921875
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0011162303853780031
        entropy: 1.6308914422988892
        entropy_coeff: 0.0017600000137463212
        kl: 0.01400325633585453
        model: {}
        policy_loss: -0.0014629429206252098
        total_loss: -0.0034328848123550415
        vf_explained_var: 7.161498069763184e-05
        vf_loss: 4.573553085327148
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011162303853780031
        entropy: 1.994111180305481
        entropy_coeff: 0.0017600000137463212
        kl: 0.00881172250956297
        model: {}
        policy_loss: -0.0010038991458714008
        total_loss: -0.003694760613143444
        vf_explained_var: -0.0001731961965560913
        vf_loss: 4.883345127105713
    load_time_ms: 13305.609
    num_steps_sampled: 2400000
    num_steps_trained: 2400000
    sample_time_ms: 104396.688
    update_time_ms: 17.253
  iterations_since_restore: 25
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.23494623655914
    ram_util_percent: 11.651075268817205
  pid: 28570
  policy_reward_max:
    agent-0: 39.5
    agent-1: 39.5
    agent-2: 27.0
    agent-3: 27.0
    agent-4: 22.5
    agent-5: 22.5
  policy_reward_mean:
    agent-0: 10.86
    agent-1: 10.86
    agent-2: 9.36
    agent-3: 9.36
    agent-4: 6.845
    agent-5: 6.845
  policy_reward_min:
    agent-0: -17.5
    agent-1: -17.5
    agent-2: -21.0
    agent-3: -21.0
    agent-4: -43.0
    agent-5: -43.0
  sampler_perf:
    mean_env_wait_ms: 25.771451738615692
    mean_inference_ms: 13.324688060631702
    mean_processing_ms: 59.795029721923086
  time_since_restore: 3334.055067539215
  time_this_iter_s: 130.71667313575745
  time_total_s: 3334.055067539215
  timestamp: 1637511861
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 2400000
  training_iteration: 25
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     25 |          3334.06 | 2400000 |    54.13 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 8.82
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 8.06
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 8.12
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 6.92
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 9.08
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 114.83
    cleaning_beam_agent-0_min: 95
    cleaning_beam_agent-1_max: 102
    cleaning_beam_agent-1_mean: 80.82
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 201
    cleaning_beam_agent-2_mean: 168.75
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 106.64
    cleaning_beam_agent-3_min: 73
    cleaning_beam_agent-4_max: 115
    cleaning_beam_agent-4_mean: 89.65
    cleaning_beam_agent-4_min: 67
    cleaning_beam_agent-5_max: 201
    cleaning_beam_agent-5_mean: 158.04
    cleaning_beam_agent-5_min: 128
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.21
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.13
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.13
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.18
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-26-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 136.0
  episode_reward_mean: 53.73
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 2496
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12854.236
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0011102400021627545
        entropy: 1.5071864128112793
        entropy_coeff: 0.0017600000137463212
        kl: 0.007234875578433275
        model: {}
        policy_loss: -0.0029474464245140553
        total_loss: -0.004996330477297306
        vf_explained_var: -0.0013921260833740234
        vf_loss: 4.22892427444458
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0011102400021627545
        entropy: 1.7056076526641846
        entropy_coeff: 0.0017600000137463212
        kl: 0.013584349304437637
        model: {}
        policy_loss: -0.0010566937271505594
        total_loss: -0.0033469449263066053
        vf_explained_var: 0.0023806989192962646
        vf_loss: 4.5691118240356445
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0011102400021627545
        entropy: 1.864331603050232
        entropy_coeff: 0.0017600000137463212
        kl: 0.00899210013449192
        model: {}
        policy_loss: -0.004190051928162575
        total_loss: -0.006369219161570072
        vf_explained_var: -0.0033805668354034424
        vf_loss: 6.5245041847229
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.763655662536621
        entropy_coeff: 0.0017600000137463212
        kl: 0.010355369187891483
        model: {}
        policy_loss: -0.0024298967327922583
        total_loss: -0.004554044455289841
        vf_explained_var: -0.001995086669921875
        vf_loss: 5.9156084060668945
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0011102400021627545
        entropy: 1.6152911186218262
        entropy_coeff: 0.0017600000137463212
        kl: 0.010144172236323357
        model: {}
        policy_loss: -0.0010840166360139847
        total_loss: -0.0032305987551808357
        vf_explained_var: -0.0005383193492889404
        vf_loss: 3.7536280155181885
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0011102400021627545
        entropy: 1.989513874053955
        entropy_coeff: 0.0017600000137463212
        kl: 0.009227760136127472
        model: {}
        policy_loss: -0.0014266263460740447
        total_loss: -0.004189973697066307
        vf_explained_var: -0.00047528743743896484
        vf_loss: 3.9215588569641113
    load_time_ms: 13330.703
    num_steps_sampled: 2496000
    num_steps_trained: 2496000
    sample_time_ms: 104452.387
    update_time_ms: 17.457
  iterations_since_restore: 26
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.182795698924732
    ram_util_percent: 11.644086021505377
  pid: 28570
  policy_reward_max:
    agent-0: 35.0
    agent-1: 35.0
    agent-2: 30.5
    agent-3: 30.5
    agent-4: 24.5
    agent-5: 24.5
  policy_reward_mean:
    agent-0: 10.55
    agent-1: 10.55
    agent-2: 9.175
    agent-3: 9.175
    agent-4: 7.14
    agent-5: 7.14
  policy_reward_min:
    agent-0: -21.0
    agent-1: -21.0
    agent-2: -21.5
    agent-3: -21.5
    agent-4: -21.5
    agent-5: -21.5
  sampler_perf:
    mean_env_wait_ms: 25.732555980887074
    mean_inference_ms: 13.315867305438054
    mean_processing_ms: 59.754988463245084
  time_since_restore: 3464.463297843933
  time_this_iter_s: 130.40823030471802
  time_total_s: 3464.463297843933
  timestamp: 1637511992
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 2496000
  training_iteration: 26
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     26 |          3464.46 | 2496000 |    53.73 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 10.23
    apples_agent-0_min: 0
    apples_agent-1_max: 35
    apples_agent-1_mean: 6.33
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 8.02
    apples_agent-2_min: 0
    apples_agent-3_max: 33
    apples_agent-3_mean: 7.12
    apples_agent-3_min: 0
    apples_agent-4_max: 48
    apples_agent-4_mean: 9.2
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 127.95
    cleaning_beam_agent-0_min: 105
    cleaning_beam_agent-1_max: 101
    cleaning_beam_agent-1_mean: 73.03
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 209
    cleaning_beam_agent-2_mean: 166.07
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 127.1
    cleaning_beam_agent-3_min: 93
    cleaning_beam_agent-4_max: 134
    cleaning_beam_agent-4_mean: 106.76
    cleaning_beam_agent-4_min: 67
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 172.06
    cleaning_beam_agent-5_min: 138
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.17
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.17
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.2
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-28-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 141.0
  episode_reward_mean: 55.7
  episode_reward_min: -65.0
  episodes_this_iter: 96
  episodes_total: 2592
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12855.718
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001104249618947506
        entropy: 1.5288071632385254
        entropy_coeff: 0.0017600000137463212
        kl: 0.008580571971833706
        model: {}
        policy_loss: -0.003034757450222969
        total_loss: -0.005088471341878176
        vf_explained_var: -0.0001367628574371338
        vf_loss: 4.224723815917969
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.001104249618947506
        entropy: 1.8046979904174805
        entropy_coeff: 0.0017600000137463212
        kl: 0.011425130069255829
        model: {}
        policy_loss: -0.0015729309525340796
        total_loss: -0.004045351408421993
        vf_explained_var: -0.004773974418640137
        vf_loss: 4.8962554931640625
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001104249618947506
        entropy: 1.880639672279358
        entropy_coeff: 0.0017600000137463212
        kl: 0.01186556275933981
        model: {}
        policy_loss: -0.0058933766558766365
        total_loss: -0.00827227532863617
        vf_explained_var: -0.005378812551498413
        vf_loss: 3.3775129318237305
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.7369500398635864
        entropy_coeff: 0.0017600000137463212
        kl: 0.01028414350003004
        model: {}
        policy_loss: -0.004279584623873234
        total_loss: -0.006613435223698616
        vf_explained_var: 0.0024561285972595215
        vf_loss: 3.3752572536468506
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.001104249618947506
        entropy: 1.683220386505127
        entropy_coeff: 0.0017600000137463212
        kl: 0.007938400842249393
        model: {}
        policy_loss: -0.0010393406264483929
        total_loss: -0.00348581001162529
        vf_explained_var: 0.0014760196208953857
        vf_loss: 2.6482491493225098
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001104249618947506
        entropy: 1.9483846426010132
        entropy_coeff: 0.0017600000137463212
        kl: 0.009944545105099678
        model: {}
        policy_loss: -0.002026858739554882
        total_loss: -0.004806507844477892
        vf_explained_var: 5.315244197845459e-05
        vf_loss: 2.7658863067626953
    load_time_ms: 13313.66
    num_steps_sampled: 2592000
    num_steps_trained: 2592000
    sample_time_ms: 104368.793
    update_time_ms: 16.857
  iterations_since_restore: 27
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.332065217391307
    ram_util_percent: 11.546739130434782
  pid: 28570
  policy_reward_max:
    agent-0: 35.5
    agent-1: 35.5
    agent-2: 30.5
    agent-3: 30.5
    agent-4: 26.5
    agent-5: 26.5
  policy_reward_mean:
    agent-0: 9.82
    agent-1: 9.82
    agent-2: 10.69
    agent-3: 10.69
    agent-4: 7.34
    agent-5: 7.34
  policy_reward_min:
    agent-0: -20.5
    agent-1: -20.5
    agent-2: -16.5
    agent-3: -16.5
    agent-4: -21.5
    agent-5: -21.5
  sampler_perf:
    mean_env_wait_ms: 25.698016182021846
    mean_inference_ms: 13.308285841913067
    mean_processing_ms: 59.7069674260862
  time_since_restore: 3594.0986943244934
  time_this_iter_s: 129.6353964805603
  time_total_s: 3594.0986943244934
  timestamp: 1637512122
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 2592000
  training_iteration: 27
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     27 |           3594.1 | 2592000 |     55.7 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 8.71
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 6.42
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 9.13
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 7.2
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 10.39
    apples_agent-4_min: 0
    apples_agent-5_max: 33
    apples_agent-5_mean: 5.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 231
    cleaning_beam_agent-0_mean: 183.1
    cleaning_beam_agent-0_min: 110
    cleaning_beam_agent-1_max: 92
    cleaning_beam_agent-1_mean: 70.87
    cleaning_beam_agent-1_min: 47
    cleaning_beam_agent-2_max: 221
    cleaning_beam_agent-2_mean: 191.82
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 156.85
    cleaning_beam_agent-3_min: 106
    cleaning_beam_agent-4_max: 123
    cleaning_beam_agent-4_mean: 76.3
    cleaning_beam_agent-4_min: 57
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 171.54
    cleaning_beam_agent-5_min: 138
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.16
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.16
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-30-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 142.0
  episode_reward_mean: 61.54
  episode_reward_min: -52.0
  episodes_this_iter: 96
  episodes_total: 2688
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12860.527
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010982592357322574
        entropy: 1.494663119316101
        entropy_coeff: 0.0017600000137463212
        kl: 0.012149151414632797
        model: {}
        policy_loss: -0.0041180625557899475
        total_loss: -0.006221117451786995
        vf_explained_var: -0.004934400320053101
        vf_loss: 2.238248348236084
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010982592357322574
        entropy: 1.6998624801635742
        entropy_coeff: 0.0017600000137463212
        kl: 0.012906149961054325
        model: {}
        policy_loss: -0.0030698226764798164
        total_loss: -0.005572138354182243
        vf_explained_var: -0.0020454823970794678
        vf_loss: 2.474496364593506
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010982592357322574
        entropy: 1.8508927822113037
        entropy_coeff: 0.0017600000137463212
        kl: 0.00823933631181717
        model: {}
        policy_loss: -0.005027719307690859
        total_loss: -0.007626623380929232
        vf_explained_var: 0.0008214414119720459
        vf_loss: 2.4669783115386963
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.7505393028259277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061682481318712234
        model: {}
        policy_loss: -0.002510480349883437
        total_loss: -0.005082782357931137
        vf_explained_var: -0.0034289956092834473
        vf_loss: 2.7733733654022217
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0010982592357322574
        entropy: 1.4970197677612305
        entropy_coeff: 0.0017600000137463212
        kl: 0.007108571007847786
        model: {}
        policy_loss: -0.0012348159216344357
        total_loss: -0.003396646585315466
        vf_explained_var: 0.0006518661975860596
        vf_loss: 2.48003888130188
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010982592357322574
        entropy: 1.9027843475341797
        entropy_coeff: 0.0017600000137463212
        kl: 0.009256456978619099
        model: {}
        policy_loss: -0.0021784636192023754
        total_loss: -0.004930746741592884
        vf_explained_var: 0.002480059862136841
        vf_loss: 2.4949941635131836
    load_time_ms: 13310.607
    num_steps_sampled: 2688000
    num_steps_trained: 2688000
    sample_time_ms: 104262.605
    update_time_ms: 16.78
  iterations_since_restore: 28
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.313513513513513
    ram_util_percent: 11.610810810810813
  pid: 28570
  policy_reward_max:
    agent-0: 31.5
    agent-1: 31.5
    agent-2: 30.0
    agent-3: 30.0
    agent-4: 32.5
    agent-5: 32.5
  policy_reward_mean:
    agent-0: 9.9
    agent-1: 9.9
    agent-2: 11.27
    agent-3: 11.27
    agent-4: 9.6
    agent-5: 9.6
  policy_reward_min:
    agent-0: -17.0
    agent-1: -17.0
    agent-2: -18.5
    agent-3: -18.5
    agent-4: -38.0
    agent-5: -38.0
  sampler_perf:
    mean_env_wait_ms: 25.683095230213535
    mean_inference_ms: 13.30126957304193
    mean_processing_ms: 59.67679836456593
  time_since_restore: 3724.367765903473
  time_this_iter_s: 130.2690715789795
  time_total_s: 3724.367765903473
  timestamp: 1637512252
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 2688000
  training_iteration: 28
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     28 |          3724.37 | 2688000 |    61.54 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 12.25
    apples_agent-0_min: 0
    apples_agent-1_max: 44
    apples_agent-1_mean: 8.48
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 9.24
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 6.84
    apples_agent-3_min: 0
    apples_agent-4_max: 46
    apples_agent-4_mean: 11.57
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 5.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 226
    cleaning_beam_agent-0_mean: 186.27
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 94
    cleaning_beam_agent-1_mean: 77.08
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 266
    cleaning_beam_agent-2_mean: 211.22
    cleaning_beam_agent-2_min: 171
    cleaning_beam_agent-3_max: 238
    cleaning_beam_agent-3_mean: 197.58
    cleaning_beam_agent-3_min: 146
    cleaning_beam_agent-4_max: 96
    cleaning_beam_agent-4_mean: 79.33
    cleaning_beam_agent-4_min: 61
    cleaning_beam_agent-5_max: 200
    cleaning_beam_agent-5_mean: 176.31
    cleaning_beam_agent-5_min: 147
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.14
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-33-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 69.89
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 2784
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12839.48
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010922688525170088
        entropy: 1.5292346477508545
        entropy_coeff: 0.0017600000137463212
        kl: 0.012633725069463253
        model: {}
        policy_loss: -0.0045736078172922134
        total_loss: -0.006478108465671539
        vf_explained_var: 0.004977568984031677
        vf_loss: 4.711106300354004
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010922688525170088
        entropy: 1.7131781578063965
        entropy_coeff: 0.0017600000137463212
        kl: 0.01000607293099165
        model: {}
        policy_loss: -0.004069835878908634
        total_loss: -0.006402356084436178
        vf_explained_var: 0.0020322799682617188
        vf_loss: 4.950592994689941
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010922688525170088
        entropy: 1.876187801361084
        entropy_coeff: 0.0017600000137463212
        kl: 0.01005468238145113
        model: {}
        policy_loss: -0.0058747525326907635
        total_loss: -0.008456477895379066
        vf_explained_var: 0.002008378505706787
        vf_loss: 2.1763083934783936
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.7283849716186523
        entropy_coeff: 0.0017600000137463212
        kl: 0.00857376866042614
        model: {}
        policy_loss: -0.0035411790013313293
        total_loss: -0.006027376279234886
        vf_explained_var: 0.0004392862319946289
        vf_loss: 2.3424007892608643
      agent-4:
        cur_kl_coeff: 0.03164062649011612
        cur_lr: 0.0010922688525170088
        entropy: 1.5285727977752686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037737637758255005
        model: {}
        policy_loss: -0.0008654785342514515
        total_loss: -0.003087170422077179
        vf_explained_var: 0.0011988580226898193
        vf_loss: 3.491931915283203
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010922688525170088
        entropy: 1.8591318130493164
        entropy_coeff: 0.0017600000137463212
        kl: 0.00786247942596674
        model: {}
        policy_loss: -0.0029241726733744144
        total_loss: -0.005538098979741335
        vf_explained_var: 0.0021732747554779053
        vf_loss: 3.633052110671997
    load_time_ms: 13304.435
    num_steps_sampled: 2784000
    num_steps_trained: 2784000
    sample_time_ms: 104186.032
    update_time_ms: 16.798
  iterations_since_restore: 29
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.36129032258065
    ram_util_percent: 11.589247311827956
  pid: 28570
  policy_reward_max:
    agent-0: 50.0
    agent-1: 50.0
    agent-2: 24.5
    agent-3: 24.5
    agent-4: 37.5
    agent-5: 37.5
  policy_reward_mean:
    agent-0: 14.36
    agent-1: 14.36
    agent-2: 11.565
    agent-3: 11.565
    agent-4: 9.02
    agent-5: 9.02
  policy_reward_min:
    agent-0: -36.5
    agent-1: -36.5
    agent-2: -19.0
    agent-3: -19.0
    agent-4: -34.0
    agent-5: -34.0
  sampler_perf:
    mean_env_wait_ms: 25.676721316434367
    mean_inference_ms: 13.29394525090944
    mean_processing_ms: 59.65336662679847
  time_since_restore: 3855.187207698822
  time_this_iter_s: 130.81944179534912
  time_total_s: 3855.187207698822
  timestamp: 1637512383
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 2784000
  training_iteration: 29
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     29 |          3855.19 | 2784000 |    69.89 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 10.2
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 8.82
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 12.72
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 7.56
    apples_agent-3_min: 0
    apples_agent-4_max: 38
    apples_agent-4_mean: 10.09
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 6.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 253
    cleaning_beam_agent-0_mean: 218.56
    cleaning_beam_agent-0_min: 176
    cleaning_beam_agent-1_max: 115
    cleaning_beam_agent-1_mean: 91.54
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 242
    cleaning_beam_agent-2_mean: 206.56
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 251
    cleaning_beam_agent-3_mean: 208.25
    cleaning_beam_agent-3_min: 168
    cleaning_beam_agent-4_max: 101
    cleaning_beam_agent-4_mean: 80.77
    cleaning_beam_agent-4_min: 64
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 190.02
    cleaning_beam_agent-5_min: 162
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.15
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-35-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 73.5
  episode_reward_min: -13.0
  episodes_this_iter: 96
  episodes_total: 2880
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12847.361
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010862783528864384
        entropy: 1.4660173654556274
        entropy_coeff: 0.0017600000137463212
        kl: 0.006922920234501362
        model: {}
        policy_loss: -0.003988675773143768
        total_loss: -0.006066645495593548
        vf_explained_var: 0.004709526896476746
        vf_loss: 3.2914657592773438
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010862783528864384
        entropy: 1.739987850189209
        entropy_coeff: 0.0017600000137463212
        kl: 0.011297574266791344
        model: {}
        policy_loss: -0.007485669571906328
        total_loss: -0.01001182571053505
        vf_explained_var: 0.003490433096885681
        vf_loss: 3.2439162731170654
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010862783528864384
        entropy: 1.8632252216339111
        entropy_coeff: 0.0017600000137463212
        kl: 0.009067845530807972
        model: {}
        policy_loss: -0.007604183163493872
        total_loss: -0.010010233148932457
        vf_explained_var: 0.0010539442300796509
        vf_loss: 4.198337554931641
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.728659987449646
        entropy_coeff: 0.0017600000137463212
        kl: 0.008573781698942184
        model: {}
        policy_loss: -0.004364331252872944
        total_loss: -0.0066601587459445
        vf_explained_var: 0.000233575701713562
        vf_loss: 4.25096321105957
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.0010862783528864384
        entropy: 1.632244348526001
        entropy_coeff: 0.0017600000137463212
        kl: 0.009420250542461872
        model: {}
        policy_loss: -0.0011280924081802368
        total_loss: -0.0036283833906054497
        vf_explained_var: 0.0016506761312484741
        vf_loss: 2.2342753410339355
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010862783528864384
        entropy: 1.8758949041366577
        entropy_coeff: 0.0017600000137463212
        kl: 0.007984164170920849
        model: {}
        policy_loss: -0.0037390971556305885
        total_loss: -0.0064762188121676445
        vf_explained_var: -0.001070559024810791
        vf_loss: 2.6504597663879395
    load_time_ms: 13308.051
    num_steps_sampled: 2880000
    num_steps_trained: 2880000
    sample_time_ms: 104303.019
    update_time_ms: 16.584
  iterations_since_restore: 30
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.259893048128344
    ram_util_percent: 11.626203208556152
  pid: 28570
  policy_reward_max:
    agent-0: 40.5
    agent-1: 40.5
    agent-2: 52.5
    agent-3: 52.5
    agent-4: 30.5
    agent-5: 30.5
  policy_reward_mean:
    agent-0: 13.955
    agent-1: 13.955
    agent-2: 13.3
    agent-3: 13.3
    agent-4: 9.495
    agent-5: 9.495
  policy_reward_min:
    agent-0: -23.0
    agent-1: -23.0
    agent-2: -16.0
    agent-3: -16.0
    agent-4: -19.5
    agent-5: -19.5
  sampler_perf:
    mean_env_wait_ms: 25.681746418843854
    mean_inference_ms: 13.288418266199066
    mean_processing_ms: 59.62600289749432
  time_since_restore: 3986.9429705142975
  time_this_iter_s: 131.75576281547546
  time_total_s: 3986.9429705142975
  timestamp: 1637512515
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 2880000
  training_iteration: 30
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     30 |          3986.94 | 2880000 |     73.5 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 8.99
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 8.36
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 9.45
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 5.38
    apples_agent-3_min: 0
    apples_agent-4_max: 54
    apples_agent-4_mean: 7.07
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 8.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 237
    cleaning_beam_agent-0_mean: 191.28
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 125
    cleaning_beam_agent-1_mean: 96.46
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 223
    cleaning_beam_agent-2_mean: 184.73
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 264
    cleaning_beam_agent-3_mean: 225.15
    cleaning_beam_agent-3_min: 184
    cleaning_beam_agent-4_max: 119
    cleaning_beam_agent-4_mean: 95.02
    cleaning_beam_agent-4_min: 65
    cleaning_beam_agent-5_max: 210
    cleaning_beam_agent-5_mean: 178.3
    cleaning_beam_agent-5_min: 143
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-37-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 63.34
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 2976
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12837.593
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010802879696711898
        entropy: 1.4200962781906128
        entropy_coeff: 0.0017600000137463212
        kl: 0.011726495809853077
        model: {}
        policy_loss: -0.00529952859506011
        total_loss: -0.00711421063169837
        vf_explained_var: -0.0017327368259429932
        vf_loss: 3.9152557849884033
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010802879696711898
        entropy: 1.7268378734588623
        entropy_coeff: 0.0017600000137463212
        kl: 0.00972557719796896
        model: {}
        policy_loss: -0.00743579026311636
        total_loss: -0.009984978474676609
        vf_explained_var: 0.004902288317680359
        vf_loss: 3.0769245624542236
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010802879696711898
        entropy: 1.8522473573684692
        entropy_coeff: 0.0017600000137463212
        kl: 0.00846049189567566
        model: {}
        policy_loss: -0.006536317057907581
        total_loss: -0.009083561599254608
        vf_explained_var: -0.0002952665090560913
        vf_loss: 2.896880865097046
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.7259175777435303
        entropy_coeff: 0.0017600000137463212
        kl: 0.008287684991955757
        model: {}
        policy_loss: -0.0040615275502204895
        total_loss: -0.006494584493339062
        vf_explained_var: -0.003698602318763733
        vf_loss: 2.937683582305908
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.0010802879696711898
        entropy: 1.6354299783706665
        entropy_coeff: 0.0017600000137463212
        kl: 0.005021656863391399
        model: {}
        policy_loss: -0.000519223278388381
        total_loss: -0.003099520690739155
        vf_explained_var: 0.0009087175130844116
        vf_loss: 2.1861677169799805
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010802879696711898
        entropy: 1.8391096591949463
        entropy_coeff: 0.0017600000137463212
        kl: 0.012351431883871555
        model: {}
        policy_loss: -0.005874305497854948
        total_loss: -0.008420063182711601
        vf_explained_var: -0.012632161378860474
        vf_loss: 2.278980016708374
    load_time_ms: 13303.21
    num_steps_sampled: 2976000
    num_steps_trained: 2976000
    sample_time_ms: 104591.359
    update_time_ms: 16.483
  iterations_since_restore: 31
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.150793650793652
    ram_util_percent: 11.630687830687835
  pid: 28570
  policy_reward_max:
    agent-0: 36.0
    agent-1: 36.0
    agent-2: 40.5
    agent-3: 40.5
    agent-4: 37.5
    agent-5: 37.5
  policy_reward_mean:
    agent-0: 12.545
    agent-1: 12.545
    agent-2: 9.605
    agent-3: 9.605
    agent-4: 9.52
    agent-5: 9.52
  policy_reward_min:
    agent-0: -22.5
    agent-1: -22.5
    agent-2: -23.0
    agent-3: -23.0
    agent-4: -14.0
    agent-5: -14.0
  sampler_perf:
    mean_env_wait_ms: 25.691285563026558
    mean_inference_ms: 13.283036819145273
    mean_processing_ms: 59.63404543293384
  time_since_restore: 4119.6181790828705
  time_this_iter_s: 132.675208568573
  time_total_s: 4119.6181790828705
  timestamp: 1637512648
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 2976000
  training_iteration: 31
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     31 |          4119.62 | 2976000 |    63.34 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 7.84
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 7.77
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 7.73
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 4.22
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 5.15
    apples_agent-4_min: 0
    apples_agent-5_max: 37
    apples_agent-5_mean: 9.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 199
    cleaning_beam_agent-0_mean: 143.77
    cleaning_beam_agent-0_min: 109
    cleaning_beam_agent-1_max: 116
    cleaning_beam_agent-1_mean: 94.03
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 219
    cleaning_beam_agent-2_mean: 181.04
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 277
    cleaning_beam_agent-3_mean: 245.22
    cleaning_beam_agent-3_min: 204
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 104.44
    cleaning_beam_agent-4_min: 79
    cleaning_beam_agent-5_max: 203
    cleaning_beam_agent-5_mean: 167.93
    cleaning_beam_agent-5_min: 138
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-39-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 57.07
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 3072
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12839.708
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010742975864559412
        entropy: 1.4135034084320068
        entropy_coeff: 0.0017600000137463212
        kl: 0.008647851645946503
        model: {}
        policy_loss: -0.0056257047690451145
        total_loss: -0.007548837922513485
        vf_explained_var: 0.0005093812942504883
        vf_loss: 3.484356641769409
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010742975864559412
        entropy: 1.72172212600708
        entropy_coeff: 0.0017600000137463212
        kl: 0.01134705264121294
        model: {}
        policy_loss: -0.00808644387871027
        total_loss: -0.010609145276248455
        vf_explained_var: 0.0012266039848327637
        vf_loss: 2.9477105140686035
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010742975864559412
        entropy: 1.846366286277771
        entropy_coeff: 0.0017600000137463212
        kl: 0.009131889790296555
        model: {}
        policy_loss: -0.007191404700279236
        total_loss: -0.009905342012643814
        vf_explained_var: 0.0020227134227752686
        vf_loss: 0.7907518148422241
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.696786642074585
        entropy_coeff: 0.0017600000137463212
        kl: 0.011188079603016376
        model: {}
        policy_loss: -0.005945737008005381
        total_loss: -0.00843091867864132
        vf_explained_var: 0.0014166831970214844
        vf_loss: 0.8160840272903442
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.0010742975864559412
        entropy: 1.6755064725875854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0113521171733737
        model: {}
        policy_loss: -0.0015548283699899912
        total_loss: -0.004098673816770315
        vf_explained_var: 0.000960618257522583
        vf_loss: 2.2545180320739746
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010742975864559412
        entropy: 1.8135195970535278
        entropy_coeff: 0.0017600000137463212
        kl: 0.008964603766798973
        model: {}
        policy_loss: -0.006600227672606707
        total_loss: -0.00922643207013607
        vf_explained_var: -0.004670172929763794
        vf_loss: 2.294166088104248
    load_time_ms: 13309.613
    num_steps_sampled: 3072000
    num_steps_trained: 3072000
    sample_time_ms: 104853.569
    update_time_ms: 16.328
  iterations_since_restore: 32
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.15478723404255
    ram_util_percent: 11.637234042553192
  pid: 28570
  policy_reward_max:
    agent-0: 46.0
    agent-1: 46.0
    agent-2: 24.5
    agent-3: 24.5
    agent-4: 26.5
    agent-5: 26.5
  policy_reward_mean:
    agent-0: 11.69
    agent-1: 11.69
    agent-2: 8.665
    agent-3: 8.665
    agent-4: 8.18
    agent-5: 8.18
  policy_reward_min:
    agent-0: -35.5
    agent-1: -35.5
    agent-2: -19.5
    agent-3: -19.5
    agent-4: -22.0
    agent-5: -22.0
  sampler_perf:
    mean_env_wait_ms: 25.69049681396511
    mean_inference_ms: 13.276725345695233
    mean_processing_ms: 59.62372947054961
  time_since_restore: 4251.50946187973
  time_this_iter_s: 131.89128279685974
  time_total_s: 4251.50946187973
  timestamp: 1637512780
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 3072000
  training_iteration: 32
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     32 |          4251.51 | 3072000 |    57.07 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 6.63
    apples_agent-0_min: 0
    apples_agent-1_max: 25
    apples_agent-1_mean: 8.08
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 8.62
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.88
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 4.78
    apples_agent-4_min: 0
    apples_agent-5_max: 44
    apples_agent-5_mean: 10.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 154.43
    cleaning_beam_agent-0_min: 114
    cleaning_beam_agent-1_max: 101
    cleaning_beam_agent-1_mean: 74.11
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 216
    cleaning_beam_agent-2_mean: 169.97
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 288
    cleaning_beam_agent-3_mean: 249.08
    cleaning_beam_agent-3_min: 213
    cleaning_beam_agent-4_max: 144
    cleaning_beam_agent-4_mean: 127.67
    cleaning_beam_agent-4_min: 79
    cleaning_beam_agent-5_max: 198
    cleaning_beam_agent-5_mean: 162.54
    cleaning_beam_agent-5_min: 132
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-41-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 141.0
  episode_reward_mean: 59.31
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 3168
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12854.24
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010683072032406926
        entropy: 1.4003289937973022
        entropy_coeff: 0.0017600000137463212
        kl: 0.007747082505375147
        model: {}
        policy_loss: -0.005681667476892471
        total_loss: -0.0076962364837527275
        vf_explained_var: 0.0011772364377975464
        vf_loss: 2.5633366107940674
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010683072032406926
        entropy: 1.6369646787643433
        entropy_coeff: 0.0017600000137463212
        kl: 0.008666145615279675
        model: {}
        policy_loss: -0.007241216022521257
        total_loss: -0.009737396612763405
        vf_explained_var: -0.004088729619979858
        vf_loss: 2.223893642425537
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010683072032406926
        entropy: 1.8247913122177124
        entropy_coeff: 0.0017600000137463212
        kl: 0.011919232085347176
        model: {}
        policy_loss: -0.009508991613984108
        total_loss: -0.012014475651085377
        vf_explained_var: -0.003312617540359497
        vf_loss: 1.1018579006195068
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.6654174327850342
        entropy_coeff: 0.0017600000137463212
        kl: 0.011012090370059013
        model: {}
        policy_loss: -0.007334624417126179
        total_loss: -0.009741251356899738
        vf_explained_var: -0.012414515018463135
        vf_loss: 1.1155035495758057
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.0010683072032406926
        entropy: 1.6578174829483032
        entropy_coeff: 0.0017600000137463212
        kl: 0.01570020616054535
        model: {}
        policy_loss: -0.0010845372453331947
        total_loss: -0.003489871509373188
        vf_explained_var: 0.001171201467514038
        vf_loss: 2.6404266357421875
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010683072032406926
        entropy: 1.8011366128921509
        entropy_coeff: 0.0017600000137463212
        kl: 0.00863649696111679
        model: {}
        policy_loss: -0.006664412096142769
        total_loss: -0.009239841252565384
        vf_explained_var: -0.003751009702682495
        vf_loss: 2.7070345878601074
    load_time_ms: 13320.885
    num_steps_sampled: 3168000
    num_steps_trained: 3168000
    sample_time_ms: 104809.971
    update_time_ms: 16.236
  iterations_since_restore: 33
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.30641711229946
    ram_util_percent: 11.64973262032086
  pid: 28570
  policy_reward_max:
    agent-0: 37.0
    agent-1: 37.0
    agent-2: 23.0
    agent-3: 23.0
    agent-4: 35.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 11.995
    agent-1: 11.995
    agent-2: 7.815
    agent-3: 7.815
    agent-4: 9.845
    agent-5: 9.845
  policy_reward_min:
    agent-0: -29.5
    agent-1: -29.5
    agent-2: -21.5
    agent-3: -21.5
    agent-4: -39.5
    agent-5: -39.5
  sampler_perf:
    mean_env_wait_ms: 25.693269712003072
    mean_inference_ms: 13.272735965152417
    mean_processing_ms: 59.61070392876487
  time_since_restore: 4382.588286399841
  time_this_iter_s: 131.07882452011108
  time_total_s: 4382.588286399841
  timestamp: 1637512911
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 3168000
  training_iteration: 33
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     33 |          4382.59 | 3168000 |    59.31 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.18
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 7.9
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 10.46
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.88
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 4.89
    apples_agent-4_min: 0
    apples_agent-5_max: 36
    apples_agent-5_mean: 11.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 224
    cleaning_beam_agent-0_mean: 182.73
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 96
    cleaning_beam_agent-1_mean: 73.49
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 205
    cleaning_beam_agent-2_mean: 152.09
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 283
    cleaning_beam_agent-3_mean: 246.65
    cleaning_beam_agent-3_min: 196
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 147.59
    cleaning_beam_agent-4_min: 119
    cleaning_beam_agent-5_max: 187
    cleaning_beam_agent-5_mean: 155.54
    cleaning_beam_agent-5_min: 123
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-44-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 64.47
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 3264
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12861.558
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001062316820025444
        entropy: 1.472576379776001
        entropy_coeff: 0.0017600000137463212
        kl: 0.008197113871574402
        model: {}
        policy_loss: -0.006322791799902916
        total_loss: -0.008436605334281921
        vf_explained_var: -0.0009367614984512329
        vf_loss: 2.7299442291259766
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.001062316820025444
        entropy: 1.5998915433883667
        entropy_coeff: 0.0017600000137463212
        kl: 0.009910179302096367
        model: {}
        policy_loss: -0.007073144894093275
        total_loss: -0.009456248953938484
        vf_explained_var: 0.0011503547430038452
        vf_loss: 2.468893051147461
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001062316820025444
        entropy: 1.7881927490234375
        entropy_coeff: 0.0017600000137463212
        kl: 0.009964615106582642
        model: {}
        policy_loss: -0.009909499436616898
        total_loss: -0.012373736128211021
        vf_explained_var: -0.002305954694747925
        vf_loss: 1.847527027130127
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.636014699935913
        entropy_coeff: 0.0017600000137463212
        kl: 0.009950329549610615
        model: {}
        policy_loss: -0.008774558082222939
        total_loss: -0.011096922680735588
        vf_explained_var: -0.0006390362977981567
        vf_loss: 1.8388280868530273
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.001062316820025444
        entropy: 1.7054868936538696
        entropy_coeff: 0.0017600000137463212
        kl: 0.013810304924845695
        model: {}
        policy_loss: -0.000559271196834743
        total_loss: -0.003158705774694681
        vf_explained_var: 0.002612277865409851
        vf_loss: 1.8373807668685913
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001062316820025444
        entropy: 1.7358988523483276
        entropy_coeff: 0.0017600000137463212
        kl: 0.011270876042544842
        model: {}
        policy_loss: -0.009632613509893417
        total_loss: -0.0120797548443079
        vf_explained_var: 0.0030791163444519043
        vf_loss: 1.8538486957550049
    load_time_ms: 13338.878
    num_steps_sampled: 3264000
    num_steps_trained: 3264000
    sample_time_ms: 104792.104
    update_time_ms: 15.991
  iterations_since_restore: 34
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.373796791443848
    ram_util_percent: 11.648663101604281
  pid: 28570
  policy_reward_max:
    agent-0: 41.5
    agent-1: 41.5
    agent-2: 31.5
    agent-3: 31.5
    agent-4: 29.5
    agent-5: 29.5
  policy_reward_mean:
    agent-0: 12.755
    agent-1: 12.755
    agent-2: 9.59
    agent-3: 9.59
    agent-4: 9.89
    agent-5: 9.89
  policy_reward_min:
    agent-0: -17.5
    agent-1: -17.5
    agent-2: -22.5
    agent-3: -22.5
    agent-4: -34.0
    agent-5: -34.0
  sampler_perf:
    mean_env_wait_ms: 25.70020001993065
    mean_inference_ms: 13.270806156423367
    mean_processing_ms: 59.60310711945607
  time_since_restore: 4514.300802707672
  time_this_iter_s: 131.7125163078308
  time_total_s: 4514.300802707672
  timestamp: 1637513043
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 3264000
  training_iteration: 34
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     34 |           4514.3 | 3264000 |    64.47 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 4.8
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 8.05
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 10.31
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 4.07
    apples_agent-4_min: 0
    apples_agent-5_max: 39
    apples_agent-5_mean: 12.53
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 217
    cleaning_beam_agent-0_mean: 162.26
    cleaning_beam_agent-0_min: 114
    cleaning_beam_agent-1_max: 105
    cleaning_beam_agent-1_mean: 79.49
    cleaning_beam_agent-1_min: 49
    cleaning_beam_agent-2_max: 184
    cleaning_beam_agent-2_mean: 141.47
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 323
    cleaning_beam_agent-3_mean: 277.16
    cleaning_beam_agent-3_min: 229
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 152.49
    cleaning_beam_agent-4_min: 115
    cleaning_beam_agent-5_max: 186
    cleaning_beam_agent-5_mean: 147.35
    cleaning_beam_agent-5_min: 117
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.11
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.13
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.1
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-46-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 59.14
  episode_reward_min: -56.0
  episodes_this_iter: 96
  episodes_total: 3360
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12839.659
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010563264368101954
        entropy: 1.406262755393982
        entropy_coeff: 0.0017600000137463212
        kl: 0.008535444736480713
        model: {}
        policy_loss: -0.006410251371562481
        total_loss: -0.008457442745566368
        vf_explained_var: -0.004453986883163452
        vf_loss: 2.1444523334503174
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010563264368101954
        entropy: 1.6974985599517822
        entropy_coeff: 0.0017600000137463212
        kl: 0.01406935416162014
        model: {}
        policy_loss: -0.008916317485272884
        total_loss: -0.011433182284235954
        vf_explained_var: -0.0035269558429718018
        vf_loss: 2.0693166255950928
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010563264368101954
        entropy: 1.7527916431427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.012026628479361534
        model: {}
        policy_loss: -0.010287780314683914
        total_loss: -0.012568198144435883
        vf_explained_var: 0.0039855241775512695
        vf_loss: 2.031632900238037
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.600224494934082
        entropy_coeff: 0.0017600000137463212
        kl: 0.00971243716776371
        model: {}
        policy_loss: -0.008043298497796059
        total_loss: -0.010293683037161827
        vf_explained_var: 0.003626123070716858
        vf_loss: 2.017967462539673
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.0010563264368101954
        entropy: 1.63337242603302
        entropy_coeff: 0.0017600000137463212
        kl: 0.008579071611166
        model: {}
        policy_loss: -0.0006774482317268848
        total_loss: -0.0032282532192766666
        vf_explained_var: 0.001699402928352356
        vf_loss: 1.8820903301239014
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010563264368101954
        entropy: 1.7256391048431396
        entropy_coeff: 0.0017600000137463212
        kl: 0.01280046347528696
        model: {}
        policy_loss: -0.009815615601837635
        total_loss: -0.012186309322714806
        vf_explained_var: 0.002598091959953308
        vf_loss: 1.8641492128372192
    load_time_ms: 13330.348
    num_steps_sampled: 3360000
    num_steps_trained: 3360000
    sample_time_ms: 104988.708
    update_time_ms: 16.202
  iterations_since_restore: 35
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.181481481481477
    ram_util_percent: 11.665079365079368
  pid: 28570
  policy_reward_max:
    agent-0: 43.0
    agent-1: 43.0
    agent-2: 23.5
    agent-3: 23.5
    agent-4: 36.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 11.135
    agent-1: 11.135
    agent-2: 8.505
    agent-3: 8.505
    agent-4: 9.93
    agent-5: 9.93
  policy_reward_min:
    agent-0: -21.0
    agent-1: -21.0
    agent-2: -20.0
    agent-3: -20.0
    agent-4: -22.0
    agent-5: -22.0
  sampler_perf:
    mean_env_wait_ms: 25.707803840080164
    mean_inference_ms: 13.271702752883987
    mean_processing_ms: 59.60646418392887
  time_since_restore: 4646.648318052292
  time_this_iter_s: 132.34751534461975
  time_total_s: 4646.648318052292
  timestamp: 1637513175
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 3360000
  training_iteration: 35
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     35 |          4646.65 | 3360000 |    59.14 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 5.52
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 9.77
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 13.16
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.08
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 4.31
    apples_agent-4_min: 0
    apples_agent-5_max: 45
    apples_agent-5_mean: 15.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 273
    cleaning_beam_agent-0_mean: 177.59
    cleaning_beam_agent-0_min: 114
    cleaning_beam_agent-1_max: 108
    cleaning_beam_agent-1_mean: 81.63
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 213
    cleaning_beam_agent-2_mean: 139.94
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 387
    cleaning_beam_agent-3_mean: 324.53
    cleaning_beam_agent-3_min: 246
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 138.92
    cleaning_beam_agent-4_min: 112
    cleaning_beam_agent-5_max: 214
    cleaning_beam_agent-5_mean: 164.13
    cleaning_beam_agent-5_min: 121
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-48-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 72.57
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 3456
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12833.937
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010503360535949469
        entropy: 1.3843903541564941
        entropy_coeff: 0.0017600000137463212
        kl: 0.007723715156316757
        model: {}
        policy_loss: -0.006110536400228739
        total_loss: -0.008072293363511562
        vf_explained_var: 0.0009924471378326416
        vf_loss: 2.8168420791625977
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010503360535949469
        entropy: 1.586369276046753
        entropy_coeff: 0.0017600000137463212
        kl: 0.013074420392513275
        model: {}
        policy_loss: -0.009801564738154411
        total_loss: -0.012082982808351517
        vf_explained_var: 0.006009742617607117
        vf_loss: 2.6544456481933594
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010503360535949469
        entropy: 1.7221379280090332
        entropy_coeff: 0.0017600000137463212
        kl: 0.01019519753754139
        model: {}
        policy_loss: -0.011263961903750896
        total_loss: -0.013650571927428246
        vf_explained_var: -0.004034966230392456
        vf_loss: 1.3459181785583496
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.5651943683624268
        entropy_coeff: 0.0017600000137463212
        kl: 0.008488977327942848
        model: {}
        policy_loss: -0.008216134272515774
        total_loss: -0.01052047498524189
        vf_explained_var: 0.0007813423871994019
        vf_loss: 1.3206570148468018
      agent-4:
        cur_kl_coeff: 0.01582031324505806
        cur_lr: 0.0010503360535949469
        entropy: 1.6555752754211426
        entropy_coeff: 0.0017600000137463212
        kl: 0.003348404774442315
        model: {}
        policy_loss: -0.00023046915885061026
        total_loss: -0.0029656230472028255
        vf_explained_var: 0.001853838562965393
        vf_loss: 1.2568516731262207
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010503360535949469
        entropy: 1.7225096225738525
        entropy_coeff: 0.0017600000137463212
        kl: 0.014138130471110344
        model: {}
        policy_loss: -0.012070899829268456
        total_loss: -0.014441795647144318
        vf_explained_var: -0.01900133490562439
        vf_loss: 1.305437684059143
    load_time_ms: 13304.724
    num_steps_sampled: 3456000
    num_steps_trained: 3456000
    sample_time_ms: 105265.564
    update_time_ms: 16.427
  iterations_since_restore: 36
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.194708994708996
    ram_util_percent: 11.589947089947092
  pid: 28570
  policy_reward_max:
    agent-0: 41.5
    agent-1: 41.5
    agent-2: 29.0
    agent-3: 29.0
    agent-4: 27.5
    agent-5: 27.5
  policy_reward_mean:
    agent-0: 13.935
    agent-1: 13.935
    agent-2: 10.185
    agent-3: 10.185
    agent-4: 12.165
    agent-5: 12.165
  policy_reward_min:
    agent-0: -29.5
    agent-1: -29.5
    agent-2: -14.0
    agent-3: -14.0
    agent-4: -22.0
    agent-5: -22.0
  sampler_perf:
    mean_env_wait_ms: 25.720913111592093
    mean_inference_ms: 13.268736850783377
    mean_processing_ms: 59.60977845596641
  time_since_restore: 4779.549370288849
  time_this_iter_s: 132.901052236557
  time_total_s: 4779.549370288849
  timestamp: 1637513308
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 3456000
  training_iteration: 36
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     36 |          4779.55 | 3456000 |    72.57 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 4.68
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 9.87
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 13.89
    apples_agent-2_min: 3
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.36
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 4.27
    apples_agent-4_min: 0
    apples_agent-5_max: 52
    apples_agent-5_mean: 15.99
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 294
    cleaning_beam_agent-0_mean: 236.73
    cleaning_beam_agent-0_min: 140
    cleaning_beam_agent-1_max: 110
    cleaning_beam_agent-1_mean: 81.4
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 232
    cleaning_beam_agent-2_mean: 133.9
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 456
    cleaning_beam_agent-3_mean: 356.41
    cleaning_beam_agent-3_min: 269
    cleaning_beam_agent-4_max: 166
    cleaning_beam_agent-4_mean: 139.36
    cleaning_beam_agent-4_min: 111
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 177.17
    cleaning_beam_agent-5_min: 127
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 4
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-50-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 67.23
  episode_reward_min: -112.0
  episodes_this_iter: 96
  episodes_total: 3552
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12830.936
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010443455539643764
        entropy: 1.5544804334640503
        entropy_coeff: 0.0017600000137463212
        kl: 0.011193370446562767
        model: {}
        policy_loss: -0.006616014987230301
        total_loss: -0.008662351407110691
        vf_explained_var: 0.0016482174396514893
        vf_loss: 4.0971269607543945
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010443455539643764
        entropy: 1.545241355895996
        entropy_coeff: 0.0017600000137463212
        kl: 0.012128278613090515
        model: {}
        policy_loss: -0.008477604016661644
        total_loss: -0.010564560070633888
        vf_explained_var: 0.004328638315200806
        vf_loss: 4.052618980407715
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010443455539643764
        entropy: 1.712998390197754
        entropy_coeff: 0.0017600000137463212
        kl: 0.011167820543050766
        model: {}
        policy_loss: -0.011486104689538479
        total_loss: -0.013809235766530037
        vf_explained_var: -0.0008184760808944702
        vf_loss: 1.3335614204406738
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.5281132459640503
        entropy_coeff: 0.0017600000137463212
        kl: 0.007952691987156868
        model: {}
        policy_loss: -0.008761473000049591
        total_loss: -0.011020911857485771
        vf_explained_var: 0.002736017107963562
        vf_loss: 1.3181788921356201
      agent-4:
        cur_kl_coeff: 0.00791015662252903
        cur_lr: 0.0010443455539643764
        entropy: 1.5835041999816895
        entropy_coeff: 0.0017600000137463212
        kl: 0.020367169752717018
        model: {}
        policy_loss: -0.00047939782962203026
        total_loss: -0.0028437450528144836
        vf_explained_var: 0.002060920000076294
        vf_loss: 2.6151046752929688
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010443455539643764
        entropy: 1.6963684558868408
        entropy_coeff: 0.0017600000137463212
        kl: 0.012520389631390572
        model: {}
        policy_loss: -0.01205740962177515
        total_loss: -0.014307668432593346
        vf_explained_var: -0.0014881044626235962
        vf_loss: 2.6583449840545654
    load_time_ms: 13303.899
    num_steps_sampled: 3552000
    num_steps_trained: 3552000
    sample_time_ms: 105407.871
    update_time_ms: 16.347
  iterations_since_restore: 37
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.210695187165772
    ram_util_percent: 11.646524064171127
  pid: 28570
  policy_reward_max:
    agent-0: 38.0
    agent-1: 38.0
    agent-2: 27.5
    agent-3: 27.5
    agent-4: 39.5
    agent-5: 39.5
  policy_reward_mean:
    agent-0: 12.745
    agent-1: 12.745
    agent-2: 9.98
    agent-3: 9.98
    agent-4: 10.89
    agent-5: 10.89
  policy_reward_min:
    agent-0: -44.5
    agent-1: -44.5
    agent-2: -43.5
    agent-3: -43.5
    agent-4: -36.0
    agent-5: -36.0
  sampler_perf:
    mean_env_wait_ms: 25.731612021708198
    mean_inference_ms: 13.26240639737148
    mean_processing_ms: 59.588541400577526
  time_since_restore: 4910.559860229492
  time_this_iter_s: 131.0104899406433
  time_total_s: 4910.559860229492
  timestamp: 1637513439
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 3552000
  training_iteration: 37
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     37 |          4910.56 | 3552000 |    67.23 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 4.76
    apples_agent-0_min: 0
    apples_agent-1_max: 31
    apples_agent-1_mean: 9.23
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 13.22
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.55
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 4.3
    apples_agent-4_min: 0
    apples_agent-5_max: 41
    apples_agent-5_mean: 14.67
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 292
    cleaning_beam_agent-0_mean: 197.04
    cleaning_beam_agent-0_min: 125
    cleaning_beam_agent-1_max: 120
    cleaning_beam_agent-1_mean: 88.49
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 189
    cleaning_beam_agent-2_mean: 127.31
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 510
    cleaning_beam_agent-3_mean: 378.54
    cleaning_beam_agent-3_min: 268
    cleaning_beam_agent-4_max: 149
    cleaning_beam_agent-4_mean: 118.94
    cleaning_beam_agent-4_min: 91
    cleaning_beam_agent-5_max: 228
    cleaning_beam_agent-5_mean: 172.67
    cleaning_beam_agent-5_min: 116
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-52-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 211.0
  episode_reward_mean: 66.37
  episode_reward_min: -57.0
  episodes_this_iter: 96
  episodes_total: 3648
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12835.746
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010383551707491279
        entropy: 1.4730788469314575
        entropy_coeff: 0.0017600000137463212
        kl: 0.010288018733263016
        model: {}
        policy_loss: -0.00868840329349041
        total_loss: -0.010787365026772022
        vf_explained_var: -0.0013710260391235352
        vf_loss: 2.364553451538086
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010383551707491279
        entropy: 1.487898826599121
        entropy_coeff: 0.0017600000137463212
        kl: 0.009669012390077114
        model: {}
        policy_loss: -0.008811508305370808
        total_loss: -0.011023908853530884
        vf_explained_var: 0.005495339632034302
        vf_loss: 2.250070095062256
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010383551707491279
        entropy: 1.6986050605773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.011032076552510262
        model: {}
        policy_loss: -0.011584056541323662
        total_loss: -0.013836974278092384
        vf_explained_var: 0.0016116499900817871
        vf_loss: 1.8502177000045776
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.529426097869873
        entropy_coeff: 0.0017600000137463212
        kl: 0.01030398067086935
        model: {}
        policy_loss: -0.009033652022480965
        total_loss: -0.011155636049807072
        vf_explained_var: 0.00457841157913208
        vf_loss: 1.8340766429901123
      agent-4:
        cur_kl_coeff: 0.01186523400247097
        cur_lr: 0.0010383551707491279
        entropy: 1.5803501605987549
        entropy_coeff: 0.0017600000137463212
        kl: 0.00871307123452425
        model: {}
        policy_loss: -0.00047091254964470863
        total_loss: -0.002927222056314349
        vf_explained_var: 0.0004201531410217285
        vf_loss: 2.2172152996063232
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010383551707491279
        entropy: 1.6842174530029297
        entropy_coeff: 0.0017600000137463212
        kl: 0.011752160266041756
        model: {}
        policy_loss: -0.01242869719862938
        total_loss: -0.014729893766343594
        vf_explained_var: 0.004465639591217041
        vf_loss: 2.2231621742248535
    load_time_ms: 13297.019
    num_steps_sampled: 3648000
    num_steps_trained: 3648000
    sample_time_ms: 105460.31
    update_time_ms: 16.295
  iterations_since_restore: 38
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.316129032258065
    ram_util_percent: 11.660752688172046
  pid: 28570
  policy_reward_max:
    agent-0: 47.0
    agent-1: 47.0
    agent-2: 43.0
    agent-3: 43.0
    agent-4: 33.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 13.385
    agent-1: 13.385
    agent-2: 8.135
    agent-3: 8.135
    agent-4: 11.665
    agent-5: 11.665
  policy_reward_min:
    agent-0: -36.0
    agent-1: -36.0
    agent-2: -40.5
    agent-3: -40.5
    agent-4: -19.5
    agent-5: -19.5
  sampler_perf:
    mean_env_wait_ms: 25.740497057201193
    mean_inference_ms: 13.256963172153359
    mean_processing_ms: 59.57759207991584
  time_since_restore: 5041.310636043549
  time_this_iter_s: 130.7507758140564
  time_total_s: 5041.310636043549
  timestamp: 1637513570
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 3648000
  training_iteration: 38
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     38 |          5041.31 | 3648000 |    66.37 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 4.59
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 9.3
    apples_agent-1_min: 1
    apples_agent-2_max: 52
    apples_agent-2_mean: 15.93
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.66
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 4.69
    apples_agent-4_min: 0
    apples_agent-5_max: 72
    apples_agent-5_mean: 18.88
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 291
    cleaning_beam_agent-0_mean: 208.75
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 111
    cleaning_beam_agent-1_mean: 85.98
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 236
    cleaning_beam_agent-2_mean: 137.64
    cleaning_beam_agent-2_min: 75
    cleaning_beam_agent-3_max: 482
    cleaning_beam_agent-3_mean: 371.83
    cleaning_beam_agent-3_min: 239
    cleaning_beam_agent-4_max: 164
    cleaning_beam_agent-4_mean: 136.3
    cleaning_beam_agent-4_min: 98
    cleaning_beam_agent-5_max: 228
    cleaning_beam_agent-5_mean: 174.82
    cleaning_beam_agent-5_min: 122
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-55-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 216.0
  episode_reward_mean: 75.5
  episode_reward_min: -66.0
  episodes_this_iter: 96
  episodes_total: 3744
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12854.503
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010323647875338793
        entropy: 1.4740232229232788
        entropy_coeff: 0.0017600000137463212
        kl: 0.011218976229429245
        model: {}
        policy_loss: -0.010008996352553368
        total_loss: -0.011973672546446323
        vf_explained_var: 0.0007663965225219727
        vf_loss: 3.4912941455841064
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010323647875338793
        entropy: 1.4425687789916992
        entropy_coeff: 0.0017600000137463212
        kl: 0.012205852195620537
        model: {}
        policy_loss: -0.010395171120762825
        total_loss: -0.012366503477096558
        vf_explained_var: 0.0051750242710113525
        vf_loss: 3.3873090744018555
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010323647875338793
        entropy: 1.6808438301086426
        entropy_coeff: 0.0017600000137463212
        kl: 0.013397719711065292
        model: {}
        policy_loss: -0.011820679530501366
        total_loss: -0.013871602714061737
        vf_explained_var: -0.0052605122327804565
        vf_loss: 2.374748945236206
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.5233064889907837
        entropy_coeff: 0.0017600000137463212
        kl: 0.011026186868548393
        model: {}
        policy_loss: -0.009740783832967281
        total_loss: -0.011773125268518925
        vf_explained_var: 0.0009364336729049683
        vf_loss: 2.3519766330718994
      agent-4:
        cur_kl_coeff: 0.01186523400247097
        cur_lr: 0.0010323647875338793
        entropy: 1.67336106300354
        entropy_coeff: 0.0017600000137463212
        kl: 0.013227900490164757
        model: {}
        policy_loss: -0.0012125270441174507
        total_loss: -0.003764767199754715
        vf_explained_var: 0.0025531798601150513
        vf_loss: 2.3591816425323486
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010323647875338793
        entropy: 1.662360429763794
        entropy_coeff: 0.0017600000137463212
        kl: 0.013645390048623085
        model: {}
        policy_loss: -0.013588473200798035
        total_loss: -0.01576097682118416
        vf_explained_var: -0.0001071244478225708
        vf_loss: 2.415482997894287
    load_time_ms: 13291.518
    num_steps_sampled: 3744000
    num_steps_trained: 3744000
    sample_time_ms: 105559.72
    update_time_ms: 16.475
  iterations_since_restore: 39
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.245212765957444
    ram_util_percent: 11.601063829787234
  pid: 28570
  policy_reward_max:
    agent-0: 42.5
    agent-1: 42.5
    agent-2: 46.0
    agent-3: 46.0
    agent-4: 38.0
    agent-5: 38.0
  policy_reward_mean:
    agent-0: 14.8
    agent-1: 14.8
    agent-2: 10.51
    agent-3: 10.51
    agent-4: 12.44
    agent-5: 12.44
  policy_reward_min:
    agent-0: -30.5
    agent-1: -30.5
    agent-2: -15.0
    agent-3: -15.0
    agent-4: -17.5
    agent-5: -17.5
  sampler_perf:
    mean_env_wait_ms: 25.75369846369564
    mean_inference_ms: 13.252639237521937
    mean_processing_ms: 59.5622412747165
  time_since_restore: 5173.245988845825
  time_this_iter_s: 131.9353528022766
  time_total_s: 5173.245988845825
  timestamp: 1637513702
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 3744000
  training_iteration: 39
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     39 |          5173.25 | 3744000 |     75.5 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 5.31
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 10.34
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 17.95
    apples_agent-2_min: 3
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.32
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 5.19
    apples_agent-4_min: 0
    apples_agent-5_max: 56
    apples_agent-5_mean: 22.68
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 287
    cleaning_beam_agent-0_mean: 216.9
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 126
    cleaning_beam_agent-1_mean: 92.6
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 182
    cleaning_beam_agent-2_mean: 112.32
    cleaning_beam_agent-2_min: 61
    cleaning_beam_agent-3_max: 492
    cleaning_beam_agent-3_mean: 368.4
    cleaning_beam_agent-3_min: 264
    cleaning_beam_agent-4_max: 213
    cleaning_beam_agent-4_mean: 185.18
    cleaning_beam_agent-4_min: 113
    cleaning_beam_agent-5_max: 252
    cleaning_beam_agent-5_mean: 177.59
    cleaning_beam_agent-5_min: 117
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-57-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 216.0
  episode_reward_mean: 88.62
  episode_reward_min: -50.0
  episodes_this_iter: 96
  episodes_total: 3840
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12855.437
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010263744043186307
        entropy: 1.4434691667556763
        entropy_coeff: 0.0017600000137463212
        kl: 0.010397806763648987
        model: {}
        policy_loss: -0.007874292321503162
        total_loss: -0.009776733815670013
        vf_explained_var: 0.002485930919647217
        vf_loss: 3.7811999320983887
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010263744043186307
        entropy: 1.5538158416748047
        entropy_coeff: 0.0017600000137463212
        kl: 0.009732258506119251
        model: {}
        policy_loss: -0.009914406575262547
        total_loss: -0.012096809223294258
        vf_explained_var: 0.004396006464958191
        vf_loss: 3.6983418464660645
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010263744043186307
        entropy: 1.6743967533111572
        entropy_coeff: 0.0017600000137463212
        kl: 0.012970187701284885
        model: {}
        policy_loss: -0.013961014337837696
        total_loss: -0.016051117330789566
        vf_explained_var: -0.022436082363128662
        vf_loss: 2.083282470703125
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.5398422479629517
        entropy_coeff: 0.0017600000137463212
        kl: 0.01067378930747509
        model: {}
        policy_loss: -0.010005583986639977
        total_loss: -0.012134191580116749
        vf_explained_var: 0.007694676518440247
        vf_loss: 1.8124827146530151
      agent-4:
        cur_kl_coeff: 0.01186523400247097
        cur_lr: 0.0010263744043186307
        entropy: 1.5549023151397705
        entropy_coeff: 0.0017600000137463212
        kl: 0.006198287010192871
        model: {}
        policy_loss: -6.692251190543175e-05
        total_loss: -0.002438765484839678
        vf_explained_var: 0.0015522092580795288
        vf_loss: 2.912404775619507
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010263744043186307
        entropy: 1.6383347511291504
        entropy_coeff: 0.0017600000137463212
        kl: 0.013811581768095493
        model: {}
        policy_loss: -0.013753797858953476
        total_loss: -0.015818674117326736
        vf_explained_var: -0.002437874674797058
        vf_loss: 3.0065441131591797
    load_time_ms: 13293.988
    num_steps_sampled: 3840000
    num_steps_trained: 3840000
    sample_time_ms: 105587.169
    update_time_ms: 16.498
  iterations_since_restore: 40
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.17978723404255
    ram_util_percent: 11.662234042553195
  pid: 28570
  policy_reward_max:
    agent-0: 44.5
    agent-1: 44.5
    agent-2: 30.0
    agent-3: 30.0
    agent-4: 44.5
    agent-5: 44.5
  policy_reward_mean:
    agent-0: 15.645
    agent-1: 15.645
    agent-2: 12.815
    agent-3: 12.815
    agent-4: 15.85
    agent-5: 15.85
  policy_reward_min:
    agent-0: -18.0
    agent-1: -18.0
    agent-2: -15.0
    agent-3: -15.0
    agent-4: -22.5
    agent-5: -22.5
  sampler_perf:
    mean_env_wait_ms: 25.770050435301318
    mean_inference_ms: 13.24881677291766
    mean_processing_ms: 59.542520074886596
  time_since_restore: 5305.2791538238525
  time_this_iter_s: 132.03316497802734
  time_total_s: 5305.2791538238525
  timestamp: 1637513834
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 3840000
  training_iteration: 40
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     40 |          5305.28 | 3840000 |    88.62 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 5.33
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 9.97
    apples_agent-1_min: 1
    apples_agent-2_max: 59
    apples_agent-2_mean: 21.64
    apples_agent-2_min: 3
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.28
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 5.6
    apples_agent-4_min: 0
    apples_agent-5_max: 69
    apples_agent-5_mean: 26.75
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 311
    cleaning_beam_agent-0_mean: 214.48
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 113
    cleaning_beam_agent-1_mean: 86.73
    cleaning_beam_agent-1_min: 52
    cleaning_beam_agent-2_max: 193
    cleaning_beam_agent-2_mean: 117.14
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 473
    cleaning_beam_agent-3_mean: 314.58
    cleaning_beam_agent-3_min: 158
    cleaning_beam_agent-4_max: 226
    cleaning_beam_agent-4_mean: 201.03
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 254
    cleaning_beam_agent-5_mean: 195.15
    cleaning_beam_agent-5_min: 135
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.11
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_11-59-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 238.0
  episode_reward_mean: 98.51
  episode_reward_min: -114.0
  episodes_this_iter: 96
  episodes_total: 3936
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12855.451
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010203840211033821
        entropy: 1.4033164978027344
        entropy_coeff: 0.0017600000137463212
        kl: 0.00975917000323534
        model: {}
        policy_loss: -0.010443044826388359
        total_loss: -0.01219020877033472
        vf_explained_var: -0.01032257080078125
        vf_loss: 4.78691291809082
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010203840211033821
        entropy: 1.5732157230377197
        entropy_coeff: 0.0017600000137463212
        kl: 0.010383631102740765
        model: {}
        policy_loss: -0.01063552312552929
        total_loss: -0.012747777625918388
        vf_explained_var: 0.003995820879936218
        vf_loss: 4.619129180908203
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010203840211033821
        entropy: 1.659024953842163
        entropy_coeff: 0.0017600000137463212
        kl: 0.01244475319981575
        model: {}
        policy_loss: -0.01414916105568409
        total_loss: -0.016175447031855583
        vf_explained_var: -0.009784102439880371
        vf_loss: 2.713573455810547
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.509718418121338
        entropy_coeff: 0.0017600000137463212
        kl: 0.00908756349235773
        model: {}
        policy_loss: -0.010137174278497696
        total_loss: -0.012206640094518661
        vf_explained_var: 0.009877845644950867
        vf_loss: 2.46854305267334
      agent-4:
        cur_kl_coeff: 0.01186523400247097
        cur_lr: 0.0010203840211033821
        entropy: 1.702575922012329
        entropy_coeff: 0.0017600000137463212
        kl: 0.023401878774166107
        model: {}
        policy_loss: -0.0008381740190088749
        total_loss: -0.0031403470784425735
        vf_explained_var: 0.0014100074768066406
        vf_loss: 4.166936874389648
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010203840211033821
        entropy: 1.5960643291473389
        entropy_coeff: 0.0017600000137463212
        kl: 0.012404859997332096
        model: {}
        policy_loss: -0.014900044538080692
        total_loss: -0.01682022586464882
        vf_explained_var: 0.0015272349119186401
        vf_loss: 4.237090110778809
    load_time_ms: 13295.004
    num_steps_sampled: 3936000
    num_steps_trained: 3936000
    sample_time_ms: 105431.325
    update_time_ms: 16.455
  iterations_since_restore: 41
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.248663101604283
    ram_util_percent: 11.61818181818182
  pid: 28570
  policy_reward_max:
    agent-0: 47.5
    agent-1: 47.5
    agent-2: 37.0
    agent-3: 37.0
    agent-4: 46.0
    agent-5: 46.0
  policy_reward_mean:
    agent-0: 16.515
    agent-1: 16.515
    agent-2: 16.03
    agent-3: 16.03
    agent-4: 16.71
    agent-5: 16.71
  policy_reward_min:
    agent-0: -21.5
    agent-1: -21.5
    agent-2: -19.0
    agent-3: -19.0
    agent-4: -20.0
    agent-5: -20.0
  sampler_perf:
    mean_env_wait_ms: 25.786462136898752
    mean_inference_ms: 13.246185174121008
    mean_processing_ms: 59.52952604653642
  time_since_restore: 5436.395579814911
  time_this_iter_s: 131.11642599105835
  time_total_s: 5436.395579814911
  timestamp: 1637513966
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 3936000
  training_iteration: 41
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     41 |           5436.4 | 3936000 |    98.51 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 4.2
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 10.08
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 21.07
    apples_agent-2_min: 3
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.76
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 5.68
    apples_agent-4_min: 0
    apples_agent-5_max: 96
    apples_agent-5_mean: 23.99
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 353
    cleaning_beam_agent-0_mean: 218.87
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 114
    cleaning_beam_agent-1_mean: 79.98
    cleaning_beam_agent-1_min: 58
    cleaning_beam_agent-2_max: 203
    cleaning_beam_agent-2_mean: 112.0
    cleaning_beam_agent-2_min: 59
    cleaning_beam_agent-3_max: 475
    cleaning_beam_agent-3_mean: 333.27
    cleaning_beam_agent-3_min: 229
    cleaning_beam_agent-4_max: 221
    cleaning_beam_agent-4_mean: 153.19
    cleaning_beam_agent-4_min: 126
    cleaning_beam_agent-5_max: 262
    cleaning_beam_agent-5_mean: 205.8
    cleaning_beam_agent-5_min: 152
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.14
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.15
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-01-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 247.0
  episode_reward_mean: 92.31
  episode_reward_min: -78.0
  episodes_this_iter: 96
  episodes_total: 4032
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12870.035
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010143936378881335
        entropy: 1.3747342824935913
        entropy_coeff: 0.0017600000137463212
        kl: 0.012508337385952473
        model: {}
        policy_loss: -0.011596609838306904
        total_loss: -0.013334276154637337
        vf_explained_var: -0.010659217834472656
        vf_loss: 3.691601276397705
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010143936378881335
        entropy: 1.4887515306472778
        entropy_coeff: 0.0017600000137463212
        kl: 0.012293582782149315
        model: {}
        policy_loss: -0.010702727362513542
        total_loss: -0.012739798985421658
        vf_explained_var: 0.007869288325309753
        vf_loss: 3.526233196258545
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010143936378881335
        entropy: 1.6217660903930664
        entropy_coeff: 0.0017600000137463212
        kl: 0.012107916176319122
        model: {}
        policy_loss: -0.013244005851447582
        total_loss: -0.015260500833392143
        vf_explained_var: -0.027037382125854492
        vf_loss: 2.3241868019104004
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.4883931875228882
        entropy_coeff: 0.0017600000137463212
        kl: 0.010353230871260166
        model: {}
        policy_loss: -0.011081205680966377
        total_loss: -0.013097965158522129
        vf_explained_var: 0.010700330138206482
        vf_loss: 2.1456680297851562
      agent-4:
        cur_kl_coeff: 0.01779785193502903
        cur_lr: 0.0010143936378881335
        entropy: 1.5413119792938232
        entropy_coeff: 0.0017600000137463212
        kl: 0.006760448683053255
        model: {}
        policy_loss: -0.00136203458532691
        total_loss: -0.003469355870038271
        vf_explained_var: 0.0017437934875488281
        vf_loss: 4.85067081451416
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010143936378881335
        entropy: 1.586545705795288
        entropy_coeff: 0.0017600000137463212
        kl: 0.015992091968655586
        model: {}
        policy_loss: -0.011292927898466587
        total_loss: -0.013003004714846611
        vf_explained_var: 0.011448949575424194
        vf_loss: 4.825394153594971
    load_time_ms: 13308.391
    num_steps_sampled: 4032000
    num_steps_trained: 4032000
    sample_time_ms: 105380.197
    update_time_ms: 16.886
  iterations_since_restore: 42
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.428877005347598
    ram_util_percent: 11.667379679144384
  pid: 28570
  policy_reward_max:
    agent-0: 57.0
    agent-1: 57.0
    agent-2: 38.5
    agent-3: 38.5
    agent-4: 63.5
    agent-5: 63.5
  policy_reward_mean:
    agent-0: 17.515
    agent-1: 17.515
    agent-2: 13.585
    agent-3: 13.585
    agent-4: 15.055
    agent-5: 15.055
  policy_reward_min:
    agent-0: -14.5
    agent-1: -14.5
    agent-2: -45.0
    agent-3: -45.0
    agent-4: -41.0
    agent-5: -41.0
  sampler_perf:
    mean_env_wait_ms: 25.798252255416266
    mean_inference_ms: 13.242674935698233
    mean_processing_ms: 59.51609222776577
  time_since_restore: 5568.124413967133
  time_this_iter_s: 131.72883415222168
  time_total_s: 5568.124413967133
  timestamp: 1637514097
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 4032000
  training_iteration: 42
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     42 |          5568.12 | 4032000 |    92.31 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 4.67
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 10.4
    apples_agent-1_min: 2
    apples_agent-2_max: 64
    apples_agent-2_mean: 23.76
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.65
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 5.69
    apples_agent-4_min: 0
    apples_agent-5_max: 87
    apples_agent-5_mean: 27.9
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 300
    cleaning_beam_agent-0_mean: 228.59
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 104
    cleaning_beam_agent-1_mean: 73.1
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 186
    cleaning_beam_agent-2_mean: 105.55
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 429
    cleaning_beam_agent-3_mean: 303.03
    cleaning_beam_agent-3_min: 193
    cleaning_beam_agent-4_max: 202
    cleaning_beam_agent-4_mean: 171.66
    cleaning_beam_agent-4_min: 130
    cleaning_beam_agent-5_max: 255
    cleaning_beam_agent-5_mean: 180.32
    cleaning_beam_agent-5_min: 114
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-03-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 236.0
  episode_reward_mean: 108.12
  episode_reward_min: -111.0
  episodes_this_iter: 96
  episodes_total: 4128
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12852.974
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.001008403254672885
        entropy: 1.4004054069519043
        entropy_coeff: 0.0017600000137463212
        kl: 0.013432209379971027
        model: {}
        policy_loss: -0.012916351668536663
        total_loss: -0.014659849926829338
        vf_explained_var: -0.004011720418930054
        vf_loss: 3.8541083335876465
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.001008403254672885
        entropy: 1.436198353767395
        entropy_coeff: 0.0017600000137463212
        kl: 0.012884391471743584
        model: {}
        policy_loss: -0.011408375576138496
        total_loss: -0.013330301269888878
        vf_explained_var: 0.01375339925289154
        vf_loss: 3.6420226097106934
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.001008403254672885
        entropy: 1.5967868566513062
        entropy_coeff: 0.0017600000137463212
        kl: 0.01299360767006874
        model: {}
        policy_loss: -0.015138091519474983
        total_loss: -0.0169898122549057
        vf_explained_var: -0.009848922491073608
        vf_loss: 3.0894150733947754
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.4810259342193604
        entropy_coeff: 0.0017600000137463212
        kl: 0.010808423161506653
        model: {}
        policy_loss: -0.010414741933345795
        total_loss: -0.012309586629271507
        vf_explained_var: 0.011244967579841614
        vf_loss: 3.0644631385803223
      agent-4:
        cur_kl_coeff: 0.01779785193502903
        cur_lr: 0.001008403254672885
        entropy: 1.6514997482299805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027823359705507755
        model: {}
        policy_loss: -0.0004251343198120594
        total_loss: -0.002912668976932764
        vf_explained_var: 0.001532822847366333
        vf_loss: 3.695863723754883
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.001008403254672885
        entropy: 1.540431022644043
        entropy_coeff: 0.0017600000137463212
        kl: 0.01152134407311678
        model: {}
        policy_loss: -0.013866055756807327
        total_loss: -0.015779519453644753
        vf_explained_var: 0.007978856563568115
        vf_loss: 3.656463146209717
    load_time_ms: 13281.108
    num_steps_sampled: 4128000
    num_steps_trained: 4128000
    sample_time_ms: 105496.516
    update_time_ms: 17.123
  iterations_since_restore: 43
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.79255319148936
    ram_util_percent: 11.626063829787238
  pid: 28570
  policy_reward_max:
    agent-0: 42.0
    agent-1: 42.0
    agent-2: 37.5
    agent-3: 37.5
    agent-4: 56.0
    agent-5: 56.0
  policy_reward_mean:
    agent-0: 18.915
    agent-1: 18.915
    agent-2: 16.52
    agent-3: 16.52
    agent-4: 18.625
    agent-5: 18.625
  policy_reward_min:
    agent-0: -40.5
    agent-1: -40.5
    agent-2: -19.5
    agent-3: -19.5
    agent-4: -18.5
    agent-5: -18.5
  sampler_perf:
    mean_env_wait_ms: 25.80735828172905
    mean_inference_ms: 13.238701140863645
    mean_processing_ms: 59.50789984404826
  time_since_restore: 5699.876662969589
  time_this_iter_s: 131.75224900245667
  time_total_s: 5699.876662969589
  timestamp: 1637514229
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 4128000
  training_iteration: 43
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     43 |          5699.88 | 4128000 |   108.12 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 4.3
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 10.95
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 26.88
    apples_agent-2_min: 8
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.48
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 6.55
    apples_agent-4_min: 0
    apples_agent-5_max: 102
    apples_agent-5_mean: 30.52
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 288
    cleaning_beam_agent-0_mean: 206.32
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 109
    cleaning_beam_agent-1_mean: 83.87
    cleaning_beam_agent-1_min: 58
    cleaning_beam_agent-2_max: 171
    cleaning_beam_agent-2_mean: 109.58
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 461
    cleaning_beam_agent-3_mean: 309.56
    cleaning_beam_agent-3_min: 171
    cleaning_beam_agent-4_max: 202
    cleaning_beam_agent-4_mean: 166.03
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 184.77
    cleaning_beam_agent-5_min: 139
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.12
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.14
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-06-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 297.0
  episode_reward_mean: 116.57
  episode_reward_min: -57.0
  episodes_this_iter: 96
  episodes_total: 4224
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12845.208
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0010024127550423145
        entropy: 1.3325542211532593
        entropy_coeff: 0.0017600000137463212
        kl: 0.012668841518461704
        model: {}
        policy_loss: -0.013204356655478477
        total_loss: -0.014728671871125698
        vf_explained_var: 0.0010897666215896606
        vf_loss: 5.042572975158691
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0010024127550423145
        entropy: 1.4420462846755981
        entropy_coeff: 0.0017600000137463212
        kl: 0.013188166543841362
        model: {}
        policy_loss: -0.011443503201007843
        total_loss: -0.013247720897197723
        vf_explained_var: 0.018554508686065674
        vf_loss: 4.865066051483154
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0010024127550423145
        entropy: 1.58704674243927
        entropy_coeff: 0.0017600000137463212
        kl: 0.012194446288049221
        model: {}
        policy_loss: -0.015491010621190071
        total_loss: -0.017379919067025185
        vf_explained_var: -0.001659497618675232
        vf_loss: 2.945709466934204
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.463836669921875
        entropy_coeff: 0.0017600000137463212
        kl: 0.012262952513992786
        model: {}
        policy_loss: -0.01147518865764141
        total_loss: -0.013297038152813911
        vf_explained_var: 0.016903355717658997
        vf_loss: 2.946439743041992
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0010024127550423145
        entropy: 1.5938503742218018
        entropy_coeff: 0.0017600000137463212
        kl: 0.01748143881559372
        model: {}
        policy_loss: -0.00034691253677010536
        total_loss: -0.002500495407730341
        vf_explained_var: 0.0017681270837783813
        vf_loss: 4.960280418395996
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0010024127550423145
        entropy: 1.4970123767852783
        entropy_coeff: 0.0017600000137463212
        kl: 0.012058243155479431
        model: {}
        policy_loss: -0.014066006056964397
        total_loss: -0.015755534172058105
        vf_explained_var: 0.0007296651601791382
        vf_loss: 4.930310249328613
    load_time_ms: 13274.097
    num_steps_sampled: 4224000
    num_steps_trained: 4224000
    sample_time_ms: 105448.283
    update_time_ms: 17.141
  iterations_since_restore: 44
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.266844919786095
    ram_util_percent: 11.54331550802139
  pid: 28570
  policy_reward_max:
    agent-0: 53.5
    agent-1: 53.5
    agent-2: 41.0
    agent-3: 41.0
    agent-4: 71.5
    agent-5: 71.5
  policy_reward_mean:
    agent-0: 20.82
    agent-1: 20.82
    agent-2: 18.8
    agent-3: 18.8
    agent-4: 18.665
    agent-5: 18.665
  policy_reward_min:
    agent-0: -39.5
    agent-1: -39.5
    agent-2: -9.5
    agent-3: -9.5
    agent-4: -42.0
    agent-5: -42.0
  sampler_perf:
    mean_env_wait_ms: 25.818043393207244
    mean_inference_ms: 13.235744053452022
    mean_processing_ms: 59.503348867678326
  time_since_restore: 5830.949826478958
  time_this_iter_s: 131.0731635093689
  time_total_s: 5830.949826478958
  timestamp: 1637514360
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 4224000
  training_iteration: 44
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     44 |          5830.95 | 4224000 |   116.57 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 4.8
    apples_agent-0_min: 0
    apples_agent-1_max: 42
    apples_agent-1_mean: 12.19
    apples_agent-1_min: 2
    apples_agent-2_max: 97
    apples_agent-2_mean: 32.11
    apples_agent-2_min: 3
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.82
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 6.91
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 36.94
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 268
    cleaning_beam_agent-0_mean: 212.45
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 122
    cleaning_beam_agent-1_mean: 91.43
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 162
    cleaning_beam_agent-2_mean: 119.08
    cleaning_beam_agent-2_min: 69
    cleaning_beam_agent-3_max: 486
    cleaning_beam_agent-3_mean: 316.76
    cleaning_beam_agent-3_min: 199
    cleaning_beam_agent-4_max: 210
    cleaning_beam_agent-4_mean: 184.49
    cleaning_beam_agent-4_min: 143
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 152.88
    cleaning_beam_agent-5_min: 103
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.18
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.16
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-08-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 443.0
  episode_reward_mean: 140.04
  episode_reward_min: -4.0
  episodes_this_iter: 96
  episodes_total: 4320
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12847.751
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000996422371827066
        entropy: 1.3664644956588745
        entropy_coeff: 0.0017600000137463212
        kl: 0.014027735218405724
        model: {}
        policy_loss: -0.012836336158216
        total_loss: -0.014273060485720634
        vf_explained_var: -0.0029118359088897705
        vf_loss: 6.175602912902832
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000996422371827066
        entropy: 1.4908775091171265
        entropy_coeff: 0.0017600000137463212
        kl: 0.011891582980751991
        model: {}
        policy_loss: -0.011189818382263184
        total_loss: -0.012998719699680805
        vf_explained_var: 0.018627822399139404
        vf_loss: 5.920732498168945
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000996422371827066
        entropy: 1.547682762145996
        entropy_coeff: 0.0017600000137463212
        kl: 0.01285675447434187
        model: {}
        policy_loss: -0.015826180577278137
        total_loss: -0.017452610656619072
        vf_explained_var: -0.019397974014282227
        vf_loss: 4.546570777893066
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.4551570415496826
        entropy_coeff: 0.0017600000137463212
        kl: 0.012331470847129822
        model: {}
        policy_loss: -0.013350581750273705
        total_loss: -0.015008077025413513
        vf_explained_var: 0.00747014582157135
        vf_loss: 4.411502838134766
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000996422371827066
        entropy: 1.5088591575622559
        entropy_coeff: 0.0017600000137463212
        kl: 0.014335373416543007
        model: {}
        policy_loss: -0.0017598490230739117
        total_loss: -0.0036868094466626644
        vf_explained_var: 0.0008872002363204956
        vf_loss: 6.01060676574707
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000996422371827066
        entropy: 1.4056466817855835
        entropy_coeff: 0.0017600000137463212
        kl: 0.012891355901956558
        model: {}
        policy_loss: -0.014354766346514225
        total_loss: -0.015749948099255562
        vf_explained_var: -0.0006778687238693237
        vf_loss: 5.953286170959473
    load_time_ms: 13276.514
    num_steps_sampled: 4320000
    num_steps_trained: 4320000
    sample_time_ms: 105298.322
    update_time_ms: 16.937
  iterations_since_restore: 45
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.37043010752688
    ram_util_percent: 11.655376344086024
  pid: 28570
  policy_reward_max:
    agent-0: 83.5
    agent-1: 83.5
    agent-2: 56.5
    agent-3: 56.5
    agent-4: 81.5
    agent-5: 81.5
  policy_reward_mean:
    agent-0: 24.945
    agent-1: 24.945
    agent-2: 20.68
    agent-3: 20.68
    agent-4: 24.395
    agent-5: 24.395
  policy_reward_min:
    agent-0: -12.0
    agent-1: -12.0
    agent-2: -11.5
    agent-3: -11.5
    agent-4: -16.5
    agent-5: -16.5
  sampler_perf:
    mean_env_wait_ms: 25.825848763136133
    mean_inference_ms: 13.232389969275328
    mean_processing_ms: 59.48964791817398
  time_since_restore: 5961.879728555679
  time_this_iter_s: 130.9299020767212
  time_total_s: 5961.879728555679
  timestamp: 1637514492
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 4320000
  training_iteration: 45
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     45 |          5961.88 | 4320000 |   140.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 3.91
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 11.16
    apples_agent-1_min: 2
    apples_agent-2_max: 81
    apples_agent-2_mean: 33.39
    apples_agent-2_min: 3
    apples_agent-3_max: 17
    apples_agent-3_mean: 4.76
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 7.0
    apples_agent-4_min: 0
    apples_agent-5_max: 95
    apples_agent-5_mean: 37.28
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 327
    cleaning_beam_agent-0_mean: 233.84
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 114
    cleaning_beam_agent-1_mean: 75.44
    cleaning_beam_agent-1_min: 51
    cleaning_beam_agent-2_max: 159
    cleaning_beam_agent-2_mean: 91.53
    cleaning_beam_agent-2_min: 38
    cleaning_beam_agent-3_max: 444
    cleaning_beam_agent-3_mean: 251.44
    cleaning_beam_agent-3_min: 149
    cleaning_beam_agent-4_max: 210
    cleaning_beam_agent-4_mean: 171.19
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 174
    cleaning_beam_agent-5_mean: 123.34
    cleaning_beam_agent-5_min: 92
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.14
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.1
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.11
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-10-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 320.0
  episode_reward_mean: 144.31
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 4416
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12861.158
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009904319886118174
        entropy: 1.393005132675171
        entropy_coeff: 0.0017600000137463212
        kl: 0.012324774637818336
        model: {}
        policy_loss: -0.012870430946350098
        total_loss: -0.01457889936864376
        vf_explained_var: -0.013916939496994019
        vf_loss: 4.351012229919434
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009904319886118174
        entropy: 1.459651231765747
        entropy_coeff: 0.0017600000137463212
        kl: 0.011351308785378933
        model: {}
        policy_loss: -0.011469393968582153
        total_loss: -0.013412670232355595
        vf_explained_var: 0.026617348194122314
        vf_loss: 4.12869930267334
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009904319886118174
        entropy: 1.5027364492416382
        entropy_coeff: 0.0017600000137463212
        kl: 0.011436747387051582
        model: {}
        policy_loss: -0.016078881919384003
        total_loss: -0.01779188960790634
        vf_explained_var: 0.00018030405044555664
        vf_loss: 3.599748373031616
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.4499921798706055
        entropy_coeff: 0.0017600000137463212
        kl: 0.011807221919298172
        model: {}
        policy_loss: -0.011973187327384949
        total_loss: -0.01372334361076355
        vf_explained_var: 0.02361215651035309
        vf_loss: 3.5905838012695312
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009904319886118174
        entropy: 1.505858063697815
        entropy_coeff: 0.0017600000137463212
        kl: 0.014868650585412979
        model: {}
        policy_loss: -0.001928067533299327
        total_loss: -0.003901724237948656
        vf_explained_var: 0.0019337385892868042
        vf_loss: 5.4433817863464355
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009904319886118174
        entropy: 1.3977991342544556
        entropy_coeff: 0.0017600000137463212
        kl: 0.013587208464741707
        model: {}
        policy_loss: -0.014269900508224964
        total_loss: -0.01567872241139412
        vf_explained_var: -0.002151891589164734
        vf_loss: 5.417829990386963
    load_time_ms: 13277.967
    num_steps_sampled: 4416000
    num_steps_trained: 4416000
    sample_time_ms: 105206.244
    update_time_ms: 17.019
  iterations_since_restore: 46
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.188297872340428
    ram_util_percent: 11.590425531914896
  pid: 28570
  policy_reward_max:
    agent-0: 63.0
    agent-1: 63.0
    agent-2: 56.5
    agent-3: 56.5
    agent-4: 59.5
    agent-5: 59.5
  policy_reward_mean:
    agent-0: 23.21
    agent-1: 23.21
    agent-2: 23.425
    agent-3: 23.425
    agent-4: 25.52
    agent-5: 25.52
  policy_reward_min:
    agent-0: -5.0
    agent-1: -5.0
    agent-2: -0.5
    agent-3: -0.5
    agent-4: -8.5
    agent-5: -8.5
  sampler_perf:
    mean_env_wait_ms: 25.828050170685096
    mean_inference_ms: 13.231685364679734
    mean_processing_ms: 59.48472181223511
  time_since_restore: 6093.9728899002075
  time_this_iter_s: 132.0931613445282
  time_total_s: 6093.9728899002075
  timestamp: 1637514624
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 4416000
  training_iteration: 46
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     46 |          6093.97 | 4416000 |   144.31 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 4.13
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 13.63
    apples_agent-1_min: 1
    apples_agent-2_max: 90
    apples_agent-2_mean: 38.87
    apples_agent-2_min: 7
    apples_agent-3_max: 18
    apples_agent-3_mean: 4.74
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 5.54
    apples_agent-4_min: 0
    apples_agent-5_max: 113
    apples_agent-5_mean: 46.7
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 384
    cleaning_beam_agent-0_mean: 291.02
    cleaning_beam_agent-0_min: 165
    cleaning_beam_agent-1_max: 106
    cleaning_beam_agent-1_mean: 73.84
    cleaning_beam_agent-1_min: 45
    cleaning_beam_agent-2_max: 143
    cleaning_beam_agent-2_mean: 98.82
    cleaning_beam_agent-2_min: 63
    cleaning_beam_agent-3_max: 379
    cleaning_beam_agent-3_mean: 258.7
    cleaning_beam_agent-3_min: 147
    cleaning_beam_agent-4_max: 308
    cleaning_beam_agent-4_mean: 270.12
    cleaning_beam_agent-4_min: 160
    cleaning_beam_agent-5_max: 151
    cleaning_beam_agent-5_mean: 104.94
    cleaning_beam_agent-5_min: 66
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-12-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 360.0
  episode_reward_mean: 168.29
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 4512
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12870.403
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009844416053965688
        entropy: 1.3586572408676147
        entropy_coeff: 0.0017600000137463212
        kl: 0.01346773374825716
        model: {}
        policy_loss: -0.013665623962879181
        total_loss: -0.014919331297278404
        vf_explained_var: -0.01819092035293579
        vf_loss: 8.008397102355957
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009844416053965688
        entropy: 1.431379795074463
        entropy_coeff: 0.0017600000137463212
        kl: 0.013939866796135902
        model: {}
        policy_loss: -0.010834787040948868
        total_loss: -0.01233175490051508
        vf_explained_var: 0.03434568643569946
        vf_loss: 7.608888626098633
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009844416053965688
        entropy: 1.472368597984314
        entropy_coeff: 0.0017600000137463212
        kl: 0.013444814831018448
        model: {}
        policy_loss: -0.015496510080993176
        total_loss: -0.016830764710903168
        vf_explained_var: -0.003926068544387817
        vf_loss: 5.848736763000488
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.4591435194015503
        entropy_coeff: 0.0017600000137463212
        kl: 0.011920420452952385
        model: {}
        policy_loss: -0.012514840811491013
        total_loss: -0.014073153026401997
        vf_explained_var: 0.013051837682723999
        vf_loss: 5.627630233764648
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009844416053965688
        entropy: 1.517427682876587
        entropy_coeff: 0.0017600000137463212
        kl: 0.01537651289254427
        model: {}
        policy_loss: -0.00392723735421896
        total_loss: -0.005788497161120176
        vf_explained_var: 0.002570405602455139
        vf_loss: 6.725786209106445
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009844416053965688
        entropy: 1.3543283939361572
        entropy_coeff: 0.0017600000137463212
        kl: 0.01206126157194376
        model: {}
        policy_loss: -0.014129992574453354
        total_loss: -0.015381025150418282
        vf_explained_var: 0.0018851310014724731
        vf_loss: 6.802896499633789
    load_time_ms: 13272.385
    num_steps_sampled: 4512000
    num_steps_trained: 4512000
    sample_time_ms: 105315.671
    update_time_ms: 17.19
  iterations_since_restore: 47
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.186702127659572
    ram_util_percent: 11.654255319148936
  pid: 28570
  policy_reward_max:
    agent-0: 72.5
    agent-1: 72.5
    agent-2: 60.0
    agent-3: 60.0
    agent-4: 82.0
    agent-5: 82.0
  policy_reward_mean:
    agent-0: 27.57
    agent-1: 27.57
    agent-2: 27.38
    agent-3: 27.38
    agent-4: 29.195
    agent-5: 29.195
  policy_reward_min:
    agent-0: -14.5
    agent-1: -14.5
    agent-2: -6.0
    agent-3: -6.0
    agent-4: -12.5
    agent-5: -12.5
  sampler_perf:
    mean_env_wait_ms: 25.841394548445752
    mean_inference_ms: 13.22938804118181
    mean_processing_ms: 59.47922516086671
  time_since_restore: 6226.163375854492
  time_this_iter_s: 132.19048595428467
  time_total_s: 6226.163375854492
  timestamp: 1637514756
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 4512000
  training_iteration: 47
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     47 |          6226.16 | 4512000 |   168.29 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 4.6
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 14.24
    apples_agent-1_min: 2
    apples_agent-2_max: 123
    apples_agent-2_mean: 48.46
    apples_agent-2_min: 8
    apples_agent-3_max: 30
    apples_agent-3_mean: 5.99
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 4.07
    apples_agent-4_min: 0
    apples_agent-5_max: 107
    apples_agent-5_mean: 51.94
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 391
    cleaning_beam_agent-0_mean: 294.73
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 103
    cleaning_beam_agent-1_mean: 61.56
    cleaning_beam_agent-1_min: 36
    cleaning_beam_agent-2_max: 172
    cleaning_beam_agent-2_mean: 92.3
    cleaning_beam_agent-2_min: 44
    cleaning_beam_agent-3_max: 379
    cleaning_beam_agent-3_mean: 256.16
    cleaning_beam_agent-3_min: 172
    cleaning_beam_agent-4_max: 287
    cleaning_beam_agent-4_mean: 250.35
    cleaning_beam_agent-4_min: 209
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 104.75
    cleaning_beam_agent-5_min: 74
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.11
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-14-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 345.0
  episode_reward_mean: 185.52
  episode_reward_min: -158.0
  episodes_this_iter: 96
  episodes_total: 4608
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12873.86
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009784512221813202
        entropy: 1.3293943405151367
        entropy_coeff: 0.0017600000137463212
        kl: 0.015232658945024014
        model: {}
        policy_loss: -0.01548156701028347
        total_loss: -0.01666160114109516
        vf_explained_var: -0.01093226671218872
        vf_loss: 7.788819313049316
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009784512221813202
        entropy: 1.4448351860046387
        entropy_coeff: 0.0017600000137463212
        kl: 0.013385765254497528
        model: {}
        policy_loss: -0.010511624626815319
        total_loss: -0.012061364948749542
        vf_explained_var: 0.04512867331504822
        vf_loss: 7.421838760375977
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009784512221813202
        entropy: 1.4595108032226562
        entropy_coeff: 0.0017600000137463212
        kl: 0.013189475052058697
        model: {}
        policy_loss: -0.015416989102959633
        total_loss: -0.01661555841565132
        vf_explained_var: -0.0069532692432403564
        vf_loss: 7.106947898864746
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.4180079698562622
        entropy_coeff: 0.0017600000137463212
        kl: 0.012799112126231194
        model: {}
        policy_loss: -0.013373308815062046
        total_loss: -0.014702101238071918
        vf_explained_var: 0.016371503472328186
        vf_loss: 6.869368553161621
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009784512221813202
        entropy: 1.6665194034576416
        entropy_coeff: 0.0017600000137463212
        kl: 0.014027072116732597
        model: {}
        policy_loss: -0.004677139222621918
        total_loss: -0.006741117686033249
        vf_explained_var: 0.00019524991512298584
        vf_loss: 7.442718505859375
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009784512221813202
        entropy: 1.3888874053955078
        entropy_coeff: 0.0017600000137463212
        kl: 0.011915705166757107
        model: {}
        policy_loss: -0.014299849979579449
        total_loss: -0.015562321990728378
        vf_explained_var: 0.009069100022315979
        vf_loss: 7.351323127746582
    load_time_ms: 13260.198
    num_steps_sampled: 4608000
    num_steps_trained: 4608000
    sample_time_ms: 105436.754
    update_time_ms: 17.204
  iterations_since_restore: 48
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.244148936170212
    ram_util_percent: 11.593617021276598
  pid: 28570
  policy_reward_max:
    agent-0: 65.5
    agent-1: 65.5
    agent-2: 64.5
    agent-3: 64.5
    agent-4: 64.0
    agent-5: 64.0
  policy_reward_mean:
    agent-0: 30.345
    agent-1: 30.345
    agent-2: 31.805
    agent-3: 31.805
    agent-4: 30.61
    agent-5: 30.61
  policy_reward_min:
    agent-0: -36.0
    agent-1: -36.0
    agent-2: -34.5
    agent-3: -34.5
    agent-4: -8.5
    agent-5: -8.5
  sampler_perf:
    mean_env_wait_ms: 25.852333199377334
    mean_inference_ms: 13.227620908336883
    mean_processing_ms: 59.471460308525224
  time_since_restore: 6358.032962322235
  time_this_iter_s: 131.86958646774292
  time_total_s: 6358.032962322235
  timestamp: 1637514888
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 4608000
  training_iteration: 48
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     48 |          6358.03 | 4608000 |   185.52 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 5.14
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 15.26
    apples_agent-1_min: 2
    apples_agent-2_max: 108
    apples_agent-2_mean: 56.63
    apples_agent-2_min: 18
    apples_agent-3_max: 26
    apples_agent-3_mean: 5.6
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 4.44
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 62.21
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 383
    cleaning_beam_agent-0_mean: 296.74
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 83
    cleaning_beam_agent-1_mean: 57.51
    cleaning_beam_agent-1_min: 40
    cleaning_beam_agent-2_max: 144
    cleaning_beam_agent-2_mean: 90.88
    cleaning_beam_agent-2_min: 44
    cleaning_beam_agent-3_max: 323
    cleaning_beam_agent-3_mean: 242.12
    cleaning_beam_agent-3_min: 136
    cleaning_beam_agent-4_max: 306
    cleaning_beam_agent-4_mean: 260.73
    cleaning_beam_agent-4_min: 219
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 96.28
    cleaning_beam_agent-5_min: 57
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.17
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-16-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 401.0
  episode_reward_mean: 223.19
  episode_reward_min: 45.0
  episodes_this_iter: 96
  episodes_total: 4704
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12868.116
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009724607807584107
        entropy: 1.268306851387024
        entropy_coeff: 0.0017600000137463212
        kl: 0.014362964779138565
        model: {}
        policy_loss: -0.01657828316092491
        total_loss: -0.017443377524614334
        vf_explained_var: -0.03929048776626587
        vf_loss: 10.080526351928711
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009724607807584107
        entropy: 1.3649752140045166
        entropy_coeff: 0.0017600000137463212
        kl: 0.011668861843645573
        model: {}
        policy_loss: -0.013043923303484917
        total_loss: -0.014289544895291328
        vf_explained_var: 0.04668435454368591
        vf_loss: 9.379456520080566
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009724607807584107
        entropy: 1.4235037565231323
        entropy_coeff: 0.0017600000137463212
        kl: 0.01166814286261797
        model: {}
        policy_loss: -0.016906213015317917
        total_loss: -0.017841869965195656
        vf_explained_var: -0.005423486232757568
        vf_loss: 9.863036155700684
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.4332890510559082
        entropy_coeff: 0.0017600000137463212
        kl: 0.014108659699559212
        model: {}
        policy_loss: -0.01603691279888153
        total_loss: -0.017073161900043488
        vf_explained_var: 0.011331096291542053
        vf_loss: 9.572649002075195
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009724607807584107
        entropy: 1.6398162841796875
        entropy_coeff: 0.0017600000137463212
        kl: 0.011769777163863182
        model: {}
        policy_loss: -0.005542629398405552
        total_loss: -0.007376939058303833
        vf_explained_var: 0.0011114180088043213
        vf_loss: 9.470260620117188
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009724607807584107
        entropy: 1.2724871635437012
        entropy_coeff: 0.0017600000137463212
        kl: 0.011664526537060738
        model: {}
        policy_loss: -0.014883304946124554
        total_loss: -0.01572415977716446
        vf_explained_var: 0.011785060167312622
        vf_loss: 9.613051414489746
    load_time_ms: 13265.139
    num_steps_sampled: 4704000
    num_steps_trained: 4704000
    sample_time_ms: 105359.251
    update_time_ms: 17.086
  iterations_since_restore: 49
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.833155080213906
    ram_util_percent: 11.663101604278076
  pid: 28570
  policy_reward_max:
    agent-0: 75.5
    agent-1: 75.5
    agent-2: 74.0
    agent-3: 74.0
    agent-4: 81.5
    agent-5: 81.5
  policy_reward_mean:
    agent-0: 36.45
    agent-1: 36.45
    agent-2: 38.83
    agent-3: 38.83
    agent-4: 36.315
    agent-5: 36.315
  policy_reward_min:
    agent-0: -6.0
    agent-1: -6.0
    agent-2: 4.0
    agent-3: 4.0
    agent-4: -6.0
    agent-5: -6.0
  sampler_perf:
    mean_env_wait_ms: 25.861632080779003
    mean_inference_ms: 13.226360539363357
    mean_processing_ms: 59.46387876788361
  time_since_restore: 6489.185826539993
  time_this_iter_s: 131.15286421775818
  time_total_s: 6489.185826539993
  timestamp: 1637515019
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 4704000
  training_iteration: 49
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     49 |          6489.19 | 4704000 |   223.19 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 4.9
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 13.99
    apples_agent-1_min: 1
    apples_agent-2_max: 135
    apples_agent-2_mean: 54.53
    apples_agent-2_min: 1
    apples_agent-3_max: 21
    apples_agent-3_mean: 4.88
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 62.17
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 399
    cleaning_beam_agent-0_mean: 285.19
    cleaning_beam_agent-0_min: 199
    cleaning_beam_agent-1_max: 70
    cleaning_beam_agent-1_mean: 49.47
    cleaning_beam_agent-1_min: 33
    cleaning_beam_agent-2_max: 149
    cleaning_beam_agent-2_mean: 88.76
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 379
    cleaning_beam_agent-3_mean: 271.45
    cleaning_beam_agent-3_min: 154
    cleaning_beam_agent-4_max: 317
    cleaning_beam_agent-4_mean: 279.68
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 93.36
    cleaning_beam_agent-5_min: 57
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.28
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-19-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 434.0
  episode_reward_mean: 202.22
  episode_reward_min: -153.0
  episodes_this_iter: 96
  episodes_total: 4800
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12856.79
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009664703975431621
        entropy: 1.3364335298538208
        entropy_coeff: 0.0017600000137463212
        kl: 0.015473974868655205
        model: {}
        policy_loss: -0.017694661393761635
        total_loss: -0.018796809017658234
        vf_explained_var: -0.0007436573505401611
        vf_loss: 8.631254196166992
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009664703975431621
        entropy: 1.4167749881744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.011734196916222572
        model: {}
        policy_loss: -0.013777908869087696
        total_loss: -0.01521694753319025
        vf_explained_var: 0.051928624510765076
        vf_loss: 8.344712257385254
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009664703975431621
        entropy: 1.3891456127166748
        entropy_coeff: 0.0017600000137463212
        kl: 0.013042666018009186
        model: {}
        policy_loss: -0.015839364379644394
        total_loss: -0.016744978725910187
        vf_explained_var: 0.008781537413597107
        vf_loss: 8.871479034423828
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.38534414768219
        entropy_coeff: 0.0017600000137463212
        kl: 0.013971759006381035
        model: {}
        policy_loss: -0.015375726856291294
        total_loss: -0.016419265419244766
        vf_explained_var: 0.01597975194454193
        vf_loss: 8.70728588104248
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009664703975431621
        entropy: 1.637465238571167
        entropy_coeff: 0.0017600000137463212
        kl: 0.010072679258883
        model: {}
        policy_loss: -0.007077925838530064
        total_loss: -0.00874953344464302
        vf_explained_var: 0.0014221370220184326
        vf_loss: 11.20693302154541
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009664703975431621
        entropy: 1.2181404829025269
        entropy_coeff: 0.0017600000137463212
        kl: 0.011209470219910145
        model: {}
        policy_loss: -0.014160878956317902
        total_loss: -0.014757296070456505
        vf_explained_var: 0.008997976779937744
        vf_loss: 11.271540641784668
    load_time_ms: 13257.195
    num_steps_sampled: 4800000
    num_steps_trained: 4800000
    sample_time_ms: 105285.106
    update_time_ms: 17.454
  iterations_since_restore: 50
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.28930481283422
    ram_util_percent: 11.654545454545458
  pid: 28570
  policy_reward_max:
    agent-0: 84.0
    agent-1: 84.0
    agent-2: 77.0
    agent-3: 77.0
    agent-4: 85.5
    agent-5: 85.5
  policy_reward_mean:
    agent-0: 30.52
    agent-1: 30.52
    agent-2: 35.435
    agent-3: 35.435
    agent-4: 35.155
    agent-5: 35.155
  policy_reward_min:
    agent-0: -25.5
    agent-1: -25.5
    agent-2: -39.5
    agent-3: -39.5
    agent-4: -25.0
    agent-5: -25.0
  sampler_perf:
    mean_env_wait_ms: 25.86936051037234
    mean_inference_ms: 13.223437137341131
    mean_processing_ms: 59.45473004380639
  time_since_restore: 6620.2889316082
  time_this_iter_s: 131.1031050682068
  time_total_s: 6620.2889316082
  timestamp: 1637515150
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 4800000
  training_iteration: 50
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     50 |          6620.29 | 4800000 |   202.22 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 40
    apples_agent-1_mean: 16.75
    apples_agent-1_min: 3
    apples_agent-2_max: 169
    apples_agent-2_mean: 76.22
    apples_agent-2_min: 14
    apples_agent-3_max: 27
    apples_agent-3_mean: 9.12
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 2.22
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 95.2
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 371.03
    cleaning_beam_agent-0_min: 229
    cleaning_beam_agent-1_max: 72
    cleaning_beam_agent-1_mean: 53.65
    cleaning_beam_agent-1_min: 38
    cleaning_beam_agent-2_max: 144
    cleaning_beam_agent-2_mean: 88.88
    cleaning_beam_agent-2_min: 49
    cleaning_beam_agent-3_max: 371
    cleaning_beam_agent-3_mean: 282.86
    cleaning_beam_agent-3_min: 163
    cleaning_beam_agent-4_max: 355
    cleaning_beam_agent-4_mean: 312.15
    cleaning_beam_agent-4_min: 247
    cleaning_beam_agent-5_max: 109
    cleaning_beam_agent-5_mean: 73.97
    cleaning_beam_agent-5_min: 39
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.27
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.14
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-21-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 557.0
  episode_reward_mean: 287.84
  episode_reward_min: 27.0
  episodes_this_iter: 96
  episodes_total: 4896
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12854.079
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009604800143279135
        entropy: 1.2567912340164185
        entropy_coeff: 0.0017600000137463212
        kl: 0.015112016350030899
        model: {}
        policy_loss: -0.015863588079810143
        total_loss: -0.016680438071489334
        vf_explained_var: 0.0061822980642318726
        vf_loss: 10.173020362854004
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009604800143279135
        entropy: 1.4144728183746338
        entropy_coeff: 0.0017600000137463212
        kl: 0.01433146744966507
        model: {}
        policy_loss: -0.01349039375782013
        total_loss: -0.01469817291945219
        vf_explained_var: 0.057679012417793274
        vf_loss: 10.12977409362793
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009604800143279135
        entropy: 1.3648874759674072
        entropy_coeff: 0.0017600000137463212
        kl: 0.013205940835177898
        model: {}
        policy_loss: -0.016969483345746994
        total_loss: -0.01707897149026394
        vf_explained_var: -0.010055720806121826
        vf_loss: 16.324180603027344
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.3600631952285767
        entropy_coeff: 0.0017600000137463212
        kl: 0.011553670279681683
        model: {}
        policy_loss: -0.014066565781831741
        total_loss: -0.014500748366117477
        vf_explained_var: 0.02173765003681183
        vf_loss: 15.26268482208252
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009604800143279135
        entropy: 1.6063165664672852
        entropy_coeff: 0.0017600000137463212
        kl: 0.007989365607500076
        model: {}
        policy_loss: -0.0061414483934640884
        total_loss: -0.007120268419384956
        vf_explained_var: 0.0004204362630844116
        vf_loss: 17.772003173828125
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009604800143279135
        entropy: 1.1076107025146484
        entropy_coeff: 0.0017600000137463212
        kl: 0.01207244023680687
        model: {}
        policy_loss: -0.01251339539885521
        total_loss: -0.012233292683959007
        vf_explained_var: 0.011485978960990906
        vf_loss: 17.7678279876709
    load_time_ms: 13264.625
    num_steps_sampled: 4896000
    num_steps_trained: 4896000
    sample_time_ms: 105278.506
    update_time_ms: 17.57
  iterations_since_restore: 51
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.31871657754011
    ram_util_percent: 11.619251336898397
  pid: 28570
  policy_reward_max:
    agent-0: 82.0
    agent-1: 82.0
    agent-2: 93.0
    agent-3: 93.0
    agent-4: 118.0
    agent-5: 118.0
  policy_reward_mean:
    agent-0: 39.445
    agent-1: 39.445
    agent-2: 50.975
    agent-3: 50.975
    agent-4: 53.5
    agent-5: 53.5
  policy_reward_min:
    agent-0: -2.5
    agent-1: -2.5
    agent-2: -5.5
    agent-3: -5.5
    agent-4: 4.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 25.888288467197093
    mean_inference_ms: 13.220511337319074
    mean_processing_ms: 59.44065688597686
  time_since_restore: 6751.421247959137
  time_this_iter_s: 131.1323163509369
  time_total_s: 6751.421247959137
  timestamp: 1637515282
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 4896000
  training_iteration: 51
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     51 |          6751.42 | 4896000 |   287.84 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 5.43
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 17.17
    apples_agent-1_min: 2
    apples_agent-2_max: 163
    apples_agent-2_mean: 76.11
    apples_agent-2_min: 17
    apples_agent-3_max: 27
    apples_agent-3_mean: 6.36
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 198
    apples_agent-5_mean: 80.73
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 337.03
    cleaning_beam_agent-0_min: 246
    cleaning_beam_agent-1_max: 70
    cleaning_beam_agent-1_mean: 51.3
    cleaning_beam_agent-1_min: 33
    cleaning_beam_agent-2_max: 144
    cleaning_beam_agent-2_mean: 89.21
    cleaning_beam_agent-2_min: 49
    cleaning_beam_agent-3_max: 494
    cleaning_beam_agent-3_mean: 361.61
    cleaning_beam_agent-3_min: 187
    cleaning_beam_agent-4_max: 316
    cleaning_beam_agent-4_mean: 262.74
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 128
    cleaning_beam_agent-5_mean: 89.57
    cleaning_beam_agent-5_min: 54
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.24
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-23-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 544.0
  episode_reward_mean: 271.92
  episode_reward_min: 16.0
  episodes_this_iter: 96
  episodes_total: 4992
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12837.135
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000954489572905004
        entropy: 1.2785916328430176
        entropy_coeff: 0.0017600000137463212
        kl: 0.01456725038588047
        model: {}
        policy_loss: -0.016866208985447884
        total_loss: -0.017544221132993698
        vf_explained_var: 0.013535857200622559
        vf_loss: 12.081262588500977
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000954489572905004
        entropy: 1.37389075756073
        entropy_coeff: 0.0017600000137463212
        kl: 0.013264988549053669
        model: {}
        policy_loss: -0.013099233619868755
        total_loss: -0.01409056968986988
        vf_explained_var: 0.06538493931293488
        vf_loss: 11.779937744140625
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000954489572905004
        entropy: 1.3443231582641602
        entropy_coeff: 0.0017600000137463212
        kl: 0.012726892717182636
        model: {}
        policy_loss: -0.016350774094462395
        total_loss: -0.0165870264172554
        vf_explained_var: 0.001974835991859436
        vf_loss: 14.934099197387695
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.305678129196167
        entropy_coeff: 0.0017600000137463212
        kl: 0.01232866570353508
        model: {}
        policy_loss: -0.014049635268747807
        total_loss: -0.014437675476074219
        vf_explained_var: 0.01890067756175995
        vf_loss: 14.476285934448242
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000954489572905004
        entropy: 1.5854285955429077
        entropy_coeff: 0.0017600000137463212
        kl: 0.011377627961337566
        model: {}
        policy_loss: -0.008706755004823208
        total_loss: -0.010065397247672081
        vf_explained_var: 0.00017580389976501465
        vf_loss: 13.304640769958496
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000954489572905004
        entropy: 1.1779649257659912
        entropy_coeff: 0.0017600000137463212
        kl: 0.010830700397491455
        model: {}
        policy_loss: -0.01338422391563654
        total_loss: -0.013717389665544033
        vf_explained_var: 0.019020870327949524
        vf_loss: 13.339038848876953
    load_time_ms: 13251.131
    num_steps_sampled: 4992000
    num_steps_trained: 4992000
    sample_time_ms: 105354.692
    update_time_ms: 17.155
  iterations_since_restore: 52
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.16755319148936
    ram_util_percent: 11.628723404255323
  pid: 28570
  policy_reward_max:
    agent-0: 106.5
    agent-1: 106.5
    agent-2: 100.5
    agent-3: 100.5
    agent-4: 102.5
    agent-5: 102.5
  policy_reward_mean:
    agent-0: 41.205
    agent-1: 41.205
    agent-2: 49.435
    agent-3: 49.435
    agent-4: 45.32
    agent-5: 45.32
  policy_reward_min:
    agent-0: -27.5
    agent-1: -27.5
    agent-2: -9.0
    agent-3: -9.0
    agent-4: 10.5
    agent-5: 10.5
  sampler_perf:
    mean_env_wait_ms: 25.905544358939895
    mean_inference_ms: 13.216851921928383
    mean_processing_ms: 59.423497774694134
  time_since_restore: 6883.538470029831
  time_this_iter_s: 132.11722207069397
  time_total_s: 6883.538470029831
  timestamp: 1637515414
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 4992000
  training_iteration: 52
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     52 |          6883.54 | 4992000 |   271.92 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 4.61
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 17.87
    apples_agent-1_min: 1
    apples_agent-2_max: 220
    apples_agent-2_mean: 78.54
    apples_agent-2_min: 14
    apples_agent-3_max: 33
    apples_agent-3_mean: 9.55
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 3.65
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 83.5
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 347.74
    cleaning_beam_agent-0_min: 240
    cleaning_beam_agent-1_max: 79
    cleaning_beam_agent-1_mean: 47.41
    cleaning_beam_agent-1_min: 25
    cleaning_beam_agent-2_max: 165
    cleaning_beam_agent-2_mean: 93.12
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 429
    cleaning_beam_agent-3_mean: 286.17
    cleaning_beam_agent-3_min: 135
    cleaning_beam_agent-4_max: 311
    cleaning_beam_agent-4_mean: 262.13
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 80.17
    cleaning_beam_agent-5_min: 44
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.16
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-25-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 577.0
  episode_reward_mean: 292.12
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 5088
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12841.256
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009484991896897554
        entropy: 1.2218265533447266
        entropy_coeff: 0.0017600000137463212
        kl: 0.014716723933815956
        model: {}
        policy_loss: -0.017613481730222702
        total_loss: -0.018095893785357475
        vf_explained_var: 0.0056086331605911255
        vf_loss: 13.00081729888916
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009484991896897554
        entropy: 1.3050305843353271
        entropy_coeff: 0.0017600000137463212
        kl: 0.0126869548112154
        model: {}
        policy_loss: -0.010879351757466793
        total_loss: -0.011705870740115643
        vf_explained_var: 0.06858617067337036
        vf_loss: 12.324562072753906
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009484991896897554
        entropy: 1.294689416885376
        entropy_coeff: 0.0017600000137463212
        kl: 0.012970184907317162
        model: {}
        policy_loss: -0.017202340066432953
        total_loss: -0.017369234934449196
        vf_explained_var: -0.0054775625467300415
        vf_loss: 14.632516860961914
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.3519896268844604
        entropy_coeff: 0.0017600000137463212
        kl: 0.012138806283473969
        model: {}
        policy_loss: -0.014014540240168571
        total_loss: -0.014509644359350204
        vf_explained_var: 0.019752278923988342
        vf_loss: 14.291946411132812
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009484991896897554
        entropy: 1.5152429342269897
        entropy_coeff: 0.0017600000137463212
        kl: 0.01080776285380125
        model: {}
        policy_loss: -0.009357994422316551
        total_loss: -0.010512647219002247
        vf_explained_var: 0.0018600821495056152
        vf_loss: 14.159966468811035
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009484991896897554
        entropy: 1.1008894443511963
        entropy_coeff: 0.0017600000137463212
        kl: 0.010604131035506725
        model: {}
        policy_loss: -0.013886530883610249
        total_loss: -0.014006131328642368
        vf_explained_var: 0.018018484115600586
        vf_loss: 14.203109741210938
    load_time_ms: 13256.462
    num_steps_sampled: 5088000
    num_steps_trained: 5088000
    sample_time_ms: 105225.398
    update_time_ms: 17.069
  iterations_since_restore: 53
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.285483870967745
    ram_util_percent: 11.643010752688173
  pid: 28570
  policy_reward_max:
    agent-0: 94.5
    agent-1: 94.5
    agent-2: 120.0
    agent-3: 120.0
    agent-4: 103.0
    agent-5: 103.0
  policy_reward_mean:
    agent-0: 45.505
    agent-1: 45.505
    agent-2: 55.07
    agent-3: 55.07
    agent-4: 45.485
    agent-5: 45.485
  policy_reward_min:
    agent-0: 7.0
    agent-1: 7.0
    agent-2: -1.0
    agent-3: -1.0
    agent-4: -66.0
    agent-5: -66.0
  sampler_perf:
    mean_env_wait_ms: 25.914236440142066
    mean_inference_ms: 13.213561785725197
    mean_processing_ms: 59.406633594995604
  time_since_restore: 7014.12607049942
  time_this_iter_s: 130.58760046958923
  time_total_s: 7014.12607049942
  timestamp: 1637515545
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 5088000
  training_iteration: 53
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     53 |          7014.13 | 5088000 |   292.12 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 4.61
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 19.25
    apples_agent-1_min: 3
    apples_agent-2_max: 210
    apples_agent-2_mean: 89.26
    apples_agent-2_min: 15
    apples_agent-3_max: 30
    apples_agent-3_mean: 6.52
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 2.37
    apples_agent-4_min: 0
    apples_agent-5_max: 235
    apples_agent-5_mean: 106.01
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 324.09
    cleaning_beam_agent-0_min: 111
    cleaning_beam_agent-1_max: 78
    cleaning_beam_agent-1_mean: 42.22
    cleaning_beam_agent-1_min: 25
    cleaning_beam_agent-2_max: 157
    cleaning_beam_agent-2_mean: 98.01
    cleaning_beam_agent-2_min: 38
    cleaning_beam_agent-3_max: 511
    cleaning_beam_agent-3_mean: 335.77
    cleaning_beam_agent-3_min: 208
    cleaning_beam_agent-4_max: 336
    cleaning_beam_agent-4_mean: 271.58
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 79.65
    cleaning_beam_agent-5_min: 51
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.15
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-27-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 690.0
  episode_reward_mean: 332.79
  episode_reward_min: 31.0
  episodes_this_iter: 96
  episodes_total: 5184
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12846.09
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009425088064745069
        entropy: 1.2883110046386719
        entropy_coeff: 0.0017600000137463212
        kl: 0.013731306418776512
        model: {}
        policy_loss: -0.017604190856218338
        total_loss: -0.018027760088443756
        vf_explained_var: 0.03991997241973877
        vf_loss: 15.005729675292969
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009425088064745069
        entropy: 1.3748139142990112
        entropy_coeff: 0.0017600000137463212
        kl: 0.012955703772604465
        model: {}
        policy_loss: -0.01373411063104868
        total_loss: -0.014463067054748535
        vf_explained_var: 0.08327420055866241
        vf_loss: 14.477967262268066
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009425088064745069
        entropy: 1.2677438259124756
        entropy_coeff: 0.0017600000137463212
        kl: 0.01339561864733696
        model: {}
        policy_loss: -0.017139647156000137
        total_loss: -0.017077026888728142
        vf_explained_var: -0.0031138956546783447
        vf_loss: 16.240697860717773
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.3255223035812378
        entropy_coeff: 0.0017600000137463212
        kl: 0.012185847386717796
        model: {}
        policy_loss: -0.013983650133013725
        total_loss: -0.014259899966418743
        vf_explained_var: 0.01724371314048767
        vf_loss: 15.997020721435547
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009425088064745069
        entropy: 1.4752639532089233
        entropy_coeff: 0.0017600000137463212
        kl: 0.013041382655501366
        model: {}
        policy_loss: -0.009593946859240532
        total_loss: -0.010106180794537067
        vf_explained_var: 0.0012403875589370728
        vf_loss: 19.681795120239258
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009425088064745069
        entropy: 1.0105218887329102
        entropy_coeff: 0.0017600000137463212
        kl: 0.01134980283677578
        model: {}
        policy_loss: -0.014920802786946297
        total_loss: -0.014279858209192753
        vf_explained_var: 0.021064341068267822
        vf_loss: 19.938467025756836
    load_time_ms: 13262.528
    num_steps_sampled: 5184000
    num_steps_trained: 5184000
    sample_time_ms: 105240.209
    update_time_ms: 16.989
  iterations_since_restore: 54
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.27005347593583
    ram_util_percent: 11.633689839572192
  pid: 28570
  policy_reward_max:
    agent-0: 116.5
    agent-1: 116.5
    agent-2: 115.0
    agent-3: 115.0
    agent-4: 128.5
    agent-5: 128.5
  policy_reward_mean:
    agent-0: 48.195
    agent-1: 48.195
    agent-2: 59.72
    agent-3: 59.72
    agent-4: 58.48
    agent-5: 58.48
  policy_reward_min:
    agent-0: -3.0
    agent-1: -3.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: -4.5
    agent-5: -4.5
  sampler_perf:
    mean_env_wait_ms: 25.924460904455096
    mean_inference_ms: 13.209789864378383
    mean_processing_ms: 59.39174806317591
  time_since_restore: 7145.430016040802
  time_this_iter_s: 131.30394554138184
  time_total_s: 7145.430016040802
  timestamp: 1637515676
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 5184000
  training_iteration: 54
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     54 |          7145.43 | 5184000 |   332.79 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 4.91
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 20.75
    apples_agent-1_min: 6
    apples_agent-2_max: 216
    apples_agent-2_mean: 101.45
    apples_agent-2_min: 20
    apples_agent-3_max: 33
    apples_agent-3_mean: 6.32
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 2.36
    apples_agent-4_min: 0
    apples_agent-5_max: 237
    apples_agent-5_mean: 115.81
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 368.32
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 77
    cleaning_beam_agent-1_mean: 42.41
    cleaning_beam_agent-1_min: 25
    cleaning_beam_agent-2_max: 165
    cleaning_beam_agent-2_mean: 92.89
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 507
    cleaning_beam_agent-3_mean: 350.12
    cleaning_beam_agent-3_min: 166
    cleaning_beam_agent-4_max: 369
    cleaning_beam_agent-4_mean: 309.19
    cleaning_beam_agent-4_min: 242
    cleaning_beam_agent-5_max: 103
    cleaning_beam_agent-5_mean: 71.3
    cleaning_beam_agent-5_min: 38
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.12
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.17
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-30-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 707.0
  episode_reward_mean: 364.46
  episode_reward_min: 51.0
  episodes_this_iter: 96
  episodes_total: 5280
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12853.109
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009365184232592583
        entropy: 1.1955286264419556
        entropy_coeff: 0.0017600000137463212
        kl: 0.015214554034173489
        model: {}
        policy_loss: -0.017609920352697372
        total_loss: -0.01748409867286682
        vf_explained_var: -0.005706340074539185
        vf_loss: 18.49590492248535
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009365184232592583
        entropy: 1.2794454097747803
        entropy_coeff: 0.0017600000137463212
        kl: 0.013054930604994297
        model: {}
        policy_loss: -0.015020573511719704
        total_loss: -0.015428001061081886
        vf_explained_var: 0.07801379263401031
        vf_loss: 15.996172904968262
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009365184232592583
        entropy: 1.2411959171295166
        entropy_coeff: 0.0017600000137463212
        kl: 0.012675315141677856
        model: {}
        policy_loss: -0.0175695288926363
        total_loss: -0.01719926856458187
        vf_explained_var: -6.213784217834473e-05
        vf_loss: 19.20998191833496
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009365184232592583
        entropy: 1.3384087085723877
        entropy_coeff: 0.0017600000137463212
        kl: 0.011446988210082054
        model: {}
        policy_loss: -0.01544342003762722
        total_loss: -0.015451377257704735
        vf_explained_var: 0.014810889959335327
        vf_loss: 19.18377685546875
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009365184232592583
        entropy: 1.4524258375167847
        entropy_coeff: 0.0017600000137463212
        kl: 0.011875788681209087
        model: {}
        policy_loss: -0.009986413642764091
        total_loss: -0.010359448380768299
        vf_explained_var: 0.0018185228109359741
        vf_loss: 20.7755184173584
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009365184232592583
        entropy: 0.9830629825592041
        entropy_coeff: 0.0017600000137463212
        kl: 0.010192180052399635
        model: {}
        policy_loss: -0.014460352249443531
        total_loss: -0.013684178702533245
        vf_explained_var: 0.025542840361595154
        vf_loss: 21.24156379699707
    load_time_ms: 13260.05
    num_steps_sampled: 5280000
    num_steps_trained: 5280000
    sample_time_ms: 105262.811
    update_time_ms: 16.927
  iterations_since_restore: 55
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.280213903743313
    ram_util_percent: 11.610160427807488
  pid: 28570
  policy_reward_max:
    agent-0: 108.0
    agent-1: 108.0
    agent-2: 127.5
    agent-3: 127.5
    agent-4: 132.0
    agent-5: 132.0
  policy_reward_mean:
    agent-0: 55.005
    agent-1: 55.005
    agent-2: 64.835
    agent-3: 64.835
    agent-4: 62.39
    agent-5: 62.39
  policy_reward_min:
    agent-0: -12.0
    agent-1: -12.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 1.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 25.94277177214893
    mean_inference_ms: 13.20676975736431
    mean_processing_ms: 59.37538136015935
  time_since_restore: 7276.619984865189
  time_this_iter_s: 131.1899688243866
  time_total_s: 7276.619984865189
  timestamp: 1637515807
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 5280000
  training_iteration: 55
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     55 |          7276.62 | 5280000 |   364.46 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 5.33
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 23.68
    apples_agent-1_min: 3
    apples_agent-2_max: 231
    apples_agent-2_mean: 108.94
    apples_agent-2_min: 29
    apples_agent-3_max: 35
    apples_agent-3_mean: 8.26
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 249
    apples_agent-5_mean: 121.46
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 587
    cleaning_beam_agent-0_mean: 400.39
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 73
    cleaning_beam_agent-1_mean: 43.75
    cleaning_beam_agent-1_min: 25
    cleaning_beam_agent-2_max: 147
    cleaning_beam_agent-2_mean: 94.35
    cleaning_beam_agent-2_min: 51
    cleaning_beam_agent-3_max: 508
    cleaning_beam_agent-3_mean: 324.25
    cleaning_beam_agent-3_min: 180
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 368.25
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 108
    cleaning_beam_agent-5_mean: 65.78
    cleaning_beam_agent-5_min: 37
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.17
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.21
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-32-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 700.0
  episode_reward_mean: 381.96
  episode_reward_min: -53.0
  episodes_this_iter: 96
  episodes_total: 5376
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12845.244
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009305279818363488
        entropy: 1.144059419631958
        entropy_coeff: 0.0017600000137463212
        kl: 0.014294615015387535
        model: {}
        policy_loss: -0.01819240301847458
        total_loss: -0.018132178112864494
        vf_explained_var: 0.02282300591468811
        vf_loss: 17.16402816772461
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009305279818363488
        entropy: 1.2744553089141846
        entropy_coeff: 0.0017600000137463212
        kl: 0.012407297268509865
        model: {}
        policy_loss: -0.015103471465408802
        total_loss: -0.0155747439712286
        vf_explained_var: 0.07527975738048553
        vf_loss: 15.391298294067383
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009305279818363488
        entropy: 1.2061691284179688
        entropy_coeff: 0.0017600000137463212
        kl: 0.011739691719412804
        model: {}
        policy_loss: -0.017913689836859703
        total_loss: -0.01763741672039032
        vf_explained_var: 0.005363181233406067
        vf_loss: 18.121501922607422
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009305279818363488
        entropy: 1.3215982913970947
        entropy_coeff: 0.0017600000137463212
        kl: 0.01380246877670288
        model: {}
        policy_loss: -0.014938600361347198
        total_loss: -0.014891545288264751
        vf_explained_var: 0.023362457752227783
        vf_loss: 18.554737091064453
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009305279818363488
        entropy: 1.3598909378051758
        entropy_coeff: 0.0017600000137463212
        kl: 0.013095147907733917
        model: {}
        policy_loss: -0.009744176641106606
        total_loss: -0.010132750496268272
        vf_explained_var: -4.92781400680542e-05
        vf_loss: 18.88304328918457
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009305279818363488
        entropy: 0.9403718113899231
        entropy_coeff: 0.0017600000137463212
        kl: 0.010903876274824142
        model: {}
        policy_loss: -0.01461419090628624
        total_loss: -0.013948754407465458
        vf_explained_var: 0.03780464828014374
        vf_loss: 19.115942001342773
    load_time_ms: 13272.967
    num_steps_sampled: 5376000
    num_steps_trained: 5376000
    sample_time_ms: 105300.875
    update_time_ms: 16.591
  iterations_since_restore: 56
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.229787234042558
    ram_util_percent: 11.59627659574468
  pid: 28570
  policy_reward_max:
    agent-0: 124.0
    agent-1: 124.0
    agent-2: 139.0
    agent-3: 139.0
    agent-4: 139.0
    agent-5: 139.0
  policy_reward_mean:
    agent-0: 55.735
    agent-1: 55.735
    agent-2: 70.78
    agent-3: 70.78
    agent-4: 64.465
    agent-5: 64.465
  policy_reward_min:
    agent-0: -25.5
    agent-1: -25.5
    agent-2: -18.5
    agent-3: -18.5
    agent-4: -30.0
    agent-5: -30.0
  sampler_perf:
    mean_env_wait_ms: 25.96690107272161
    mean_inference_ms: 13.204726318068419
    mean_processing_ms: 59.364927975599855
  time_since_restore: 7409.217475414276
  time_this_iter_s: 132.59749054908752
  time_total_s: 7409.217475414276
  timestamp: 1637515940
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 5376000
  training_iteration: 56
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 19.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     56 |          7409.22 | 5376000 |   381.96 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 6.81
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 27.83
    apples_agent-1_min: 4
    apples_agent-2_max: 231
    apples_agent-2_mean: 129.69
    apples_agent-2_min: 38
    apples_agent-3_max: 41
    apples_agent-3_mean: 9.86
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.7
    apples_agent-4_min: 0
    apples_agent-5_max: 288
    apples_agent-5_mean: 141.88
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 607
    cleaning_beam_agent-0_mean: 402.89
    cleaning_beam_agent-0_min: 218
    cleaning_beam_agent-1_max: 71
    cleaning_beam_agent-1_mean: 45.1
    cleaning_beam_agent-1_min: 22
    cleaning_beam_agent-2_max: 126
    cleaning_beam_agent-2_mean: 87.33
    cleaning_beam_agent-2_min: 50
    cleaning_beam_agent-3_max: 581
    cleaning_beam_agent-3_mean: 326.15
    cleaning_beam_agent-3_min: 189
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 499.65
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 66.11
    cleaning_beam_agent-5_min: 39
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.14
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-34-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 840.0
  episode_reward_mean: 462.22
  episode_reward_min: 123.0
  episodes_this_iter: 96
  episodes_total: 5472
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12824.36
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009245375986211002
        entropy: 1.1012507677078247
        entropy_coeff: 0.0017600000137463212
        kl: 0.014993507415056229
        model: {}
        policy_loss: -0.01991322636604309
        total_loss: -0.018853671848773956
        vf_explained_var: 0.022549137473106384
        vf_loss: 26.22918128967285
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009245375986211002
        entropy: 1.158242106437683
        entropy_coeff: 0.0017600000137463212
        kl: 0.012228621169924736
        model: {}
        policy_loss: -0.014959447085857391
        total_loss: -0.014448113739490509
        vf_explained_var: 0.085979163646698
        vf_loss: 23.20551109313965
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009245375986211002
        entropy: 1.155062198638916
        entropy_coeff: 0.0017600000137463212
        kl: 0.012410128489136696
        model: {}
        policy_loss: -0.015584791079163551
        total_loss: -0.014403793960809708
        vf_explained_var: -0.00665283203125
        vf_loss: 25.934017181396484
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009245375986211002
        entropy: 1.3054417371749878
        entropy_coeff: 0.0017600000137463212
        kl: 0.01276986114680767
        model: {}
        policy_loss: -0.016117513179779053
        total_loss: -0.01522162277251482
        vf_explained_var: 0.020342394709587097
        vf_loss: 27.146028518676758
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009245375986211002
        entropy: 1.252764105796814
        entropy_coeff: 0.0017600000137463212
        kl: 0.011386320926249027
        model: {}
        policy_loss: -0.009222028777003288
        total_loss: -0.0088772838935256
        vf_explained_var: 0.0012021362781524658
        vf_loss: 24.482818603515625
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009245375986211002
        entropy: 0.8941541314125061
        entropy_coeff: 0.0017600000137463212
        kl: 0.011127489618957043
        model: {}
        policy_loss: -0.014532459899783134
        total_loss: -0.013238180428743362
        vf_explained_var: 0.04180748760700226
        vf_loss: 24.507097244262695
    load_time_ms: 13276.765
    num_steps_sampled: 5472000
    num_steps_trained: 5472000
    sample_time_ms: 105136.962
    update_time_ms: 16.495
  iterations_since_restore: 57
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.16182795698925
    ram_util_percent: 10.583333333333334
  pid: 28570
  policy_reward_max:
    agent-0: 149.0
    agent-1: 149.0
    agent-2: 140.5
    agent-3: 140.5
    agent-4: 158.0
    agent-5: 158.0
  policy_reward_mean:
    agent-0: 72.73
    agent-1: 72.73
    agent-2: 82.275
    agent-3: 82.275
    agent-4: 76.105
    agent-5: 76.105
  policy_reward_min:
    agent-0: 5.0
    agent-1: 5.0
    agent-2: 27.5
    agent-3: 27.5
    agent-4: 22.5
    agent-5: 22.5
  sampler_perf:
    mean_env_wait_ms: 25.99110176930958
    mean_inference_ms: 13.198811152548572
    mean_processing_ms: 59.33077741332465
  time_since_restore: 7539.553056240082
  time_this_iter_s: 130.33558082580566
  time_total_s: 7539.553056240082
  timestamp: 1637516070
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 5472000
  training_iteration: 57
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 19.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     57 |          7539.55 | 5472000 |   462.22 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 4.43
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 26.62
    apples_agent-1_min: 5
    apples_agent-2_max: 243
    apples_agent-2_mean: 117.77
    apples_agent-2_min: 41
    apples_agent-3_max: 35
    apples_agent-3_mean: 11.61
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 269
    apples_agent-5_mean: 135.27
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 684
    cleaning_beam_agent-0_mean: 462.7
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 77
    cleaning_beam_agent-1_mean: 40.63
    cleaning_beam_agent-1_min: 19
    cleaning_beam_agent-2_max: 158
    cleaning_beam_agent-2_mean: 89.29
    cleaning_beam_agent-2_min: 47
    cleaning_beam_agent-3_max: 420
    cleaning_beam_agent-3_mean: 264.01
    cleaning_beam_agent-3_min: 205
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 484.4
    cleaning_beam_agent-4_min: 411
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 64.74
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.12
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.14
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-36-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 727.0
  episode_reward_mean: 436.66
  episode_reward_min: 117.0
  episodes_this_iter: 96
  episodes_total: 5568
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12801.352
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009185472154058516
        entropy: 1.02443265914917
        entropy_coeff: 0.0017600000137463212
        kl: 0.013001786544919014
        model: {}
        policy_loss: -0.017877252772450447
        total_loss: -0.017394401133060455
        vf_explained_var: 0.022391334176063538
        vf_loss: 19.60805892944336
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009185472154058516
        entropy: 1.1764347553253174
        entropy_coeff: 0.0017600000137463212
        kl: 0.01140194945037365
        model: {}
        policy_loss: -0.01537569984793663
        total_loss: -0.015529876574873924
        vf_explained_var: 0.1124628484249115
        vf_loss: 17.025617599487305
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009185472154058516
        entropy: 1.1994314193725586
        entropy_coeff: 0.0017600000137463212
        kl: 0.012606516480445862
        model: {}
        policy_loss: -0.01696379855275154
        total_loss: -0.016452731564641
        vf_explained_var: -0.0022623389959335327
        vf_loss: 19.917438507080078
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009185472154058516
        entropy: 1.2523612976074219
        entropy_coeff: 0.0017600000137463212
        kl: 0.01158786378800869
        model: {}
        policy_loss: -0.016069725155830383
        total_loss: -0.015837108716368675
        vf_explained_var: 0.025828316807746887
        vf_loss: 20.022293090820312
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009185472154058516
        entropy: 1.3378324508666992
        entropy_coeff: 0.0017600000137463212
        kl: 0.011399203911423683
        model: {}
        policy_loss: -0.008741933852434158
        total_loss: -0.0087230009958148
        vf_explained_var: 0.002494022250175476
        vf_loss: 22.72077751159668
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009185472154058516
        entropy: 0.8933843374252319
        entropy_coeff: 0.0017600000137463212
        kl: 0.01074855774641037
        model: {}
        policy_loss: -0.014762181788682938
        total_loss: -0.013716582208871841
        vf_explained_var: 0.05197732150554657
        vf_loss: 22.14882469177246
    load_time_ms: 13294.67
    num_steps_sampled: 5568000
    num_steps_trained: 5568000
    sample_time_ms: 104996.9
    update_time_ms: 16.557
  iterations_since_restore: 58
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.625945945945944
    ram_util_percent: 11.280000000000001
  pid: 28570
  policy_reward_max:
    agent-0: 118.0
    agent-1: 118.0
    agent-2: 142.5
    agent-3: 142.5
    agent-4: 147.0
    agent-5: 147.0
  policy_reward_mean:
    agent-0: 67.155
    agent-1: 67.155
    agent-2: 78.375
    agent-3: 78.375
    agent-4: 72.8
    agent-5: 72.8
  policy_reward_min:
    agent-0: 5.0
    agent-1: 5.0
    agent-2: 24.0
    agent-3: 24.0
    agent-4: -3.5
    agent-5: -3.5
  sampler_perf:
    mean_env_wait_ms: 26.01823859277476
    mean_inference_ms: 13.196290323481717
    mean_processing_ms: 59.30356965253972
  time_since_restore: 7670.001697063446
  time_this_iter_s: 130.44864082336426
  time_total_s: 7670.001697063446
  timestamp: 1637516201
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 5568000
  training_iteration: 58
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     58 |             7670 | 5568000 |   436.66 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 4.15
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 25.04
    apples_agent-1_min: 3
    apples_agent-2_max: 265
    apples_agent-2_mean: 121.23
    apples_agent-2_min: 51
    apples_agent-3_max: 69
    apples_agent-3_mean: 14.39
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 269
    apples_agent-5_mean: 146.8
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 684
    cleaning_beam_agent-0_mean: 475.47
    cleaning_beam_agent-0_min: 183
    cleaning_beam_agent-1_max: 71
    cleaning_beam_agent-1_mean: 45.07
    cleaning_beam_agent-1_min: 20
    cleaning_beam_agent-2_max: 167
    cleaning_beam_agent-2_mean: 85.8
    cleaning_beam_agent-2_min: 35
    cleaning_beam_agent-3_max: 435
    cleaning_beam_agent-3_mean: 296.57
    cleaning_beam_agent-3_min: 199
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 514.94
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 104
    cleaning_beam_agent-5_mean: 61.27
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.2
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-38-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 767.0
  episode_reward_mean: 461.89
  episode_reward_min: 56.0
  episodes_this_iter: 96
  episodes_total: 5664
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12802.805
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009125567739829421
        entropy: 1.0036792755126953
        entropy_coeff: 0.0017600000137463212
        kl: 0.013821232132613659
        model: {}
        policy_loss: -0.01799503155052662
        total_loss: -0.017328603193163872
        vf_explained_var: 0.03451022505760193
        vf_loss: 20.873722076416016
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009125567739829421
        entropy: 1.1339597702026367
        entropy_coeff: 0.0017600000137463212
        kl: 0.011489532887935638
        model: {}
        policy_loss: -0.014997139573097229
        total_loss: -0.014965217560529709
        vf_explained_var: 0.11416344344615936
        vf_loss: 18.122663497924805
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009125567739829421
        entropy: 1.1801707744598389
        entropy_coeff: 0.0017600000137463212
        kl: 0.012729311361908913
        model: {}
        policy_loss: -0.017515769228339195
        total_loss: -0.01683349534869194
        vf_explained_var: -0.025355398654937744
        vf_loss: 21.229122161865234
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009125567739829421
        entropy: 1.2059662342071533
        entropy_coeff: 0.0017600000137463212
        kl: 0.011343312449753284
        model: {}
        policy_loss: -0.015493993647396564
        total_loss: -0.015168814919888973
        vf_explained_var: 0.011879146099090576
        vf_loss: 20.2230224609375
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009125567739829421
        entropy: 1.2616279125213623
        entropy_coeff: 0.0017600000137463212
        kl: 0.009573355317115784
        model: {}
        policy_loss: -0.009264920838177204
        total_loss: -0.00927277747541666
        vf_explained_var: -0.00033655762672424316
        vf_loss: 21.2741641998291
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009125567739829421
        entropy: 0.8581953048706055
        entropy_coeff: 0.0017600000137463212
        kl: 0.009651347063481808
        model: {}
        policy_loss: -0.015037156641483307
        total_loss: -0.014135828241705894
        vf_explained_var: 0.04671567678451538
        vf_loss: 20.498291015625
    load_time_ms: 13317.202
    num_steps_sampled: 5664000
    num_steps_trained: 5664000
    sample_time_ms: 105054.171
    update_time_ms: 16.624
  iterations_since_restore: 59
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.337234042553188
    ram_util_percent: 11.688829787234043
  pid: 28570
  policy_reward_max:
    agent-0: 143.0
    agent-1: 143.0
    agent-2: 126.5
    agent-3: 126.5
    agent-4: 147.0
    agent-5: 147.0
  policy_reward_mean:
    agent-0: 72.305
    agent-1: 72.305
    agent-2: 79.275
    agent-3: 79.275
    agent-4: 79.365
    agent-5: 79.365
  policy_reward_min:
    agent-0: -0.5
    agent-1: -0.5
    agent-2: -13.5
    agent-3: -13.5
    agent-4: 9.5
    agent-5: 9.5
  sampler_perf:
    mean_env_wait_ms: 26.05091636413186
    mean_inference_ms: 13.192370255166198
    mean_processing_ms: 59.274746162142286
  time_since_restore: 7801.9898953437805
  time_this_iter_s: 131.98819828033447
  time_total_s: 7801.9898953437805
  timestamp: 1637516333
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 5664000
  training_iteration: 59
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     59 |          7801.99 | 5664000 |   461.89 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 5.18
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 25.55
    apples_agent-1_min: 6
    apples_agent-2_max: 204
    apples_agent-2_mean: 119.99
    apples_agent-2_min: 47
    apples_agent-3_max: 51
    apples_agent-3_mean: 14.01
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.64
    apples_agent-4_min: 0
    apples_agent-5_max: 236
    apples_agent-5_mean: 145.02
    apples_agent-5_min: 72
    cleaning_beam_agent-0_max: 623
    cleaning_beam_agent-0_mean: 431.18
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 66
    cleaning_beam_agent-1_mean: 39.97
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 204
    cleaning_beam_agent-2_mean: 95.68
    cleaning_beam_agent-2_min: 41
    cleaning_beam_agent-3_max: 394
    cleaning_beam_agent-3_mean: 247.94
    cleaning_beam_agent-3_min: 162
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 529.58
    cleaning_beam_agent-4_min: 418
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 59.84
    cleaning_beam_agent-5_min: 35
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.12
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-41-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 729.0
  episode_reward_mean: 459.75
  episode_reward_min: 200.0
  episodes_this_iter: 96
  episodes_total: 5760
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12811.176
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009065663907676935
        entropy: 1.1135437488555908
        entropy_coeff: 0.0017600000137463212
        kl: 0.015285002067685127
        model: {}
        policy_loss: -0.021667690947651863
        total_loss: -0.02124457247555256
        vf_explained_var: 0.04103764891624451
        vf_loss: 20.00832176208496
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009065663907676935
        entropy: 1.09701669216156
        entropy_coeff: 0.0017600000137463212
        kl: 0.01230693981051445
        model: {}
        policy_loss: -0.015296140685677528
        total_loss: -0.01519869640469551
        vf_explained_var: 0.07983942329883575
        vf_loss: 17.97439956665039
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009065663907676935
        entropy: 1.14028000831604
        entropy_coeff: 0.0017600000137463212
        kl: 0.01222122460603714
        model: {}
        policy_loss: -0.018795745447278023
        total_loss: -0.018019162118434906
        vf_explained_var: -0.022448301315307617
        vf_loss: 21.72418785095215
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009065663907676935
        entropy: 1.1783065795898438
        entropy_coeff: 0.0017600000137463212
        kl: 0.011829085648059845
        model: {}
        policy_loss: -0.01624147593975067
        total_loss: -0.01582052931189537
        vf_explained_var: 0.020687013864517212
        vf_loss: 20.511760711669922
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009065663907676935
        entropy: 1.2230114936828613
        entropy_coeff: 0.0017600000137463212
        kl: 0.01049951184540987
        model: {}
        policy_loss: -0.010341250337660313
        total_loss: -0.010275810956954956
        vf_explained_var: 0.0012933313846588135
        vf_loss: 21.245023727416992
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009065663907676935
        entropy: 0.8927850127220154
        entropy_coeff: 0.0017600000137463212
        kl: 0.010200290940701962
        model: {}
        policy_loss: -0.014251327142119408
        total_loss: -0.013426622375845909
        vf_explained_var: 0.05898398160934448
        vf_loss: 20.134946823120117
    load_time_ms: 13330.807
    num_steps_sampled: 5760000
    num_steps_trained: 5760000
    sample_time_ms: 105205.724
    update_time_ms: 16.351
  iterations_since_restore: 60
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.253157894736844
    ram_util_percent: 11.61789473684211
  pid: 28570
  policy_reward_max:
    agent-0: 120.0
    agent-1: 120.0
    agent-2: 126.5
    agent-3: 126.5
    agent-4: 130.0
    agent-5: 130.0
  policy_reward_mean:
    agent-0: 71.4
    agent-1: 71.4
    agent-2: 81.27
    agent-3: 81.27
    agent-4: 77.205
    agent-5: 77.205
  policy_reward_min:
    agent-0: 5.5
    agent-1: 5.5
    agent-2: 9.5
    agent-3: 9.5
    agent-4: 33.5
    agent-5: 33.5
  sampler_perf:
    mean_env_wait_ms: 26.08150195384446
    mean_inference_ms: 13.189510374260024
    mean_processing_ms: 59.2575905712155
  time_since_restore: 7934.825093507767
  time_this_iter_s: 132.8351981639862
  time_total_s: 7934.825093507767
  timestamp: 1637516466
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 5760000
  training_iteration: 60
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     60 |          7934.83 | 5760000 |   459.75 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 4.91
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 22.84
    apples_agent-1_min: 5
    apples_agent-2_max: 192
    apples_agent-2_mean: 112.8
    apples_agent-2_min: 32
    apples_agent-3_max: 55
    apples_agent-3_mean: 13.36
    apples_agent-3_min: 0
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 256
    apples_agent-5_mean: 138.45
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 647
    cleaning_beam_agent-0_mean: 480.55
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 65
    cleaning_beam_agent-1_mean: 45.82
    cleaning_beam_agent-1_min: 22
    cleaning_beam_agent-2_max: 175
    cleaning_beam_agent-2_mean: 114.4
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 369
    cleaning_beam_agent-3_mean: 257.14
    cleaning_beam_agent-3_min: 194
    cleaning_beam_agent-4_max: 608
    cleaning_beam_agent-4_mean: 545.52
    cleaning_beam_agent-4_min: 445
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 60.19
    cleaning_beam_agent-5_min: 33
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.21
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-43-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 692.0
  episode_reward_mean: 443.78
  episode_reward_min: 155.0
  episodes_this_iter: 96
  episodes_total: 5856
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12799.178
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0009005760075524449
        entropy: 1.0348693132400513
        entropy_coeff: 0.0017600000137463212
        kl: 0.015000762417912483
        model: {}
        policy_loss: -0.018516086041927338
        total_loss: -0.018391305580735207
        vf_explained_var: 0.05120594799518585
        vf_loss: 15.711309432983398
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0009005760075524449
        entropy: 1.1148974895477295
        entropy_coeff: 0.0017600000137463212
        kl: 0.012302909046411514
        model: {}
        policy_loss: -0.01633455976843834
        total_loss: -0.01660350151360035
        vf_explained_var: 0.09861654043197632
        vf_loss: 14.625993728637695
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0009005760075524449
        entropy: 1.1688168048858643
        entropy_coeff: 0.0017600000137463212
        kl: 0.013105944730341434
        model: {}
        policy_loss: -0.01846918836236
        total_loss: -0.018024958670139313
        vf_explained_var: 0.00776788592338562
        vf_loss: 18.46048355102539
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009005760075524449
        entropy: 1.2036972045898438
        entropy_coeff: 0.0017600000137463212
        kl: 0.012642798945307732
        model: {}
        policy_loss: -0.015933813527226448
        total_loss: -0.015762194991111755
        vf_explained_var: 0.02364899218082428
        vf_loss: 18.16019058227539
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0009005760075524449
        entropy: 1.2372769117355347
        entropy_coeff: 0.0017600000137463212
        kl: 0.008843274787068367
        model: {}
        policy_loss: -0.009221172891557217
        total_loss: -0.00927550345659256
        vf_explained_var: -0.0003246814012527466
        vf_loss: 20.445804595947266
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0009005760075524449
        entropy: 0.8952362537384033
        entropy_coeff: 0.0017600000137463212
        kl: 0.010607077740132809
        model: {}
        policy_loss: -0.01553987618535757
        total_loss: -0.014773484319448471
        vf_explained_var: 0.052332475781440735
        vf_loss: 19.442398071289062
    load_time_ms: 13328.149
    num_steps_sampled: 5856000
    num_steps_trained: 5856000
    sample_time_ms: 105201.603
    update_time_ms: 16.407
  iterations_since_restore: 61
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.429411764705883
    ram_util_percent: 11.619251336898397
  pid: 28570
  policy_reward_max:
    agent-0: 122.0
    agent-1: 122.0
    agent-2: 138.5
    agent-3: 138.5
    agent-4: 124.5
    agent-5: 124.5
  policy_reward_mean:
    agent-0: 68.49
    agent-1: 68.49
    agent-2: 78.455
    agent-3: 78.455
    agent-4: 74.945
    agent-5: 74.945
  policy_reward_min:
    agent-0: 23.5
    agent-1: 23.5
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 14.5
    agent-5: 14.5
  sampler_perf:
    mean_env_wait_ms: 26.11345321787621
    mean_inference_ms: 13.186459696854836
    mean_processing_ms: 59.23591813286434
  time_since_restore: 8065.778698682785
  time_this_iter_s: 130.9536051750183
  time_total_s: 8065.778698682785
  timestamp: 1637516597
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 5856000
  training_iteration: 61
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     61 |          8065.78 | 5856000 |   443.78 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 5.29
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 26.84
    apples_agent-1_min: 5
    apples_agent-2_max: 238
    apples_agent-2_mean: 125.03
    apples_agent-2_min: 29
    apples_agent-3_max: 67
    apples_agent-3_mean: 13.55
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 265
    apples_agent-5_mean: 150.3
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 672
    cleaning_beam_agent-0_mean: 465.81
    cleaning_beam_agent-0_min: 112
    cleaning_beam_agent-1_max: 70
    cleaning_beam_agent-1_mean: 46.47
    cleaning_beam_agent-1_min: 15
    cleaning_beam_agent-2_max: 159
    cleaning_beam_agent-2_mean: 109.9
    cleaning_beam_agent-2_min: 73
    cleaning_beam_agent-3_max: 415
    cleaning_beam_agent-3_mean: 302.64
    cleaning_beam_agent-3_min: 177
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 511.34
    cleaning_beam_agent-4_min: 442
    cleaning_beam_agent-5_max: 140
    cleaning_beam_agent-5_mean: 67.21
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.11
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-45-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 810.0
  episode_reward_mean: 492.23
  episode_reward_min: 157.0
  episodes_this_iter: 96
  episodes_total: 5952
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12806.88
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008945856243371964
        entropy: 1.0336476564407349
        entropy_coeff: 0.0017600000137463212
        kl: 0.014417948201298714
        model: {}
        policy_loss: -0.02223973721265793
        total_loss: -0.02171953022480011
        vf_explained_var: 0.06548422574996948
        vf_loss: 19.7897891998291
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008945856243371964
        entropy: 1.0446220636367798
        entropy_coeff: 0.0017600000137463212
        kl: 0.012884235009551048
        model: {}
        policy_loss: -0.016185712069272995
        total_loss: -0.016008581966161728
        vf_explained_var: 0.10203775763511658
        vf_loss: 17.740861892700195
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008945856243371964
        entropy: 1.1564052104949951
        entropy_coeff: 0.0017600000137463212
        kl: 0.013512367382645607
        model: {}
        policy_loss: -0.01600142940878868
        total_loss: -0.01518087089061737
        vf_explained_var: 0.015935018658638
        vf_loss: 21.802141189575195
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008945856243371964
        entropy: 1.190699577331543
        entropy_coeff: 0.0017600000137463212
        kl: 0.01268274150788784
        model: {}
        policy_loss: -0.015529242344200611
        total_loss: -0.014966449700295925
        vf_explained_var: 0.009079426527023315
        vf_loss: 21.828227996826172
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008945856243371964
        entropy: 1.2562284469604492
        entropy_coeff: 0.0017600000137463212
        kl: 0.010607447475194931
        model: {}
        policy_loss: -0.008522817865014076
        total_loss: -0.008372345007956028
        vf_explained_var: 0.001405671238899231
        vf_loss: 22.670394897460938
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008945856243371964
        entropy: 0.853184163570404
        entropy_coeff: 0.0017600000137463212
        kl: 0.011492183431982994
        model: {}
        policy_loss: -0.01498996838927269
        total_loss: -0.013883990235626698
        vf_explained_var: 0.05913352966308594
        vf_loss: 21.76627540588379
    load_time_ms: 13338.783
    num_steps_sampled: 5952000
    num_steps_trained: 5952000
    sample_time_ms: 105085.82
    update_time_ms: 16.332
  iterations_since_restore: 62
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.448387096774198
    ram_util_percent: 11.612903225806452
  pid: 28570
  policy_reward_max:
    agent-0: 131.5
    agent-1: 131.5
    agent-2: 150.5
    agent-3: 150.5
    agent-4: 144.5
    agent-5: 144.5
  policy_reward_mean:
    agent-0: 78.845
    agent-1: 78.845
    agent-2: 86.105
    agent-3: 86.105
    agent-4: 81.165
    agent-5: 81.165
  policy_reward_min:
    agent-0: 23.0
    agent-1: 23.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 22.0
    agent-5: 22.0
  sampler_perf:
    mean_env_wait_ms: 26.142334301089853
    mean_inference_ms: 13.183080748167225
    mean_processing_ms: 59.20826252274562
  time_since_restore: 8196.955387592316
  time_this_iter_s: 131.17668890953064
  time_total_s: 8196.955387592316
  timestamp: 1637516728
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 5952000
  training_iteration: 62
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     62 |          8196.96 | 5952000 |   492.23 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 5.23
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 24.34
    apples_agent-1_min: 3
    apples_agent-2_max: 242
    apples_agent-2_mean: 125.81
    apples_agent-2_min: 44
    apples_agent-3_max: 60
    apples_agent-3_mean: 16.85
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 278
    apples_agent-5_mean: 153.19
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 654
    cleaning_beam_agent-0_mean: 490.96
    cleaning_beam_agent-0_min: 229
    cleaning_beam_agent-1_max: 72
    cleaning_beam_agent-1_mean: 47.07
    cleaning_beam_agent-1_min: 25
    cleaning_beam_agent-2_max: 156
    cleaning_beam_agent-2_mean: 101.98
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 443
    cleaning_beam_agent-3_mean: 325.88
    cleaning_beam_agent-3_min: 199
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 487.11
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 226
    cleaning_beam_agent-5_mean: 102.04
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.14
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.16
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-47-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 861.0
  episode_reward_mean: 495.83
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 6048
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12785.778
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008885951829142869
        entropy: 0.9834539890289307
        entropy_coeff: 0.0017600000137463212
        kl: 0.013610132038593292
        model: {}
        policy_loss: -0.022282488644123077
        total_loss: -0.021660033613443375
        vf_explained_var: 0.031307414174079895
        vf_loss: 20.130773544311523
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008885951829142869
        entropy: 1.0459949970245361
        entropy_coeff: 0.0017600000137463212
        kl: 0.01405783835798502
        model: {}
        policy_loss: -0.014775445684790611
        total_loss: -0.014537441544234753
        vf_explained_var: 0.08903928101062775
        vf_loss: 18.153663635253906
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008885951829142869
        entropy: 1.1565155982971191
        entropy_coeff: 0.0017600000137463212
        kl: 0.012899111956357956
        model: {}
        policy_loss: -0.018291588872671127
        total_loss: -0.017373595386743546
        vf_explained_var: 0.017143428325653076
        vf_loss: 23.08504867553711
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008885951829142869
        entropy: 1.1519055366516113
        entropy_coeff: 0.0017600000137463212
        kl: 0.01218593493103981
        model: {}
        policy_loss: -0.016090350225567818
        total_loss: -0.015335692092776299
        vf_explained_var: 0.011276662349700928
        vf_loss: 23.250394821166992
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008885951829142869
        entropy: 1.2087790966033936
        entropy_coeff: 0.0017600000137463212
        kl: 0.010157870128750801
        model: {}
        policy_loss: -0.010276475921273232
        total_loss: -0.010148826986551285
        vf_explained_var: -0.0009541213512420654
        vf_loss: 21.647062301635742
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008885951829142869
        entropy: 0.8946384191513062
        entropy_coeff: 0.0017600000137463212
        kl: 0.011638404801487923
        model: {}
        policy_loss: -0.014308221638202667
        total_loss: -0.013424860313534737
        vf_explained_var: 0.09115861356258392
        vf_loss: 20.21483612060547
    load_time_ms: 13329.928
    num_steps_sampled: 6048000
    num_steps_trained: 6048000
    sample_time_ms: 105335.253
    update_time_ms: 16.3
  iterations_since_restore: 63
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.308994708994707
    ram_util_percent: 11.613227513227516
  pid: 28570
  policy_reward_max:
    agent-0: 134.5
    agent-1: 134.5
    agent-2: 149.0
    agent-3: 149.0
    agent-4: 149.0
    agent-5: 149.0
  policy_reward_mean:
    agent-0: 79.37
    agent-1: 79.37
    agent-2: 87.575
    agent-3: 87.575
    agent-4: 80.97
    agent-5: 80.97
  policy_reward_min:
    agent-0: -19.0
    agent-1: -19.0
    agent-2: -15.5
    agent-3: -15.5
    agent-4: -20.0
    agent-5: -20.0
  sampler_perf:
    mean_env_wait_ms: 26.177200240753805
    mean_inference_ms: 13.180526330245234
    mean_processing_ms: 59.18742171773423
  time_since_restore: 8329.706501483917
  time_this_iter_s: 132.75111389160156
  time_total_s: 8329.706501483917
  timestamp: 1637516861
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 6048000
  training_iteration: 63
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     63 |          8329.71 | 6048000 |   495.83 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 4.95
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 26.69
    apples_agent-1_min: 6
    apples_agent-2_max: 223
    apples_agent-2_mean: 134.96
    apples_agent-2_min: 38
    apples_agent-3_max: 49
    apples_agent-3_mean: 14.19
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.35
    apples_agent-4_min: 0
    apples_agent-5_max: 278
    apples_agent-5_mean: 157.33
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 627
    cleaning_beam_agent-0_mean: 482.41
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 72
    cleaning_beam_agent-1_mean: 41.77
    cleaning_beam_agent-1_min: 21
    cleaning_beam_agent-2_max: 158
    cleaning_beam_agent-2_mean: 87.35
    cleaning_beam_agent-2_min: 46
    cleaning_beam_agent-3_max: 476
    cleaning_beam_agent-3_mean: 345.1
    cleaning_beam_agent-3_min: 178
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 491.7
    cleaning_beam_agent-4_min: 406
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 64.83
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-49-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 884.0
  episode_reward_mean: 510.29
  episode_reward_min: 169.0
  episodes_this_iter: 96
  episodes_total: 6144
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12764.959
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008826047996990383
        entropy: 0.9974527955055237
        entropy_coeff: 0.0017600000137463212
        kl: 0.013842622749507427
        model: {}
        policy_loss: -0.021777141839265823
        total_loss: -0.02114769071340561
        vf_explained_var: 0.038211971521377563
        vf_loss: 20.388975143432617
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008826047996990383
        entropy: 0.9972430467605591
        entropy_coeff: 0.0017600000137463212
        kl: 0.011285355314612389
        model: {}
        policy_loss: -0.01540315430611372
        total_loss: -0.015063739381730556
        vf_explained_var: 0.09808023273944855
        vf_loss: 18.82965087890625
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008826047996990383
        entropy: 1.1258008480072021
        entropy_coeff: 0.0017600000137463212
        kl: 0.013053683564066887
        model: {}
        policy_loss: -0.019583716988563538
        total_loss: -0.018737534061074257
        vf_explained_var: 0.023296043276786804
        vf_loss: 21.749082565307617
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008826047996990383
        entropy: 1.1589939594268799
        entropy_coeff: 0.0017600000137463212
        kl: 0.011197200044989586
        model: {}
        policy_loss: -0.01611807383596897
        total_loss: -0.015526099130511284
        vf_explained_var: 0.017545655369758606
        vf_loss: 22.11908721923828
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008826047996990383
        entropy: 1.2166826725006104
        entropy_coeff: 0.0017600000137463212
        kl: 0.011868786066770554
        model: {}
        policy_loss: -0.010291827842593193
        total_loss: -0.010069877840578556
        vf_explained_var: 0.000815659761428833
        vf_loss: 22.576929092407227
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008826047996990383
        entropy: 0.781578779220581
        entropy_coeff: 0.0017600000137463212
        kl: 0.012116113677620888
        model: {}
        policy_loss: -0.01597318798303604
        total_loss: -0.014795826748013496
        vf_explained_var: 0.0755653977394104
        vf_loss: 20.985864639282227
    load_time_ms: 13321.09
    num_steps_sampled: 6144000
    num_steps_trained: 6144000
    sample_time_ms: 105361.625
    update_time_ms: 16.245
  iterations_since_restore: 64
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.282978723404256
    ram_util_percent: 11.615425531914894
  pid: 28570
  policy_reward_max:
    agent-0: 143.0
    agent-1: 143.0
    agent-2: 162.0
    agent-3: 162.0
    agent-4: 149.0
    agent-5: 149.0
  policy_reward_mean:
    agent-0: 80.88
    agent-1: 80.88
    agent-2: 89.855
    agent-3: 89.855
    agent-4: 84.41
    agent-5: 84.41
  policy_reward_min:
    agent-0: 27.5
    agent-1: 27.5
    agent-2: 34.0
    agent-3: 34.0
    agent-4: 20.5
    agent-5: 20.5
  sampler_perf:
    mean_env_wait_ms: 26.207200054562104
    mean_inference_ms: 13.177927374798667
    mean_processing_ms: 59.16599216128296
  time_since_restore: 8461.012100696564
  time_this_iter_s: 131.30559921264648
  time_total_s: 8461.012100696564
  timestamp: 1637516993
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 6144000
  training_iteration: 64
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     64 |          8461.01 | 6144000 |   510.29 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 26.3
    apples_agent-1_min: 8
    apples_agent-2_max: 246
    apples_agent-2_mean: 149.15
    apples_agent-2_min: 67
    apples_agent-3_max: 61
    apples_agent-3_mean: 13.41
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 328
    apples_agent-5_mean: 169.81
    apples_agent-5_min: 78
    cleaning_beam_agent-0_max: 671
    cleaning_beam_agent-0_mean: 524.36
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 59
    cleaning_beam_agent-1_mean: 37.43
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 71.06
    cleaning_beam_agent-2_min: 44
    cleaning_beam_agent-3_max: 482
    cleaning_beam_agent-3_mean: 378.93
    cleaning_beam_agent-3_min: 178
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 523.19
    cleaning_beam_agent-4_min: 438
    cleaning_beam_agent-5_max: 155
    cleaning_beam_agent-5_mean: 59.5
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.08
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.13
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-52-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 912.0
  episode_reward_mean: 547.65
  episode_reward_min: 136.0
  episodes_this_iter: 96
  episodes_total: 6240
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12770.337
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008766144164837897
        entropy: 0.9372948408126831
        entropy_coeff: 0.0017600000137463212
        kl: 0.015510806813836098
        model: {}
        policy_loss: -0.01947975717484951
        total_loss: -0.018488433212041855
        vf_explained_var: 0.02863357961177826
        vf_loss: 22.531932830810547
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008766144164837897
        entropy: 0.951836109161377
        entropy_coeff: 0.0017600000137463212
        kl: 0.01226813718676567
        model: {}
        policy_loss: -0.016157187521457672
        total_loss: -0.01562538743019104
        vf_explained_var: 0.11940322816371918
        vf_loss: 19.77004051208496
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008766144164837897
        entropy: 1.0533254146575928
        entropy_coeff: 0.0017600000137463212
        kl: 0.01123441569507122
        model: {}
        policy_loss: -0.01875046268105507
        total_loss: -0.017764030024409294
        vf_explained_var: 0.025711849331855774
        vf_loss: 22.785621643066406
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008766144164837897
        entropy: 1.2002828121185303
        entropy_coeff: 0.0017600000137463212
        kl: 0.012975843623280525
        model: {}
        policy_loss: -0.016372807323932648
        total_loss: -0.015667755156755447
        vf_explained_var: 0.026897072792053223
        vf_loss: 23.30954360961914
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008766144164837897
        entropy: 1.2140600681304932
        entropy_coeff: 0.0017600000137463212
        kl: 0.010387380607426167
        model: {}
        policy_loss: -0.010606872849166393
        total_loss: -0.010047223418951035
        vf_explained_var: 0.0037801116704940796
        vf_loss: 26.03957748413086
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008766144164837897
        entropy: 0.790604829788208
        entropy_coeff: 0.0017600000137463212
        kl: 0.011032691225409508
        model: {}
        policy_loss: -0.01808910258114338
        total_loss: -0.01658201590180397
        vf_explained_var: 0.0794588178396225
        vf_loss: 24.848268508911133
    load_time_ms: 13346.241
    num_steps_sampled: 6240000
    num_steps_trained: 6240000
    sample_time_ms: 105509.654
    update_time_ms: 16.27
  iterations_since_restore: 65
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.34444444444444
    ram_util_percent: 11.616931216931219
  pid: 28570
  policy_reward_max:
    agent-0: 143.5
    agent-1: 143.5
    agent-2: 153.5
    agent-3: 153.5
    agent-4: 171.5
    agent-5: 171.5
  policy_reward_mean:
    agent-0: 85.65
    agent-1: 85.65
    agent-2: 97.81
    agent-3: 97.81
    agent-4: 90.365
    agent-5: 90.365
  policy_reward_min:
    agent-0: 10.5
    agent-1: 10.5
    agent-2: 1.5
    agent-3: 1.5
    agent-4: -3.0
    agent-5: -3.0
  sampler_perf:
    mean_env_wait_ms: 26.24253991616955
    mean_inference_ms: 13.174717250593853
    mean_processing_ms: 59.147611989193045
  time_since_restore: 8594.008764743805
  time_this_iter_s: 132.9966640472412
  time_total_s: 8594.008764743805
  timestamp: 1637517126
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 6240000
  training_iteration: 65
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     65 |          8594.01 | 6240000 |   547.65 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 5.02
    apples_agent-0_min: 0
    apples_agent-1_max: 123
    apples_agent-1_mean: 31.62
    apples_agent-1_min: 8
    apples_agent-2_max: 251
    apples_agent-2_mean: 146.41
    apples_agent-2_min: 49
    apples_agent-3_max: 40
    apples_agent-3_mean: 13.69
    apples_agent-3_min: 0
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 328
    apples_agent-5_mean: 164.1
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 674
    cleaning_beam_agent-0_mean: 490.86
    cleaning_beam_agent-0_min: 302
    cleaning_beam_agent-1_max: 53
    cleaning_beam_agent-1_mean: 27.6
    cleaning_beam_agent-1_min: 14
    cleaning_beam_agent-2_max: 139
    cleaning_beam_agent-2_mean: 76.7
    cleaning_beam_agent-2_min: 51
    cleaning_beam_agent-3_max: 512
    cleaning_beam_agent-3_mean: 360.52
    cleaning_beam_agent-3_min: 193
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 519.92
    cleaning_beam_agent-4_min: 433
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 55.73
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-54-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 912.0
  episode_reward_mean: 545.61
  episode_reward_min: 262.0
  episodes_this_iter: 96
  episodes_total: 6336
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12771.565
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008706239750608802
        entropy: 0.9698310494422913
        entropy_coeff: 0.0017600000137463212
        kl: 0.01890385150909424
        model: {}
        policy_loss: -0.018208768218755722
        total_loss: -0.01736447401344776
        vf_explained_var: 0.03355534374713898
        vf_loss: 20.786006927490234
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008706239750608802
        entropy: 0.9189510345458984
        entropy_coeff: 0.0017600000137463212
        kl: 0.011598682031035423
        model: {}
        policy_loss: -0.014840930700302124
        total_loss: -0.014365147799253464
        vf_explained_var: 0.10115613043308258
        vf_loss: 18.756610870361328
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008706239750608802
        entropy: 1.0630040168762207
        entropy_coeff: 0.0017600000137463212
        kl: 0.013037221506237984
        model: {}
        policy_loss: -0.01922551728785038
        total_loss: -0.018108749762177467
        vf_explained_var: 0.02038109302520752
        vf_loss: 23.35797882080078
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008706239750608802
        entropy: 1.1286649703979492
        entropy_coeff: 0.0017600000137463212
        kl: 0.012706298381090164
        model: {}
        policy_loss: -0.016287904232740402
        total_loss: -0.015361931174993515
        vf_explained_var: -0.005891025066375732
        vf_loss: 24.359392166137695
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008706239750608802
        entropy: 1.2131354808807373
        entropy_coeff: 0.0017600000137463212
        kl: 0.009482803754508495
        model: {}
        policy_loss: -0.010597875341773033
        total_loss: -0.010162631049752235
        vf_explained_var: 0.001410260796546936
        vf_loss: 24.859760284423828
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008706239750608802
        entropy: 0.7850884199142456
        entropy_coeff: 0.0017600000137463212
        kl: 0.01071284431964159
        model: {}
        policy_loss: -0.016335705295205116
        total_loss: -0.014984294772148132
        vf_explained_var: 0.08460502326488495
        vf_loss: 23.314359664916992
    load_time_ms: 13325.594
    num_steps_sampled: 6336000
    num_steps_trained: 6336000
    sample_time_ms: 105596.7
    update_time_ms: 16.406
  iterations_since_restore: 66
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.22684210526316
    ram_util_percent: 11.692105263157895
  pid: 28570
  policy_reward_max:
    agent-0: 143.5
    agent-1: 143.5
    agent-2: 144.5
    agent-3: 144.5
    agent-4: 169.0
    agent-5: 169.0
  policy_reward_mean:
    agent-0: 87.41
    agent-1: 87.41
    agent-2: 95.985
    agent-3: 95.985
    agent-4: 89.41
    agent-5: 89.41
  policy_reward_min:
    agent-0: 36.0
    agent-1: 36.0
    agent-2: 45.0
    agent-3: 45.0
    agent-4: -4.5
    agent-5: -4.5
  sampler_perf:
    mean_env_wait_ms: 26.276600615005727
    mean_inference_ms: 13.172037237983682
    mean_processing_ms: 59.130752550340176
  time_since_restore: 8727.206953287125
  time_this_iter_s: 133.1981885433197
  time_total_s: 8727.206953287125
  timestamp: 1637517259
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 6336000
  training_iteration: 66
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     66 |          8727.21 | 6336000 |   545.61 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 4.72
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 28.01
    apples_agent-1_min: 11
    apples_agent-2_max: 217
    apples_agent-2_mean: 136.14
    apples_agent-2_min: 61
    apples_agent-3_max: 53
    apples_agent-3_mean: 12.91
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 258
    apples_agent-5_mean: 161.45
    apples_agent-5_min: 71
    cleaning_beam_agent-0_max: 688
    cleaning_beam_agent-0_mean: 500.9
    cleaning_beam_agent-0_min: 247
    cleaning_beam_agent-1_max: 50
    cleaning_beam_agent-1_mean: 25.89
    cleaning_beam_agent-1_min: 10
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 76.9
    cleaning_beam_agent-2_min: 47
    cleaning_beam_agent-3_max: 518
    cleaning_beam_agent-3_mean: 345.44
    cleaning_beam_agent-3_min: 185
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 507.03
    cleaning_beam_agent-4_min: 391
    cleaning_beam_agent-5_max: 146
    cleaning_beam_agent-5_mean: 48.93
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.18
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.22
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-56-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 829.0
  episode_reward_mean: 522.62
  episode_reward_min: 266.0
  episodes_this_iter: 96
  episodes_total: 6432
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12795.443
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008646335918456316
        entropy: 0.9704399108886719
        entropy_coeff: 0.0017600000137463212
        kl: 0.015348650515079498
        model: {}
        policy_loss: -0.0207949411123991
        total_loss: -0.01987205632030964
        vf_explained_var: 0.0029722601175308228
        vf_loss: 22.471405029296875
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008646335918456316
        entropy: 0.9230831265449524
        entropy_coeff: 0.0017600000137463212
        kl: 0.011287925764918327
        model: {}
        policy_loss: -0.01641392894089222
        total_loss: -0.015842637047171593
        vf_explained_var: 0.10498525202274323
        vf_loss: 19.84269905090332
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008646335918456316
        entropy: 1.063642144203186
        entropy_coeff: 0.0017600000137463212
        kl: 0.012913905084133148
        model: {}
        policy_loss: -0.018212145194411278
        total_loss: -0.017254197970032692
        vf_explained_var: 0.017690658569335938
        vf_loss: 21.842613220214844
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008646335918456316
        entropy: 1.1471608877182007
        entropy_coeff: 0.0017600000137463212
        kl: 0.011028595268726349
        model: {}
        policy_loss: -0.0161723755300045
        total_loss: -0.015516162849962711
        vf_explained_var: -0.014060556888580322
        vf_loss: 22.616405487060547
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008646335918456316
        entropy: 1.3127797842025757
        entropy_coeff: 0.0017600000137463212
        kl: 0.009192381054162979
        model: {}
        policy_loss: -0.011122655123472214
        total_loss: -0.010984731838107109
        vf_explained_var: -0.0010175108909606934
        vf_loss: 23.6661376953125
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008646335918456316
        entropy: 0.7426385879516602
        entropy_coeff: 0.0017600000137463212
        kl: 0.010728910565376282
        model: {}
        policy_loss: -0.016029566526412964
        total_loss: -0.014797957614064217
        vf_explained_var: 0.10715028643608093
        vf_loss: 21.3631591796875
    load_time_ms: 13333.835
    num_steps_sampled: 6432000
    num_steps_trained: 6432000
    sample_time_ms: 105698.061
    update_time_ms: 16.569
  iterations_since_restore: 67
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.41176470588235
    ram_util_percent: 11.52620320855615
  pid: 28570
  policy_reward_max:
    agent-0: 135.0
    agent-1: 135.0
    agent-2: 144.5
    agent-3: 144.5
    agent-4: 145.0
    agent-5: 145.0
  policy_reward_mean:
    agent-0: 83.52
    agent-1: 83.52
    agent-2: 89.44
    agent-3: 89.44
    agent-4: 88.35
    agent-5: 88.35
  policy_reward_min:
    agent-0: 39.5
    agent-1: 39.5
    agent-2: 30.0
    agent-3: 30.0
    agent-4: 23.5
    agent-5: 23.5
  sampler_perf:
    mean_env_wait_ms: 26.307809102792294
    mean_inference_ms: 13.169289618119096
    mean_processing_ms: 59.1112321649043
  time_since_restore: 8858.922946214676
  time_this_iter_s: 131.71599292755127
  time_total_s: 8858.922946214676
  timestamp: 1637517391
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 6432000
  training_iteration: 67
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     67 |          8858.92 | 6432000 |   522.62 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 4.91
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 27.35
    apples_agent-1_min: 5
    apples_agent-2_max: 228
    apples_agent-2_mean: 139.63
    apples_agent-2_min: 36
    apples_agent-3_max: 43
    apples_agent-3_mean: 11.77
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 332
    apples_agent-5_mean: 177.92
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 681
    cleaning_beam_agent-0_mean: 497.28
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 44
    cleaning_beam_agent-1_mean: 26.24
    cleaning_beam_agent-1_min: 7
    cleaning_beam_agent-2_max: 112
    cleaning_beam_agent-2_mean: 70.93
    cleaning_beam_agent-2_min: 37
    cleaning_beam_agent-3_max: 536
    cleaning_beam_agent-3_mean: 384.08
    cleaning_beam_agent-3_min: 177
    cleaning_beam_agent-4_max: 615
    cleaning_beam_agent-4_mean: 552.6
    cleaning_beam_agent-4_min: 459
    cleaning_beam_agent-5_max: 124
    cleaning_beam_agent-5_mean: 52.79
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.19
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.16
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_12-58-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 884.0
  episode_reward_mean: 559.18
  episode_reward_min: 167.0
  episodes_this_iter: 96
  episodes_total: 6528
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12811.443
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000858643208630383
        entropy: 0.9419909715652466
        entropy_coeff: 0.0017600000137463212
        kl: 0.013243495486676693
        model: {}
        policy_loss: -0.021251047030091286
        total_loss: -0.020104411989450455
        vf_explained_var: 0.06982463598251343
        vf_loss: 24.734535217285156
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000858643208630383
        entropy: 0.8962115049362183
        entropy_coeff: 0.0017600000137463212
        kl: 0.01125717256218195
        model: {}
        policy_loss: -0.016913894563913345
        total_loss: -0.015946723520755768
        vf_explained_var: 0.11437608301639557
        vf_loss: 23.334278106689453
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000858643208630383
        entropy: 1.0549352169036865
        entropy_coeff: 0.0017600000137463212
        kl: 0.013490993529558182
        model: {}
        policy_loss: -0.017503825947642326
        total_loss: -0.016582176089286804
        vf_explained_var: 0.02977493405342102
        vf_loss: 21.037912368774414
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000858643208630383
        entropy: 1.1412808895111084
        entropy_coeff: 0.0017600000137463212
        kl: 0.011625166982412338
        model: {}
        policy_loss: -0.017572887241840363
        total_loss: -0.016999095678329468
        vf_explained_var: 0.014722436666488647
        vf_loss: 21.465003967285156
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000858643208630383
        entropy: 1.1961207389831543
        entropy_coeff: 0.0017600000137463212
        kl: 0.011317006312310696
        model: {}
        policy_loss: -0.010068882256746292
        total_loss: -0.009318722411990166
        vf_explained_var: 0.004366457462310791
        vf_loss: 27.546232223510742
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000858643208630383
        entropy: 0.7264854907989502
        entropy_coeff: 0.0017600000137463212
        kl: 0.011006830260157585
        model: {}
        policy_loss: -0.016891974955797195
        total_loss: -0.01522087026387453
        vf_explained_var: 0.10638374090194702
        vf_loss: 25.369651794433594
    load_time_ms: 13337.862
    num_steps_sampled: 6528000
    num_steps_trained: 6528000
    sample_time_ms: 105897.733
    update_time_ms: 16.9
  iterations_since_restore: 68
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.34656084656085
    ram_util_percent: 11.679894179894182
  pid: 28570
  policy_reward_max:
    agent-0: 158.0
    agent-1: 158.0
    agent-2: 130.0
    agent-3: 130.0
    agent-4: 170.0
    agent-5: 170.0
  policy_reward_mean:
    agent-0: 90.845
    agent-1: 90.845
    agent-2: 91.695
    agent-3: 91.695
    agent-4: 97.05
    agent-5: 97.05
  policy_reward_min:
    agent-0: 10.5
    agent-1: 10.5
    agent-2: 39.0
    agent-3: 39.0
    agent-4: 30.5
    agent-5: 30.5
  sampler_perf:
    mean_env_wait_ms: 26.340936187480725
    mean_inference_ms: 13.166041776734582
    mean_processing_ms: 59.09310743375336
  time_since_restore: 8991.57768034935
  time_this_iter_s: 132.65473413467407
  time_total_s: 8991.57768034935
  timestamp: 1637517524
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 6528000
  training_iteration: 68
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     68 |          8991.58 | 6528000 |   559.18 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 28.56
    apples_agent-1_min: 5
    apples_agent-2_max: 250
    apples_agent-2_mean: 145.91
    apples_agent-2_min: 41
    apples_agent-3_max: 41
    apples_agent-3_mean: 12.16
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 319
    apples_agent-5_mean: 168.9
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 689
    cleaning_beam_agent-0_mean: 510.25
    cleaning_beam_agent-0_min: 188
    cleaning_beam_agent-1_max: 42
    cleaning_beam_agent-1_mean: 21.64
    cleaning_beam_agent-1_min: 10
    cleaning_beam_agent-2_max: 148
    cleaning_beam_agent-2_mean: 81.64
    cleaning_beam_agent-2_min: 49
    cleaning_beam_agent-3_max: 478
    cleaning_beam_agent-3_mean: 338.17
    cleaning_beam_agent-3_min: 219
    cleaning_beam_agent-4_max: 637
    cleaning_beam_agent-4_mean: 506.84
    cleaning_beam_agent-4_min: 339
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 83.99
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-00-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 883.0
  episode_reward_mean: 561.49
  episode_reward_min: 218.0
  episodes_this_iter: 96
  episodes_total: 6624
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12808.291
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008526528254151344
        entropy: 0.9281213879585266
        entropy_coeff: 0.0017600000137463212
        kl: 0.01245717890560627
        model: {}
        policy_loss: -0.019684946164488792
        total_loss: -0.018603865057229996
        vf_explained_var: 0.029089197516441345
        vf_loss: 24.03143882751465
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008526528254151344
        entropy: 0.9006732702255249
        entropy_coeff: 0.0017600000137463212
        kl: 0.011111073195934296
        model: {}
        policy_loss: -0.01629164069890976
        total_loss: -0.015413066372275352
        vf_explained_var: 0.08840814232826233
        vf_loss: 22.554285049438477
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008526528254151344
        entropy: 1.0361789464950562
        entropy_coeff: 0.0017600000137463212
        kl: 0.01183470617979765
        model: {}
        policy_loss: -0.018124161288142204
        total_loss: -0.016827061772346497
        vf_explained_var: 0.021521583199501038
        vf_loss: 25.290380477905273
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008526528254151344
        entropy: 1.1281781196594238
        entropy_coeff: 0.0017600000137463212
        kl: 0.013255789875984192
        model: {}
        policy_loss: -0.015874207019805908
        total_loss: -0.014736400917172432
        vf_explained_var: -0.0085219144821167
        vf_loss: 26.26310157775879
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008526528254151344
        entropy: 1.2849162817001343
        entropy_coeff: 0.0017600000137463212
        kl: 0.010191439650952816
        model: {}
        policy_loss: -0.010466942563652992
        total_loss: -0.010019795037806034
        vf_explained_var: 0.0032774507999420166
        vf_loss: 26.179080963134766
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008526528254151344
        entropy: 0.78443443775177
        entropy_coeff: 0.0017600000137463212
        kl: 0.012231274507939816
        model: {}
        policy_loss: -0.018763957545161247
        total_loss: -0.01728563755750656
        vf_explained_var: 0.11680324375629425
        vf_loss: 24.002546310424805
    load_time_ms: 13309.923
    num_steps_sampled: 6624000
    num_steps_trained: 6624000
    sample_time_ms: 105932.582
    update_time_ms: 16.865
  iterations_since_restore: 69
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.376063829787235
    ram_util_percent: 11.613297872340427
  pid: 28570
  policy_reward_max:
    agent-0: 144.0
    agent-1: 144.0
    agent-2: 156.5
    agent-3: 156.5
    agent-4: 173.0
    agent-5: 173.0
  policy_reward_mean:
    agent-0: 90.575
    agent-1: 90.575
    agent-2: 96.695
    agent-3: 96.695
    agent-4: 93.475
    agent-5: 93.475
  policy_reward_min:
    agent-0: 40.5
    agent-1: 40.5
    agent-2: 17.5
    agent-3: 17.5
    agent-4: 37.0
    agent-5: 37.0
  sampler_perf:
    mean_env_wait_ms: 26.3695346679341
    mean_inference_ms: 13.16342626135313
    mean_processing_ms: 59.07604382079532
  time_since_restore: 9123.556046009064
  time_this_iter_s: 131.97836565971375
  time_total_s: 9123.556046009064
  timestamp: 1637517656
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 6624000
  training_iteration: 69
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     69 |          9123.56 | 6624000 |   561.49 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 25.47
    apples_agent-1_min: 7
    apples_agent-2_max: 217
    apples_agent-2_mean: 138.72
    apples_agent-2_min: 60
    apples_agent-3_max: 44
    apples_agent-3_mean: 12.99
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.36
    apples_agent-4_min: 0
    apples_agent-5_max: 286
    apples_agent-5_mean: 170.3
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 657
    cleaning_beam_agent-0_mean: 502.19
    cleaning_beam_agent-0_min: 203
    cleaning_beam_agent-1_max: 38
    cleaning_beam_agent-1_mean: 17.59
    cleaning_beam_agent-1_min: 6
    cleaning_beam_agent-2_max: 116
    cleaning_beam_agent-2_mean: 68.56
    cleaning_beam_agent-2_min: 34
    cleaning_beam_agent-3_max: 456
    cleaning_beam_agent-3_mean: 328.62
    cleaning_beam_agent-3_min: 150
    cleaning_beam_agent-4_max: 698
    cleaning_beam_agent-4_mean: 579.05
    cleaning_beam_agent-4_min: 421
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 79.41
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.18
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.25
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-03-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 844.0
  episode_reward_mean: 550.13
  episode_reward_min: 111.0
  episodes_this_iter: 96
  episodes_total: 6720
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12812.933
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008466623839922249
        entropy: 0.9403555989265442
        entropy_coeff: 0.0017600000137463212
        kl: 0.012672153301537037
        model: {}
        policy_loss: -0.021767301484942436
        total_loss: -0.02078843116760254
        vf_explained_var: 0.06738962233066559
        vf_loss: 23.170957565307617
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008466623839922249
        entropy: 0.9102891683578491
        entropy_coeff: 0.0017600000137463212
        kl: 0.01142100803554058
        model: {}
        policy_loss: -0.017627207562327385
        total_loss: -0.016813334077596664
        vf_explained_var: 0.12228544056415558
        vf_loss: 22.01837158203125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008466623839922249
        entropy: 1.033846139907837
        entropy_coeff: 0.0017600000137463212
        kl: 0.012686023488640785
        model: {}
        policy_loss: -0.020470034331083298
        total_loss: -0.019457655027508736
        vf_explained_var: 0.034591883420944214
        vf_loss: 21.976505279541016
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008466623839922249
        entropy: 1.116676688194275
        entropy_coeff: 0.0017600000137463212
        kl: 0.012095525860786438
        model: {}
        policy_loss: -0.017181985080242157
        total_loss: -0.016376838088035583
        vf_explained_var: -0.015033930540084839
        vf_loss: 23.16913604736328
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008466623839922249
        entropy: 1.152105450630188
        entropy_coeff: 0.0017600000137463212
        kl: 0.010374082252383232
        model: {}
        policy_loss: -0.011105932295322418
        total_loss: -0.01036371011286974
        vf_explained_var: 0.004416763782501221
        vf_loss: 26.776084899902344
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008466623839922249
        entropy: 0.7992327213287354
        entropy_coeff: 0.0017600000137463212
        kl: 0.012078560888767242
        model: {}
        policy_loss: -0.019168388098478317
        total_loss: -0.017715049907565117
        vf_explained_var: 0.12669691443443298
        vf_loss: 24.0704288482666
    load_time_ms: 13307.875
    num_steps_sampled: 6720000
    num_steps_trained: 6720000
    sample_time_ms: 105931.798
    update_time_ms: 16.62
  iterations_since_restore: 70
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.290476190476188
    ram_util_percent: 11.685185185185185
  pid: 28570
  policy_reward_max:
    agent-0: 140.0
    agent-1: 140.0
    agent-2: 137.0
    agent-3: 137.0
    agent-4: 155.0
    agent-5: 155.0
  policy_reward_mean:
    agent-0: 87.2
    agent-1: 87.2
    agent-2: 94.01
    agent-3: 94.01
    agent-4: 93.855
    agent-5: 93.855
  policy_reward_min:
    agent-0: 9.0
    agent-1: 9.0
    agent-2: -3.0
    agent-3: -3.0
    agent-4: 22.5
    agent-5: 22.5
  sampler_perf:
    mean_env_wait_ms: 26.39957842796938
    mean_inference_ms: 13.160368698329139
    mean_processing_ms: 59.06132362287108
  time_since_restore: 9256.430198431015
  time_this_iter_s: 132.8741524219513
  time_total_s: 9256.430198431015
  timestamp: 1637517789
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 6720000
  training_iteration: 70
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     70 |          9256.43 | 6720000 |   550.13 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 3.95
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 25.86
    apples_agent-1_min: 9
    apples_agent-2_max: 251
    apples_agent-2_mean: 143.36
    apples_agent-2_min: 32
    apples_agent-3_max: 73
    apples_agent-3_mean: 17.58
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 297
    apples_agent-5_mean: 165.8
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 615
    cleaning_beam_agent-0_mean: 493.48
    cleaning_beam_agent-0_min: 161
    cleaning_beam_agent-1_max: 30
    cleaning_beam_agent-1_mean: 14.9
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 125
    cleaning_beam_agent-2_mean: 61.58
    cleaning_beam_agent-2_min: 32
    cleaning_beam_agent-3_max: 445
    cleaning_beam_agent-3_mean: 273.91
    cleaning_beam_agent-3_min: 148
    cleaning_beam_agent-4_max: 678
    cleaning_beam_agent-4_mean: 565.77
    cleaning_beam_agent-4_min: 469
    cleaning_beam_agent-5_max: 149
    cleaning_beam_agent-5_mean: 80.99
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.14
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.34
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-05-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 866.0
  episode_reward_mean: 557.85
  episode_reward_min: 136.0
  episodes_this_iter: 96
  episodes_total: 6816
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12834.695
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008406720007769763
        entropy: 0.8862108588218689
        entropy_coeff: 0.0017600000137463212
        kl: 0.013246540911495686
        model: {}
        policy_loss: -0.019680244848132133
        total_loss: -0.01873615011572838
        vf_explained_var: 0.024827346205711365
        vf_loss: 21.72658920288086
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008406720007769763
        entropy: 0.8834468126296997
        entropy_coeff: 0.0017600000137463212
        kl: 0.010997693054378033
        model: {}
        policy_loss: -0.0184096097946167
        total_loss: -0.01770184189081192
        vf_explained_var: 0.07687625288963318
        vf_loss: 20.56426239013672
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008406720007769763
        entropy: 1.013413667678833
        entropy_coeff: 0.0017600000137463212
        kl: 0.013284258544445038
        model: {}
        policy_loss: -0.020711995661258698
        total_loss: -0.019664490595459938
        vf_explained_var: 0.029035240411758423
        vf_loss: 21.66895866394043
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008406720007769763
        entropy: 1.0828423500061035
        entropy_coeff: 0.0017600000137463212
        kl: 0.013665670529007912
        model: {}
        policy_loss: -0.01585868000984192
        total_loss: -0.014982317574322224
        vf_explained_var: -0.004136368632316589
        vf_loss: 22.69700813293457
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008406720007769763
        entropy: 1.176546573638916
        entropy_coeff: 0.0017600000137463212
        kl: 0.00994520727545023
        model: {}
        policy_loss: -0.010793725959956646
        total_loss: -0.01052914373576641
        vf_explained_var: 0.004384160041809082
        vf_loss: 22.468029022216797
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008406720007769763
        entropy: 0.7907228469848633
        entropy_coeff: 0.0017600000137463212
        kl: 0.012553094886243343
        model: {}
        policy_loss: -0.018923070281744003
        total_loss: -0.017851268872618675
        vf_explained_var: 0.13418032228946686
        vf_loss: 19.927343368530273
    load_time_ms: 13308.777
    num_steps_sampled: 6816000
    num_steps_trained: 6816000
    sample_time_ms: 106125.079
    update_time_ms: 16.46
  iterations_since_restore: 71
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.333333333333332
    ram_util_percent: 11.611640211640214
  pid: 28570
  policy_reward_max:
    agent-0: 134.5
    agent-1: 134.5
    agent-2: 152.5
    agent-3: 152.5
    agent-4: 166.5
    agent-5: 166.5
  policy_reward_mean:
    agent-0: 89.02
    agent-1: 89.02
    agent-2: 99.215
    agent-3: 99.215
    agent-4: 90.69
    agent-5: 90.69
  policy_reward_min:
    agent-0: 22.5
    agent-1: 22.5
    agent-2: 26.5
    agent-3: 26.5
    agent-4: 19.0
    agent-5: 19.0
  sampler_perf:
    mean_env_wait_ms: 26.42712574873981
    mean_inference_ms: 13.158822612922886
    mean_processing_ms: 59.05012593585263
  time_since_restore: 9389.499454259872
  time_this_iter_s: 133.06925582885742
  time_total_s: 9389.499454259872
  timestamp: 1637517922
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 6816000
  training_iteration: 71
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     71 |           9389.5 | 6816000 |   557.85 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 5.59
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 23.14
    apples_agent-1_min: 6
    apples_agent-2_max: 252
    apples_agent-2_mean: 144.16
    apples_agent-2_min: 50
    apples_agent-3_max: 68
    apples_agent-3_mean: 17.54
    apples_agent-3_min: 1
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 297
    apples_agent-5_mean: 157.97
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 624
    cleaning_beam_agent-0_mean: 453.61
    cleaning_beam_agent-0_min: 216
    cleaning_beam_agent-1_max: 27
    cleaning_beam_agent-1_mean: 13.5
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 52.14
    cleaning_beam_agent-2_min: 29
    cleaning_beam_agent-3_max: 388
    cleaning_beam_agent-3_mean: 242.87
    cleaning_beam_agent-3_min: 138
    cleaning_beam_agent-4_max: 644
    cleaning_beam_agent-4_mean: 560.4
    cleaning_beam_agent-4_min: 454
    cleaning_beam_agent-5_max: 168
    cleaning_beam_agent-5_mean: 88.6
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.11
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.42
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-07-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 898.0
  episode_reward_mean: 548.86
  episode_reward_min: 169.0
  episodes_this_iter: 96
  episodes_total: 6912
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12829.867
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008346816175617278
        entropy: 0.9185231924057007
        entropy_coeff: 0.0017600000137463212
        kl: 0.014267179183661938
        model: {}
        policy_loss: -0.02065730094909668
        total_loss: -0.019616879522800446
        vf_explained_var: 0.04877611994743347
        vf_loss: 23.00338363647461
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008346816175617278
        entropy: 0.8917772769927979
        entropy_coeff: 0.0017600000137463212
        kl: 0.012557228095829487
        model: {}
        policy_loss: -0.017766207456588745
        total_loss: -0.01695994846522808
        vf_explained_var: 0.11982828378677368
        vf_loss: 21.403406143188477
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008346816175617278
        entropy: 0.9900150299072266
        entropy_coeff: 0.0017600000137463212
        kl: 0.012834051623940468
        model: {}
        policy_loss: -0.017924828454852104
        total_loss: -0.016613444313406944
        vf_explained_var: 0.03605648875236511
        vf_loss: 24.121068954467773
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008346816175617278
        entropy: 1.063119649887085
        entropy_coeff: 0.0017600000137463212
        kl: 0.012358114123344421
        model: {}
        policy_loss: -0.017689663916826248
        total_loss: -0.016520511358976364
        vf_explained_var: -0.01335442066192627
        vf_loss: 25.76812744140625
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008346816175617278
        entropy: 1.160068392753601
        entropy_coeff: 0.0017600000137463212
        kl: 0.009912164881825447
        model: {}
        policy_loss: -0.01025906391441822
        total_loss: -0.009777155704796314
        vf_explained_var: 0.004690125584602356
        vf_loss: 24.354188919067383
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008346816175617278
        entropy: 0.8017896413803101
        entropy_coeff: 0.0017600000137463212
        kl: 0.011686696670949459
        model: {}
        policy_loss: -0.018961602821946144
        total_loss: -0.01782246120274067
        vf_explained_var: 0.13897141814231873
        vf_loss: 21.120372772216797
    load_time_ms: 13298.724
    num_steps_sampled: 6912000
    num_steps_trained: 6912000
    sample_time_ms: 106163.561
    update_time_ms: 16.332
  iterations_since_restore: 72
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.412299465240643
    ram_util_percent: 11.605347593582891
  pid: 28570
  policy_reward_max:
    agent-0: 163.0
    agent-1: 163.0
    agent-2: 159.0
    agent-3: 159.0
    agent-4: 161.5
    agent-5: 161.5
  policy_reward_mean:
    agent-0: 90.38
    agent-1: 90.38
    agent-2: 98.62
    agent-3: 98.62
    agent-4: 85.43
    agent-5: 85.43
  policy_reward_min:
    agent-0: 26.0
    agent-1: 26.0
    agent-2: 32.0
    agent-3: 32.0
    agent-4: 21.0
    agent-5: 21.0
  sampler_perf:
    mean_env_wait_ms: 26.450135135639947
    mean_inference_ms: 13.156387668202662
    mean_processing_ms: 59.03536685165467
  time_since_restore: 9520.878253221512
  time_this_iter_s: 131.3787989616394
  time_total_s: 9520.878253221512
  timestamp: 1637518053
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 6912000
  training_iteration: 72
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     72 |          9520.88 | 6912000 |   548.86 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 4.92
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 25.09
    apples_agent-1_min: 10
    apples_agent-2_max: 244
    apples_agent-2_mean: 147.45
    apples_agent-2_min: 61
    apples_agent-3_max: 79
    apples_agent-3_mean: 16.68
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.55
    apples_agent-4_min: 0
    apples_agent-5_max: 337
    apples_agent-5_mean: 170.19
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 595
    cleaning_beam_agent-0_mean: 459.11
    cleaning_beam_agent-0_min: 183
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 13.31
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 54.71
    cleaning_beam_agent-2_min: 24
    cleaning_beam_agent-3_max: 456
    cleaning_beam_agent-3_mean: 295.31
    cleaning_beam_agent-3_min: 172
    cleaning_beam_agent-4_max: 666
    cleaning_beam_agent-4_mean: 542.34
    cleaning_beam_agent-4_min: 431
    cleaning_beam_agent-5_max: 205
    cleaning_beam_agent-5_mean: 72.85
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.1
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.38
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-09-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1024.0
  episode_reward_mean: 576.2
  episode_reward_min: 179.0
  episodes_this_iter: 96
  episodes_total: 7008
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12824.807
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008286911761388183
        entropy: 0.9049913883209229
        entropy_coeff: 0.0017600000137463212
        kl: 0.013641683384776115
        model: {}
        policy_loss: -0.02044285461306572
        total_loss: -0.019469929859042168
        vf_explained_var: 0.0566643625497818
        vf_loss: 22.246719360351562
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008286911761388183
        entropy: 0.8717542886734009
        entropy_coeff: 0.0017600000137463212
        kl: 0.013241018168628216
        model: {}
        policy_loss: -0.015506277792155743
        total_loss: -0.0146959712728858
        vf_explained_var: 0.11175094544887543
        vf_loss: 20.96323013305664
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008286911761388183
        entropy: 0.9714058637619019
        entropy_coeff: 0.0017600000137463212
        kl: 0.012687386013567448
        model: {}
        policy_loss: -0.019621558487415314
        total_loss: -0.018058618530631065
        vf_explained_var: 0.03887595236301422
        vf_loss: 26.382463455200195
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008286911761388183
        entropy: 1.087031364440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.011901522055268288
        model: {}
        policy_loss: -0.01732308231294155
        total_loss: -0.016018280759453773
        vf_explained_var: -0.007803216576576233
        vf_loss: 27.71670913696289
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008286911761388183
        entropy: 1.2025012969970703
        entropy_coeff: 0.0017600000137463212
        kl: 0.010190209373831749
        model: {}
        policy_loss: -0.011141424998641014
        total_loss: -0.01057581976056099
        vf_explained_var: 0.008047118782997131
        vf_loss: 25.913267135620117
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008286911761388183
        entropy: 0.7332887053489685
        entropy_coeff: 0.0017600000137463212
        kl: 0.01091205608099699
        model: {}
        policy_loss: -0.018601447343826294
        total_loss: -0.01726953685283661
        vf_explained_var: 0.14730887115001678
        vf_loss: 22.13296890258789
    load_time_ms: 13295.014
    num_steps_sampled: 7008000
    num_steps_trained: 7008000
    sample_time_ms: 106101.551
    update_time_ms: 16.269
  iterations_since_restore: 73
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.37754010695187
    ram_util_percent: 11.59732620320856
  pid: 28570
  policy_reward_max:
    agent-0: 155.5
    agent-1: 155.5
    agent-2: 171.0
    agent-3: 171.0
    agent-4: 185.5
    agent-5: 185.5
  policy_reward_mean:
    agent-0: 94.05
    agent-1: 94.05
    agent-2: 101.685
    agent-3: 101.685
    agent-4: 92.365
    agent-5: 92.365
  policy_reward_min:
    agent-0: 26.0
    agent-1: 26.0
    agent-2: 17.5
    agent-3: 17.5
    agent-4: 32.5
    agent-5: 32.5
  sampler_perf:
    mean_env_wait_ms: 26.47056918371254
    mean_inference_ms: 13.154790660805359
    mean_processing_ms: 59.02048395687146
  time_since_restore: 9652.917655944824
  time_this_iter_s: 132.03940272331238
  time_total_s: 9652.917655944824
  timestamp: 1637518185
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 7008000
  training_iteration: 73
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     73 |          9652.92 | 7008000 |    576.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 24.87
    apples_agent-1_min: 5
    apples_agent-2_max: 246
    apples_agent-2_mean: 158.46
    apples_agent-2_min: 45
    apples_agent-3_max: 65
    apples_agent-3_mean: 16.44
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 310
    apples_agent-5_mean: 171.16
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 605
    cleaning_beam_agent-0_mean: 471.1
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 30
    cleaning_beam_agent-1_mean: 12.34
    cleaning_beam_agent-1_min: 5
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 51.82
    cleaning_beam_agent-2_min: 32
    cleaning_beam_agent-3_max: 458
    cleaning_beam_agent-3_mean: 269.02
    cleaning_beam_agent-3_min: 120
    cleaning_beam_agent-4_max: 660
    cleaning_beam_agent-4_mean: 555.43
    cleaning_beam_agent-4_min: 431
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 87.5
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.22
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-11-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 919.0
  episode_reward_mean: 590.08
  episode_reward_min: 197.0
  episodes_this_iter: 96
  episodes_total: 7104
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12853.922
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008227007929235697
        entropy: 0.908771812915802
        entropy_coeff: 0.0017600000137463212
        kl: 0.013003182597458363
        model: {}
        policy_loss: -0.021003976464271545
        total_loss: -0.019586192443966866
        vf_explained_var: 0.03807425498962402
        vf_loss: 26.92142105102539
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008227007929235697
        entropy: 0.8588143587112427
        entropy_coeff: 0.0017600000137463212
        kl: 0.011687088757753372
        model: {}
        policy_loss: -0.01670209690928459
        total_loss: -0.015550745651125908
        vf_explained_var: 0.12122201919555664
        vf_loss: 24.437328338623047
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008227007929235697
        entropy: 0.9288324117660522
        entropy_coeff: 0.0017600000137463212
        kl: 0.01091316994279623
        model: {}
        policy_loss: -0.0199719425290823
        total_loss: -0.01835361123085022
        vf_explained_var: 0.03611697256565094
        vf_loss: 27.07415199279785
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008227007929235697
        entropy: 1.0848650932312012
        entropy_coeff: 0.0017600000137463212
        kl: 0.014538610354065895
        model: {}
        policy_loss: -0.016041172668337822
        total_loss: -0.01449771411716938
        vf_explained_var: -0.019943326711654663
        vf_loss: 29.07622528076172
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008227007929235697
        entropy: 1.1911808252334595
        entropy_coeff: 0.0017600000137463212
        kl: 0.008781869895756245
        model: {}
        policy_loss: -0.01139122899621725
        total_loss: -0.01088642980903387
        vf_explained_var: 0.0053057074546813965
        vf_loss: 25.231321334838867
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008227007929235697
        entropy: 0.7586359977722168
        entropy_coeff: 0.0017600000137463212
        kl: 0.01068063173443079
        model: {}
        policy_loss: -0.019114533439278603
        total_loss: -0.017827918753027916
        vf_explained_var: 0.13314896821975708
        vf_loss: 22.212902069091797
    load_time_ms: 13293.395
    num_steps_sampled: 7104000
    num_steps_trained: 7104000
    sample_time_ms: 106125.1
    update_time_ms: 16.294
  iterations_since_restore: 74
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.432978723404258
    ram_util_percent: 11.684574468085106
  pid: 28570
  policy_reward_max:
    agent-0: 158.0
    agent-1: 158.0
    agent-2: 166.0
    agent-3: 166.0
    agent-4: 167.0
    agent-5: 167.0
  policy_reward_mean:
    agent-0: 95.22
    agent-1: 95.22
    agent-2: 106.885
    agent-3: 106.885
    agent-4: 92.935
    agent-5: 92.935
  policy_reward_min:
    agent-0: 17.0
    agent-1: 17.0
    agent-2: 41.0
    agent-3: 41.0
    agent-4: 25.5
    agent-5: 25.5
  sampler_perf:
    mean_env_wait_ms: 26.494324157495402
    mean_inference_ms: 13.153336429697642
    mean_processing_ms: 59.00778151132756
  time_since_restore: 9784.751489162445
  time_this_iter_s: 131.83383321762085
  time_total_s: 9784.751489162445
  timestamp: 1637518317
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 7104000
  training_iteration: 74
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     74 |          9784.75 | 7104000 |   590.08 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 4.38
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 25.78
    apples_agent-1_min: 10
    apples_agent-2_max: 267
    apples_agent-2_mean: 147.18
    apples_agent-2_min: 63
    apples_agent-3_max: 94
    apples_agent-3_mean: 15.67
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 275
    apples_agent-5_mean: 164.79
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 657
    cleaning_beam_agent-0_mean: 492.96
    cleaning_beam_agent-0_min: 314
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 11.4
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 131
    cleaning_beam_agent-2_mean: 52.2
    cleaning_beam_agent-2_min: 22
    cleaning_beam_agent-3_max: 479
    cleaning_beam_agent-3_mean: 284.43
    cleaning_beam_agent-3_min: 158
    cleaning_beam_agent-4_max: 644
    cleaning_beam_agent-4_mean: 544.32
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 80.22
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.14
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.16
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-14-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 860.0
  episode_reward_mean: 573.81
  episode_reward_min: 292.0
  episodes_this_iter: 96
  episodes_total: 7200
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12846.382
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008167104097083211
        entropy: 0.895251989364624
        entropy_coeff: 0.0017600000137463212
        kl: 0.012718958780169487
        model: {}
        policy_loss: -0.020116014406085014
        total_loss: -0.019211892038583755
        vf_explained_var: 0.03936213254928589
        vf_loss: 21.617950439453125
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008167104097083211
        entropy: 0.852831244468689
        entropy_coeff: 0.0017600000137463212
        kl: 0.010810027830302715
        model: {}
        policy_loss: -0.017534807324409485
        total_loss: -0.01686994731426239
        vf_explained_var: 0.11359855532646179
        vf_loss: 19.63153076171875
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008167104097083211
        entropy: 0.9696909189224243
        entropy_coeff: 0.0017600000137463212
        kl: 0.013593491166830063
        model: {}
        policy_loss: -0.020539863035082817
        total_loss: -0.01937202177941799
        vf_explained_var: 0.03974568843841553
        vf_loss: 21.948198318481445
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008167104097083211
        entropy: 1.0609357357025146
        entropy_coeff: 0.0017600000137463212
        kl: 0.012105610221624374
        model: {}
        policy_loss: -0.01914379373192787
        total_loss: -0.018239952623844147
        vf_explained_var: -0.013232290744781494
        vf_loss: 23.171268463134766
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008167104097083211
        entropy: 1.2119414806365967
        entropy_coeff: 0.0017600000137463212
        kl: 0.010203513316810131
        model: {}
        policy_loss: -0.011939320713281631
        total_loss: -0.01171145774424076
        vf_explained_var: 0.0064811259508132935
        vf_loss: 22.700790405273438
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008167104097083211
        entropy: 0.75534987449646
        entropy_coeff: 0.0017600000137463212
        kl: 0.011573090218007565
        model: {}
        policy_loss: -0.019744710996747017
        total_loss: -0.01866004429757595
        vf_explained_var: 0.1351773589849472
        vf_loss: 19.8008975982666
    load_time_ms: 13270.925
    num_steps_sampled: 7200000
    num_steps_trained: 7200000
    sample_time_ms: 106116.35
    update_time_ms: 16.429
  iterations_since_restore: 75
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.296296296296298
    ram_util_percent: 11.612698412698418
  pid: 28570
  policy_reward_max:
    agent-0: 158.0
    agent-1: 158.0
    agent-2: 148.0
    agent-3: 148.0
    agent-4: 146.5
    agent-5: 146.5
  policy_reward_mean:
    agent-0: 96.54
    agent-1: 96.54
    agent-2: 100.65
    agent-3: 100.65
    agent-4: 89.715
    agent-5: 89.715
  policy_reward_min:
    agent-0: 37.5
    agent-1: 37.5
    agent-2: 40.5
    agent-3: 40.5
    agent-4: 30.0
    agent-5: 30.0
  sampler_perf:
    mean_env_wait_ms: 26.515562533972787
    mean_inference_ms: 13.151324548355257
    mean_processing_ms: 58.99443928983902
  time_since_restore: 9917.315304756165
  time_this_iter_s: 132.56381559371948
  time_total_s: 9917.315304756165
  timestamp: 1637518450
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 7200000
  training_iteration: 75
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     75 |          9917.32 | 7200000 |   573.81 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 26.74
    apples_agent-1_min: 7
    apples_agent-2_max: 226
    apples_agent-2_mean: 148.95
    apples_agent-2_min: 72
    apples_agent-3_max: 69
    apples_agent-3_mean: 18.77
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.26
    apples_agent-4_min: 0
    apples_agent-5_max: 260
    apples_agent-5_mean: 167.41
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 706
    cleaning_beam_agent-0_mean: 536.56
    cleaning_beam_agent-0_min: 360
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 11.61
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 175
    cleaning_beam_agent-2_mean: 46.2
    cleaning_beam_agent-2_min: 20
    cleaning_beam_agent-3_max: 489
    cleaning_beam_agent-3_mean: 291.64
    cleaning_beam_agent-3_min: 185
    cleaning_beam_agent-4_max: 654
    cleaning_beam_agent-4_mean: 578.35
    cleaning_beam_agent-4_min: 501
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 70.29
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-16-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 888.0
  episode_reward_mean: 588.82
  episode_reward_min: 287.0
  episodes_this_iter: 96
  episodes_total: 7296
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12834.554
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0008107200264930725
        entropy: 0.844668447971344
        entropy_coeff: 0.0017600000137463212
        kl: 0.01262191217392683
        model: {}
        policy_loss: -0.020914867520332336
        total_loss: -0.019942941144108772
        vf_explained_var: 0.02106650173664093
        vf_loss: 21.429948806762695
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0008107200264930725
        entropy: 0.8319588899612427
        entropy_coeff: 0.0017600000137463212
        kl: 0.010455821640789509
        model: {}
        policy_loss: -0.017770608887076378
        total_loss: -0.01710621453821659
        vf_explained_var: 0.11428287625312805
        vf_loss: 19.32596206665039
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0008107200264930725
        entropy: 0.9488083124160767
        entropy_coeff: 0.0017600000137463212
        kl: 0.012036693282425404
        model: {}
        policy_loss: -0.021593747660517693
        total_loss: -0.020125096663832664
        vf_explained_var: 0.03666248917579651
        vf_loss: 25.3671875
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008107200264930725
        entropy: 1.0629750490188599
        entropy_coeff: 0.0017600000137463212
        kl: 0.011944727972149849
        model: {}
        policy_loss: -0.0192810520529747
        total_loss: -0.018113339319825172
        vf_explained_var: 0.013101786375045776
        vf_loss: 25.906213760375977
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0008107200264930725
        entropy: 1.1795339584350586
        entropy_coeff: 0.0017600000137463212
        kl: 0.010070215910673141
        model: {}
        policy_loss: -0.012305825017392635
        total_loss: -0.012156227603554726
        vf_explained_var: 0.009045213460922241
        vf_loss: 21.359649658203125
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0008107200264930725
        entropy: 0.7309311032295227
        entropy_coeff: 0.0017600000137463212
        kl: 0.011112199164927006
        model: {}
        policy_loss: -0.019399121403694153
        total_loss: -0.01838712766766548
        vf_explained_var: 0.12275122106075287
        vf_loss: 18.81729507446289
    load_time_ms: 13262.124
    num_steps_sampled: 7296000
    num_steps_trained: 7296000
    sample_time_ms: 106006.047
    update_time_ms: 16.453
  iterations_since_restore: 76
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.378191489361704
    ram_util_percent: 11.610638297872342
  pid: 28570
  policy_reward_max:
    agent-0: 142.0
    agent-1: 142.0
    agent-2: 163.0
    agent-3: 163.0
    agent-4: 150.0
    agent-5: 150.0
  policy_reward_mean:
    agent-0: 97.78
    agent-1: 97.78
    agent-2: 104.575
    agent-3: 104.575
    agent-4: 92.055
    agent-5: 92.055
  policy_reward_min:
    agent-0: 47.5
    agent-1: 47.5
    agent-2: 25.5
    agent-3: 25.5
    agent-4: 41.0
    agent-5: 41.0
  sampler_perf:
    mean_env_wait_ms: 26.5414811924148
    mean_inference_ms: 13.148891122142908
    mean_processing_ms: 58.981140626411815
  time_since_restore: 10049.25778579712
  time_this_iter_s: 131.9424810409546
  time_total_s: 10049.25778579712
  timestamp: 1637518582
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 7296000
  training_iteration: 76
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     76 |          10049.3 | 7296000 |   588.82 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 4.99
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 22.4
    apples_agent-1_min: 8
    apples_agent-2_max: 257
    apples_agent-2_mean: 153.82
    apples_agent-2_min: 65
    apples_agent-3_max: 89
    apples_agent-3_mean: 18.3
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 311
    apples_agent-5_mean: 175.64
    apples_agent-5_min: 71
    cleaning_beam_agent-0_max: 707
    cleaning_beam_agent-0_mean: 514.69
    cleaning_beam_agent-0_min: 269
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 12.01
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 84
    cleaning_beam_agent-2_mean: 48.56
    cleaning_beam_agent-2_min: 23
    cleaning_beam_agent-3_max: 468
    cleaning_beam_agent-3_mean: 295.25
    cleaning_beam_agent-3_min: 187
    cleaning_beam_agent-4_max: 679
    cleaning_beam_agent-4_mean: 590.71
    cleaning_beam_agent-4_min: 525
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 77.75
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.11
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.14
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-18-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 932.0
  episode_reward_mean: 603.74
  episode_reward_min: 208.0
  episodes_this_iter: 96
  episodes_total: 7392
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12816.214
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000804729585070163
        entropy: 0.8639817237854004
        entropy_coeff: 0.0017600000137463212
        kl: 0.014232903718948364
        model: {}
        policy_loss: -0.01750967837870121
        total_loss: -0.016418926417827606
        vf_explained_var: 0.057329416275024414
        vf_loss: 22.555383682250977
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000804729585070163
        entropy: 0.8333614468574524
        entropy_coeff: 0.0017600000137463212
        kl: 0.011919500306248665
        model: {}
        policy_loss: -0.017517298460006714
        total_loss: -0.016629796475172043
        vf_explained_var: 0.11591385304927826
        vf_loss: 21.30723762512207
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000804729585070163
        entropy: 0.9456597566604614
        entropy_coeff: 0.0017600000137463212
        kl: 0.013531532138586044
        model: {}
        policy_loss: -0.02138870768249035
        total_loss: -0.019962307065725327
        vf_explained_var: 0.039887890219688416
        vf_loss: 24.141878128051758
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000804729585070163
        entropy: 1.0708705186843872
        entropy_coeff: 0.0017600000137463212
        kl: 0.013000858016312122
        model: {}
        policy_loss: -0.019533053040504456
        total_loss: -0.01838671788573265
        vf_explained_var: -0.005173772573471069
        vf_loss: 25.43531608581543
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000804729585070163
        entropy: 1.1062575578689575
        entropy_coeff: 0.0017600000137463212
        kl: 0.010377288796007633
        model: {}
        policy_loss: -0.012273426167666912
        total_loss: -0.011469024233520031
        vf_explained_var: 0.0002186596393585205
        vf_loss: 26.590667724609375
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000804729585070163
        entropy: 0.7438554763793945
        entropy_coeff: 0.0017600000137463212
        kl: 0.010238992050290108
        model: {}
        policy_loss: -0.01991770975291729
        total_loss: -0.01849478855729103
        vf_explained_var: 0.12208639085292816
        vf_loss: 23.481473922729492
    load_time_ms: 13258.037
    num_steps_sampled: 7392000
    num_steps_trained: 7392000
    sample_time_ms: 106089.693
    update_time_ms: 16.482
  iterations_since_restore: 77
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.373936170212765
    ram_util_percent: 11.597872340425535
  pid: 28570
  policy_reward_max:
    agent-0: 150.0
    agent-1: 150.0
    agent-2: 165.5
    agent-3: 165.5
    agent-4: 175.0
    agent-5: 175.0
  policy_reward_mean:
    agent-0: 96.93
    agent-1: 96.93
    agent-2: 108.295
    agent-3: 108.295
    agent-4: 96.645
    agent-5: 96.645
  policy_reward_min:
    agent-0: 24.5
    agent-1: 24.5
    agent-2: 45.0
    agent-3: 45.0
    agent-4: 21.0
    agent-5: 21.0
  sampler_perf:
    mean_env_wait_ms: 26.568500782149208
    mean_inference_ms: 13.146838214705154
    mean_processing_ms: 58.96447346801742
  time_since_restore: 10181.540412664413
  time_this_iter_s: 132.2826268672943
  time_total_s: 10181.540412664413
  timestamp: 1637518714
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 7392000
  training_iteration: 77
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     77 |          10181.5 | 7392000 |   603.74 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.99
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 25.39
    apples_agent-1_min: 10
    apples_agent-2_max: 259
    apples_agent-2_mean: 154.87
    apples_agent-2_min: 45
    apples_agent-3_max: 55
    apples_agent-3_mean: 17.1
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.2
    apples_agent-4_min: 0
    apples_agent-5_max: 290
    apples_agent-5_mean: 178.58
    apples_agent-5_min: 66
    cleaning_beam_agent-0_max: 733
    cleaning_beam_agent-0_mean: 558.25
    cleaning_beam_agent-0_min: 404
    cleaning_beam_agent-1_max: 35
    cleaning_beam_agent-1_mean: 13.22
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 263
    cleaning_beam_agent-2_mean: 50.38
    cleaning_beam_agent-2_min: 24
    cleaning_beam_agent-3_max: 367
    cleaning_beam_agent-3_mean: 260.76
    cleaning_beam_agent-3_min: 166
    cleaning_beam_agent-4_max: 643
    cleaning_beam_agent-4_mean: 573.74
    cleaning_beam_agent-4_min: 494
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 83.63
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.08
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.18
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.19
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.22
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-20-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 980.0
  episode_reward_mean: 611.94
  episode_reward_min: 309.0
  episodes_this_iter: 96
  episodes_total: 7488
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12805.96
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007987392018549144
        entropy: 0.8123700618743896
        entropy_coeff: 0.0017600000137463212
        kl: 0.011648541316390038
        model: {}
        policy_loss: -0.01776801235973835
        total_loss: -0.016440533101558685
        vf_explained_var: 0.04502628743648529
        vf_loss: 24.660430908203125
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007987392018549144
        entropy: 0.814702033996582
        entropy_coeff: 0.0017600000137463212
        kl: 0.013136541470885277
        model: {}
        policy_loss: -0.018325336277484894
        total_loss: -0.01718471385538578
        vf_explained_var: 0.11072339117527008
        vf_loss: 23.281917572021484
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007987392018549144
        entropy: 0.9439047574996948
        entropy_coeff: 0.0017600000137463212
        kl: 0.014498277567327023
        model: {}
        policy_loss: -0.021618569269776344
        total_loss: -0.019945679232478142
        vf_explained_var: 0.05435873568058014
        vf_loss: 26.092466354370117
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007987392018549144
        entropy: 1.0957919359207153
        entropy_coeff: 0.0017600000137463212
        kl: 0.013689624145627022
        model: {}
        policy_loss: -0.02013198286294937
        total_loss: -0.01876429095864296
        vf_explained_var: 0.004604458808898926
        vf_loss: 27.8292236328125
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007987392018549144
        entropy: 1.0736361742019653
        entropy_coeff: 0.0017600000137463212
        kl: 0.010150181129574776
        model: {}
        policy_loss: -0.01167669054120779
        total_loss: -0.010719691403210163
        vf_explained_var: 0.007928892970085144
        vf_loss: 27.562721252441406
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007987392018549144
        entropy: 0.7458442449569702
        entropy_coeff: 0.0017600000137463212
        kl: 0.011977190151810646
        model: {}
        policy_loss: -0.019290218129754066
        total_loss: -0.017825033515691757
        vf_explained_var: 0.1735062152147293
        vf_loss: 23.287240982055664
    load_time_ms: 13248.551
    num_steps_sampled: 7488000
    num_steps_trained: 7488000
    sample_time_ms: 106006.385
    update_time_ms: 16.24
  iterations_since_restore: 78
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.54064171122995
    ram_util_percent: 11.686096256684493
  pid: 28570
  policy_reward_max:
    agent-0: 155.0
    agent-1: 155.0
    agent-2: 176.0
    agent-3: 176.0
    agent-4: 160.0
    agent-5: 160.0
  policy_reward_mean:
    agent-0: 98.32
    agent-1: 98.32
    agent-2: 108.585
    agent-3: 108.585
    agent-4: 99.065
    agent-5: 99.065
  policy_reward_min:
    agent-0: 39.5
    agent-1: 39.5
    agent-2: 47.0
    agent-3: 47.0
    agent-4: 38.5
    agent-5: 38.5
  sampler_perf:
    mean_env_wait_ms: 26.59401641953869
    mean_inference_ms: 13.145124221956205
    mean_processing_ms: 58.95035216761581
  time_since_restore: 10313.160945653915
  time_this_iter_s: 131.62053298950195
  time_total_s: 10313.160945653915
  timestamp: 1637518846
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 7488000
  training_iteration: 78
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     78 |          10313.2 | 7488000 |   611.94 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 23.9
    apples_agent-1_min: 9
    apples_agent-2_max: 265
    apples_agent-2_mean: 160.96
    apples_agent-2_min: 83
    apples_agent-3_max: 47
    apples_agent-3_mean: 14.69
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 308
    apples_agent-5_mean: 176.61
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 725
    cleaning_beam_agent-0_mean: 562.19
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 34
    cleaning_beam_agent-1_mean: 15.79
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 51.92
    cleaning_beam_agent-2_min: 24
    cleaning_beam_agent-3_max: 336
    cleaning_beam_agent-3_mean: 242.88
    cleaning_beam_agent-3_min: 175
    cleaning_beam_agent-4_max: 621
    cleaning_beam_agent-4_mean: 544.66
    cleaning_beam_agent-4_min: 448
    cleaning_beam_agent-5_max: 157
    cleaning_beam_agent-5_mean: 79.28
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.15
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.18
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.26
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-22-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 930.0
  episode_reward_mean: 616.51
  episode_reward_min: 186.0
  episodes_this_iter: 96
  episodes_total: 7584
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12796.906
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007927488186396658
        entropy: 0.7971959114074707
        entropy_coeff: 0.0017600000137463212
        kl: 0.01112351007759571
        model: {}
        policy_loss: -0.019557978957891464
        total_loss: -0.018462926149368286
        vf_explained_var: 0.030082598328590393
        vf_loss: 22.20026397705078
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007927488186396658
        entropy: 0.818493664264679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0116630420088768
        model: {}
        policy_loss: -0.017876002937555313
        total_loss: -0.016958389431238174
        vf_explained_var: 0.08737654983997345
        vf_loss: 21.394798278808594
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007927488186396658
        entropy: 0.951525092124939
        entropy_coeff: 0.0017600000137463212
        kl: 0.014634493738412857
        model: {}
        policy_loss: -0.01961243897676468
        total_loss: -0.018165871500968933
        vf_explained_var: 0.043641313910484314
        vf_loss: 23.895261764526367
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007927488186396658
        entropy: 1.1046850681304932
        entropy_coeff: 0.0017600000137463212
        kl: 0.013397537171840668
        model: {}
        policy_loss: -0.020553285256028175
        total_loss: -0.01939501240849495
        vf_explained_var: -0.04177141189575195
        vf_loss: 26.00111198425293
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007927488186396658
        entropy: 1.0919344425201416
        entropy_coeff: 0.0017600000137463212
        kl: 0.011156938970088959
        model: {}
        policy_loss: -0.012376464903354645
        total_loss: -0.011603763327002525
        vf_explained_var: 0.016148418188095093
        vf_loss: 25.952245712280273
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007927488186396658
        entropy: 0.7586840987205505
        entropy_coeff: 0.0017600000137463212
        kl: 0.011200441047549248
        model: {}
        policy_loss: -0.019891008734703064
        total_loss: -0.01853029802441597
        vf_explained_var: 0.14752034842967987
        vf_loss: 22.759777069091797
    load_time_ms: 13246.731
    num_steps_sampled: 7584000
    num_steps_trained: 7584000
    sample_time_ms: 106081.801
    update_time_ms: 15.922
  iterations_since_restore: 79
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.38677248677249
    ram_util_percent: 11.552380952380952
  pid: 28570
  policy_reward_max:
    agent-0: 157.0
    agent-1: 157.0
    agent-2: 184.0
    agent-3: 184.0
    agent-4: 165.0
    agent-5: 165.0
  policy_reward_mean:
    agent-0: 100.19
    agent-1: 100.19
    agent-2: 110.47
    agent-3: 110.47
    agent-4: 97.595
    agent-5: 97.595
  policy_reward_min:
    agent-0: 13.5
    agent-1: 13.5
    agent-2: 36.0
    agent-3: 36.0
    agent-4: -3.5
    agent-5: -3.5
  sampler_perf:
    mean_env_wait_ms: 26.616080969020686
    mean_inference_ms: 13.143590413642814
    mean_processing_ms: 58.93730101824405
  time_since_restore: 10445.782284498215
  time_this_iter_s: 132.62133884429932
  time_total_s: 10445.782284498215
  timestamp: 1637518979
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 7584000
  training_iteration: 79
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     79 |          10445.8 | 7584000 |   616.51 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 4.68
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 25.26
    apples_agent-1_min: 8
    apples_agent-2_max: 272
    apples_agent-2_mean: 155.45
    apples_agent-2_min: 65
    apples_agent-3_max: 74
    apples_agent-3_mean: 16.66
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.44
    apples_agent-4_min: 0
    apples_agent-5_max: 280
    apples_agent-5_mean: 182.51
    apples_agent-5_min: 82
    cleaning_beam_agent-0_max: 743
    cleaning_beam_agent-0_mean: 555.58
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 31
    cleaning_beam_agent-1_mean: 15.44
    cleaning_beam_agent-1_min: 5
    cleaning_beam_agent-2_max: 117
    cleaning_beam_agent-2_mean: 55.32
    cleaning_beam_agent-2_min: 24
    cleaning_beam_agent-3_max: 368
    cleaning_beam_agent-3_mean: 262.62
    cleaning_beam_agent-3_min: 171
    cleaning_beam_agent-4_max: 632
    cleaning_beam_agent-4_mean: 532.01
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 79.75
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.13
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-25-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 882.0
  episode_reward_mean: 612.73
  episode_reward_min: 168.0
  episodes_this_iter: 96
  episodes_total: 7680
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12790.841
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007867583772167563
        entropy: 0.7871550917625427
        entropy_coeff: 0.0017600000137463212
        kl: 0.012458628043532372
        model: {}
        policy_loss: -0.01862509921193123
        total_loss: -0.0171287190169096
        vf_explained_var: 0.004599213600158691
        vf_loss: 25.703060150146484
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007867583772167563
        entropy: 0.8248279094696045
        entropy_coeff: 0.0017600000137463212
        kl: 0.010710816830396652
        model: {}
        policy_loss: -0.01809590496122837
        total_loss: -0.01700838841497898
        vf_explained_var: 0.10190810263156891
        vf_loss: 23.383846282958984
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007867583772167563
        entropy: 0.9826507568359375
        entropy_coeff: 0.0017600000137463212
        kl: 0.012357557192444801
        model: {}
        policy_loss: -0.022060908377170563
        total_loss: -0.020644735544919968
        vf_explained_var: 0.057673633098602295
        vf_loss: 25.277591705322266
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007867583772167563
        entropy: 1.091099739074707
        entropy_coeff: 0.0017600000137463212
        kl: 0.012563095428049564
        model: {}
        policy_loss: -0.01996331661939621
        total_loss: -0.018724050372838974
        vf_explained_var: -0.012343913316726685
        vf_loss: 26.884891510009766
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007867583772167563
        entropy: 1.1420962810516357
        entropy_coeff: 0.0017600000137463212
        kl: 0.011096273548901081
        model: {}
        policy_loss: -0.01384095661342144
        total_loss: -0.013112334534525871
        vf_explained_var: 0.01832793653011322
        vf_loss: 26.399656295776367
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007867583772167563
        entropy: 0.7240435481071472
        entropy_coeff: 0.0017600000137463212
        kl: 0.011321010068058968
        model: {}
        policy_loss: -0.020560022443532944
        total_loss: -0.019106369465589523
        vf_explained_var: 0.14941950142383575
        vf_loss: 23.034339904785156
    load_time_ms: 13249.176
    num_steps_sampled: 7680000
    num_steps_trained: 7680000
    sample_time_ms: 106134.094
    update_time_ms: 16.094
  iterations_since_restore: 80
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.324736842105267
    ram_util_percent: 11.61473684210527
  pid: 28570
  policy_reward_max:
    agent-0: 159.0
    agent-1: 159.0
    agent-2: 171.5
    agent-3: 171.5
    agent-4: 155.0
    agent-5: 155.0
  policy_reward_mean:
    agent-0: 98.58
    agent-1: 98.58
    agent-2: 107.515
    agent-3: 107.515
    agent-4: 100.27
    agent-5: 100.27
  policy_reward_min:
    agent-0: 16.0
    agent-1: 16.0
    agent-2: 40.0
    agent-3: 40.0
    agent-4: 28.0
    agent-5: 28.0
  sampler_perf:
    mean_env_wait_ms: 26.64093276832627
    mean_inference_ms: 13.142652205715194
    mean_processing_ms: 58.93128815858294
  time_since_restore: 10579.152563333511
  time_this_iter_s: 133.37027883529663
  time_total_s: 10579.152563333511
  timestamp: 1637519112
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 7680000
  training_iteration: 80
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     80 |          10579.2 | 7680000 |   612.73 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 3.63
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 24.24
    apples_agent-1_min: 7
    apples_agent-2_max: 279
    apples_agent-2_mean: 169.65
    apples_agent-2_min: 41
    apples_agent-3_max: 53
    apples_agent-3_mean: 17.64
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 311
    apples_agent-5_mean: 200.05
    apples_agent-5_min: 94
    cleaning_beam_agent-0_max: 679
    cleaning_beam_agent-0_mean: 545.69
    cleaning_beam_agent-0_min: 389
    cleaning_beam_agent-1_max: 38
    cleaning_beam_agent-1_mean: 16.63
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 135
    cleaning_beam_agent-2_mean: 49.96
    cleaning_beam_agent-2_min: 21
    cleaning_beam_agent-3_max: 342
    cleaning_beam_agent-3_mean: 250.62
    cleaning_beam_agent-3_min: 131
    cleaning_beam_agent-4_max: 653
    cleaning_beam_agent-4_mean: 542.66
    cleaning_beam_agent-4_min: 302
    cleaning_beam_agent-5_max: 132
    cleaning_beam_agent-5_mean: 77.44
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.21
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.23
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-27-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 928.0
  episode_reward_mean: 671.24
  episode_reward_min: 364.0
  episodes_this_iter: 96
  episodes_total: 7776
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12774.598
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007807679940015078
        entropy: 0.7737237215042114
        entropy_coeff: 0.0017600000137463212
        kl: 0.011270320974290371
        model: {}
        policy_loss: -0.018467694520950317
        total_loss: -0.016865000128746033
        vf_explained_var: 0.014037981629371643
        vf_loss: 26.826927185058594
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007807679940015078
        entropy: 0.8075916171073914
        entropy_coeff: 0.0017600000137463212
        kl: 0.011803295463323593
        model: {}
        policy_loss: -0.018463393673300743
        total_loss: -0.017236284911632538
        vf_explained_var: 0.11782123148441315
        vf_loss: 24.271617889404297
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007807679940015078
        entropy: 0.9297221899032593
        entropy_coeff: 0.0017600000137463212
        kl: 0.013776732608675957
        model: {}
        policy_loss: -0.023349575698375702
        total_loss: -0.021381519734859467
        vf_explained_var: 0.05250099301338196
        vf_loss: 29.155284881591797
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007807679940015078
        entropy: 1.095048427581787
        entropy_coeff: 0.0017600000137463212
        kl: 0.01266692765057087
        model: {}
        policy_loss: -0.020902711898088455
        total_loss: -0.019237764179706573
        vf_explained_var: -0.005601093173027039
        vf_loss: 31.172222137451172
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007807679940015078
        entropy: 1.1149311065673828
        entropy_coeff: 0.0017600000137463212
        kl: 0.011168131604790688
        model: {}
        policy_loss: -0.013950493186712265
        total_loss: -0.012867797166109085
        vf_explained_var: 0.016315817832946777
        vf_loss: 29.45587730407715
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007807679940015078
        entropy: 0.6705538034439087
        entropy_coeff: 0.0017600000137463212
        kl: 0.011062528938055038
        model: {}
        policy_loss: -0.019482769072055817
        total_loss: -0.017640668898820877
        vf_explained_var: 0.14548730850219727
        vf_loss: 26.074277877807617
    load_time_ms: 13242.558
    num_steps_sampled: 7776000
    num_steps_trained: 7776000
    sample_time_ms: 106200.5
    update_time_ms: 16.172
  iterations_since_restore: 81
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.33052631578947
    ram_util_percent: 11.591578947368424
  pid: 28570
  policy_reward_max:
    agent-0: 158.0
    agent-1: 158.0
    agent-2: 170.0
    agent-3: 170.0
    agent-4: 171.5
    agent-5: 171.5
  policy_reward_mean:
    agent-0: 108.745
    agent-1: 108.745
    agent-2: 117.1
    agent-3: 117.1
    agent-4: 109.775
    agent-5: 109.775
  policy_reward_min:
    agent-0: 25.5
    agent-1: 25.5
    agent-2: 51.5
    agent-3: 51.5
    agent-4: 55.0
    agent-5: 55.0
  sampler_perf:
    mean_env_wait_ms: 26.663879590219203
    mean_inference_ms: 13.14244046808426
    mean_processing_ms: 58.924119850313254
  time_since_restore: 10712.663979530334
  time_this_iter_s: 133.51141619682312
  time_total_s: 10712.663979530334
  timestamp: 1637519246
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 7776000
  training_iteration: 81
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     81 |          10712.7 | 7776000 |   671.24 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.77
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 28.23
    apples_agent-1_min: 5
    apples_agent-2_max: 247
    apples_agent-2_mean: 164.4
    apples_agent-2_min: 51
    apples_agent-3_max: 56
    apples_agent-3_mean: 18.95
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.27
    apples_agent-4_min: 0
    apples_agent-5_max: 328
    apples_agent-5_mean: 190.71
    apples_agent-5_min: 95
    cleaning_beam_agent-0_max: 683
    cleaning_beam_agent-0_mean: 540.42
    cleaning_beam_agent-0_min: 318
    cleaning_beam_agent-1_max: 33
    cleaning_beam_agent-1_mean: 15.95
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 139
    cleaning_beam_agent-2_mean: 52.0
    cleaning_beam_agent-2_min: 18
    cleaning_beam_agent-3_max: 367
    cleaning_beam_agent-3_mean: 246.43
    cleaning_beam_agent-3_min: 159
    cleaning_beam_agent-4_max: 670
    cleaning_beam_agent-4_mean: 561.29
    cleaning_beam_agent-4_min: 422
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 69.09
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-29-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1016.0
  episode_reward_mean: 640.67
  episode_reward_min: 267.0
  episodes_this_iter: 96
  episodes_total: 7872
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12771.342
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007747776107862592
        entropy: 0.7909097075462341
        entropy_coeff: 0.0017600000137463212
        kl: 0.01279174629598856
        model: {}
        policy_loss: -0.020373478531837463
        total_loss: -0.01908857747912407
        vf_explained_var: 0.029135972261428833
        vf_loss: 23.571117401123047
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007747776107862592
        entropy: 0.8065439462661743
        entropy_coeff: 0.0017600000137463212
        kl: 0.013287462294101715
        model: {}
        policy_loss: -0.01854368858039379
        total_loss: -0.017477914690971375
        vf_explained_var: 0.07926853001117706
        vf_loss: 22.361509323120117
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007747776107862592
        entropy: 0.9309622049331665
        entropy_coeff: 0.0017600000137463212
        kl: 0.012637770734727383
        model: {}
        policy_loss: -0.0232328362762928
        total_loss: -0.021580766886472702
        vf_explained_var: 0.07343921065330505
        vf_loss: 26.586816787719727
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007747776107862592
        entropy: 1.0818631649017334
        entropy_coeff: 0.0017600000137463212
        kl: 0.01316588744521141
        model: {}
        policy_loss: -0.021214906126260757
        total_loss: -0.019716981798410416
        vf_explained_var: -0.006538987159729004
        vf_loss: 29.082780838012695
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007747776107862592
        entropy: 1.0571857690811157
        entropy_coeff: 0.0017600000137463212
        kl: 0.010816809721291065
        model: {}
        policy_loss: -0.013259027153253555
        total_loss: -0.012190954759716988
        vf_explained_var: 0.014642372727394104
        vf_loss: 28.324604034423828
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007747776107862592
        entropy: 0.6863702535629272
        entropy_coeff: 0.0017600000137463212
        kl: 0.010791930370032787
        model: {}
        policy_loss: -0.020584657788276672
        total_loss: -0.018869826570153236
        vf_explained_var: 0.12953804433345795
        vf_loss: 25.18144989013672
    load_time_ms: 13241.046
    num_steps_sampled: 7872000
    num_steps_trained: 7872000
    sample_time_ms: 106300.773
    update_time_ms: 16.689
  iterations_since_restore: 82
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.445744680851064
    ram_util_percent: 11.601595744680854
  pid: 28570
  policy_reward_max:
    agent-0: 175.0
    agent-1: 175.0
    agent-2: 160.0
    agent-3: 160.0
    agent-4: 179.5
    agent-5: 179.5
  policy_reward_mean:
    agent-0: 101.245
    agent-1: 101.245
    agent-2: 113.77
    agent-3: 113.77
    agent-4: 105.32
    agent-5: 105.32
  policy_reward_min:
    agent-0: 25.5
    agent-1: 25.5
    agent-2: 39.0
    agent-3: 39.0
    agent-4: 39.5
    agent-5: 39.5
  sampler_perf:
    mean_env_wait_ms: 26.686304809903827
    mean_inference_ms: 13.14148474453407
    mean_processing_ms: 58.91479139706829
  time_since_restore: 10845.002582073212
  time_this_iter_s: 132.3386025428772
  time_total_s: 10845.002582073212
  timestamp: 1637519378
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 7872000
  training_iteration: 82
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     82 |            10845 | 7872000 |   640.67 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.87
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 24.84
    apples_agent-1_min: 7
    apples_agent-2_max: 272
    apples_agent-2_mean: 168.17
    apples_agent-2_min: 62
    apples_agent-3_max: 60
    apples_agent-3_mean: 17.08
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.37
    apples_agent-4_min: 0
    apples_agent-5_max: 354
    apples_agent-5_mean: 192.23
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 624
    cleaning_beam_agent-0_mean: 480.84
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 48
    cleaning_beam_agent-1_mean: 20.02
    cleaning_beam_agent-1_min: 6
    cleaning_beam_agent-2_max: 138
    cleaning_beam_agent-2_mean: 54.46
    cleaning_beam_agent-2_min: 26
    cleaning_beam_agent-3_max: 417
    cleaning_beam_agent-3_mean: 247.86
    cleaning_beam_agent-3_min: 118
    cleaning_beam_agent-4_max: 637
    cleaning_beam_agent-4_mean: 546.68
    cleaning_beam_agent-4_min: 355
    cleaning_beam_agent-5_max: 203
    cleaning_beam_agent-5_mean: 80.72
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.13
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.27
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-31-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1029.0
  episode_reward_mean: 663.78
  episode_reward_min: 220.0
  episodes_this_iter: 96
  episodes_total: 7968
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12780.5
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007687872275710106
        entropy: 0.8410395383834839
        entropy_coeff: 0.0017600000137463212
        kl: 0.013176018372178078
        model: {}
        policy_loss: -0.020036540925502777
        total_loss: -0.01845768466591835
        vf_explained_var: 0.054342105984687805
        vf_loss: 27.29686737060547
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007687872275710106
        entropy: 0.797540545463562
        entropy_coeff: 0.0017600000137463212
        kl: 0.010845297947525978
        model: {}
        policy_loss: -0.0193110853433609
        total_loss: -0.017940636724233627
        vf_explained_var: 0.09447379410266876
        vf_loss: 25.70771026611328
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007687872275710106
        entropy: 0.9098483324050903
        entropy_coeff: 0.0017600000137463212
        kl: 0.01220198255032301
        model: {}
        policy_loss: -0.023668671026825905
        total_loss: -0.021871022880077362
        vf_explained_var: 0.09496235847473145
        vf_loss: 27.888824462890625
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007687872275710106
        entropy: 1.0928337574005127
        entropy_coeff: 0.0017600000137463212
        kl: 0.013083390891551971
        model: {}
        policy_loss: -0.02196888066828251
        total_loss: -0.020368386059999466
        vf_explained_var: 0.029042527079582214
        vf_loss: 30.332542419433594
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007687872275710106
        entropy: 1.0558501482009888
        entropy_coeff: 0.0017600000137463212
        kl: 0.010300952009856701
        model: {}
        policy_loss: -0.013294332660734653
        total_loss: -0.012225116603076458
        vf_explained_var: 0.02887377142906189
        vf_loss: 28.358476638793945
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007687872275710106
        entropy: 0.6886568665504456
        entropy_coeff: 0.0017600000137463212
        kl: 0.01134507730603218
        model: {}
        policy_loss: -0.02127835899591446
        total_loss: -0.019576843827962875
        vf_explained_var: 0.172532856464386
        vf_loss: 24.881128311157227
    load_time_ms: 13249.6
    num_steps_sampled: 7968000
    num_steps_trained: 7968000
    sample_time_ms: 106415.677
    update_time_ms: 16.568
  iterations_since_restore: 83
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.374210526315792
    ram_util_percent: 11.597368421052634
  pid: 28570
  policy_reward_max:
    agent-0: 175.0
    agent-1: 175.0
    agent-2: 191.5
    agent-3: 191.5
    agent-4: 187.5
    agent-5: 187.5
  policy_reward_mean:
    agent-0: 107.24
    agent-1: 107.24
    agent-2: 117.03
    agent-3: 117.03
    agent-4: 107.62
    agent-5: 107.62
  policy_reward_min:
    agent-0: 26.0
    agent-1: 26.0
    agent-2: 45.0
    agent-3: 45.0
    agent-4: 35.5
    agent-5: 35.5
  sampler_perf:
    mean_env_wait_ms: 26.70651670385339
    mean_inference_ms: 13.140228112623424
    mean_processing_ms: 58.908144576600336
  time_since_restore: 10978.368433713913
  time_this_iter_s: 133.3658516407013
  time_total_s: 10978.368433713913
  timestamp: 1637519512
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 7968000
  training_iteration: 83
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     83 |          10978.4 | 7968000 |   663.78 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 4.48
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 27.74
    apples_agent-1_min: 10
    apples_agent-2_max: 271
    apples_agent-2_mean: 175.32
    apples_agent-2_min: 62
    apples_agent-3_max: 73
    apples_agent-3_mean: 19.5
    apples_agent-3_min: 0
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 297
    apples_agent-5_mean: 194.21
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 614
    cleaning_beam_agent-0_mean: 458.49
    cleaning_beam_agent-0_min: 294
    cleaning_beam_agent-1_max: 27
    cleaning_beam_agent-1_mean: 14.08
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 138
    cleaning_beam_agent-2_mean: 49.54
    cleaning_beam_agent-2_min: 23
    cleaning_beam_agent-3_max: 384
    cleaning_beam_agent-3_mean: 257.7
    cleaning_beam_agent-3_min: 125
    cleaning_beam_agent-4_max: 671
    cleaning_beam_agent-4_mean: 590.44
    cleaning_beam_agent-4_min: 406
    cleaning_beam_agent-5_max: 199
    cleaning_beam_agent-5_mean: 86.12
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.16
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.13
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-34-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 940.0
  episode_reward_mean: 682.02
  episode_reward_min: 220.0
  episodes_this_iter: 96
  episodes_total: 8064
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12759.41
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007627967861481011
        entropy: 0.8512536287307739
        entropy_coeff: 0.0017600000137463212
        kl: 0.016986077651381493
        model: {}
        policy_loss: -0.019952910020947456
        total_loss: -0.018522370606660843
        vf_explained_var: 0.07082238793373108
        vf_loss: 25.04092788696289
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007627967861481011
        entropy: 0.7932112216949463
        entropy_coeff: 0.0017600000137463212
        kl: 0.012022408656775951
        model: {}
        policy_loss: -0.019694577902555466
        total_loss: -0.01840784028172493
        vf_explained_var: 0.0738336592912674
        vf_loss: 24.573707580566406
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007627967861481011
        entropy: 0.8892878293991089
        entropy_coeff: 0.0017600000137463212
        kl: 0.013096457347273827
        model: {}
        policy_loss: -0.024183738976716995
        total_loss: -0.02209087833762169
        vf_explained_var: 0.055081769824028015
        vf_loss: 30.031822204589844
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007627967861481011
        entropy: 1.0637736320495605
        entropy_coeff: 0.0017600000137463212
        kl: 0.013597331941127777
        model: {}
        policy_loss: -0.019002482295036316
        total_loss: -0.01716000959277153
        vf_explained_var: -0.011020809412002563
        vf_loss: 32.048118591308594
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007627967861481011
        entropy: 0.9782427549362183
        entropy_coeff: 0.0017600000137463212
        kl: 0.009870532900094986
        model: {}
        policy_loss: -0.013646448031067848
        total_loss: -0.012383253313601017
        vf_explained_var: 0.01634778082370758
        vf_loss: 28.97062873840332
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007627967861481011
        entropy: 0.7207047343254089
        entropy_coeff: 0.0017600000137463212
        kl: 0.011293258517980576
        model: {}
        policy_loss: -0.021094564348459244
        total_loss: -0.019442815333604813
        vf_explained_var: 0.1736811399459839
        vf_loss: 24.966861724853516
    load_time_ms: 13259.125
    num_steps_sampled: 8064000
    num_steps_trained: 8064000
    sample_time_ms: 106445.208
    update_time_ms: 16.646
  iterations_since_restore: 84
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.43351063829787
    ram_util_percent: 11.627127659574471
  pid: 28570
  policy_reward_max:
    agent-0: 167.5
    agent-1: 167.5
    agent-2: 178.0
    agent-3: 178.0
    agent-4: 164.5
    agent-5: 164.5
  policy_reward_mean:
    agent-0: 110.465
    agent-1: 110.465
    agent-2: 123.965
    agent-3: 123.965
    agent-4: 106.58
    agent-5: 106.58
  policy_reward_min:
    agent-0: 26.0
    agent-1: 26.0
    agent-2: 45.0
    agent-3: 45.0
    agent-4: 39.0
    agent-5: 39.0
  sampler_perf:
    mean_env_wait_ms: 26.7260537946968
    mean_inference_ms: 13.139151352024294
    mean_processing_ms: 58.89883132131779
  time_since_restore: 11110.390625238419
  time_this_iter_s: 132.02219152450562
  time_total_s: 11110.390625238419
  timestamp: 1637519644
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 8064000
  training_iteration: 84
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     84 |          11110.4 | 8064000 |   682.02 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 4.78
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 25.89
    apples_agent-1_min: 8
    apples_agent-2_max: 272
    apples_agent-2_mean: 175.91
    apples_agent-2_min: 59
    apples_agent-3_max: 61
    apples_agent-3_mean: 20.16
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 296
    apples_agent-5_mean: 201.75
    apples_agent-5_min: 103
    cleaning_beam_agent-0_max: 727
    cleaning_beam_agent-0_mean: 482.67
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 12.76
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 150
    cleaning_beam_agent-2_mean: 56.11
    cleaning_beam_agent-2_min: 22
    cleaning_beam_agent-3_max: 384
    cleaning_beam_agent-3_mean: 255.79
    cleaning_beam_agent-3_min: 143
    cleaning_beam_agent-4_max: 646
    cleaning_beam_agent-4_mean: 563.6
    cleaning_beam_agent-4_min: 423
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 88.68
    cleaning_beam_agent-5_min: 35
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-36-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 945.0
  episode_reward_mean: 712.08
  episode_reward_min: 390.0
  episodes_this_iter: 96
  episodes_total: 8160
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12747.538
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007568064029328525
        entropy: 0.839644730091095
        entropy_coeff: 0.0017600000137463212
        kl: 0.01267718244343996
        model: {}
        policy_loss: -0.019270267337560654
        total_loss: -0.01791497692465782
        vf_explained_var: 0.018701404333114624
        vf_loss: 25.161357879638672
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007568064029328525
        entropy: 0.76275634765625
        entropy_coeff: 0.0017600000137463212
        kl: 0.010674656368792057
        model: {}
        policy_loss: -0.019025150686502457
        total_loss: -0.017806246876716614
        vf_explained_var: 0.08385945856571198
        vf_loss: 23.612075805664062
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007568064029328525
        entropy: 0.9075472950935364
        entropy_coeff: 0.0017600000137463212
        kl: 0.011825849302113056
        model: {}
        policy_loss: -0.02387102320790291
        total_loss: -0.022019514814019203
        vf_explained_var: 0.056523457169532776
        vf_loss: 28.57495880126953
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007568064029328525
        entropy: 1.0435681343078613
        entropy_coeff: 0.0017600000137463212
        kl: 0.011015372350811958
        model: {}
        policy_loss: -0.01993977651000023
        total_loss: -0.018364297226071358
        vf_explained_var: 0.011624068021774292
        vf_loss: 29.990808486938477
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007568064029328525
        entropy: 0.9992164969444275
        entropy_coeff: 0.0017600000137463212
        kl: 0.010857177898287773
        model: {}
        policy_loss: -0.01387760229408741
        total_loss: -0.01275353692471981
        vf_explained_var: 0.019265025854110718
        vf_loss: 27.860689163208008
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007568064029328525
        entropy: 0.6861609220504761
        entropy_coeff: 0.0017600000137463212
        kl: 0.010657121427357197
        model: {}
        policy_loss: -0.019478200003504753
        total_loss: -0.017826812341809273
        vf_explained_var: 0.1476471722126007
        vf_loss: 24.59388542175293
    load_time_ms: 13242.761
    num_steps_sampled: 8160000
    num_steps_trained: 8160000
    sample_time_ms: 106438.143
    update_time_ms: 17.178
  iterations_since_restore: 85
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.412765957446812
    ram_util_percent: 11.67127659574468
  pid: 28570
  policy_reward_max:
    agent-0: 236.5
    agent-1: 236.5
    agent-2: 182.0
    agent-3: 182.0
    agent-4: 166.0
    agent-5: 166.0
  policy_reward_mean:
    agent-0: 116.92
    agent-1: 116.92
    agent-2: 125.86
    agent-3: 125.86
    agent-4: 113.26
    agent-5: 113.26
  policy_reward_min:
    agent-0: 67.0
    agent-1: 67.0
    agent-2: 53.5
    agent-3: 53.5
    agent-4: 42.0
    agent-5: 42.0
  sampler_perf:
    mean_env_wait_ms: 26.74501075434891
    mean_inference_ms: 13.137147886920232
    mean_processing_ms: 58.888011824914436
  time_since_restore: 11242.60784316063
  time_this_iter_s: 132.2172179222107
  time_total_s: 11242.60784316063
  timestamp: 1637519776
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 8160000
  training_iteration: 85
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     85 |          11242.6 | 8160000 |   712.08 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 3.49
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 29.32
    apples_agent-1_min: 12
    apples_agent-2_max: 322
    apples_agent-2_mean: 174.76
    apples_agent-2_min: 38
    apples_agent-3_max: 53
    apples_agent-3_mean: 17.82
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 389
    apples_agent-5_mean: 204.16
    apples_agent-5_min: 64
    cleaning_beam_agent-0_max: 645
    cleaning_beam_agent-0_mean: 460.06
    cleaning_beam_agent-0_min: 279
    cleaning_beam_agent-1_max: 39
    cleaning_beam_agent-1_mean: 10.83
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 165
    cleaning_beam_agent-2_mean: 59.74
    cleaning_beam_agent-2_min: 17
    cleaning_beam_agent-3_max: 498
    cleaning_beam_agent-3_mean: 273.65
    cleaning_beam_agent-3_min: 127
    cleaning_beam_agent-4_max: 680
    cleaning_beam_agent-4_mean: 589.46
    cleaning_beam_agent-4_min: 441
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 95.54
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.14
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-38-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.0
  episode_reward_mean: 696.71
  episode_reward_min: 195.0
  episodes_this_iter: 96
  episodes_total: 8256
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12744.11
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007508160197176039
        entropy: 0.8394302129745483
        entropy_coeff: 0.0017600000137463212
        kl: 0.01244446076452732
        model: {}
        policy_loss: -0.019888702780008316
        total_loss: -0.018500806763768196
        vf_explained_var: 0.06531734764575958
        vf_loss: 25.541839599609375
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007508160197176039
        entropy: 0.7159273028373718
        entropy_coeff: 0.0017600000137463212
        kl: 0.011226220056414604
        model: {}
        policy_loss: -0.019370103254914284
        total_loss: -0.0179872028529644
        vf_explained_var: 0.1037135124206543
        vf_loss: 24.32440185546875
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007508160197176039
        entropy: 0.9183260798454285
        entropy_coeff: 0.0017600000137463212
        kl: 0.014162031933665276
        model: {}
        policy_loss: -0.026408076286315918
        total_loss: -0.024479439482092857
        vf_explained_var: 0.09905184805393219
        vf_loss: 28.36789321899414
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007508160197176039
        entropy: 1.059278130531311
        entropy_coeff: 0.0017600000137463212
        kl: 0.012171214446425438
        model: {}
        policy_loss: -0.020764166489243507
        total_loss: -0.01910914108157158
        vf_explained_var: 0.02200959622859955
        vf_loss: 30.629329681396484
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007508160197176039
        entropy: 0.9595544338226318
        entropy_coeff: 0.0017600000137463212
        kl: 0.010941644199192524
        model: {}
        policy_loss: -0.014764602296054363
        total_loss: -0.013282853178679943
        vf_explained_var: 0.014954134821891785
        vf_loss: 30.731964111328125
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007508160197176039
        entropy: 0.7018145322799683
        entropy_coeff: 0.0017600000137463212
        kl: 0.011241979897022247
        model: {}
        policy_loss: -0.0204732958227396
        total_loss: -0.018680285662412643
        vf_explained_var: 0.1795600801706314
        vf_loss: 26.066314697265625
    load_time_ms: 13252.911
    num_steps_sampled: 8256000
    num_steps_trained: 8256000
    sample_time_ms: 106480.712
    update_time_ms: 17.199
  iterations_since_restore: 86
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.476063829787233
    ram_util_percent: 11.592021276595746
  pid: 28570
  policy_reward_max:
    agent-0: 184.0
    agent-1: 184.0
    agent-2: 194.5
    agent-3: 194.5
    agent-4: 218.5
    agent-5: 218.5
  policy_reward_mean:
    agent-0: 111.605
    agent-1: 111.605
    agent-2: 122.1
    agent-3: 122.1
    agent-4: 114.65
    agent-5: 114.65
  policy_reward_min:
    agent-0: 9.5
    agent-1: 9.5
    agent-2: 38.0
    agent-3: 38.0
    agent-4: 34.0
    agent-5: 34.0
  sampler_perf:
    mean_env_wait_ms: 26.766059485107135
    mean_inference_ms: 13.136347134001578
    mean_processing_ms: 58.882532826056284
  time_since_restore: 11375.02665901184
  time_this_iter_s: 132.41881585121155
  time_total_s: 11375.02665901184
  timestamp: 1637519909
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 8256000
  training_iteration: 86
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     86 |            11375 | 8256000 |   696.71 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 5.02
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 29.89
    apples_agent-1_min: 8
    apples_agent-2_max: 299
    apples_agent-2_mean: 170.0
    apples_agent-2_min: 0
    apples_agent-3_max: 50
    apples_agent-3_mean: 15.69
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 0.49
    apples_agent-4_min: 0
    apples_agent-5_max: 358
    apples_agent-5_mean: 207.52
    apples_agent-5_min: 68
    cleaning_beam_agent-0_max: 666
    cleaning_beam_agent-0_mean: 465.89
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 10.64
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 141
    cleaning_beam_agent-2_mean: 61.84
    cleaning_beam_agent-2_min: 30
    cleaning_beam_agent-3_max: 432
    cleaning_beam_agent-3_mean: 274.6
    cleaning_beam_agent-3_min: 159
    cleaning_beam_agent-4_max: 684
    cleaning_beam_agent-4_mean: 585.39
    cleaning_beam_agent-4_min: 418
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 92.91
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.22
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.22
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-40-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.0
  episode_reward_mean: 706.28
  episode_reward_min: 243.0
  episodes_this_iter: 96
  episodes_total: 8352
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12748.633
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007448255782946944
        entropy: 0.8213598132133484
        entropy_coeff: 0.0017600000137463212
        kl: 0.01268676109611988
        model: {}
        policy_loss: -0.020192772150039673
        total_loss: -0.01832868531346321
        vf_explained_var: 0.06902800500392914
        vf_loss: 29.92511558532715
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007448255782946944
        entropy: 0.6842963695526123
        entropy_coeff: 0.0017600000137463212
        kl: 0.01135153416544199
        model: {}
        policy_loss: -0.01830315589904785
        total_loss: -0.016414102166891098
        vf_explained_var: 0.09664209187030792
        vf_loss: 28.80573081970215
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007448255782946944
        entropy: 0.940188467502594
        entropy_coeff: 0.0017600000137463212
        kl: 0.013914906419813633
        model: {}
        policy_loss: -0.025380682200193405
        total_loss: -0.023426314815878868
        vf_explained_var: 0.10086870193481445
        vf_loss: 29.13353729248047
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007448255782946944
        entropy: 1.044909954071045
        entropy_coeff: 0.0017600000137463212
        kl: 0.012849779799580574
        model: {}
        policy_loss: -0.020620036870241165
        total_loss: -0.018716566264629364
        vf_explained_var: -0.007918715476989746
        vf_loss: 32.6064567565918
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007448255782946944
        entropy: 0.9532054662704468
        entropy_coeff: 0.0017600000137463212
        kl: 0.01227300614118576
        model: {}
        policy_loss: -0.014752818271517754
        total_loss: -0.013151796534657478
        vf_explained_var: 0.02480471134185791
        vf_loss: 31.694486618041992
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007448255782946944
        entropy: 0.6952993869781494
        entropy_coeff: 0.0017600000137463212
        kl: 0.01200263760983944
        model: {}
        policy_loss: -0.02125679701566696
        total_loss: -0.019232437014579773
        vf_explained_var: 0.15585124492645264
        vf_loss: 27.97986602783203
    load_time_ms: 13243.942
    num_steps_sampled: 8352000
    num_steps_trained: 8352000
    sample_time_ms: 106587.569
    update_time_ms: 16.902
  iterations_since_restore: 87
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.291578947368425
    ram_util_percent: 11.627894736842109
  pid: 28570
  policy_reward_max:
    agent-0: 186.0
    agent-1: 186.0
    agent-2: 196.5
    agent-3: 196.5
    agent-4: 177.0
    agent-5: 177.0
  policy_reward_mean:
    agent-0: 118.84
    agent-1: 118.84
    agent-2: 119.86
    agent-3: 119.86
    agent-4: 114.44
    agent-5: 114.44
  policy_reward_min:
    agent-0: 23.5
    agent-1: 23.5
    agent-2: 10.0
    agent-3: 10.0
    agent-4: 32.5
    agent-5: 32.5
  sampler_perf:
    mean_env_wait_ms: 26.78598793230721
    mean_inference_ms: 13.135224652959678
    mean_processing_ms: 58.87514040943833
  time_since_restore: 11508.329870939255
  time_this_iter_s: 133.30321192741394
  time_total_s: 11508.329870939255
  timestamp: 1637520042
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 8352000
  training_iteration: 87
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     87 |          11508.3 | 8352000 |   706.28 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 5.64
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 29.39
    apples_agent-1_min: 5
    apples_agent-2_max: 248
    apples_agent-2_mean: 157.83
    apples_agent-2_min: 71
    apples_agent-3_max: 95
    apples_agent-3_mean: 20.27
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.38
    apples_agent-4_min: 0
    apples_agent-5_max: 268
    apples_agent-5_mean: 178.15
    apples_agent-5_min: 81
    cleaning_beam_agent-0_max: 693
    cleaning_beam_agent-0_mean: 461.21
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 33
    cleaning_beam_agent-1_mean: 11.68
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 137
    cleaning_beam_agent-2_mean: 69.85
    cleaning_beam_agent-2_min: 25
    cleaning_beam_agent-3_max: 409
    cleaning_beam_agent-3_mean: 221.63
    cleaning_beam_agent-3_min: 123
    cleaning_beam_agent-4_max: 714
    cleaning_beam_agent-4_mean: 624.88
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 212
    cleaning_beam_agent-5_mean: 95.75
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.1
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.2
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-42-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 885.0
  episode_reward_mean: 639.67
  episode_reward_min: 366.0
  episodes_this_iter: 96
  episodes_total: 8448
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12739.205
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007388351950794458
        entropy: 0.8336033821105957
        entropy_coeff: 0.0017600000137463212
        kl: 0.012620462104678154
        model: {}
        policy_loss: -0.020164761692285538
        total_loss: -0.019049521535634995
        vf_explained_var: 0.061145007610321045
        vf_loss: 22.66868782043457
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007388351950794458
        entropy: 0.7300636172294617
        entropy_coeff: 0.0017600000137463212
        kl: 0.010359397158026695
        model: {}
        policy_loss: -0.01859653741121292
        total_loss: -0.017442824319005013
        vf_explained_var: 0.07111755013465881
        vf_loss: 22.443872451782227
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007388351950794458
        entropy: 0.9539350271224976
        entropy_coeff: 0.0017600000137463212
        kl: 0.013732732273638248
        model: {}
        policy_loss: -0.025389524176716805
        total_loss: -0.0236143060028553
        vf_explained_var: 0.08452464640140533
        vf_loss: 27.67507553100586
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007388351950794458
        entropy: 1.0106496810913086
        entropy_coeff: 0.0017600000137463212
        kl: 0.01218953263014555
        model: {}
        policy_loss: -0.018686123192310333
        total_loss: -0.016953028738498688
        vf_explained_var: -0.012338206171989441
        vf_loss: 30.547300338745117
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007388351950794458
        entropy: 0.8983548879623413
        entropy_coeff: 0.0017600000137463212
        kl: 0.00962039828300476
        model: {}
        policy_loss: -0.013991071842610836
        total_loss: -0.01267644390463829
        vf_explained_var: -0.02422812581062317
        vf_loss: 28.10120391845703
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007388351950794458
        entropy: 0.7325642108917236
        entropy_coeff: 0.0017600000137463212
        kl: 0.012135776691138744
        model: {}
        policy_loss: -0.02085600420832634
        total_loss: -0.019358471035957336
        vf_explained_var: 0.15341942012310028
        vf_loss: 23.317588806152344
    load_time_ms: 13243.248
    num_steps_sampled: 8448000
    num_steps_trained: 8448000
    sample_time_ms: 106690.872
    update_time_ms: 16.945
  iterations_since_restore: 88
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.320105820105816
    ram_util_percent: 11.604232804232808
  pid: 28570
  policy_reward_max:
    agent-0: 160.5
    agent-1: 160.5
    agent-2: 172.0
    agent-3: 172.0
    agent-4: 151.0
    agent-5: 151.0
  policy_reward_mean:
    agent-0: 106.67
    agent-1: 106.67
    agent-2: 114.54
    agent-3: 114.54
    agent-4: 98.625
    agent-5: 98.625
  policy_reward_min:
    agent-0: 56.5
    agent-1: 56.5
    agent-2: 60.0
    agent-3: 60.0
    agent-4: 42.5
    agent-5: 42.5
  sampler_perf:
    mean_env_wait_ms: 26.80565739318935
    mean_inference_ms: 13.134046562433845
    mean_processing_ms: 58.867340713945524
  time_since_restore: 11640.880140304565
  time_this_iter_s: 132.55026936531067
  time_total_s: 11640.880140304565
  timestamp: 1637520175
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 8448000
  training_iteration: 88
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     88 |          11640.9 | 8448000 |   639.67 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 4.43
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 24.32
    apples_agent-1_min: 6
    apples_agent-2_max: 236
    apples_agent-2_mean: 152.25
    apples_agent-2_min: 16
    apples_agent-3_max: 52
    apples_agent-3_mean: 16.76
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 272
    apples_agent-5_mean: 174.98
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 646
    cleaning_beam_agent-0_mean: 460.56
    cleaning_beam_agent-0_min: 188
    cleaning_beam_agent-1_max: 31
    cleaning_beam_agent-1_mean: 12.76
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 204
    cleaning_beam_agent-2_mean: 72.44
    cleaning_beam_agent-2_min: 32
    cleaning_beam_agent-3_max: 320
    cleaning_beam_agent-3_mean: 212.93
    cleaning_beam_agent-3_min: 144
    cleaning_beam_agent-4_max: 771
    cleaning_beam_agent-4_mean: 657.45
    cleaning_beam_agent-4_min: 421
    cleaning_beam_agent-5_max: 196
    cleaning_beam_agent-5_mean: 97.54
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.12
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.22
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.11
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.35
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-45-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 950.0
  episode_reward_mean: 624.97
  episode_reward_min: 106.0
  episodes_this_iter: 96
  episodes_total: 8544
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12740.955
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007328448118641973
        entropy: 0.8215320110321045
        entropy_coeff: 0.0017600000137463212
        kl: 0.013776029460132122
        model: {}
        policy_loss: -0.02230975776910782
        total_loss: -0.020809348672628403
        vf_explained_var: 0.09572947025299072
        vf_loss: 26.01905632019043
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007328448118641973
        entropy: 0.7941804528236389
        entropy_coeff: 0.0017600000137463212
        kl: 0.012836586683988571
        model: {}
        policy_loss: -0.021664366126060486
        total_loss: -0.02026156708598137
        vf_explained_var: 0.11653311550617218
        vf_loss: 25.598722457885742
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007328448118641973
        entropy: 0.9679149389266968
        entropy_coeff: 0.0017600000137463212
        kl: 0.013839534483850002
        model: {}
        policy_loss: -0.02546805515885353
        total_loss: -0.023687727749347687
        vf_explained_var: 0.09528672695159912
        vf_loss: 27.918777465820312
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007328448118641973
        entropy: 0.998258113861084
        entropy_coeff: 0.0017600000137463212
        kl: 0.01206993032246828
        model: {}
        policy_loss: -0.02003517560660839
        total_loss: -0.018210571259260178
        vf_explained_var: -0.017860203981399536
        vf_loss: 31.289159774780273
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007328448118641973
        entropy: 0.8253185749053955
        entropy_coeff: 0.0017600000137463212
        kl: 0.010239502415060997
        model: {}
        policy_loss: -0.014138667844235897
        total_loss: -0.012904241681098938
        vf_explained_var: -0.00532957911491394
        vf_loss: 25.958654403686523
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007328448118641973
        entropy: 0.7296551465988159
        entropy_coeff: 0.0017600000137463212
        kl: 0.011524206958711147
        model: {}
        policy_loss: -0.023343432694673538
        total_loss: -0.022010844200849533
        vf_explained_var: 0.15902067720890045
        vf_loss: 21.846240997314453
    load_time_ms: 13242.975
    num_steps_sampled: 8544000
    num_steps_trained: 8544000
    sample_time_ms: 106704.6
    update_time_ms: 16.999
  iterations_since_restore: 89
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.439682539682536
    ram_util_percent: 11.614285714285716
  pid: 28570
  policy_reward_max:
    agent-0: 165.5
    agent-1: 165.5
    agent-2: 172.5
    agent-3: 172.5
    agent-4: 158.5
    agent-5: 158.5
  policy_reward_mean:
    agent-0: 104.16
    agent-1: 104.16
    agent-2: 110.125
    agent-3: 110.125
    agent-4: 98.2
    agent-5: 98.2
  policy_reward_min:
    agent-0: 9.5
    agent-1: 9.5
    agent-2: 7.5
    agent-3: 7.5
    agent-4: 32.0
    agent-5: 32.0
  sampler_perf:
    mean_env_wait_ms: 26.8251462589522
    mean_inference_ms: 13.132339427613033
    mean_processing_ms: 58.856775303438106
  time_since_restore: 11773.65162396431
  time_this_iter_s: 132.77148365974426
  time_total_s: 11773.65162396431
  timestamp: 1637520308
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 8544000
  training_iteration: 89
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     89 |          11773.7 | 8544000 |   624.97 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.43
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 24.96
    apples_agent-1_min: 8
    apples_agent-2_max: 262
    apples_agent-2_mean: 155.59
    apples_agent-2_min: 59
    apples_agent-3_max: 62
    apples_agent-3_mean: 17.01
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.28
    apples_agent-4_min: 0
    apples_agent-5_max: 302
    apples_agent-5_mean: 173.61
    apples_agent-5_min: 72
    cleaning_beam_agent-0_max: 675
    cleaning_beam_agent-0_mean: 478.87
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 39
    cleaning_beam_agent-1_mean: 14.47
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 120
    cleaning_beam_agent-2_mean: 62.59
    cleaning_beam_agent-2_min: 25
    cleaning_beam_agent-3_max: 416
    cleaning_beam_agent-3_mean: 212.6
    cleaning_beam_agent-3_min: 119
    cleaning_beam_agent-4_max: 757
    cleaning_beam_agent-4_mean: 628.83
    cleaning_beam_agent-4_min: 396
    cleaning_beam_agent-5_max: 264
    cleaning_beam_agent-5_mean: 99.31
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.13
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.33
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-47-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 910.0
  episode_reward_mean: 623.7
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 8640
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12729.891
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007268544286489487
        entropy: 0.8042155504226685
        entropy_coeff: 0.0017600000137463212
        kl: 0.013275830075144768
        model: {}
        policy_loss: -0.020517420023679733
        total_loss: -0.018908601254224777
        vf_explained_var: 0.07763561606407166
        vf_loss: 26.923410415649414
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007268544286489487
        entropy: 0.8043658137321472
        entropy_coeff: 0.0017600000137463212
        kl: 0.011832485906779766
        model: {}
        policy_loss: -0.02209741249680519
        total_loss: -0.020655903965234756
        vf_explained_var: 0.10713334381580353
        vf_loss: 26.353343963623047
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007268544286489487
        entropy: 0.9409042000770569
        entropy_coeff: 0.0017600000137463212
        kl: 0.01351294293999672
        model: {}
        policy_loss: -0.02506420947611332
        total_loss: -0.02330872043967247
        vf_explained_var: 0.08977600932121277
        vf_loss: 27.358314514160156
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007268544286489487
        entropy: 0.9724547863006592
        entropy_coeff: 0.0017600000137463212
        kl: 0.01269221305847168
        model: {}
        policy_loss: -0.02000226080417633
        total_loss: -0.018263310194015503
        vf_explained_var: 0.012007072567939758
        vf_loss: 29.745126724243164
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007268544286489487
        entropy: 0.8911234140396118
        entropy_coeff: 0.0017600000137463212
        kl: 0.011524633504450321
        model: {}
        policy_loss: -0.014725057408213615
        total_loss: -0.013587220571935177
        vf_explained_var: -0.018193364143371582
        vf_loss: 26.036558151245117
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007268544286489487
        entropy: 0.722854495048523
        entropy_coeff: 0.0017600000137463212
        kl: 0.011901667341589928
        model: {}
        policy_loss: -0.02302652597427368
        total_loss: -0.02168663591146469
        vf_explained_var: 0.1525554358959198
        vf_loss: 21.658048629760742
    load_time_ms: 13241.931
    num_steps_sampled: 8640000
    num_steps_trained: 8640000
    sample_time_ms: 106567.162
    update_time_ms: 17.132
  iterations_since_restore: 90
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.36808510638298
    ram_util_percent: 11.606382978723406
  pid: 28570
  policy_reward_max:
    agent-0: 151.5
    agent-1: 151.5
    agent-2: 168.5
    agent-3: 168.5
    agent-4: 160.0
    agent-5: 160.0
  policy_reward_mean:
    agent-0: 104.18
    agent-1: 104.18
    agent-2: 110.54
    agent-3: 110.54
    agent-4: 97.13
    agent-5: 97.13
  policy_reward_min:
    agent-0: -18.5
    agent-1: -18.5
    agent-2: -0.5
    agent-3: -0.5
    agent-4: 29.0
    agent-5: 29.0
  sampler_perf:
    mean_env_wait_ms: 26.842953915492608
    mean_inference_ms: 13.130729783347242
    mean_processing_ms: 58.84649651722588
  time_since_restore: 11905.529816150665
  time_this_iter_s: 131.8781921863556
  time_total_s: 11905.529816150665
  timestamp: 1637520440
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 8640000
  training_iteration: 90
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     90 |          11905.5 | 8640000 |    623.7 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 96
    apples_agent-0_mean: 4.86
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 24.27
    apples_agent-1_min: 7
    apples_agent-2_max: 276
    apples_agent-2_mean: 160.02
    apples_agent-2_min: 68
    apples_agent-3_max: 46
    apples_agent-3_mean: 17.59
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 263
    apples_agent-5_mean: 180.71
    apples_agent-5_min: 79
    cleaning_beam_agent-0_max: 639
    cleaning_beam_agent-0_mean: 456.82
    cleaning_beam_agent-0_min: 209
    cleaning_beam_agent-1_max: 47
    cleaning_beam_agent-1_mean: 14.39
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 153
    cleaning_beam_agent-2_mean: 61.29
    cleaning_beam_agent-2_min: 21
    cleaning_beam_agent-3_max: 488
    cleaning_beam_agent-3_mean: 220.97
    cleaning_beam_agent-3_min: 150
    cleaning_beam_agent-4_max: 731
    cleaning_beam_agent-4_mean: 644.42
    cleaning_beam_agent-4_min: 452
    cleaning_beam_agent-5_max: 208
    cleaning_beam_agent-5_mean: 98.85
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.11
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.28
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 4
    fire_beam_agent-3_mean: 0.14
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.25
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-49-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1039.0
  episode_reward_mean: 640.86
  episode_reward_min: 296.0
  episodes_this_iter: 96
  episodes_total: 8736
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12740.065
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007208639872260392
        entropy: 0.8157491087913513
        entropy_coeff: 0.0017600000137463212
        kl: 0.012111322022974491
        model: {}
        policy_loss: -0.0214809849858284
        total_loss: -0.020188167691230774
        vf_explained_var: 0.0868891179561615
        vf_loss: 24.257526397705078
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007208639872260392
        entropy: 0.8053066730499268
        entropy_coeff: 0.0017600000137463212
        kl: 0.011978816241025925
        model: {}
        policy_loss: -0.022924374788999557
        total_loss: -0.021761076524853706
        vf_explained_var: 0.12074249982833862
        vf_loss: 23.560354232788086
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007208639872260392
        entropy: 0.918736457824707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0130503224208951
        model: {}
        policy_loss: -0.024169709533452988
        total_loss: -0.02193264663219452
        vf_explained_var: 0.10854421555995941
        vf_loss: 32.01525115966797
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007208639872260392
        entropy: 0.9792497158050537
        entropy_coeff: 0.0017600000137463212
        kl: 0.012988445349037647
        model: {}
        policy_loss: -0.02023419737815857
        total_loss: -0.017816634848713875
        vf_explained_var: -0.0013776123523712158
        vf_loss: 36.539764404296875
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007208639872260392
        entropy: 0.8855019807815552
        entropy_coeff: 0.0017600000137463212
        kl: 0.009338701143860817
        model: {}
        policy_loss: -0.01434289664030075
        total_loss: -0.012969712726771832
        vf_explained_var: 0.0019717365503311157
        vf_loss: 28.485620498657227
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007208639872260392
        entropy: 0.714250922203064
        entropy_coeff: 0.0017600000137463212
        kl: 0.012273133732378483
        model: {}
        policy_loss: -0.022545892745256424
        total_loss: -0.020917212590575218
        vf_explained_var: 0.14945609867572784
        vf_loss: 24.255218505859375
    load_time_ms: 13241.747
    num_steps_sampled: 8736000
    num_steps_trained: 8736000
    sample_time_ms: 106463.304
    update_time_ms: 17.158
  iterations_since_restore: 91
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.316402116402113
    ram_util_percent: 11.698941798941801
  pid: 28570
  policy_reward_max:
    agent-0: 158.0
    agent-1: 158.0
    agent-2: 202.5
    agent-3: 202.5
    agent-4: 159.0
    agent-5: 159.0
  policy_reward_mean:
    agent-0: 104.5
    agent-1: 104.5
    agent-2: 115.72
    agent-3: 115.72
    agent-4: 100.21
    agent-5: 100.21
  policy_reward_min:
    agent-0: 47.0
    agent-1: 47.0
    agent-2: 47.5
    agent-3: 47.5
    agent-4: 29.0
    agent-5: 29.0
  sampler_perf:
    mean_env_wait_ms: 26.860477835799692
    mean_inference_ms: 13.129001401444482
    mean_processing_ms: 58.83371225284547
  time_since_restore: 12038.100019454956
  time_this_iter_s: 132.57020330429077
  time_total_s: 12038.100019454956
  timestamp: 1637520572
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 8736000
  training_iteration: 91
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     91 |          12038.1 | 8736000 |   640.86 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.84
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 25.85
    apples_agent-1_min: 9
    apples_agent-2_max: 282
    apples_agent-2_mean: 157.73
    apples_agent-2_min: 32
    apples_agent-3_max: 54
    apples_agent-3_mean: 18.48
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.48
    apples_agent-4_min: 0
    apples_agent-5_max: 291
    apples_agent-5_mean: 179.16
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 693
    cleaning_beam_agent-0_mean: 476.52
    cleaning_beam_agent-0_min: 250
    cleaning_beam_agent-1_max: 46
    cleaning_beam_agent-1_mean: 18.39
    cleaning_beam_agent-1_min: 5
    cleaning_beam_agent-2_max: 114
    cleaning_beam_agent-2_mean: 55.92
    cleaning_beam_agent-2_min: 19
    cleaning_beam_agent-3_max: 386
    cleaning_beam_agent-3_mean: 216.16
    cleaning_beam_agent-3_min: 126
    cleaning_beam_agent-4_max: 723
    cleaning_beam_agent-4_mean: 608.88
    cleaning_beam_agent-4_min: 278
    cleaning_beam_agent-5_max: 218
    cleaning_beam_agent-5_mean: 93.53
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.26
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.2
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-51-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 967.0
  episode_reward_mean: 634.04
  episode_reward_min: 216.0
  episodes_this_iter: 96
  episodes_total: 8832
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12744.837
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007148736040107906
        entropy: 0.7817955613136292
        entropy_coeff: 0.0017600000137463212
        kl: 0.012790598906576633
        model: {}
        policy_loss: -0.019322354346513748
        total_loss: -0.018088823184370995
        vf_explained_var: 0.06498700380325317
        vf_loss: 22.89722442626953
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007148736040107906
        entropy: 0.8049157857894897
        entropy_coeff: 0.0017600000137463212
        kl: 0.011432820931077003
        model: {}
        policy_loss: -0.022103887051343918
        total_loss: -0.02113305777311325
        vf_explained_var: 0.1190100908279419
        vf_loss: 21.73113250732422
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007148736040107906
        entropy: 0.9179793000221252
        entropy_coeff: 0.0017600000137463212
        kl: 0.012860902585089207
        model: {}
        policy_loss: -0.02564164251089096
        total_loss: -0.023865720257163048
        vf_explained_var: 0.11956129968166351
        vf_loss: 27.485231399536133
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007148736040107906
        entropy: 0.9495267271995544
        entropy_coeff: 0.0017600000137463212
        kl: 0.013425404205918312
        model: {}
        policy_loss: -0.02186267077922821
        total_loss: -0.019823402166366577
        vf_explained_var: -0.015281379222869873
        vf_loss: 32.06985092163086
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007148736040107906
        entropy: 0.9344655275344849
        entropy_coeff: 0.0017600000137463212
        kl: 0.010219527408480644
        model: {}
        policy_loss: -0.016628259792923927
        total_loss: -0.015445169061422348
        vf_explained_var: 0.022126972675323486
        vf_loss: 27.36805534362793
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007148736040107906
        entropy: 0.6896069645881653
        entropy_coeff: 0.0017600000137463212
        kl: 0.011616562493145466
        model: {}
        policy_loss: -0.0222377460449934
        total_loss: -0.020681019872426987
        vf_explained_var: 0.1601642370223999
        vf_loss: 23.348125457763672
    load_time_ms: 13238.975
    num_steps_sampled: 8832000
    num_steps_trained: 8832000
    sample_time_ms: 106392.298
    update_time_ms: 16.728
  iterations_since_restore: 92
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.57058823529412
    ram_util_percent: 11.681818181818182
  pid: 28570
  policy_reward_max:
    agent-0: 164.5
    agent-1: 164.5
    agent-2: 185.0
    agent-3: 185.0
    agent-4: 155.5
    agent-5: 155.5
  policy_reward_mean:
    agent-0: 103.1
    agent-1: 103.1
    agent-2: 113.88
    agent-3: 113.88
    agent-4: 100.04
    agent-5: 100.04
  policy_reward_min:
    agent-0: 37.5
    agent-1: 37.5
    agent-2: 29.5
    agent-3: 29.5
    agent-4: 12.0
    agent-5: 12.0
  sampler_perf:
    mean_env_wait_ms: 26.87615327725403
    mean_inference_ms: 13.127445572763099
    mean_processing_ms: 58.824365544616484
  time_since_restore: 12169.794464111328
  time_this_iter_s: 131.69444465637207
  time_total_s: 12169.794464111328
  timestamp: 1637520704
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 8832000
  training_iteration: 92
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     92 |          12169.8 | 8832000 |   634.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 24.62
    apples_agent-1_min: 7
    apples_agent-2_max: 271
    apples_agent-2_mean: 160.36
    apples_agent-2_min: 68
    apples_agent-3_max: 59
    apples_agent-3_mean: 19.24
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 291
    apples_agent-5_mean: 174.16
    apples_agent-5_min: 80
    cleaning_beam_agent-0_max: 603
    cleaning_beam_agent-0_mean: 430.94
    cleaning_beam_agent-0_min: 279
    cleaning_beam_agent-1_max: 46
    cleaning_beam_agent-1_mean: 15.55
    cleaning_beam_agent-1_min: 6
    cleaning_beam_agent-2_max: 148
    cleaning_beam_agent-2_mean: 55.65
    cleaning_beam_agent-2_min: 21
    cleaning_beam_agent-3_max: 393
    cleaning_beam_agent-3_mean: 221.82
    cleaning_beam_agent-3_min: 129
    cleaning_beam_agent-4_max: 733
    cleaning_beam_agent-4_mean: 618.17
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 88.3
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.14
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.16
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.16
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.15
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-53-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 958.0
  episode_reward_mean: 623.78
  episode_reward_min: 259.0
  episodes_this_iter: 96
  episodes_total: 8928
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12756.887
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000708883220795542
        entropy: 0.8331280946731567
        entropy_coeff: 0.0017600000137463212
        kl: 0.013154950924217701
        model: {}
        policy_loss: -0.021827831864356995
        total_loss: -0.020645027980208397
        vf_explained_var: 0.10300856828689575
        vf_loss: 23.202333450317383
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000708883220795542
        entropy: 0.7992984056472778
        entropy_coeff: 0.0017600000137463212
        kl: 0.012134518474340439
        model: {}
        policy_loss: -0.022223837673664093
        total_loss: -0.021142300218343735
        vf_explained_var: 0.1231410801410675
        vf_loss: 22.607755661010742
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000708883220795542
        entropy: 0.9093278646469116
        entropy_coeff: 0.0017600000137463212
        kl: 0.013168352656066418
        model: {}
        policy_loss: -0.022730030119419098
        total_loss: -0.020774507895112038
        vf_explained_var: 0.11228901147842407
        vf_loss: 28.97520637512207
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000708883220795542
        entropy: 0.955605149269104
        entropy_coeff: 0.0017600000137463212
        kl: 0.012943487614393234
        model: {}
        policy_loss: -0.021044237539172173
        total_loss: -0.01885368488729
        vf_explained_var: -0.0270138680934906
        vf_loss: 33.870338439941406
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000708883220795542
        entropy: 0.9035404920578003
        entropy_coeff: 0.0017600000137463212
        kl: 0.010653121396899223
        model: {}
        policy_loss: -0.016231173649430275
        total_loss: -0.01530823577195406
        vf_explained_var: 0.01723329722881317
        vf_loss: 24.183685302734375
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000708883220795542
        entropy: 0.6953823566436768
        entropy_coeff: 0.0017600000137463212
        kl: 0.012062462978065014
        model: {}
        policy_loss: -0.022940797731280327
        total_loss: -0.021645160391926765
        vf_explained_var: 0.15909485518932343
        vf_loss: 20.671674728393555
    load_time_ms: 13221.759
    num_steps_sampled: 8928000
    num_steps_trained: 8928000
    sample_time_ms: 106362.74
    update_time_ms: 16.782
  iterations_since_restore: 93
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.31
    ram_util_percent: 11.554736842105266
  pid: 28570
  policy_reward_max:
    agent-0: 151.0
    agent-1: 151.0
    agent-2: 186.5
    agent-3: 186.5
    agent-4: 164.0
    agent-5: 164.0
  policy_reward_mean:
    agent-0: 100.775
    agent-1: 100.775
    agent-2: 114.23
    agent-3: 114.23
    agent-4: 96.885
    agent-5: 96.885
  policy_reward_min:
    agent-0: 11.0
    agent-1: 11.0
    agent-2: 48.5
    agent-3: 48.5
    agent-4: 46.5
    agent-5: 46.5
  sampler_perf:
    mean_env_wait_ms: 26.890780458777364
    mean_inference_ms: 13.125868972188005
    mean_processing_ms: 58.81778436407984
  time_since_restore: 12302.8132417202
  time_this_iter_s: 133.01877760887146
  time_total_s: 12302.8132417202
  timestamp: 1637520837
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 8928000
  training_iteration: 93
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     93 |          12302.8 | 8928000 |   623.78 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.52
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 28.01
    apples_agent-1_min: 8
    apples_agent-2_max: 292
    apples_agent-2_mean: 177.13
    apples_agent-2_min: 64
    apples_agent-3_max: 73
    apples_agent-3_mean: 20.54
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 292
    apples_agent-5_mean: 197.74
    apples_agent-5_min: 97
    cleaning_beam_agent-0_max: 648
    cleaning_beam_agent-0_mean: 454.0
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 36
    cleaning_beam_agent-1_mean: 14.96
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 167
    cleaning_beam_agent-2_mean: 55.93
    cleaning_beam_agent-2_min: 18
    cleaning_beam_agent-3_max: 448
    cleaning_beam_agent-3_mean: 221.02
    cleaning_beam_agent-3_min: 133
    cleaning_beam_agent-4_max: 755
    cleaning_beam_agent-4_mean: 587.49
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 164
    cleaning_beam_agent-5_mean: 70.96
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.11
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.13
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.22
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-56-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 990.0
  episode_reward_mean: 693.22
  episode_reward_min: 363.0
  episodes_this_iter: 96
  episodes_total: 9024
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12759.189
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0007028927793726325
        entropy: 0.8103922009468079
        entropy_coeff: 0.0017600000137463212
        kl: 0.013420457020401955
        model: {}
        policy_loss: -0.020496554672718048
        total_loss: -0.018888454884290695
        vf_explained_var: 0.09465174376964569
        vf_loss: 26.988794326782227
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0007028927793726325
        entropy: 0.7566810846328735
        entropy_coeff: 0.0017600000137463212
        kl: 0.012258479371666908
        model: {}
        policy_loss: -0.023374082520604134
        total_loss: -0.02182776667177677
        vf_explained_var: 0.11250102519989014
        vf_loss: 26.482261657714844
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0007028927793726325
        entropy: 0.8509687781333923
        entropy_coeff: 0.0017600000137463212
        kl: 0.011829440481960773
        model: {}
        policy_loss: -0.023667503148317337
        total_loss: -0.021398749202489853
        vf_explained_var: 0.10182517766952515
        vf_loss: 31.749813079833984
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007028927793726325
        entropy: 0.9439029097557068
        entropy_coeff: 0.0017600000137463212
        kl: 0.013531018048524857
        model: {}
        policy_loss: -0.022459831088781357
        total_loss: -0.019976593554019928
        vf_explained_var: 0.013079449534416199
        vf_loss: 36.370906829833984
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0007028927793726325
        entropy: 0.9732456803321838
        entropy_coeff: 0.0017600000137463212
        kl: 0.011895235627889633
        model: {}
        policy_loss: -0.017902303487062454
        total_loss: -0.016697168350219727
        vf_explained_var: 0.005729496479034424
        vf_loss: 28.121932983398438
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0007028927793726325
        entropy: 0.5997106432914734
        entropy_coeff: 0.0017600000137463212
        kl: 0.011371963657438755
        model: {}
        policy_loss: -0.021792558953166008
        total_loss: -0.020033471286296844
        vf_explained_var: 0.1524273008108139
        vf_loss: 23.881305694580078
    load_time_ms: 13215.96
    num_steps_sampled: 9024000
    num_steps_trained: 9024000
    sample_time_ms: 106377.831
    update_time_ms: 16.638
  iterations_since_restore: 94
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.396808510638294
    ram_util_percent: 11.686702127659576
  pid: 28570
  policy_reward_max:
    agent-0: 168.0
    agent-1: 168.0
    agent-2: 186.5
    agent-3: 186.5
    agent-4: 165.5
    agent-5: 165.5
  policy_reward_mean:
    agent-0: 112.09
    agent-1: 112.09
    agent-2: 125.36
    agent-3: 125.36
    agent-4: 109.16
    agent-5: 109.16
  policy_reward_min:
    agent-0: 55.0
    agent-1: 55.0
    agent-2: 54.0
    agent-3: 54.0
    agent-4: 49.0
    agent-5: 49.0
  sampler_perf:
    mean_env_wait_ms: 26.90451085757572
    mean_inference_ms: 13.125324559576514
    mean_processing_ms: 58.81057585210026
  time_since_restore: 12434.937654972076
  time_this_iter_s: 132.12441325187683
  time_total_s: 12434.937654972076
  timestamp: 1637520970
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 9024000
  training_iteration: 94
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     94 |          12434.9 | 9024000 |   693.22 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.11
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 25.65
    apples_agent-1_min: 8
    apples_agent-2_max: 265
    apples_agent-2_mean: 179.85
    apples_agent-2_min: 86
    apples_agent-3_max: 140
    apples_agent-3_mean: 22.28
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.2
    apples_agent-4_min: 0
    apples_agent-5_max: 308
    apples_agent-5_mean: 192.74
    apples_agent-5_min: 102
    cleaning_beam_agent-0_max: 645
    cleaning_beam_agent-0_mean: 473.88
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 59
    cleaning_beam_agent-1_mean: 14.52
    cleaning_beam_agent-1_min: 5
    cleaning_beam_agent-2_max: 125
    cleaning_beam_agent-2_mean: 45.72
    cleaning_beam_agent-2_min: 16
    cleaning_beam_agent-3_max: 559
    cleaning_beam_agent-3_mean: 248.34
    cleaning_beam_agent-3_min: 118
    cleaning_beam_agent-4_max: 772
    cleaning_beam_agent-4_mean: 606.73
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 179
    cleaning_beam_agent-5_mean: 80.17
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.19
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_13-58-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1051.0
  episode_reward_mean: 693.94
  episode_reward_min: 372.0
  episodes_this_iter: 96
  episodes_total: 9120
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12757.96
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006969023961573839
        entropy: 0.780031144618988
        entropy_coeff: 0.0017600000137463212
        kl: 0.012346796691417694
        model: {}
        policy_loss: -0.020409084856510162
        total_loss: -0.018889740109443665
        vf_explained_var: 0.05723030865192413
        vf_loss: 25.835302352905273
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006969023961573839
        entropy: 0.7330126762390137
        entropy_coeff: 0.0017600000137463212
        kl: 0.011579285375773907
        model: {}
        policy_loss: -0.021608619019389153
        total_loss: -0.020264599472284317
        vf_explained_var: 0.12570172548294067
        vf_loss: 24.170089721679688
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006969023961573839
        entropy: 0.837324857711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.012031637132167816
        model: {}
        policy_loss: -0.024503031745553017
        total_loss: -0.022302817553281784
        vf_explained_var: 0.10756276547908783
        vf_loss: 30.72322654724121
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006969023961573839
        entropy: 0.9547549486160278
        entropy_coeff: 0.0017600000137463212
        kl: 0.013398788869380951
        model: {}
        policy_loss: -0.02208355814218521
        total_loss: -0.01970866322517395
        vf_explained_var: -0.007003918290138245
        vf_loss: 35.52808380126953
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006969023961573839
        entropy: 0.9207521677017212
        entropy_coeff: 0.0017600000137463212
        kl: 0.010678725317120552
        model: {}
        policy_loss: -0.017370887100696564
        total_loss: -0.016049126163125038
        vf_explained_var: 0.01701195538043976
        vf_loss: 28.47251319885254
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006969023961573839
        entropy: 0.6350314617156982
        entropy_coeff: 0.0017600000137463212
        kl: 0.011448895558714867
        model: {}
        policy_loss: -0.022809328511357307
        total_loss: -0.02106216549873352
        vf_explained_var: 0.15700435638427734
        vf_loss: 24.35480499267578
    load_time_ms: 13224.669
    num_steps_sampled: 9120000
    num_steps_trained: 9120000
    sample_time_ms: 106370.837
    update_time_ms: 16.294
  iterations_since_restore: 95
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.411170212765963
    ram_util_percent: 11.519148936170215
  pid: 28570
  policy_reward_max:
    agent-0: 171.0
    agent-1: 171.0
    agent-2: 182.0
    agent-3: 182.0
    agent-4: 173.5
    agent-5: 173.5
  policy_reward_mean:
    agent-0: 112.26
    agent-1: 112.26
    agent-2: 127.775
    agent-3: 127.775
    agent-4: 106.935
    agent-5: 106.935
  policy_reward_min:
    agent-0: 48.0
    agent-1: 48.0
    agent-2: 65.0
    agent-3: 65.0
    agent-4: 36.0
    agent-5: 36.0
  sampler_perf:
    mean_env_wait_ms: 26.919392310682653
    mean_inference_ms: 13.124573983614303
    mean_processing_ms: 58.79929243413562
  time_since_restore: 12567.155425548553
  time_this_iter_s: 132.21777057647705
  time_total_s: 12567.155425548553
  timestamp: 1637521102
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 9120000
  training_iteration: 95
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     95 |          12567.2 | 9120000 |   693.94 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 23.19
    apples_agent-1_min: 8
    apples_agent-2_max: 295
    apples_agent-2_mean: 181.34
    apples_agent-2_min: 51
    apples_agent-3_max: 88
    apples_agent-3_mean: 20.37
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.57
    apples_agent-4_min: 0
    apples_agent-5_max: 312
    apples_agent-5_mean: 204.6
    apples_agent-5_min: 77
    cleaning_beam_agent-0_max: 635
    cleaning_beam_agent-0_mean: 471.14
    cleaning_beam_agent-0_min: 289
    cleaning_beam_agent-1_max: 32
    cleaning_beam_agent-1_mean: 14.35
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 113
    cleaning_beam_agent-2_mean: 41.57
    cleaning_beam_agent-2_min: 14
    cleaning_beam_agent-3_max: 461
    cleaning_beam_agent-3_mean: 260.02
    cleaning_beam_agent-3_min: 139
    cleaning_beam_agent-4_max: 754
    cleaning_beam_agent-4_mean: 603.65
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 155
    cleaning_beam_agent-5_mean: 69.38
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.08
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.13
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.11
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.23
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-00-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 995.0
  episode_reward_mean: 716.56
  episode_reward_min: 272.0
  episodes_this_iter: 96
  episodes_total: 9216
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12769.582
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006909120129421353
        entropy: 0.7748810052871704
        entropy_coeff: 0.0017600000137463212
        kl: 0.013536477461457253
        model: {}
        policy_loss: -0.019887883216142654
        total_loss: -0.018269436433911324
        vf_explained_var: 0.06272673606872559
        vf_loss: 26.438249588012695
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006909120129421353
        entropy: 0.7276170253753662
        entropy_coeff: 0.0017600000137463212
        kl: 0.011032545007765293
        model: {}
        policy_loss: -0.022003378719091415
        total_loss: -0.02062247321009636
        vf_explained_var: 0.13900482654571533
        vf_loss: 24.546493530273438
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006909120129421353
        entropy: 0.8170087337493896
        entropy_coeff: 0.0017600000137463212
        kl: 0.012163916602730751
        model: {}
        policy_loss: -0.02354763075709343
        total_loss: -0.021485041826963425
        vf_explained_var: 0.1113278865814209
        vf_loss: 28.923290252685547
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006909120129421353
        entropy: 0.9658846259117126
        entropy_coeff: 0.0017600000137463212
        kl: 0.014879323542118073
        model: {}
        policy_loss: -0.021714117377996445
        total_loss: -0.019453857094049454
        vf_explained_var: -0.023171067237854004
        vf_loss: 34.02244567871094
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006909120129421353
        entropy: 0.9274544715881348
        entropy_coeff: 0.0017600000137463212
        kl: 0.01227194257080555
        model: {}
        policy_loss: -0.01891520991921425
        total_loss: -0.01765262708067894
        vf_explained_var: 0.02176062762737274
        vf_loss: 27.856952667236328
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006909120129421353
        entropy: 0.6039485931396484
        entropy_coeff: 0.0017600000137463212
        kl: 0.010874660685658455
        model: {}
        policy_loss: -0.022666484117507935
        total_loss: -0.020966652780771255
        vf_explained_var: 0.1611921489238739
        vf_loss: 23.549827575683594
    load_time_ms: 13215.558
    num_steps_sampled: 9216000
    num_steps_trained: 9216000
    sample_time_ms: 106307.638
    update_time_ms: 16.028
  iterations_since_restore: 96
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.506417112299463
    ram_util_percent: 11.605882352941178
  pid: 28570
  policy_reward_max:
    agent-0: 174.0
    agent-1: 174.0
    agent-2: 188.5
    agent-3: 188.5
    agent-4: 178.0
    agent-5: 178.0
  policy_reward_mean:
    agent-0: 112.59
    agent-1: 112.59
    agent-2: 129.735
    agent-3: 129.735
    agent-4: 115.955
    agent-5: 115.955
  policy_reward_min:
    agent-0: 44.0
    agent-1: 44.0
    agent-2: 36.5
    agent-3: 36.5
    agent-4: 52.5
    agent-5: 52.5
  sampler_perf:
    mean_env_wait_ms: 26.934172437574375
    mean_inference_ms: 13.122689951972621
    mean_processing_ms: 58.79228910935339
  time_since_restore: 12698.994042873383
  time_this_iter_s: 131.8386173248291
  time_total_s: 12698.994042873383
  timestamp: 1637521234
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 9216000
  training_iteration: 96
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     96 |            12699 | 9216000 |   716.56 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 1.68
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 24.21
    apples_agent-1_min: 11
    apples_agent-2_max: 281
    apples_agent-2_mean: 182.18
    apples_agent-2_min: 69
    apples_agent-3_max: 62
    apples_agent-3_mean: 19.77
    apples_agent-3_min: 0
    apples_agent-4_max: 54
    apples_agent-4_mean: 0.91
    apples_agent-4_min: 0
    apples_agent-5_max: 312
    apples_agent-5_mean: 197.32
    apples_agent-5_min: 91
    cleaning_beam_agent-0_max: 666
    cleaning_beam_agent-0_mean: 482.67
    cleaning_beam_agent-0_min: 299
    cleaning_beam_agent-1_max: 40
    cleaning_beam_agent-1_mean: 14.71
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 43.31
    cleaning_beam_agent-2_min: 18
    cleaning_beam_agent-3_max: 540
    cleaning_beam_agent-3_mean: 274.04
    cleaning_beam_agent-3_min: 122
    cleaning_beam_agent-4_max: 734
    cleaning_beam_agent-4_mean: 605.28
    cleaning_beam_agent-4_min: 296
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 87.95
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-02-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1032.0
  episode_reward_mean: 705.86
  episode_reward_min: 337.0
  episodes_this_iter: 96
  episodes_total: 9312
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12757.252
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006849215715192258
        entropy: 0.7722929120063782
        entropy_coeff: 0.0017600000137463212
        kl: 0.011519737541675568
        model: {}
        policy_loss: -0.019840549677610397
        total_loss: -0.01840515062212944
        vf_explained_var: 0.07768544554710388
        vf_loss: 25.066421508789062
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006849215715192258
        entropy: 0.7142437100410461
        entropy_coeff: 0.0017600000137463212
        kl: 0.013145212084054947
        model: {}
        policy_loss: -0.022367510944604874
        total_loss: -0.02099701017141342
        vf_explained_var: 0.12278509140014648
        vf_loss: 23.810970306396484
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006849215715192258
        entropy: 0.8161223530769348
        entropy_coeff: 0.0017600000137463212
        kl: 0.011808585375547409
        model: {}
        policy_loss: -0.02345002070069313
        total_loss: -0.021435528993606567
        vf_explained_var: 0.13669444620609283
        vf_loss: 28.60437774658203
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006849215715192258
        entropy: 1.0027270317077637
        entropy_coeff: 0.0017600000137463212
        kl: 0.013364134356379509
        model: {}
        policy_loss: -0.022901594638824463
        total_loss: -0.020744934678077698
        vf_explained_var: -0.005560934543609619
        vf_loss: 34.203041076660156
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006849215715192258
        entropy: 0.8915709853172302
        entropy_coeff: 0.0017600000137463212
        kl: 0.011559003964066505
        model: {}
        policy_loss: -0.017435945570468903
        total_loss: -0.016077253967523575
        vf_explained_var: 0.017549946904182434
        vf_loss: 28.249919891357422
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006849215715192258
        entropy: 0.6304003596305847
        entropy_coeff: 0.0017600000137463212
        kl: 0.011599106714129448
        model: {}
        policy_loss: -0.02265312895178795
        total_loss: -0.020926065742969513
        vf_explained_var: 0.17044594883918762
        vf_loss: 24.01602554321289
    load_time_ms: 13223.966
    num_steps_sampled: 9312000
    num_steps_trained: 9312000
    sample_time_ms: 106266.08
    update_time_ms: 16.14
  iterations_since_restore: 97
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.386772486772482
    ram_util_percent: 11.673015873015874
  pid: 28570
  policy_reward_max:
    agent-0: 172.5
    agent-1: 172.5
    agent-2: 205.0
    agent-3: 205.0
    agent-4: 174.0
    agent-5: 174.0
  policy_reward_mean:
    agent-0: 113.385
    agent-1: 113.385
    agent-2: 129.13
    agent-3: 129.13
    agent-4: 110.415
    agent-5: 110.415
  policy_reward_min:
    agent-0: 57.0
    agent-1: 57.0
    agent-2: 64.5
    agent-3: 64.5
    agent-4: 39.5
    agent-5: 39.5
  sampler_perf:
    mean_env_wait_ms: 26.951120214381387
    mean_inference_ms: 13.121653457786765
    mean_processing_ms: 58.78263976959256
  time_since_restore: 12831.843105793
  time_this_iter_s: 132.8490629196167
  time_total_s: 12831.843105793
  timestamp: 1637521367
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 9312000
  training_iteration: 97
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     97 |          12831.8 | 9312000 |   705.86 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 1.49
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.65
    apples_agent-1_min: 10
    apples_agent-2_max: 285
    apples_agent-2_mean: 185.23
    apples_agent-2_min: 44
    apples_agent-3_max: 66
    apples_agent-3_mean: 20.0
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.57
    apples_agent-4_min: 0
    apples_agent-5_max: 311
    apples_agent-5_mean: 207.36
    apples_agent-5_min: 78
    cleaning_beam_agent-0_max: 674
    cleaning_beam_agent-0_mean: 466.62
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 46
    cleaning_beam_agent-1_mean: 12.5
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 195
    cleaning_beam_agent-2_mean: 46.42
    cleaning_beam_agent-2_min: 11
    cleaning_beam_agent-3_max: 577
    cleaning_beam_agent-3_mean: 270.12
    cleaning_beam_agent-3_min: 108
    cleaning_beam_agent-4_max: 747
    cleaning_beam_agent-4_mean: 560.14
    cleaning_beam_agent-4_min: 221
    cleaning_beam_agent-5_max: 191
    cleaning_beam_agent-5_mean: 80.56
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.2
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-04-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1027.0
  episode_reward_mean: 718.31
  episode_reward_min: 344.0
  episodes_this_iter: 96
  episodes_total: 9408
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12775.858
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006789311883039773
        entropy: 0.7619569897651672
        entropy_coeff: 0.0017600000137463212
        kl: 0.011424309574067593
        model: {}
        policy_loss: -0.02030927687883377
        total_loss: -0.01832534372806549
        vf_explained_var: 0.10104256868362427
        vf_loss: 30.393709182739258
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006789311883039773
        entropy: 0.7075337171554565
        entropy_coeff: 0.0017600000137463212
        kl: 0.011931413784623146
        model: {}
        policy_loss: -0.021027369424700737
        total_loss: -0.01911655068397522
        vf_explained_var: 0.12914150953292847
        vf_loss: 29.32364273071289
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006789311883039773
        entropy: 0.8368513584136963
        entropy_coeff: 0.0017600000137463212
        kl: 0.012483794242143631
        model: {}
        policy_loss: -0.024855734780430794
        total_loss: -0.02247694320976734
        vf_explained_var: 0.15030887722969055
        vf_loss: 32.27461242675781
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006789311883039773
        entropy: 0.9742509126663208
        entropy_coeff: 0.0017600000137463212
        kl: 0.013660195283591747
        model: {}
        policy_loss: -0.024350006133317947
        total_loss: -0.021765533834695816
        vf_explained_var: 0.008402422070503235
        vf_loss: 37.86900329589844
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006789311883039773
        entropy: 0.9492305517196655
        entropy_coeff: 0.0017600000137463212
        kl: 0.011433630250394344
        model: {}
        policy_loss: -0.017772303894162178
        total_loss: -0.015932265669107437
        vf_explained_var: -0.0011433959007263184
        vf_loss: 34.0893669128418
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006789311883039773
        entropy: 0.5977675914764404
        entropy_coeff: 0.0017600000137463212
        kl: 0.011514854617416859
        model: {}
        policy_loss: -0.02048058807849884
        total_loss: -0.01835007220506668
        vf_explained_var: 0.19864369928836823
        vf_loss: 27.507783889770508
    load_time_ms: 13234.668
    num_steps_sampled: 9408000
    num_steps_trained: 9408000
    sample_time_ms: 106120.028
    update_time_ms: 15.588
  iterations_since_restore: 98
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.394117647058824
    ram_util_percent: 11.621390374331554
  pid: 28570
  policy_reward_max:
    agent-0: 170.0
    agent-1: 170.0
    agent-2: 186.0
    agent-3: 186.0
    agent-4: 175.0
    agent-5: 175.0
  policy_reward_mean:
    agent-0: 115.74
    agent-1: 115.74
    agent-2: 128.14
    agent-3: 128.14
    agent-4: 115.275
    agent-5: 115.275
  policy_reward_min:
    agent-0: 57.5
    agent-1: 57.5
    agent-2: 47.0
    agent-3: 47.0
    agent-4: 45.0
    agent-5: 45.0
  sampler_perf:
    mean_env_wait_ms: 26.96221245030637
    mean_inference_ms: 13.120680390042397
    mean_processing_ms: 58.77112894804819
  time_since_restore: 12963.242941856384
  time_this_iter_s: 131.399836063385
  time_total_s: 12963.242941856384
  timestamp: 1637521498
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 9408000
  training_iteration: 98
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     98 |          12963.2 | 9408000 |   718.31 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 24.61
    apples_agent-1_min: 6
    apples_agent-2_max: 291
    apples_agent-2_mean: 177.34
    apples_agent-2_min: 57
    apples_agent-3_max: 84
    apples_agent-3_mean: 19.01
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.57
    apples_agent-4_min: 0
    apples_agent-5_max: 347
    apples_agent-5_mean: 202.09
    apples_agent-5_min: 80
    cleaning_beam_agent-0_max: 600
    cleaning_beam_agent-0_mean: 417.75
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 39
    cleaning_beam_agent-1_mean: 11.94
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 129
    cleaning_beam_agent-2_mean: 48.61
    cleaning_beam_agent-2_min: 11
    cleaning_beam_agent-3_max: 414
    cleaning_beam_agent-3_mean: 269.61
    cleaning_beam_agent-3_min: 140
    cleaning_beam_agent-4_max: 716
    cleaning_beam_agent-4_mean: 536.45
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 66.1
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.13
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-07-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1124.0
  episode_reward_mean: 707.76
  episode_reward_min: 270.0
  episodes_this_iter: 96
  episodes_total: 9504
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12767.261
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006729408050887287
        entropy: 0.8196048736572266
        entropy_coeff: 0.0017600000137463212
        kl: 0.011863166466355324
        model: {}
        policy_loss: -0.0220645684748888
        total_loss: -0.02053031325340271
        vf_explained_var: 0.09156528115272522
        vf_loss: 26.801794052124023
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006729408050887287
        entropy: 0.703667163848877
        entropy_coeff: 0.0017600000137463212
        kl: 0.011390498839318752
        model: {}
        policy_loss: -0.02120707556605339
        total_loss: -0.019566360861063004
        vf_explained_var: 0.0900963544845581
        vf_loss: 26.65597915649414
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006729408050887287
        entropy: 0.8606226444244385
        entropy_coeff: 0.0017600000137463212
        kl: 0.013213777914643288
        model: {}
        policy_loss: -0.025216156616806984
        total_loss: -0.02319188416004181
        vf_explained_var: 0.15705691277980804
        vf_loss: 28.782791137695312
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006729408050887287
        entropy: 0.9844893217086792
        entropy_coeff: 0.0017600000137463212
        kl: 0.01242698822170496
        model: {}
        policy_loss: -0.023157913237810135
        total_loss: -0.020927704870700836
        vf_explained_var: -0.025212794542312622
        vf_loss: 34.968963623046875
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006729408050887287
        entropy: 0.9766443371772766
        entropy_coeff: 0.0017600000137463212
        kl: 0.011914588510990143
        model: {}
        policy_loss: -0.018859608098864555
        total_loss: -0.017810648307204247
        vf_explained_var: 0.025804534554481506
        vf_loss: 26.61823081970215
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006729408050887287
        entropy: 0.561428964138031
        entropy_coeff: 0.0017600000137463212
        kl: 0.010098730213940144
        model: {}
        policy_loss: -0.020403940230607986
        total_loss: -0.018730882555246353
        vf_explained_var: 0.16179338097572327
        vf_loss: 22.824668884277344
    load_time_ms: 13234.752
    num_steps_sampled: 9504000
    num_steps_trained: 9504000
    sample_time_ms: 106060.178
    update_time_ms: 16.052
  iterations_since_restore: 99
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.324468085106382
    ram_util_percent: 11.604787234042558
  pid: 28570
  policy_reward_max:
    agent-0: 185.5
    agent-1: 185.5
    agent-2: 210.5
    agent-3: 210.5
    agent-4: 187.0
    agent-5: 187.0
  policy_reward_mean:
    agent-0: 113.62
    agent-1: 113.62
    agent-2: 125.02
    agent-3: 125.02
    agent-4: 115.24
    agent-5: 115.24
  policy_reward_min:
    agent-0: 44.5
    agent-1: 44.5
    agent-2: 43.0
    agent-3: 43.0
    agent-4: 47.0
    agent-5: 47.0
  sampler_perf:
    mean_env_wait_ms: 26.971722529815885
    mean_inference_ms: 13.119246783213525
    mean_processing_ms: 58.76215004026399
  time_since_restore: 13095.334644794464
  time_this_iter_s: 132.09170293807983
  time_total_s: 13095.334644794464
  timestamp: 1637521630
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 9504000
  training_iteration: 99
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |     99 |          13095.3 | 9504000 |   707.76 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.39
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 23.87
    apples_agent-1_min: 4
    apples_agent-2_max: 286
    apples_agent-2_mean: 169.5
    apples_agent-2_min: 76
    apples_agent-3_max: 60
    apples_agent-3_mean: 16.7
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 330
    apples_agent-5_mean: 195.47
    apples_agent-5_min: 72
    cleaning_beam_agent-0_max: 680
    cleaning_beam_agent-0_mean: 436.48
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 47
    cleaning_beam_agent-1_mean: 12.93
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 48.84
    cleaning_beam_agent-2_min: 19
    cleaning_beam_agent-3_max: 467
    cleaning_beam_agent-3_mean: 283.53
    cleaning_beam_agent-3_min: 128
    cleaning_beam_agent-4_max: 754
    cleaning_beam_agent-4_mean: 520.54
    cleaning_beam_agent-4_min: 186
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 71.1
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.19
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-09-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1047.0
  episode_reward_mean: 685.11
  episode_reward_min: 291.0
  episodes_this_iter: 96
  episodes_total: 9600
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12764.6
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006669504218734801
        entropy: 0.8071885704994202
        entropy_coeff: 0.0017600000137463212
        kl: 0.012241563759744167
        model: {}
        policy_loss: -0.02194204553961754
        total_loss: -0.020459484308958054
        vf_explained_var: 0.05701591074466705
        vf_loss: 25.971755981445312
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006669504218734801
        entropy: 0.7238403558731079
        entropy_coeff: 0.0017600000137463212
        kl: 0.01128601934760809
        model: {}
        policy_loss: -0.02185603231191635
        total_loss: -0.020419791340827942
        vf_explained_var: 0.08974331617355347
        vf_loss: 24.98587417602539
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006669504218734801
        entropy: 0.8787222504615784
        entropy_coeff: 0.0017600000137463212
        kl: 0.014496417716145515
        model: {}
        policy_loss: -0.024434903636574745
        total_loss: -0.022572945803403854
        vf_explained_var: 0.14403444528579712
        vf_loss: 26.836904525756836
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006669504218734801
        entropy: 0.968722403049469
        entropy_coeff: 0.0017600000137463212
        kl: 0.011997444555163383
        model: {}
        policy_loss: -0.02282099798321724
        total_loss: -0.020931066945195198
        vf_explained_var: -0.0027537494897842407
        vf_loss: 31.44980239868164
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006669504218734801
        entropy: 0.988173246383667
        entropy_coeff: 0.0017600000137463212
        kl: 0.01152205839753151
        model: {}
        policy_loss: -0.019778525456786156
        total_loss: -0.0186102706938982
        vf_explained_var: -0.044809579849243164
        vf_loss: 28.04909324645996
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006669504218734801
        entropy: 0.554705023765564
        entropy_coeff: 0.0017600000137463212
        kl: 0.010457178577780724
        model: {}
        policy_loss: -0.021079540252685547
        total_loss: -0.01939435675740242
        vf_explained_var: 0.14849688112735748
        vf_loss: 22.693220138549805
    load_time_ms: 13229.979
    num_steps_sampled: 9600000
    num_steps_trained: 9600000
    sample_time_ms: 106176.942
    update_time_ms: 15.729
  iterations_since_restore: 100
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.288359788359788
    ram_util_percent: 11.5984126984127
  pid: 28570
  policy_reward_max:
    agent-0: 171.5
    agent-1: 171.5
    agent-2: 181.0
    agent-3: 181.0
    agent-4: 184.0
    agent-5: 184.0
  policy_reward_mean:
    agent-0: 111.44
    agent-1: 111.44
    agent-2: 119.155
    agent-3: 119.155
    agent-4: 111.96
    agent-5: 111.96
  policy_reward_min:
    agent-0: 49.0
    agent-1: 49.0
    agent-2: 43.0
    agent-3: 43.0
    agent-4: 40.5
    agent-5: 40.5
  sampler_perf:
    mean_env_wait_ms: 26.983161833609127
    mean_inference_ms: 13.118146644312096
    mean_processing_ms: 58.756448779676084
  time_since_restore: 13228.273141384125
  time_this_iter_s: 132.93849658966064
  time_total_s: 13228.273141384125
  timestamp: 1637521763
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 9600000
  training_iteration: 100
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    100 |          13228.3 | 9600000 |   685.11 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 1.7
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 22.1
    apples_agent-1_min: 9
    apples_agent-2_max: 258
    apples_agent-2_mean: 157.06
    apples_agent-2_min: 24
    apples_agent-3_max: 77
    apples_agent-3_mean: 18.09
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 321
    apples_agent-5_mean: 186.27
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 631
    cleaning_beam_agent-0_mean: 395.17
    cleaning_beam_agent-0_min: 173
    cleaning_beam_agent-1_max: 38
    cleaning_beam_agent-1_mean: 13.91
    cleaning_beam_agent-1_min: 4
    cleaning_beam_agent-2_max: 143
    cleaning_beam_agent-2_mean: 54.77
    cleaning_beam_agent-2_min: 20
    cleaning_beam_agent-3_max: 471
    cleaning_beam_agent-3_mean: 298.32
    cleaning_beam_agent-3_min: 171
    cleaning_beam_agent-4_max: 748
    cleaning_beam_agent-4_mean: 546.43
    cleaning_beam_agent-4_min: 233
    cleaning_beam_agent-5_max: 269
    cleaning_beam_agent-5_mean: 78.34
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.15
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 4
    fire_beam_agent-2_mean: 0.15
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.11
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-11-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 970.0
  episode_reward_mean: 651.39
  episode_reward_min: 143.0
  episodes_this_iter: 96
  episodes_total: 9696
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12763.812
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006609599804505706
        entropy: 0.8202065229415894
        entropy_coeff: 0.0017600000137463212
        kl: 0.012481681071221828
        model: {}
        policy_loss: -0.021148763597011566
        total_loss: -0.019709784537553787
        vf_explained_var: 0.14586929976940155
        vf_loss: 25.70503044128418
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006609599804505706
        entropy: 0.7196028828620911
        entropy_coeff: 0.0017600000137463212
        kl: 0.011746581643819809
        model: {}
        policy_loss: -0.022414127364754677
        total_loss: -0.020830431953072548
        vf_explained_var: 0.12245702743530273
        vf_loss: 26.299463272094727
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006609599804505706
        entropy: 0.9207653403282166
        entropy_coeff: 0.0017600000137463212
        kl: 0.013261801563203335
        model: {}
        policy_loss: -0.025984477251768112
        total_loss: -0.02399858459830284
        vf_explained_var: 0.17108681797981262
        vf_loss: 29.43352508544922
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006609599804505706
        entropy: 0.9317510724067688
        entropy_coeff: 0.0017600000137463212
        kl: 0.012744599021971226
        model: {}
        policy_loss: -0.021083073690533638
        total_loss: -0.01869303733110428
        vf_explained_var: 0.009797647595405579
        vf_loss: 35.51995086669922
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006609599804505706
        entropy: 0.9538776278495789
        entropy_coeff: 0.0017600000137463212
        kl: 0.012512351386249065
        model: {}
        policy_loss: -0.01984100416302681
        total_loss: -0.018586257472634315
        vf_explained_var: 0.05238085985183716
        vf_loss: 28.222248077392578
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006609599804505706
        entropy: 0.5858587026596069
        entropy_coeff: 0.0017600000137463212
        kl: 0.011872388422489166
        model: {}
        policy_loss: -0.02135552279651165
        total_loss: -0.01949951983988285
        vf_explained_var: 0.18092909455299377
        vf_loss: 24.418991088867188
    load_time_ms: 13232.502
    num_steps_sampled: 9696000
    num_steps_trained: 9696000
    sample_time_ms: 106140.59
    update_time_ms: 15.617
  iterations_since_restore: 101
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.485106382978728
    ram_util_percent: 11.609574468085109
  pid: 28570
  policy_reward_max:
    agent-0: 179.5
    agent-1: 179.5
    agent-2: 179.0
    agent-3: 179.0
    agent-4: 185.5
    agent-5: 185.5
  policy_reward_mean:
    agent-0: 108.95
    agent-1: 108.95
    agent-2: 110.42
    agent-3: 110.42
    agent-4: 106.325
    agent-5: 106.325
  policy_reward_min:
    agent-0: 35.5
    agent-1: 35.5
    agent-2: 22.0
    agent-3: 22.0
    agent-4: 6.5
    agent-5: 6.5
  sampler_perf:
    mean_env_wait_ms: 26.994615873595095
    mean_inference_ms: 13.117033469231812
    mean_processing_ms: 58.752201394531745
  time_since_restore: 13360.564702272415
  time_this_iter_s: 132.2915608882904
  time_total_s: 13360.564702272415
  timestamp: 1637521896
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 9696000
  training_iteration: 101
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    101 |          13360.6 | 9696000 |   651.39 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 22.61
    apples_agent-1_min: 9
    apples_agent-2_max: 298
    apples_agent-2_mean: 161.45
    apples_agent-2_min: 53
    apples_agent-3_max: 81
    apples_agent-3_mean: 18.56
    apples_agent-3_min: 0
    apples_agent-4_max: 31
    apples_agent-4_mean: 2.01
    apples_agent-4_min: 0
    apples_agent-5_max: 325
    apples_agent-5_mean: 189.45
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 723
    cleaning_beam_agent-0_mean: 403.72
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 30
    cleaning_beam_agent-1_mean: 10.64
    cleaning_beam_agent-1_min: 3
    cleaning_beam_agent-2_max: 106
    cleaning_beam_agent-2_mean: 43.26
    cleaning_beam_agent-2_min: 11
    cleaning_beam_agent-3_max: 477
    cleaning_beam_agent-3_mean: 306.45
    cleaning_beam_agent-3_min: 173
    cleaning_beam_agent-4_max: 754
    cleaning_beam_agent-4_mean: 533.6
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 248
    cleaning_beam_agent-5_mean: 87.39
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.13
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.17
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-13-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1071.0
  episode_reward_mean: 664.42
  episode_reward_min: 222.0
  episodes_this_iter: 96
  episodes_total: 9792
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12762.251
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000654969597235322
        entropy: 0.8167845010757446
        entropy_coeff: 0.0017600000137463212
        kl: 0.011917456984519958
        model: {}
        policy_loss: -0.020489562302827835
        total_loss: -0.018812473863363266
        vf_explained_var: 0.09158794581890106
        vf_loss: 28.166913986206055
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000654969597235322
        entropy: 0.7164874076843262
        entropy_coeff: 0.0017600000137463212
        kl: 0.012095632031559944
        model: {}
        policy_loss: -0.022387634962797165
        total_loss: -0.020727520808577538
        vf_explained_var: 0.12973901629447937
        vf_loss: 26.943389892578125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000654969597235322
        entropy: 0.8722186088562012
        entropy_coeff: 0.0017600000137463212
        kl: 0.013153024017810822
        model: {}
        policy_loss: -0.026919670403003693
        total_loss: -0.025066211819648743
        vf_explained_var: 0.1560124009847641
        vf_loss: 27.309104919433594
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000654969597235322
        entropy: 0.9517130851745605
        entropy_coeff: 0.0017600000137463212
        kl: 0.012582202441990376
        model: {}
        policy_loss: -0.022487111389636993
        total_loss: -0.02041488140821457
        vf_explained_var: -0.011139899492263794
        vf_loss: 32.75412368774414
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000654969597235322
        entropy: 0.9343554377555847
        entropy_coeff: 0.0017600000137463212
        kl: 0.012693731114268303
        model: {}
        policy_loss: -0.020282544195652008
        total_loss: -0.019122673198580742
        vf_explained_var: 0.08185452222824097
        vf_loss: 26.913734436035156
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000654969597235322
        entropy: 0.5825849175453186
        entropy_coeff: 0.0017600000137463212
        kl: 0.01064593531191349
        model: {}
        policy_loss: -0.021707717329263687
        total_loss: -0.019993649795651436
        vf_explained_var: 0.19244185090065002
        vf_loss: 23.40196418762207
    load_time_ms: 13228.806
    num_steps_sampled: 9792000
    num_steps_trained: 9792000
    sample_time_ms: 106269.308
    update_time_ms: 16.0
  iterations_since_restore: 102
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.384656084656083
    ram_util_percent: 11.607936507936511
  pid: 28570
  policy_reward_max:
    agent-0: 180.0
    agent-1: 180.0
    agent-2: 182.5
    agent-3: 182.5
    agent-4: 185.0
    agent-5: 185.0
  policy_reward_mean:
    agent-0: 109.765
    agent-1: 109.765
    agent-2: 113.575
    agent-3: 113.575
    agent-4: 108.87
    agent-5: 108.87
  policy_reward_min:
    agent-0: 43.0
    agent-1: 43.0
    agent-2: 16.5
    agent-3: 16.5
    agent-4: 47.5
    agent-5: 47.5
  sampler_perf:
    mean_env_wait_ms: 27.00513278185369
    mean_inference_ms: 13.116121818565134
    mean_processing_ms: 58.746110790820985
  time_since_restore: 13493.445833921432
  time_this_iter_s: 132.88113164901733
  time_total_s: 13493.445833921432
  timestamp: 1637522029
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 9792000
  training_iteration: 102
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    102 |          13493.4 | 9792000 |   664.42 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 25.31
    apples_agent-1_min: 9
    apples_agent-2_max: 284
    apples_agent-2_mean: 177.68
    apples_agent-2_min: 48
    apples_agent-3_max: 52
    apples_agent-3_mean: 16.42
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 315
    apples_agent-5_mean: 214.46
    apples_agent-5_min: 71
    cleaning_beam_agent-0_max: 687
    cleaning_beam_agent-0_mean: 431.11
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 43
    cleaning_beam_agent-1_mean: 9.72
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 120
    cleaning_beam_agent-2_mean: 43.46
    cleaning_beam_agent-2_min: 10
    cleaning_beam_agent-3_max: 483
    cleaning_beam_agent-3_mean: 307.48
    cleaning_beam_agent-3_min: 171
    cleaning_beam_agent-4_max: 754
    cleaning_beam_agent-4_mean: 594.12
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 252
    cleaning_beam_agent-5_mean: 70.6
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.1
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.11
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-16-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1011.0
  episode_reward_mean: 735.0
  episode_reward_min: 323.0
  episodes_this_iter: 96
  episodes_total: 9888
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12751.294
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006489792140200734
        entropy: 0.7602887749671936
        entropy_coeff: 0.0017600000137463212
        kl: 0.012686355970799923
        model: {}
        policy_loss: -0.021302806213498116
        total_loss: -0.01939420960843563
        vf_explained_var: 0.1058523952960968
        vf_loss: 29.295452117919922
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006489792140200734
        entropy: 0.6604073643684387
        entropy_coeff: 0.0017600000137463212
        kl: 0.010936719365417957
        model: {}
        policy_loss: -0.02136368677020073
        total_loss: -0.019484993070364
        vf_explained_var: 0.1285989135503769
        vf_loss: 28.359434127807617
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006489792140200734
        entropy: 0.8483769297599792
        entropy_coeff: 0.0017600000137463212
        kl: 0.012462061829864979
        model: {}
        policy_loss: -0.026143964380025864
        total_loss: -0.024188179522752762
        vf_explained_var: 0.1364690363407135
        vf_loss: 28.25821304321289
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006489792140200734
        entropy: 0.9510961174964905
        entropy_coeff: 0.0017600000137463212
        kl: 0.011836225166916847
        model: {}
        policy_loss: -0.0219026617705822
        total_loss: -0.0198170505464077
        vf_explained_var: -0.02935740351676941
        vf_loss: 33.15686798095703
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006489792140200734
        entropy: 0.8653960824012756
        entropy_coeff: 0.0017600000137463212
        kl: 0.012383748777210712
        model: {}
        policy_loss: -0.018770303577184677
        total_loss: -0.017163066193461418
        vf_explained_var: 0.03240935504436493
        vf_loss: 30.201324462890625
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006489792140200734
        entropy: 0.5308192372322083
        entropy_coeff: 0.0017600000137463212
        kl: 0.009703758172690868
        model: {}
        policy_loss: -0.020380469039082527
        total_loss: -0.01834787428379059
        vf_explained_var: 0.162782222032547
        vf_loss: 26.02943992614746
    load_time_ms: 13246.902
    num_steps_sampled: 9888000
    num_steps_trained: 9888000
    sample_time_ms: 106203.463
    update_time_ms: 16.305
  iterations_since_restore: 103
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.366489361702126
    ram_util_percent: 11.615957446808512
  pid: 28570
  policy_reward_max:
    agent-0: 191.0
    agent-1: 191.0
    agent-2: 180.0
    agent-3: 180.0
    agent-4: 173.5
    agent-5: 173.5
  policy_reward_mean:
    agent-0: 123.15
    agent-1: 123.15
    agent-2: 123.18
    agent-3: 123.18
    agent-4: 121.17
    agent-5: 121.17
  policy_reward_min:
    agent-0: 57.0
    agent-1: 57.0
    agent-2: 46.5
    agent-3: 46.5
    agent-4: 45.0
    agent-5: 45.0
  sampler_perf:
    mean_env_wait_ms: 27.019677587157556
    mean_inference_ms: 13.115372990008188
    mean_processing_ms: 58.739937892145484
  time_since_restore: 13625.911665201187
  time_this_iter_s: 132.46583127975464
  time_total_s: 13625.911665201187
  timestamp: 1637522161
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 9888000
  training_iteration: 103
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    103 |          13625.9 | 9888000 |      735 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.96
    apples_agent-0_min: 0
    apples_agent-1_max: 153
    apples_agent-1_mean: 24.4
    apples_agent-1_min: 6
    apples_agent-2_max: 286
    apples_agent-2_mean: 174.06
    apples_agent-2_min: 44
    apples_agent-3_max: 71
    apples_agent-3_mean: 21.65
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.09
    apples_agent-4_min: 0
    apples_agent-5_max: 342
    apples_agent-5_mean: 203.45
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 595
    cleaning_beam_agent-0_mean: 421.9
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 31
    cleaning_beam_agent-1_mean: 9.04
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 232
    cleaning_beam_agent-2_mean: 46.22
    cleaning_beam_agent-2_min: 11
    cleaning_beam_agent-3_max: 423
    cleaning_beam_agent-3_mean: 266.38
    cleaning_beam_agent-3_min: 124
    cleaning_beam_agent-4_max: 737
    cleaning_beam_agent-4_mean: 553.99
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 267
    cleaning_beam_agent-5_mean: 71.18
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-18-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1060.0
  episode_reward_mean: 715.36
  episode_reward_min: 106.0
  episodes_this_iter: 96
  episodes_total: 9984
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12758.618
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006429887725971639
        entropy: 0.7687190771102905
        entropy_coeff: 0.0017600000137463212
        kl: 0.01172163337469101
        model: {}
        policy_loss: -0.02136707492172718
        total_loss: -0.019680410623550415
        vf_explained_var: 0.12335607409477234
        vf_loss: 27.465709686279297
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006429887725971639
        entropy: 0.687017560005188
        entropy_coeff: 0.0017600000137463212
        kl: 0.01126863807439804
        model: {}
        policy_loss: -0.02226031757891178
        total_loss: -0.02060702070593834
        vf_explained_var: 0.1538182646036148
        vf_loss: 26.511621475219727
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006429887725971639
        entropy: 0.843468427658081
        entropy_coeff: 0.0017600000137463212
        kl: 0.013500414788722992
        model: {}
        policy_loss: -0.024791575968265533
        total_loss: -0.022683342918753624
        vf_explained_var: 0.1553284227848053
        vf_loss: 29.17719841003418
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006429887725971639
        entropy: 0.932466447353363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0119544118642807
        model: {}
        policy_loss: -0.023058606311678886
        total_loss: -0.020718248561024666
        vf_explained_var: -0.028561949729919434
        vf_loss: 35.332069396972656
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006429887725971639
        entropy: 0.8917484283447266
        entropy_coeff: 0.0017600000137463212
        kl: 0.011789127252995968
        model: {}
        policy_loss: -0.017680659890174866
        total_loss: -0.016351167112588882
        vf_explained_var: 0.028931185603141785
        vf_loss: 27.940628051757812
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006429887725971639
        entropy: 0.549554705619812
        entropy_coeff: 0.0017600000137463212
        kl: 0.011393822729587555
        model: {}
        policy_loss: -0.020919756963849068
        total_loss: -0.019107451662421227
        vf_explained_var: 0.1816963404417038
        vf_loss: 23.52252769470215
    load_time_ms: 13248.816
    num_steps_sampled: 9984000
    num_steps_trained: 9984000
    sample_time_ms: 106418.293
    update_time_ms: 16.48
  iterations_since_restore: 104
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.188541666666666
    ram_util_percent: 11.613020833333335
  pid: 28570
  policy_reward_max:
    agent-0: 184.0
    agent-1: 184.0
    agent-2: 171.5
    agent-3: 171.5
    agent-4: 194.0
    agent-5: 194.0
  policy_reward_mean:
    agent-0: 116.245
    agent-1: 116.245
    agent-2: 124.51
    agent-3: 124.51
    agent-4: 116.925
    agent-5: 116.925
  policy_reward_min:
    agent-0: 11.5
    agent-1: 11.5
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 28.5
    agent-5: 28.5
  sampler_perf:
    mean_env_wait_ms: 27.030262204029068
    mean_inference_ms: 13.114819458271501
    mean_processing_ms: 58.740337554882466
  time_since_restore: 13760.232909440994
  time_this_iter_s: 134.32124423980713
  time_total_s: 13760.232909440994
  timestamp: 1637522296
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 9984000
  training_iteration: 104
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    104 |          13760.2 | 9984000 |   715.36 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.49
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 22.33
    apples_agent-1_min: 4
    apples_agent-2_max: 294
    apples_agent-2_mean: 176.0
    apples_agent-2_min: 22
    apples_agent-3_max: 63
    apples_agent-3_mean: 17.22
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 293
    apples_agent-5_mean: 204.18
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 591
    cleaning_beam_agent-0_mean: 431.14
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 8.4
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 164
    cleaning_beam_agent-2_mean: 43.67
    cleaning_beam_agent-2_min: 10
    cleaning_beam_agent-3_max: 445
    cleaning_beam_agent-3_mean: 286.08
    cleaning_beam_agent-3_min: 111
    cleaning_beam_agent-4_max: 705
    cleaning_beam_agent-4_mean: 550.81
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 233
    cleaning_beam_agent-5_mean: 81.97
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.14
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.14
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-20-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1037.0
  episode_reward_mean: 721.35
  episode_reward_min: 229.0
  episodes_this_iter: 96
  episodes_total: 10080
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12764.557
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006369983893819153
        entropy: 0.7507942914962769
        entropy_coeff: 0.0017600000137463212
        kl: 0.011981095187366009
        model: {}
        policy_loss: -0.0200466588139534
        total_loss: -0.018357831984758377
        vf_explained_var: 0.11968700587749481
        vf_loss: 27.10698699951172
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006369983893819153
        entropy: 0.6505903005599976
        entropy_coeff: 0.0017600000137463212
        kl: 0.010839157737791538
        model: {}
        policy_loss: -0.02111571840941906
        total_loss: -0.01940041221678257
        vf_explained_var: 0.13255928456783295
        vf_loss: 26.57112693786621
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006369983893819153
        entropy: 0.8137422800064087
        entropy_coeff: 0.0017600000137463212
        kl: 0.012848488055169582
        model: {}
        policy_loss: -0.02583368867635727
        total_loss: -0.02360924333333969
        vf_explained_var: 0.14968501031398773
        vf_loss: 30.14211654663086
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006369983893819153
        entropy: 0.9319603443145752
        entropy_coeff: 0.0017600000137463212
        kl: 0.012428195215761662
        model: {}
        policy_loss: -0.021405614912509918
        total_loss: -0.0189710333943367
        vf_explained_var: -0.017193302512168884
        vf_loss: 36.08775329589844
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006369983893819153
        entropy: 0.8912420272827148
        entropy_coeff: 0.0017600000137463212
        kl: 0.012103677727282047
        model: {}
        policy_loss: -0.018165532499551773
        total_loss: -0.01674274541437626
        vf_explained_var: 0.03437860310077667
        vf_loss: 28.836669921875
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006369983893819153
        entropy: 0.5433177947998047
        entropy_coeff: 0.0017600000137463212
        kl: 0.01011931523680687
        model: {}
        policy_loss: -0.02139912359416485
        total_loss: -0.019526908174157143
        vf_explained_var: 0.17868565022945404
        vf_loss: 24.48980140686035
    load_time_ms: 13256.827
    num_steps_sampled: 10080000
    num_steps_trained: 10080000
    sample_time_ms: 106423.789
    update_time_ms: 16.362
  iterations_since_restore: 105
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.096825396825395
    ram_util_percent: 11.477248677248676
  pid: 28570
  policy_reward_max:
    agent-0: 175.0
    agent-1: 175.0
    agent-2: 187.0
    agent-3: 187.0
    agent-4: 172.0
    agent-5: 172.0
  policy_reward_mean:
    agent-0: 118.84
    agent-1: 118.84
    agent-2: 124.485
    agent-3: 124.485
    agent-4: 117.35
    agent-5: 117.35
  policy_reward_min:
    agent-0: 41.5
    agent-1: 41.5
    agent-2: 24.5
    agent-3: 24.5
    agent-4: 24.0
    agent-5: 24.0
  sampler_perf:
    mean_env_wait_ms: 27.04242895219589
    mean_inference_ms: 13.114466382443796
    mean_processing_ms: 58.73571170085393
  time_since_restore: 13892.677840948105
  time_this_iter_s: 132.4449315071106
  time_total_s: 13892.677840948105
  timestamp: 1637522428
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 10080000
  training_iteration: 105
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 19.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    105 |          13892.7 | 10080000 |   721.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 1.13
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 24.23
    apples_agent-1_min: 7
    apples_agent-2_max: 262
    apples_agent-2_mean: 183.63
    apples_agent-2_min: 0
    apples_agent-3_max: 100
    apples_agent-3_mean: 19.58
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 373
    apples_agent-5_mean: 208.97
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 702
    cleaning_beam_agent-0_mean: 459.7
    cleaning_beam_agent-0_min: 232
    cleaning_beam_agent-1_max: 38
    cleaning_beam_agent-1_mean: 9.06
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 313
    cleaning_beam_agent-2_mean: 37.85
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 419
    cleaning_beam_agent-3_mean: 279.12
    cleaning_beam_agent-3_min: 165
    cleaning_beam_agent-4_max: 653
    cleaning_beam_agent-4_mean: 525.98
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 286
    cleaning_beam_agent-5_mean: 85.75
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.08
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.16
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-22-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1020.0
  episode_reward_mean: 747.76
  episode_reward_min: 204.0
  episodes_this_iter: 96
  episodes_total: 10176
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12745.754
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006310080061666667
        entropy: 0.7183102369308472
        entropy_coeff: 0.0017600000137463212
        kl: 0.011254318989813328
        model: {}
        policy_loss: -0.01977675035595894
        total_loss: -0.017956992611289024
        vf_explained_var: 0.0672273337841034
        vf_loss: 28.02626609802246
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006310080061666667
        entropy: 0.6205741763114929
        entropy_coeff: 0.0017600000137463212
        kl: 0.011196144856512547
        model: {}
        policy_loss: -0.021645884960889816
        total_loss: -0.019810033962130547
        vf_explained_var: 0.09675726294517517
        vf_loss: 27.181331634521484
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006310080061666667
        entropy: 0.7792794704437256
        entropy_coeff: 0.0017600000137463212
        kl: 0.012285476550459862
        model: {}
        policy_loss: -0.024766117334365845
        total_loss: -0.022556502372026443
        vf_explained_var: 0.14028841257095337
        vf_loss: 29.668781280517578
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006310080061666667
        entropy: 0.9302564859390259
        entropy_coeff: 0.0017600000137463212
        kl: 0.01321125216782093
        model: {}
        policy_loss: -0.022650111466646194
        total_loss: -0.020325278863310814
        vf_explained_var: 0.0033966898918151855
        vf_loss: 34.66657638549805
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006310080061666667
        entropy: 0.9355907440185547
        entropy_coeff: 0.0017600000137463212
        kl: 0.012403002008795738
        model: {}
        policy_loss: -0.018600264564156532
        total_loss: -0.017145691439509392
        vf_explained_var: 0.011586800217628479
        vf_loss: 29.90843963623047
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006310080061666667
        entropy: 0.5411543846130371
        entropy_coeff: 0.0017600000137463212
        kl: 0.01084057241678238
        model: {}
        policy_loss: -0.021547121927142143
        total_loss: -0.019556790590286255
        vf_explained_var: 0.16175754368305206
        vf_loss: 25.362398147583008
    load_time_ms: 13275.742
    num_steps_sampled: 10176000
    num_steps_trained: 10176000
    sample_time_ms: 106497.084
    update_time_ms: 16.458
  iterations_since_restore: 106
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.487234042553194
    ram_util_percent: 11.23085106382979
  pid: 28570
  policy_reward_max:
    agent-0: 219.0
    agent-1: 219.0
    agent-2: 184.5
    agent-3: 184.5
    agent-4: 204.5
    agent-5: 204.5
  policy_reward_mean:
    agent-0: 122.47
    agent-1: 122.47
    agent-2: 130.815
    agent-3: 130.815
    agent-4: 120.595
    agent-5: 120.595
  policy_reward_min:
    agent-0: 30.5
    agent-1: 30.5
    agent-2: 38.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 29.0
  sampler_perf:
    mean_env_wait_ms: 27.054544820943093
    mean_inference_ms: 13.113281879141928
    mean_processing_ms: 58.72962357652832
  time_since_restore: 14025.187432050705
  time_this_iter_s: 132.5095911026001
  time_total_s: 14025.187432050705
  timestamp: 1637522561
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 10176000
  training_iteration: 106
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    106 |          14025.2 | 10176000 |   747.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 1.01
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 23.35
    apples_agent-1_min: 6
    apples_agent-2_max: 314
    apples_agent-2_mean: 193.66
    apples_agent-2_min: 40
    apples_agent-3_max: 76
    apples_agent-3_mean: 21.11
    apples_agent-3_min: 0
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 317
    apples_agent-5_mean: 209.35
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 712
    cleaning_beam_agent-0_mean: 448.7
    cleaning_beam_agent-0_min: 217
    cleaning_beam_agent-1_max: 48
    cleaning_beam_agent-1_mean: 8.39
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 150
    cleaning_beam_agent-2_mean: 31.93
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 435
    cleaning_beam_agent-3_mean: 277.76
    cleaning_beam_agent-3_min: 135
    cleaning_beam_agent-4_max: 701
    cleaning_beam_agent-4_mean: 553.89
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 289
    cleaning_beam_agent-5_mean: 79.96
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.15
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.23
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-24-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1086.0
  episode_reward_mean: 764.43
  episode_reward_min: 118.0
  episodes_this_iter: 96
  episodes_total: 10272
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12744.617
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006250176229514182
        entropy: 0.7545071840286255
        entropy_coeff: 0.0017600000137463212
        kl: 0.011577626690268517
        model: {}
        policy_loss: -0.020393474027514458
        total_loss: -0.018599705770611763
        vf_explained_var: 0.12226040661334991
        vf_loss: 28.322616577148438
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006250176229514182
        entropy: 0.6157940626144409
        entropy_coeff: 0.0017600000137463212
        kl: 0.01182921789586544
        model: {}
        policy_loss: -0.020582290366292
        total_loss: -0.018685482442378998
        vf_explained_var: 0.1503129005432129
        vf_loss: 27.588062286376953
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006250176229514182
        entropy: 0.7661460638046265
        entropy_coeff: 0.0017600000137463212
        kl: 0.014521208591759205
        model: {}
        policy_loss: -0.022795751690864563
        total_loss: -0.020347442477941513
        vf_explained_var: 0.15237842500209808
        vf_loss: 30.706649780273438
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006250176229514182
        entropy: 0.9113560914993286
        entropy_coeff: 0.0017600000137463212
        kl: 0.012884348630905151
        model: {}
        policy_loss: -0.02095474675297737
        total_loss: -0.01833888702094555
        vf_explained_var: -0.019627436995506287
        vf_loss: 37.36685562133789
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006250176229514182
        entropy: 0.8852698802947998
        entropy_coeff: 0.0017600000137463212
        kl: 0.013181887567043304
        model: {}
        policy_loss: -0.019305871799588203
        total_loss: -0.017853140830993652
        vf_explained_var: 0.010813131928443909
        vf_loss: 28.934946060180664
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006250176229514182
        entropy: 0.5661977529525757
        entropy_coeff: 0.0017600000137463212
        kl: 0.011071962304413319
        model: {}
        policy_loss: -0.022578729316592216
        total_loss: -0.02072867378592491
        vf_explained_var: 0.1814182549715042
        vf_loss: 24.313671112060547
    load_time_ms: 13280.264
    num_steps_sampled: 10272000
    num_steps_trained: 10272000
    sample_time_ms: 106370.222
    update_time_ms: 16.456
  iterations_since_restore: 107
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.430851063829788
    ram_util_percent: 11.601595744680854
  pid: 28570
  policy_reward_max:
    agent-0: 185.5
    agent-1: 185.5
    agent-2: 207.5
    agent-3: 207.5
    agent-4: 174.5
    agent-5: 174.5
  policy_reward_mean:
    agent-0: 125.725
    agent-1: 125.725
    agent-2: 135.83
    agent-3: 135.83
    agent-4: 120.66
    agent-5: 120.66
  policy_reward_min:
    agent-0: 11.5
    agent-1: 11.5
    agent-2: 41.5
    agent-3: 41.5
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 27.065747977883003
    mean_inference_ms: 13.11169406820286
    mean_processing_ms: 58.723317457813764
  time_since_restore: 14156.846207618713
  time_this_iter_s: 131.65877556800842
  time_total_s: 14156.846207618713
  timestamp: 1637522693
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 10272000
  training_iteration: 107
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    107 |          14156.8 | 10272000 |   764.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.81
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 24.49
    apples_agent-1_min: 8
    apples_agent-2_max: 300
    apples_agent-2_mean: 193.96
    apples_agent-2_min: 47
    apples_agent-3_max: 96
    apples_agent-3_mean: 24.09
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 356
    apples_agent-5_mean: 215.88
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 655
    cleaning_beam_agent-0_mean: 472.59
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 46
    cleaning_beam_agent-1_mean: 9.29
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 135
    cleaning_beam_agent-2_mean: 33.5
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 449
    cleaning_beam_agent-3_mean: 275.78
    cleaning_beam_agent-3_min: 165
    cleaning_beam_agent-4_max: 662
    cleaning_beam_agent-4_mean: 496.02
    cleaning_beam_agent-4_min: 289
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 74.18
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.2
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-27-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1096.0
  episode_reward_mean: 771.27
  episode_reward_min: 251.0
  episodes_this_iter: 96
  episodes_total: 10368
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12729.167
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006190271815285087
        entropy: 0.7077674269676208
        entropy_coeff: 0.0017600000137463212
        kl: 0.009893043898046017
        model: {}
        policy_loss: -0.018437890335917473
        total_loss: -0.016472946852445602
        vf_explained_var: 0.10250644385814667
        vf_loss: 29.63286590576172
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006190271815285087
        entropy: 0.5954606533050537
        entropy_coeff: 0.0017600000137463212
        kl: 0.011590705253183842
        model: {}
        policy_loss: -0.02021220326423645
        total_loss: -0.01822500303387642
        vf_explained_var: 0.1548963487148285
        vf_loss: 28.178842544555664
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006190271815285087
        entropy: 0.7334665060043335
        entropy_coeff: 0.0017600000137463212
        kl: 0.011744082905352116
        model: {}
        policy_loss: -0.022930124774575233
        total_loss: -0.020363889634609222
        vf_explained_var: 0.13487723469734192
        vf_loss: 32.69932174682617
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006190271815285087
        entropy: 0.9116336703300476
        entropy_coeff: 0.0017600000137463212
        kl: 0.01229439489543438
        model: {}
        policy_loss: -0.02090362273156643
        total_loss: -0.018091775476932526
        vf_explained_var: -0.0314021110534668
        vf_loss: 39.552825927734375
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006190271815285087
        entropy: 0.974892258644104
        entropy_coeff: 0.0017600000137463212
        kl: 0.014017539098858833
        model: {}
        policy_loss: -0.021317237988114357
        total_loss: -0.019754784181714058
        vf_explained_var: 0.04238235950469971
        vf_loss: 31.535232543945312
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006190271815285087
        entropy: 0.5602476000785828
        entropy_coeff: 0.0017600000137463212
        kl: 0.010742615908384323
        model: {}
        policy_loss: -0.021452007815241814
        total_loss: -0.01942729949951172
        vf_explained_var: 0.2006807178258896
        vf_loss: 26.078947067260742
    load_time_ms: 13270.145
    num_steps_sampled: 10368000
    num_steps_trained: 10368000
    sample_time_ms: 106513.152
    update_time_ms: 17.436
  iterations_since_restore: 108
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.23439153439153
    ram_util_percent: 11.606878306878311
  pid: 28570
  policy_reward_max:
    agent-0: 225.0
    agent-1: 225.0
    agent-2: 202.0
    agent-3: 202.0
    agent-4: 204.0
    agent-5: 204.0
  policy_reward_mean:
    agent-0: 123.85
    agent-1: 123.85
    agent-2: 136.755
    agent-3: 136.755
    agent-4: 125.03
    agent-5: 125.03
  policy_reward_min:
    agent-0: 21.0
    agent-1: 21.0
    agent-2: 30.0
    agent-3: 30.0
    agent-4: 39.0
    agent-5: 39.0
  sampler_perf:
    mean_env_wait_ms: 27.07323922760351
    mean_inference_ms: 13.11071238375549
    mean_processing_ms: 58.717955544175354
  time_since_restore: 14289.375776290894
  time_this_iter_s: 132.52956867218018
  time_total_s: 14289.375776290894
  timestamp: 1637522825
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 10368000
  training_iteration: 108
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    108 |          14289.4 | 10368000 |   771.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 1.07
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 25.08
    apples_agent-1_min: 9
    apples_agent-2_max: 311
    apples_agent-2_mean: 191.63
    apples_agent-2_min: 15
    apples_agent-3_max: 122
    apples_agent-3_mean: 25.86
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 2.52
    apples_agent-4_min: 0
    apples_agent-5_max: 343
    apples_agent-5_mean: 215.05
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 673
    cleaning_beam_agent-0_mean: 480.86
    cleaning_beam_agent-0_min: 300
    cleaning_beam_agent-1_max: 78
    cleaning_beam_agent-1_mean: 11.01
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 286
    cleaning_beam_agent-2_mean: 36.69
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 471
    cleaning_beam_agent-3_mean: 277.71
    cleaning_beam_agent-3_min: 136
    cleaning_beam_agent-4_max: 620
    cleaning_beam_agent-4_mean: 450.91
    cleaning_beam_agent-4_min: 234
    cleaning_beam_agent-5_max: 175
    cleaning_beam_agent-5_mean: 82.17
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.1
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.16
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.24
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.28
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-29-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1054.0
  episode_reward_mean: 762.73
  episode_reward_min: 274.0
  episodes_this_iter: 96
  episodes_total: 10464
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12722.515
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006130367983132601
        entropy: 0.6955573558807373
        entropy_coeff: 0.0017600000137463212
        kl: 0.010242415592074394
        model: {}
        policy_loss: -0.017666026949882507
        total_loss: -0.01527555100619793
        vf_explained_var: 0.05355289578437805
        vf_loss: 33.585975646972656
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006130367983132601
        entropy: 0.6059622764587402
        entropy_coeff: 0.0017600000137463212
        kl: 0.011749442666769028
        model: {}
        policy_loss: -0.020366143435239792
        total_loss: -0.018114356324076653
        vf_explained_var: 0.12997651100158691
        vf_loss: 30.979801177978516
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006130367983132601
        entropy: 0.7424023747444153
        entropy_coeff: 0.0017600000137463212
        kl: 0.011770114302635193
        model: {}
        policy_loss: -0.02418474666774273
        total_loss: -0.02165161818265915
        vf_explained_var: 0.13200682401657104
        vf_loss: 32.51251220703125
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006130367983132601
        entropy: 0.9106077551841736
        entropy_coeff: 0.0017600000137463212
        kl: 0.011902758851647377
        model: {}
        policy_loss: -0.022524500265717506
        total_loss: -0.019790135324001312
        vf_explained_var: -0.028831034898757935
        vf_loss: 38.90678405761719
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006130367983132601
        entropy: 0.9774936437606812
        entropy_coeff: 0.0017600000137463212
        kl: 0.015317588113248348
        model: {}
        policy_loss: -0.021176230162382126
        total_loss: -0.019987696781754494
        vf_explained_var: 0.07337334752082825
        vf_loss: 27.72613525390625
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006130367983132601
        entropy: 0.5411812663078308
        entropy_coeff: 0.0017600000137463212
        kl: 0.010558592155575752
        model: {}
        policy_loss: -0.021267075091600418
        total_loss: -0.01936684548854828
        vf_explained_var: 0.17177006602287292
        vf_loss: 24.567594528198242
    load_time_ms: 13292.409
    num_steps_sampled: 10464000
    num_steps_trained: 10464000
    sample_time_ms: 106582.168
    update_time_ms: 17.014
  iterations_since_restore: 109
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.357142857142858
    ram_util_percent: 11.61481481481482
  pid: 28570
  policy_reward_max:
    agent-0: 201.5
    agent-1: 201.5
    agent-2: 187.5
    agent-3: 187.5
    agent-4: 196.5
    agent-5: 196.5
  policy_reward_mean:
    agent-0: 123.46
    agent-1: 123.46
    agent-2: 134.55
    agent-3: 134.55
    agent-4: 123.355
    agent-5: 123.355
  policy_reward_min:
    agent-0: 46.0
    agent-1: 46.0
    agent-2: 44.0
    agent-3: 44.0
    agent-4: 32.5
    agent-5: 32.5
  sampler_perf:
    mean_env_wait_ms: 27.083489044762956
    mean_inference_ms: 13.109713055153158
    mean_processing_ms: 58.71825734817386
  time_since_restore: 14422.353418588638
  time_this_iter_s: 132.97764229774475
  time_total_s: 14422.353418588638
  timestamp: 1637522958
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 10464000
  training_iteration: 109
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    109 |          14422.4 | 10464000 |   762.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 0.59
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 23.42
    apples_agent-1_min: 10
    apples_agent-2_max: 311
    apples_agent-2_mean: 185.21
    apples_agent-2_min: 94
    apples_agent-3_max: 71
    apples_agent-3_mean: 19.84
    apples_agent-3_min: 0
    apples_agent-4_max: 65
    apples_agent-4_mean: 4.81
    apples_agent-4_min: 0
    apples_agent-5_max: 295
    apples_agent-5_mean: 200.24
    apples_agent-5_min: 93
    cleaning_beam_agent-0_max: 762
    cleaning_beam_agent-0_mean: 496.16
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 137
    cleaning_beam_agent-1_mean: 10.27
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 98
    cleaning_beam_agent-2_mean: 29.56
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 487
    cleaning_beam_agent-3_mean: 276.54
    cleaning_beam_agent-3_min: 113
    cleaning_beam_agent-4_max: 677
    cleaning_beam_agent-4_mean: 461.1
    cleaning_beam_agent-4_min: 254
    cleaning_beam_agent-5_max: 288
    cleaning_beam_agent-5_mean: 89.89
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.11
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.18
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-31-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1075.0
  episode_reward_mean: 718.02
  episode_reward_min: 394.0
  episodes_this_iter: 96
  episodes_total: 10560
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12708.263
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0006070464150980115
        entropy: 0.6487588286399841
        entropy_coeff: 0.0017600000137463212
        kl: 0.008991079404950142
        model: {}
        policy_loss: -0.016686197370290756
        total_loss: -0.014724722132086754
        vf_explained_var: 0.020610451698303223
        vf_loss: 28.78510856628418
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0006070464150980115
        entropy: 0.6261940002441406
        entropy_coeff: 0.0017600000137463212
        kl: 0.013679344207048416
        model: {}
        policy_loss: -0.018448546528816223
        total_loss: -0.016736382618546486
        vf_explained_var: 0.12061814963817596
        vf_loss: 25.577810287475586
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0006070464150980115
        entropy: 0.7518090009689331
        entropy_coeff: 0.0017600000137463212
        kl: 0.011896488256752491
        model: {}
        policy_loss: -0.024853553622961044
        total_loss: -0.022785194218158722
        vf_explained_var: 0.12286432087421417
        vf_loss: 27.967193603515625
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006070464150980115
        entropy: 0.930452287197113
        entropy_coeff: 0.0017600000137463212
        kl: 0.013052704744040966
        model: {}
        policy_loss: -0.02206905372440815
        total_loss: -0.019838301464915276
        vf_explained_var: -0.058100104331970215
        vf_loss: 33.78874206542969
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0006070464150980115
        entropy: 0.9593509435653687
        entropy_coeff: 0.0017600000137463212
        kl: 0.012901395559310913
        model: {}
        policy_loss: -0.022098254412412643
        total_loss: -0.0209862869232893
        vf_explained_var: 0.04765106737613678
        vf_loss: 26.856182098388672
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0006070464150980115
        entropy: 0.557888925075531
        entropy_coeff: 0.0017600000137463212
        kl: 0.010597173124551773
        model: {}
        policy_loss: -0.021936142817139626
        total_loss: -0.020209655165672302
        vf_explained_var: 0.17537614703178406
        vf_loss: 23.10976219177246
    load_time_ms: 13301.888
    num_steps_sampled: 10560000
    num_steps_trained: 10560000
    sample_time_ms: 106481.232
    update_time_ms: 18.457
  iterations_since_restore: 110
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.43101604278075
    ram_util_percent: 11.602139037433156
  pid: 28570
  policy_reward_max:
    agent-0: 188.5
    agent-1: 188.5
    agent-2: 187.5
    agent-3: 187.5
    agent-4: 166.5
    agent-5: 166.5
  policy_reward_mean:
    agent-0: 114.635
    agent-1: 114.635
    agent-2: 127.995
    agent-3: 127.995
    agent-4: 116.38
    agent-5: 116.38
  policy_reward_min:
    agent-0: 60.5
    agent-1: 60.5
    agent-2: 40.5
    agent-3: 40.5
    agent-4: 53.5
    agent-5: 53.5
  sampler_perf:
    mean_env_wait_ms: 27.092772325027678
    mean_inference_ms: 13.108606139427781
    mean_processing_ms: 58.71266397117083
  time_since_restore: 14554.246291160583
  time_this_iter_s: 131.8928725719452
  time_total_s: 14554.246291160583
  timestamp: 1637523090
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 10560000
  training_iteration: 110
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    110 |          14554.2 | 10560000 |   718.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 0.94
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 22.51
    apples_agent-1_min: 9
    apples_agent-2_max: 315
    apples_agent-2_mean: 184.71
    apples_agent-2_min: 31
    apples_agent-3_max: 68
    apples_agent-3_mean: 21.56
    apples_agent-3_min: 0
    apples_agent-4_max: 60
    apples_agent-4_mean: 4.38
    apples_agent-4_min: 0
    apples_agent-5_max: 324
    apples_agent-5_mean: 199.36
    apples_agent-5_min: 68
    cleaning_beam_agent-0_max: 762
    cleaning_beam_agent-0_mean: 478.39
    cleaning_beam_agent-0_min: 242
    cleaning_beam_agent-1_max: 119
    cleaning_beam_agent-1_mean: 11.58
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 164
    cleaning_beam_agent-2_mean: 29.14
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 414
    cleaning_beam_agent-3_mean: 257.56
    cleaning_beam_agent-3_min: 114
    cleaning_beam_agent-4_max: 642
    cleaning_beam_agent-4_mean: 454.49
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 201
    cleaning_beam_agent-5_mean: 101.33
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.12
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 7
    fire_beam_agent-3_mean: 0.17
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.26
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.26
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-33-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1112.0
  episode_reward_mean: 716.61
  episode_reward_min: 300.0
  episodes_this_iter: 96
  episodes_total: 10656
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12675.986
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000601055973675102
        entropy: 0.6832427978515625
        entropy_coeff: 0.0017600000137463212
        kl: 0.010492390021681786
        model: {}
        policy_loss: -0.018205909058451653
        total_loss: -0.016140062361955643
        vf_explained_var: 0.0439850389957428
        vf_loss: 30.060466766357422
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000601055973675102
        entropy: 0.6267938017845154
        entropy_coeff: 0.0017600000137463212
        kl: 0.012636546976864338
        model: {}
        policy_loss: -0.01906663365662098
        total_loss: -0.01710156723856926
        vf_explained_var: 0.09788231551647186
        vf_loss: 28.31285858154297
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000601055973675102
        entropy: 0.7374691367149353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0115965586155653
        model: {}
        policy_loss: -0.024382293224334717
        total_loss: -0.021962298080325127
        vf_explained_var: 0.0841168612241745
        vf_loss: 31.38114356994629
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000601055973675102
        entropy: 0.8692505359649658
        entropy_coeff: 0.0017600000137463212
        kl: 0.011301512829959393
        model: {}
        policy_loss: -0.021037407219409943
        total_loss: -0.018705148249864578
        vf_explained_var: -0.003799259662628174
        vf_loss: 34.383365631103516
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000601055973675102
        entropy: 0.9516264200210571
        entropy_coeff: 0.0017600000137463212
        kl: 0.014321460388600826
        model: {}
        policy_loss: -0.022861024364829063
        total_loss: -0.021646562963724136
        vf_explained_var: 0.04455342888832092
        vf_loss: 27.61879539489746
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000601055973675102
        entropy: 0.5643308758735657
        entropy_coeff: 0.0017600000137463212
        kl: 0.011774716898798943
        model: {}
        policy_loss: -0.02346380613744259
        total_loss: -0.02162802778184414
        vf_explained_var: 0.17116864025592804
        vf_loss: 23.87446403503418
    load_time_ms: 13304.284
    num_steps_sampled: 10656000
    num_steps_trained: 10656000
    sample_time_ms: 106410.782
    update_time_ms: 18.743
  iterations_since_restore: 111
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.249197860962568
    ram_util_percent: 11.193582887700536
  pid: 28570
  policy_reward_max:
    agent-0: 173.5
    agent-1: 173.5
    agent-2: 197.0
    agent-3: 197.0
    agent-4: 187.0
    agent-5: 187.0
  policy_reward_mean:
    agent-0: 113.045
    agent-1: 113.045
    agent-2: 129.035
    agent-3: 129.035
    agent-4: 116.225
    agent-5: 116.225
  policy_reward_min:
    agent-0: 20.0
    agent-1: 20.0
    agent-2: 40.0
    agent-3: 40.0
    agent-4: 39.5
    agent-5: 39.5
  sampler_perf:
    mean_env_wait_ms: 27.101270000289297
    mean_inference_ms: 13.10732682463108
    mean_processing_ms: 58.704846876604904
  time_since_restore: 14685.496366024017
  time_this_iter_s: 131.25007486343384
  time_total_s: 14685.496366024017
  timestamp: 1637523222
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 10656000
  training_iteration: 111
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    111 |          14685.5 | 10656000 |   716.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 0.82
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 20.8
    apples_agent-1_min: 6
    apples_agent-2_max: 297
    apples_agent-2_mean: 172.68
    apples_agent-2_min: 65
    apples_agent-3_max: 70
    apples_agent-3_mean: 18.44
    apples_agent-3_min: 0
    apples_agent-4_max: 69
    apples_agent-4_mean: 5.12
    apples_agent-4_min: 0
    apples_agent-5_max: 328
    apples_agent-5_mean: 199.71
    apples_agent-5_min: 69
    cleaning_beam_agent-0_max: 685
    cleaning_beam_agent-0_mean: 485.06
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 152
    cleaning_beam_agent-1_mean: 12.36
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 34.64
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 431
    cleaning_beam_agent-3_mean: 272.08
    cleaning_beam_agent-3_min: 104
    cleaning_beam_agent-4_max: 680
    cleaning_beam_agent-4_mean: 421.81
    cleaning_beam_agent-4_min: 160
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 108.49
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.15
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 4
    fire_beam_agent-3_mean: 0.15
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.31
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.2
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-35-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1052.0
  episode_reward_mean: 702.68
  episode_reward_min: 284.0
  episodes_this_iter: 96
  episodes_total: 10752
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12666.253
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005950655904598534
        entropy: 0.6541709899902344
        entropy_coeff: 0.0017600000137463212
        kl: 0.010428447276353836
        model: {}
        policy_loss: -0.017579756677150726
        total_loss: -0.01582474261522293
        vf_explained_var: 0.044794172048568726
        vf_loss: 26.456451416015625
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005950655904598534
        entropy: 0.6427651047706604
        entropy_coeff: 0.0017600000137463212
        kl: 0.010747073218226433
        model: {}
        policy_loss: -0.021775679662823677
        total_loss: -0.0202188603579998
        vf_explained_var: 0.10186994075775146
        vf_loss: 24.865802764892578
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005950655904598534
        entropy: 0.7820732593536377
        entropy_coeff: 0.0017600000137463212
        kl: 0.011711502447724342
        model: {}
        policy_loss: -0.0259698536247015
        total_loss: -0.023846548050642014
        vf_explained_var: 0.12860335409641266
        vf_loss: 29.14179039001465
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005950655904598534
        entropy: 0.9080068469047546
        entropy_coeff: 0.0017600000137463212
        kl: 0.012832573615014553
        model: {}
        policy_loss: -0.020636439323425293
        total_loss: -0.018299071118235588
        vf_explained_var: -0.03281134366989136
        vf_loss: 34.54240036010742
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005950655904598534
        entropy: 0.9592505097389221
        entropy_coeff: 0.0017600000137463212
        kl: 0.01308454293757677
        model: {}
        policy_loss: -0.02221681736409664
        total_loss: -0.02068287879228592
        vf_explained_var: -0.01352091133594513
        vf_loss: 31.057819366455078
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005950655904598534
        entropy: 0.5844177007675171
        entropy_coeff: 0.0017600000137463212
        kl: 0.012070932425558567
        model: {}
        policy_loss: -0.02237778902053833
        total_loss: -0.020431309938430786
        vf_explained_var: 0.17144031822681427
        vf_loss: 25.223896026611328
    load_time_ms: 13310.404
    num_steps_sampled: 10752000
    num_steps_trained: 10752000
    sample_time_ms: 106418.599
    update_time_ms: 18.74
  iterations_since_restore: 112
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.250526315789475
    ram_util_percent: 11.674210526315791
  pid: 28570
  policy_reward_max:
    agent-0: 173.0
    agent-1: 173.0
    agent-2: 200.5
    agent-3: 200.5
    agent-4: 177.0
    agent-5: 177.0
  policy_reward_mean:
    agent-0: 112.785
    agent-1: 112.785
    agent-2: 123.445
    agent-3: 123.445
    agent-4: 115.11
    agent-5: 115.11
  policy_reward_min:
    agent-0: 41.5
    agent-1: 41.5
    agent-2: 44.0
    agent-3: 44.0
    agent-4: 43.0
    agent-5: 43.0
  sampler_perf:
    mean_env_wait_ms: 27.108741019838966
    mean_inference_ms: 13.106440053250529
    mean_processing_ms: 58.703111752098565
  time_since_restore: 14818.420076847076
  time_this_iter_s: 132.92371082305908
  time_total_s: 14818.420076847076
  timestamp: 1637523355
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 10752000
  training_iteration: 112
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    112 |          14818.4 | 10752000 |   702.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.81
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 22.62
    apples_agent-1_min: 9
    apples_agent-2_max: 279
    apples_agent-2_mean: 184.08
    apples_agent-2_min: 63
    apples_agent-3_max: 67
    apples_agent-3_mean: 19.4
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 4.46
    apples_agent-4_min: 0
    apples_agent-5_max: 318
    apples_agent-5_mean: 212.34
    apples_agent-5_min: 127
    cleaning_beam_agent-0_max: 651
    cleaning_beam_agent-0_mean: 506.25
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 117
    cleaning_beam_agent-1_mean: 15.27
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 35.12
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 396
    cleaning_beam_agent-3_mean: 253.25
    cleaning_beam_agent-3_min: 106
    cleaning_beam_agent-4_max: 616
    cleaning_beam_agent-4_mean: 443.98
    cleaning_beam_agent-4_min: 214
    cleaning_beam_agent-5_max: 206
    cleaning_beam_agent-5_mean: 84.91
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 4
    fire_beam_agent-3_mean: 0.11
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.23
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-38-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1067.0
  episode_reward_mean: 749.58
  episode_reward_min: 457.0
  episodes_this_iter: 96
  episodes_total: 10848
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12657.181
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005890752072446048
        entropy: 0.6188000440597534
        entropy_coeff: 0.0017600000137463212
        kl: 0.010746506042778492
        model: {}
        policy_loss: -0.015727652236819267
        total_loss: -0.013604611158370972
        vf_explained_var: 0.03889082372188568
        vf_loss: 29.434650421142578
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005890752072446048
        entropy: 0.6201249361038208
        entropy_coeff: 0.0017600000137463212
        kl: 0.010708961635828018
        model: {}
        policy_loss: -0.022065186873078346
        total_loss: -0.020151672884821892
        vf_explained_var: 0.10402980446815491
        vf_loss: 28.041431427001953
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005890752072446048
        entropy: 0.7386325597763062
        entropy_coeff: 0.0017600000137463212
        kl: 0.011494042351841927
        model: {}
        policy_loss: -0.0249654371291399
        total_loss: -0.022844156250357628
        vf_explained_var: 0.10425406694412231
        vf_loss: 28.465734481811523
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005890752072446048
        entropy: 0.9259687662124634
        entropy_coeff: 0.0017600000137463212
        kl: 0.011679903604090214
        model: {}
        policy_loss: -0.023018592968583107
        total_loss: -0.020894266664981842
        vf_explained_var: -0.03517574071884155
        vf_loss: 33.160369873046875
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005890752072446048
        entropy: 0.9685958027839661
        entropy_coeff: 0.0017600000137463212
        kl: 0.01251891441643238
        model: {}
        policy_loss: -0.02118007466197014
        total_loss: -0.020017430186271667
        vf_explained_var: 0.013416141271591187
        vf_loss: 27.559659957885742
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005890752072446048
        entropy: 0.5125975608825684
        entropy_coeff: 0.0017600000137463212
        kl: 0.011298206634819508
        model: {}
        policy_loss: -0.02064703404903412
        total_loss: -0.018837077543139458
        vf_explained_var: 0.16432294249534607
        vf_loss: 22.88443374633789
    load_time_ms: 13321.666
    num_steps_sampled: 10848000
    num_steps_trained: 10848000
    sample_time_ms: 106449.406
    update_time_ms: 18.723
  iterations_since_restore: 113
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.32433862433862
    ram_util_percent: 11.5984126984127
  pid: 28570
  policy_reward_max:
    agent-0: 178.0
    agent-1: 178.0
    agent-2: 201.5
    agent-3: 201.5
    agent-4: 182.5
    agent-5: 182.5
  policy_reward_mean:
    agent-0: 119.81
    agent-1: 119.81
    agent-2: 131.54
    agent-3: 131.54
    agent-4: 123.44
    agent-5: 123.44
  policy_reward_min:
    agent-0: 61.5
    agent-1: 61.5
    agent-2: 52.5
    agent-3: 52.5
    agent-4: 74.0
    agent-5: 74.0
  sampler_perf:
    mean_env_wait_ms: 27.117346840081964
    mean_inference_ms: 13.10567861407479
    mean_processing_ms: 58.701090928004284
  time_since_restore: 14951.24969792366
  time_this_iter_s: 132.82962107658386
  time_total_s: 14951.24969792366
  timestamp: 1637523488
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 10848000
  training_iteration: 113
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    113 |          14951.2 | 10848000 |   749.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 0.52
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 22.38
    apples_agent-1_min: 8
    apples_agent-2_max: 302
    apples_agent-2_mean: 203.23
    apples_agent-2_min: 113
    apples_agent-3_max: 96
    apples_agent-3_mean: 27.1
    apples_agent-3_min: 0
    apples_agent-4_max: 74
    apples_agent-4_mean: 4.37
    apples_agent-4_min: 0
    apples_agent-5_max: 354
    apples_agent-5_mean: 222.4
    apples_agent-5_min: 134
    cleaning_beam_agent-0_max: 673
    cleaning_beam_agent-0_mean: 489.88
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 145
    cleaning_beam_agent-1_mean: 14.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 26.98
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 353
    cleaning_beam_agent-3_mean: 236.49
    cleaning_beam_agent-3_min: 125
    cleaning_beam_agent-4_max: 649
    cleaning_beam_agent-4_mean: 473.52
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 74.2
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.14
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.23
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-40-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1152.0
  episode_reward_mean: 807.48
  episode_reward_min: 499.0
  episodes_this_iter: 96
  episodes_total: 10944
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12641.965
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005830848240293562
        entropy: 0.6257051229476929
        entropy_coeff: 0.0017600000137463212
        kl: 0.009549994021654129
        model: {}
        policy_loss: -0.016625484451651573
        total_loss: -0.014200826175510883
        vf_explained_var: 0.009696245193481445
        vf_loss: 32.8714599609375
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005830848240293562
        entropy: 0.5917151570320129
        entropy_coeff: 0.0017600000137463212
        kl: 0.010253839194774628
        model: {}
        policy_loss: -0.019934402778744698
        total_loss: -0.017737600952386856
        vf_explained_var: 0.08633175492286682
        vf_loss: 30.459598541259766
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005830848240293562
        entropy: 0.6944366097450256
        entropy_coeff: 0.0017600000137463212
        kl: 0.012328951619565487
        model: {}
        policy_loss: -0.0237833671271801
        total_loss: -0.021205490455031395
        vf_explained_var: 0.08097384870052338
        vf_loss: 31.836402893066406
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005830848240293562
        entropy: 0.9046342968940735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0117389727383852
        model: {}
        policy_loss: -0.02187252976000309
        total_loss: -0.019322380423545837
        vf_explained_var: -0.03913035988807678
        vf_loss: 37.02091979980469
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005830848240293562
        entropy: 0.9848586320877075
        entropy_coeff: 0.0017600000137463212
        kl: 0.013376452028751373
        model: {}
        policy_loss: -0.02021939679980278
        total_loss: -0.019223101437091827
        vf_explained_var: -0.0006190836429595947
        vf_loss: 26.106111526489258
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005830848240293562
        entropy: 0.5092931985855103
        entropy_coeff: 0.0017600000137463212
        kl: 0.01090210396796465
        model: {}
        policy_loss: -0.021371517330408096
        total_loss: -0.019542116671800613
        vf_explained_var: 0.13471142947673798
        vf_loss: 23.169267654418945
    load_time_ms: 13321.888
    num_steps_sampled: 10944000
    num_steps_trained: 10944000
    sample_time_ms: 106374.478
    update_time_ms: 18.847
  iterations_since_restore: 114
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.23894736842105
    ram_util_percent: 11.53263157894737
  pid: 28570
  policy_reward_max:
    agent-0: 186.0
    agent-1: 186.0
    agent-2: 206.0
    agent-3: 206.0
    agent-4: 199.0
    agent-5: 199.0
  policy_reward_mean:
    agent-0: 128.925
    agent-1: 128.925
    agent-2: 145.515
    agent-3: 145.515
    agent-4: 129.3
    agent-5: 129.3
  policy_reward_min:
    agent-0: 55.0
    agent-1: 55.0
    agent-2: 81.5
    agent-3: 81.5
    agent-4: 80.0
    agent-5: 80.0
  sampler_perf:
    mean_env_wait_ms: 27.1261813337049
    mean_inference_ms: 13.105049230464227
    mean_processing_ms: 58.69935138962177
  time_since_restore: 15084.673243999481
  time_this_iter_s: 133.42354607582092
  time_total_s: 15084.673243999481
  timestamp: 1637523621
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 10944000
  training_iteration: 114
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    114 |          15084.7 | 10944000 |   807.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 0.83
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 23.28
    apples_agent-1_min: 9
    apples_agent-2_max: 328
    apples_agent-2_mean: 200.33
    apples_agent-2_min: 81
    apples_agent-3_max: 76
    apples_agent-3_mean: 24.3
    apples_agent-3_min: 0
    apples_agent-4_max: 107
    apples_agent-4_mean: 3.9
    apples_agent-4_min: 0
    apples_agent-5_max: 344
    apples_agent-5_mean: 219.24
    apples_agent-5_min: 93
    cleaning_beam_agent-0_max: 683
    cleaning_beam_agent-0_mean: 502.07
    cleaning_beam_agent-0_min: 295
    cleaning_beam_agent-1_max: 147
    cleaning_beam_agent-1_mean: 14.41
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 27.33
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 397
    cleaning_beam_agent-3_mean: 221.25
    cleaning_beam_agent-3_min: 86
    cleaning_beam_agent-4_max: 682
    cleaning_beam_agent-4_mean: 486.27
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 76.88
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.25
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-42-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1125.0
  episode_reward_mean: 797.18
  episode_reward_min: 345.0
  episodes_this_iter: 96
  episodes_total: 11040
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12638.362
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005770943826064467
        entropy: 0.589532732963562
        entropy_coeff: 0.0017600000137463212
        kl: 0.009550724178552628
        model: {}
        policy_loss: -0.01625768281519413
        total_loss: -0.014367959462106228
        vf_explained_var: 0.06229376792907715
        vf_loss: 26.88532066345215
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005770943826064467
        entropy: 0.58640456199646
        entropy_coeff: 0.0017600000137463212
        kl: 0.010917680338025093
        model: {}
        policy_loss: -0.020794589072465897
        total_loss: -0.019051380455493927
        vf_explained_var: 0.11359216272830963
        vf_loss: 25.705768585205078
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005770943826064467
        entropy: 0.7016540765762329
        entropy_coeff: 0.0017600000137463212
        kl: 0.011671749874949455
        model: {}
        policy_loss: -0.024630606174468994
        total_loss: -0.022175025194883347
        vf_explained_var: 0.11058066785335541
        vf_loss: 31.06902313232422
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005770943826064467
        entropy: 0.9150776863098145
        entropy_coeff: 0.0017600000137463212
        kl: 0.013538165017962456
        model: {}
        policy_loss: -0.023555302992463112
        total_loss: -0.02080414444208145
        vf_explained_var: -0.07302019000053406
        vf_loss: 38.5401725769043
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005770943826064467
        entropy: 0.9404363632202148
        entropy_coeff: 0.0017600000137463212
        kl: 0.013085885904729366
        model: {}
        policy_loss: -0.020969707518815994
        total_loss: -0.019591422751545906
        vf_explained_var: 0.007039159536361694
        vf_loss: 29.17005157470703
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005770943826064467
        entropy: 0.5293402075767517
        entropy_coeff: 0.0017600000137463212
        kl: 0.010537451133131981
        model: {}
        policy_loss: -0.02183930203318596
        total_loss: -0.019965318962931633
        vf_explained_var: 0.1860894113779068
        vf_loss: 24.104665756225586
    load_time_ms: 13327.819
    num_steps_sampled: 11040000
    num_steps_trained: 11040000
    sample_time_ms: 106375.571
    update_time_ms: 18.636
  iterations_since_restore: 115
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.340425531914892
    ram_util_percent: 11.59840425531915
  pid: 28570
  policy_reward_max:
    agent-0: 188.5
    agent-1: 188.5
    agent-2: 220.0
    agent-3: 220.0
    agent-4: 189.5
    agent-5: 189.5
  policy_reward_mean:
    agent-0: 126.265
    agent-1: 126.265
    agent-2: 144.97
    agent-3: 144.97
    agent-4: 127.355
    agent-5: 127.355
  policy_reward_min:
    agent-0: 30.0
    agent-1: 30.0
    agent-2: 68.0
    agent-3: 68.0
    agent-4: 53.0
    agent-5: 53.0
  sampler_perf:
    mean_env_wait_ms: 27.133966252470636
    mean_inference_ms: 13.104460480379156
    mean_processing_ms: 58.696317744605494
  time_since_restore: 15217.150825023651
  time_this_iter_s: 132.47758102416992
  time_total_s: 15217.150825023651
  timestamp: 1637523754
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 11040000
  training_iteration: 115
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    115 |          15217.2 | 11040000 |   797.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.53
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 22.92
    apples_agent-1_min: 8
    apples_agent-2_max: 295
    apples_agent-2_mean: 207.5
    apples_agent-2_min: 73
    apples_agent-3_max: 87
    apples_agent-3_mean: 29.19
    apples_agent-3_min: 1
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.6
    apples_agent-4_min: 0
    apples_agent-5_max: 318
    apples_agent-5_mean: 224.59
    apples_agent-5_min: 80
    cleaning_beam_agent-0_max: 761
    cleaning_beam_agent-0_mean: 529.0
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 121
    cleaning_beam_agent-1_mean: 10.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 127
    cleaning_beam_agent-2_mean: 27.75
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 324
    cleaning_beam_agent-3_mean: 205.96
    cleaning_beam_agent-3_min: 70
    cleaning_beam_agent-4_max: 713
    cleaning_beam_agent-4_mean: 498.07
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 293
    cleaning_beam_agent-5_mean: 81.24
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.17
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.29
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-44-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1139.0
  episode_reward_mean: 826.04
  episode_reward_min: 272.0
  episodes_this_iter: 96
  episodes_total: 11136
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12647.222
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005711039993911982
        entropy: 0.5599511861801147
        entropy_coeff: 0.0017600000137463212
        kl: 0.008307977579534054
        model: {}
        policy_loss: -0.015277741476893425
        total_loss: -0.013234734535217285
        vf_explained_var: 0.045002445578575134
        vf_loss: 28.208189010620117
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005711039993911982
        entropy: 0.5776733160018921
        entropy_coeff: 0.0017600000137463212
        kl: 0.009812949225306511
        model: {}
        policy_loss: -0.020743824541568756
        total_loss: -0.018909983336925507
        vf_explained_var: 0.10715927183628082
        vf_loss: 26.665557861328125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005711039993911982
        entropy: 0.6803487539291382
        entropy_coeff: 0.0017600000137463212
        kl: 0.010201947763562202
        model: {}
        policy_loss: -0.022612884640693665
        total_loss: -0.019926048815250397
        vf_explained_var: 0.112867072224617
        vf_loss: 33.741485595703125
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005711039993911982
        entropy: 0.8892725110054016
        entropy_coeff: 0.0017600000137463212
        kl: 0.01275577861815691
        model: {}
        policy_loss: -0.021931733936071396
        total_loss: -0.019130254164338112
        vf_explained_var: -0.002503708004951477
        vf_loss: 38.88258743286133
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005711039993911982
        entropy: 0.978564977645874
        entropy_coeff: 0.0017600000137463212
        kl: 0.01289958693087101
        model: {}
        policy_loss: -0.021845508366823196
        total_loss: -0.020658336579799652
        vf_explained_var: 0.025100022554397583
        vf_loss: 27.946517944335938
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005711039993911982
        entropy: 0.4930354654788971
        entropy_coeff: 0.0017600000137463212
        kl: 0.01071785669773817
        model: {}
        policy_loss: -0.021655859425663948
        total_loss: -0.019712259992957115
        vf_explained_var: 0.16482365131378174
        vf_loss: 24.09421157836914
    load_time_ms: 13318.405
    num_steps_sampled: 11136000
    num_steps_trained: 11136000
    sample_time_ms: 106429.649
    update_time_ms: 18.76
  iterations_since_restore: 116
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.36084656084656
    ram_util_percent: 11.530687830687832
  pid: 28570
  policy_reward_max:
    agent-0: 190.0
    agent-1: 190.0
    agent-2: 208.5
    agent-3: 208.5
    agent-4: 174.0
    agent-5: 174.0
  policy_reward_mean:
    agent-0: 129.99
    agent-1: 129.99
    agent-2: 151.785
    agent-3: 151.785
    agent-4: 131.245
    agent-5: 131.245
  policy_reward_min:
    agent-0: 37.0
    agent-1: 37.0
    agent-2: 57.0
    agent-3: 57.0
    agent-4: 42.0
    agent-5: 42.0
  sampler_perf:
    mean_env_wait_ms: 27.143802192502843
    mean_inference_ms: 13.104382313752847
    mean_processing_ms: 58.694951876147904
  time_since_restore: 15350.199985265732
  time_this_iter_s: 133.0491602420807
  time_total_s: 15350.199985265732
  timestamp: 1637523887
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 11136000
  training_iteration: 116
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    116 |          15350.2 | 11136000 |   826.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 0.23
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 23.79
    apples_agent-1_min: 9
    apples_agent-2_max: 316
    apples_agent-2_mean: 216.25
    apples_agent-2_min: 24
    apples_agent-3_max: 80
    apples_agent-3_mean: 29.3
    apples_agent-3_min: 1
    apples_agent-4_max: 41
    apples_agent-4_mean: 3.96
    apples_agent-4_min: 0
    apples_agent-5_max: 384
    apples_agent-5_mean: 238.04
    apples_agent-5_min: 119
    cleaning_beam_agent-0_max: 713
    cleaning_beam_agent-0_mean: 532.16
    cleaning_beam_agent-0_min: 297
    cleaning_beam_agent-1_max: 123
    cleaning_beam_agent-1_mean: 14.94
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 345
    cleaning_beam_agent-2_mean: 33.88
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 326
    cleaning_beam_agent-3_mean: 203.02
    cleaning_beam_agent-3_min: 46
    cleaning_beam_agent-4_max: 712
    cleaning_beam_agent-4_mean: 522.27
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 65.78
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-47-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1256.0
  episode_reward_mean: 864.03
  episode_reward_min: 496.0
  episodes_this_iter: 96
  episodes_total: 11232
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12655.375
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005651136161759496
        entropy: 0.5407590270042419
        entropy_coeff: 0.0017600000137463212
        kl: 0.008049988187849522
        model: {}
        policy_loss: -0.014150084927678108
        total_loss: -0.011814028955996037
        vf_explained_var: 0.01857703924179077
        vf_loss: 30.865421295166016
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005651136161759496
        entropy: 0.5722254514694214
        entropy_coeff: 0.0017600000137463212
        kl: 0.01010897196829319
        model: {}
        policy_loss: -0.019568556919693947
        total_loss: -0.017486214637756348
        vf_explained_var: 0.10201208293437958
        vf_loss: 28.999183654785156
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005651136161759496
        entropy: 0.6580138802528381
        entropy_coeff: 0.0017600000137463212
        kl: 0.011380109935998917
        model: {}
        policy_loss: -0.02268519252538681
        total_loss: -0.019824093207716942
        vf_explained_var: 0.115169957280159
        vf_loss: 34.50198745727539
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005651136161759496
        entropy: 0.875726044178009
        entropy_coeff: 0.0017600000137463212
        kl: 0.012436289340257645
        model: {}
        policy_loss: -0.02234024554491043
        total_loss: -0.01927679032087326
        vf_explained_var: -0.034588783979415894
        vf_loss: 41.38373947143555
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005651136161759496
        entropy: 0.9487941265106201
        entropy_coeff: 0.0017600000137463212
        kl: 0.011737673543393612
        model: {}
        policy_loss: -0.02108355239033699
        total_loss: -0.01983538642525673
        vf_explained_var: 0.08826817572116852
        vf_loss: 28.135940551757812
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005651136161759496
        entropy: 0.46383875608444214
        entropy_coeff: 0.0017600000137463212
        kl: 0.010716437362134457
        model: {}
        policy_loss: -0.020228417590260506
        total_loss: -0.018067358061671257
        vf_explained_var: 0.16079799830913544
        vf_loss: 25.755475997924805
    load_time_ms: 13316.445
    num_steps_sampled: 11232000
    num_steps_trained: 11232000
    sample_time_ms: 106508.997
    update_time_ms: 18.527
  iterations_since_restore: 117
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.35608465608465
    ram_util_percent: 11.598412698412703
  pid: 28570
  policy_reward_max:
    agent-0: 223.5
    agent-1: 223.5
    agent-2: 226.5
    agent-3: 226.5
    agent-4: 212.5
    agent-5: 212.5
  policy_reward_mean:
    agent-0: 135.86
    agent-1: 135.86
    agent-2: 158.665
    agent-3: 158.665
    agent-4: 137.49
    agent-5: 137.49
  policy_reward_min:
    agent-0: 50.5
    agent-1: 50.5
    agent-2: 49.0
    agent-3: 49.0
    agent-4: 70.5
    agent-5: 70.5
  sampler_perf:
    mean_env_wait_ms: 27.15429377501924
    mean_inference_ms: 13.103673971081555
    mean_processing_ms: 58.69288366604481
  time_since_restore: 15482.713155269623
  time_this_iter_s: 132.513170003891
  time_total_s: 15482.713155269623
  timestamp: 1637524020
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 11232000
  training_iteration: 117
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    117 |          15482.7 | 11232000 |   864.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 0.67
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 23.62
    apples_agent-1_min: 8
    apples_agent-2_max: 316
    apples_agent-2_mean: 218.11
    apples_agent-2_min: 91
    apples_agent-3_max: 80
    apples_agent-3_mean: 25.92
    apples_agent-3_min: 0
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.64
    apples_agent-4_min: 0
    apples_agent-5_max: 351
    apples_agent-5_mean: 237.95
    apples_agent-5_min: 92
    cleaning_beam_agent-0_max: 721
    cleaning_beam_agent-0_mean: 564.85
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 123
    cleaning_beam_agent-1_mean: 14.25
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 190
    cleaning_beam_agent-2_mean: 27.85
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 382
    cleaning_beam_agent-3_mean: 205.97
    cleaning_beam_agent-3_min: 51
    cleaning_beam_agent-4_max: 676
    cleaning_beam_agent-4_mean: 528.8
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 180
    cleaning_beam_agent-5_mean: 62.7
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-49-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1153.0
  episode_reward_mean: 858.64
  episode_reward_min: 410.0
  episodes_this_iter: 96
  episodes_total: 11328
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12650.718
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005591231747530401
        entropy: 0.4960452616214752
        entropy_coeff: 0.0017600000137463212
        kl: 0.008288836106657982
        model: {}
        policy_loss: -0.013286178931593895
        total_loss: -0.011048576794564724
        vf_explained_var: -0.012540161609649658
        vf_loss: 29.034202575683594
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005591231747530401
        entropy: 0.5825321078300476
        entropy_coeff: 0.0017600000137463212
        kl: 0.010513118468225002
        model: {}
        policy_loss: -0.02060459554195404
        total_loss: -0.01872977241873741
        vf_explained_var: 0.0667247623205185
        vf_loss: 27.02956199645996
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005591231747530401
        entropy: 0.647716224193573
        entropy_coeff: 0.0017600000137463212
        kl: 0.010909562930464745
        model: {}
        policy_loss: -0.02229969948530197
        total_loss: -0.019607750698924065
        vf_explained_var: 0.11427493393421173
        vf_loss: 32.864505767822266
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005591231747530401
        entropy: 0.8896291255950928
        entropy_coeff: 0.0017600000137463212
        kl: 0.012535922229290009
        model: {}
        policy_loss: -0.022386979311704636
        total_loss: -0.019561314955353737
        vf_explained_var: -0.016932547092437744
        vf_loss: 39.21311950683594
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005591231747530401
        entropy: 0.9411976337432861
        entropy_coeff: 0.0017600000137463212
        kl: 0.011556284502148628
        model: {}
        policy_loss: -0.020676519721746445
        total_loss: -0.019179370254278183
        vf_explained_var: 0.016869768500328064
        vf_loss: 30.508224487304688
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005591231747530401
        entropy: 0.47414448857307434
        entropy_coeff: 0.0017600000137463212
        kl: 0.009514530189335346
        model: {}
        policy_loss: -0.01913134567439556
        total_loss: -0.01695263758301735
        vf_explained_var: 0.13615021109580994
        vf_loss: 26.564056396484375
    load_time_ms: 13316.908
    num_steps_sampled: 11328000
    num_steps_trained: 11328000
    sample_time_ms: 106539.4
    update_time_ms: 18.598
  iterations_since_restore: 118
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.444680851063833
    ram_util_percent: 11.60585106382979
  pid: 28570
  policy_reward_max:
    agent-0: 195.0
    agent-1: 195.0
    agent-2: 213.0
    agent-3: 213.0
    agent-4: 199.5
    agent-5: 199.5
  policy_reward_mean:
    agent-0: 133.225
    agent-1: 133.225
    agent-2: 158.795
    agent-3: 158.795
    agent-4: 137.3
    agent-5: 137.3
  policy_reward_min:
    agent-0: 67.5
    agent-1: 67.5
    agent-2: 66.5
    agent-3: 66.5
    agent-4: 59.0
    agent-5: 59.0
  sampler_perf:
    mean_env_wait_ms: 27.16410331926116
    mean_inference_ms: 13.102795462517944
    mean_processing_ms: 58.68974446485919
  time_since_restore: 15615.507688760757
  time_this_iter_s: 132.79453349113464
  time_total_s: 15615.507688760757
  timestamp: 1637524152
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 11328000
  training_iteration: 118
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    118 |          15615.5 | 11328000 |   858.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 0.49
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 23.57
    apples_agent-1_min: 8
    apples_agent-2_max: 312
    apples_agent-2_mean: 225.99
    apples_agent-2_min: 59
    apples_agent-3_max: 85
    apples_agent-3_mean: 27.65
    apples_agent-3_min: 0
    apples_agent-4_max: 47
    apples_agent-4_mean: 1.98
    apples_agent-4_min: 0
    apples_agent-5_max: 377
    apples_agent-5_mean: 241.66
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 772
    cleaning_beam_agent-0_mean: 580.86
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 114
    cleaning_beam_agent-1_mean: 18.33
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 26.96
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 342
    cleaning_beam_agent-3_mean: 215.3
    cleaning_beam_agent-3_min: 37
    cleaning_beam_agent-4_max: 759
    cleaning_beam_agent-4_mean: 552.22
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 142
    cleaning_beam_agent-5_mean: 48.13
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.22
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-51-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1194.0
  episode_reward_mean: 879.25
  episode_reward_min: 187.0
  episodes_this_iter: 96
  episodes_total: 11424
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12633.26
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005531327915377915
        entropy: 0.48966729640960693
        entropy_coeff: 0.0017600000137463212
        kl: 0.007741897366940975
        model: {}
        policy_loss: -0.01347622461616993
        total_loss: -0.011188896372914314
        vf_explained_var: 0.022610515356063843
        vf_loss: 29.55595588684082
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005531327915377915
        entropy: 0.5795011520385742
        entropy_coeff: 0.0017600000137463212
        kl: 0.009528450667858124
        model: {}
        policy_loss: -0.01973184198141098
        total_loss: -0.01761174574494362
        vf_explained_var: 0.05446964502334595
        vf_loss: 29.613601684570312
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005531327915377915
        entropy: 0.6438583135604858
        entropy_coeff: 0.0017600000137463212
        kl: 0.01074796449393034
        model: {}
        policy_loss: -0.023721717298030853
        total_loss: -0.02121584862470627
        vf_explained_var: 0.11877715587615967
        vf_loss: 31.01659393310547
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005531327915377915
        entropy: 0.924075722694397
        entropy_coeff: 0.0017600000137463212
        kl: 0.012485725805163383
        model: {}
        policy_loss: -0.022163555026054382
        total_loss: -0.019395146518945694
        vf_explained_var: -0.10356485843658447
        vf_loss: 39.265682220458984
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005531327915377915
        entropy: 0.9006030559539795
        entropy_coeff: 0.0017600000137463212
        kl: 0.01179827656596899
        model: {}
        policy_loss: -0.02014252543449402
        total_loss: -0.018717007711529732
        vf_explained_var: 0.044970348477363586
        vf_loss: 29.05584144592285
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005531327915377915
        entropy: 0.4394826292991638
        entropy_coeff: 0.0017600000137463212
        kl: 0.00905558280646801
        model: {}
        policy_loss: -0.01967218890786171
        total_loss: -0.017526298761367798
        vf_explained_var: 0.15379007160663605
        vf_loss: 25.797924041748047
    load_time_ms: 13304.77
    num_steps_sampled: 11424000
    num_steps_trained: 11424000
    sample_time_ms: 106510.607
    update_time_ms: 18.87
  iterations_since_restore: 119
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.353439153439155
    ram_util_percent: 11.528042328042329
  pid: 28570
  policy_reward_max:
    agent-0: 211.5
    agent-1: 211.5
    agent-2: 219.0
    agent-3: 219.0
    agent-4: 211.5
    agent-5: 211.5
  policy_reward_mean:
    agent-0: 140.085
    agent-1: 140.085
    agent-2: 161.095
    agent-3: 161.095
    agent-4: 138.445
    agent-5: 138.445
  policy_reward_min:
    agent-0: 36.0
    agent-1: 36.0
    agent-2: 38.5
    agent-3: 38.5
    agent-4: 19.0
    agent-5: 19.0
  sampler_perf:
    mean_env_wait_ms: 27.176331580599577
    mean_inference_ms: 13.10185609802784
    mean_processing_ms: 58.684969225129606
  time_since_restore: 15747.892158031464
  time_this_iter_s: 132.38446927070618
  time_total_s: 15747.892158031464
  timestamp: 1637524285
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 11424000
  training_iteration: 119
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    119 |          15747.9 | 11424000 |   879.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 0.55
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 24.31
    apples_agent-1_min: 10
    apples_agent-2_max: 325
    apples_agent-2_mean: 221.24
    apples_agent-2_min: 66
    apples_agent-3_max: 101
    apples_agent-3_mean: 30.0
    apples_agent-3_min: 0
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.06
    apples_agent-4_min: 0
    apples_agent-5_max: 334
    apples_agent-5_mean: 241.64
    apples_agent-5_min: 90
    cleaning_beam_agent-0_max: 766
    cleaning_beam_agent-0_mean: 580.5
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 69
    cleaning_beam_agent-1_mean: 13.58
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 225
    cleaning_beam_agent-2_mean: 30.33
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 440
    cleaning_beam_agent-3_mean: 200.27
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 739
    cleaning_beam_agent-4_mean: 603.84
    cleaning_beam_agent-4_min: 337
    cleaning_beam_agent-5_max: 132
    cleaning_beam_agent-5_mean: 41.37
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-53-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1184.0
  episode_reward_mean: 872.04
  episode_reward_min: 455.0
  episodes_this_iter: 96
  episodes_total: 11520
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12640.666
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005471424083225429
        entropy: 0.49118244647979736
        entropy_coeff: 0.0017600000137463212
        kl: 0.007213113829493523
        model: {}
        policy_loss: -0.012092272751033306
        total_loss: -0.009937424212694168
        vf_explained_var: 0.031860724091529846
        vf_loss: 28.3900089263916
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005471424083225429
        entropy: 0.5565237998962402
        entropy_coeff: 0.0017600000137463212
        kl: 0.009376573376357555
        model: {}
        policy_loss: -0.019183514639735222
        total_loss: -0.01725619286298752
        vf_explained_var: 0.07531648874282837
        vf_loss: 27.30991554260254
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005471424083225429
        entropy: 0.6435609459877014
        entropy_coeff: 0.0017600000137463212
        kl: 0.011439042165875435
        model: {}
        policy_loss: -0.02247069962322712
        total_loss: -0.019538147374987602
        vf_explained_var: 0.1266513168811798
        vf_loss: 34.93268585205078
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005471424083225429
        entropy: 0.8866457939147949
        entropy_coeff: 0.0017600000137463212
        kl: 0.013723624870181084
        model: {}
        policy_loss: -0.022377407178282738
        total_loss: -0.019263969734311104
        vf_explained_var: -0.021978318691253662
        vf_loss: 41.592960357666016
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005471424083225429
        entropy: 0.825649082660675
        entropy_coeff: 0.0017600000137463212
        kl: 0.011579799465835094
        model: {}
        policy_loss: -0.018946267664432526
        total_loss: -0.0173166673630476
        vf_explained_var: -0.003679901361465454
        vf_loss: 29.79695701599121
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005471424083225429
        entropy: 0.43076878786087036
        entropy_coeff: 0.0017600000137463212
        kl: 0.009224675595760345
        model: {}
        policy_loss: -0.01859964430332184
        total_loss: -0.016543304547667503
        vf_explained_var: 0.17241385579109192
        vf_loss: 24.68569564819336
    load_time_ms: 13300.888
    num_steps_sampled: 11520000
    num_steps_trained: 11520000
    sample_time_ms: 106711.233
    update_time_ms: 17.883
  iterations_since_restore: 120
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.30736842105263
    ram_util_percent: 11.601578947368424
  pid: 28570
  policy_reward_max:
    agent-0: 197.5
    agent-1: 197.5
    agent-2: 209.0
    agent-3: 209.0
    agent-4: 192.0
    agent-5: 192.0
  policy_reward_mean:
    agent-0: 136.195
    agent-1: 136.195
    agent-2: 160.03
    agent-3: 160.03
    agent-4: 139.795
    agent-5: 139.795
  policy_reward_min:
    agent-0: 68.5
    agent-1: 68.5
    agent-2: 50.0
    agent-3: 50.0
    agent-4: 56.0
    agent-5: 56.0
  sampler_perf:
    mean_env_wait_ms: 27.190042656590645
    mean_inference_ms: 13.100607271379499
    mean_processing_ms: 58.68290969182316
  time_since_restore: 15881.835119009018
  time_this_iter_s: 133.94296097755432
  time_total_s: 15881.835119009018
  timestamp: 1637524419
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 11520000
  training_iteration: 120
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    120 |          15881.8 | 11520000 |   872.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 0.51
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 25.23
    apples_agent-1_min: 10
    apples_agent-2_max: 345
    apples_agent-2_mean: 235.0
    apples_agent-2_min: 121
    apples_agent-3_max: 88
    apples_agent-3_mean: 30.85
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 377
    apples_agent-5_mean: 248.59
    apples_agent-5_min: 142
    cleaning_beam_agent-0_max: 684
    cleaning_beam_agent-0_mean: 558.91
    cleaning_beam_agent-0_min: 433
    cleaning_beam_agent-1_max: 167
    cleaning_beam_agent-1_mean: 19.66
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 88
    cleaning_beam_agent-2_mean: 22.41
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 324
    cleaning_beam_agent-3_mean: 218.37
    cleaning_beam_agent-3_min: 100
    cleaning_beam_agent-4_max: 756
    cleaning_beam_agent-4_mean: 580.46
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 130
    cleaning_beam_agent-5_mean: 42.22
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-55-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1248.0
  episode_reward_mean: 900.21
  episode_reward_min: 507.0
  episodes_this_iter: 96
  episodes_total: 11616
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12653.534
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005411520251072943
        entropy: 0.4840410053730011
        entropy_coeff: 0.0017600000137463212
        kl: 0.006811884231865406
        model: {}
        policy_loss: -0.012397227808833122
        total_loss: -0.010349919088184834
        vf_explained_var: 0.015149429440498352
        vf_loss: 27.289220809936523
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005411520251072943
        entropy: 0.5653984546661377
        entropy_coeff: 0.0017600000137463212
        kl: 0.010029684752225876
        model: {}
        policy_loss: -0.020038506016135216
        total_loss: -0.0182966161519289
        vf_explained_var: 0.09504479169845581
        vf_loss: 25.489395141601562
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005411520251072943
        entropy: 0.6057641506195068
        entropy_coeff: 0.0017600000137463212
        kl: 0.00952928327023983
        model: {}
        policy_loss: -0.020919427275657654
        total_loss: -0.018108854070305824
        vf_explained_var: 0.08867804706096649
        vf_loss: 34.002525329589844
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005411520251072943
        entropy: 0.8960785865783691
        entropy_coeff: 0.0017600000137463212
        kl: 0.012584414333105087
        model: {}
        policy_loss: -0.022567259147763252
        total_loss: -0.01970716379582882
        vf_explained_var: -0.060519397258758545
        vf_loss: 39.65277862548828
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005411520251072943
        entropy: 0.8708947896957397
        entropy_coeff: 0.0017600000137463212
        kl: 0.011718111112713814
        model: {}
        policy_loss: -0.020604286342859268
        total_loss: -0.01897755265235901
        vf_explained_var: -0.023014932870864868
        vf_loss: 30.55228042602539
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005411520251072943
        entropy: 0.43877819180488586
        entropy_coeff: 0.0017600000137463212
        kl: 0.00951174832880497
        model: {}
        policy_loss: -0.018917035311460495
        total_loss: -0.01684243604540825
        vf_explained_var: 0.15011011064052582
        vf_loss: 24.901607513427734
    load_time_ms: 13296.588
    num_steps_sampled: 11616000
    num_steps_trained: 11616000
    sample_time_ms: 106790.75
    update_time_ms: 17.647
  iterations_since_restore: 121
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.375
    ram_util_percent: 11.591489361702132
  pid: 28570
  policy_reward_max:
    agent-0: 195.0
    agent-1: 195.0
    agent-2: 231.0
    agent-3: 231.0
    agent-4: 212.5
    agent-5: 212.5
  policy_reward_mean:
    agent-0: 137.405
    agent-1: 137.405
    agent-2: 169.19
    agent-3: 169.19
    agent-4: 143.51
    agent-5: 143.51
  policy_reward_min:
    agent-0: 64.0
    agent-1: 64.0
    agent-2: 94.0
    agent-3: 94.0
    agent-4: 83.0
    agent-5: 83.0
  sampler_perf:
    mean_env_wait_ms: 27.200536157681753
    mean_inference_ms: 13.099517823577106
    mean_processing_ms: 58.67869126123033
  time_since_restore: 16013.930662155151
  time_this_iter_s: 132.09554314613342
  time_total_s: 16013.930662155151
  timestamp: 1637524551
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 11616000
  training_iteration: 121
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    121 |          16013.9 | 11616000 |   900.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 0.43
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 24.38
    apples_agent-1_min: 9
    apples_agent-2_max: 343
    apples_agent-2_mean: 226.84
    apples_agent-2_min: 105
    apples_agent-3_max: 122
    apples_agent-3_mean: 32.15
    apples_agent-3_min: 1
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 337
    apples_agent-5_mean: 236.91
    apples_agent-5_min: 85
    cleaning_beam_agent-0_max: 717
    cleaning_beam_agent-0_mean: 573.59
    cleaning_beam_agent-0_min: 402
    cleaning_beam_agent-1_max: 111
    cleaning_beam_agent-1_mean: 14.27
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 27.18
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 386
    cleaning_beam_agent-3_mean: 200.77
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 755
    cleaning_beam_agent-4_mean: 592.4
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 164
    cleaning_beam_agent-5_mean: 52.38
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.12
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.22
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.27
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_14-58-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1142.0
  episode_reward_mean: 873.21
  episode_reward_min: 350.0
  episodes_this_iter: 96
  episodes_total: 11712
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12645.657
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005351615836843848
        entropy: 0.47884073853492737
        entropy_coeff: 0.0017600000137463212
        kl: 0.007438488304615021
        model: {}
        policy_loss: -0.012298989109694958
        total_loss: -0.010106757283210754
        vf_explained_var: 0.045105502009391785
        vf_loss: 28.49030876159668
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005351615836843848
        entropy: 0.5718793869018555
        entropy_coeff: 0.0017600000137463212
        kl: 0.009771781042218208
        model: {}
        policy_loss: -0.01954592950642109
        total_loss: -0.01767662540078163
        vf_explained_var: 0.09897646307945251
        vf_loss: 26.925907135009766
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005351615836843848
        entropy: 0.6181092262268066
        entropy_coeff: 0.0017600000137463212
        kl: 0.010902654379606247
        model: {}
        policy_loss: -0.02277183160185814
        total_loss: -0.019998345524072647
        vf_explained_var: 0.10761375725269318
        vf_loss: 33.16225051879883
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005351615836843848
        entropy: 0.8618338108062744
        entropy_coeff: 0.0017600000137463212
        kl: 0.011704791337251663
        model: {}
        policy_loss: -0.022344695404171944
        total_loss: -0.01939157396554947
        vf_explained_var: -0.0672391951084137
        vf_loss: 40.3101806640625
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005351615836843848
        entropy: 0.8469687700271606
        entropy_coeff: 0.0017600000137463212
        kl: 0.011926225386559963
        model: {}
        policy_loss: -0.019569015130400658
        total_loss: -0.017953582108020782
        vf_explained_var: 0.008464515209197998
        vf_loss: 29.99966049194336
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005351615836843848
        entropy: 0.4938786029815674
        entropy_coeff: 0.0017600000137463212
        kl: 0.010251782834529877
        model: {}
        policy_loss: -0.021084865555167198
        total_loss: -0.01902041956782341
        vf_explained_var: 0.15518270432949066
        vf_loss: 25.492280960083008
    load_time_ms: 13298.14
    num_steps_sampled: 11712000
    num_steps_trained: 11712000
    sample_time_ms: 106847.482
    update_time_ms: 17.102
  iterations_since_restore: 122
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.223560209424086
    ram_util_percent: 11.600523560209428
  pid: 28570
  policy_reward_max:
    agent-0: 187.0
    agent-1: 187.0
    agent-2: 228.5
    agent-3: 228.5
    agent-4: 190.5
    agent-5: 190.5
  policy_reward_mean:
    agent-0: 134.895
    agent-1: 134.895
    agent-2: 164.845
    agent-3: 164.845
    agent-4: 136.865
    agent-5: 136.865
  policy_reward_min:
    agent-0: 52.0
    agent-1: 52.0
    agent-2: 70.5
    agent-3: 70.5
    agent-4: 50.5
    agent-5: 50.5
  sampler_perf:
    mean_env_wait_ms: 27.212531828602483
    mean_inference_ms: 13.098605572155975
    mean_processing_ms: 58.67662629500443
  time_since_restore: 16147.354770183563
  time_this_iter_s: 133.42410802841187
  time_total_s: 16147.354770183563
  timestamp: 1637524685
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 11712000
  training_iteration: 122
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    122 |          16147.4 | 11712000 |   873.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.62
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 26.32
    apples_agent-1_min: 7
    apples_agent-2_max: 342
    apples_agent-2_mean: 229.86
    apples_agent-2_min: 47
    apples_agent-3_max: 107
    apples_agent-3_mean: 31.23
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 339
    apples_agent-5_mean: 244.17
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 708
    cleaning_beam_agent-0_mean: 589.79
    cleaning_beam_agent-0_min: 279
    cleaning_beam_agent-1_max: 78
    cleaning_beam_agent-1_mean: 12.32
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 203
    cleaning_beam_agent-2_mean: 31.48
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 353
    cleaning_beam_agent-3_mean: 195.79
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 767
    cleaning_beam_agent-4_mean: 596.9
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 164
    cleaning_beam_agent-5_mean: 52.68
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.13
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-00-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1188.0
  episode_reward_mean: 886.38
  episode_reward_min: 171.0
  episodes_this_iter: 96
  episodes_total: 11808
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12637.378
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005291712004691362
        entropy: 0.4790354073047638
        entropy_coeff: 0.0017600000137463212
        kl: 0.008412233553826809
        model: {}
        policy_loss: -0.014904282055795193
        total_loss: -0.012993997894227505
        vf_explained_var: 0.08481501042842865
        vf_loss: 25.43082618713379
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005291712004691362
        entropy: 0.5547690391540527
        entropy_coeff: 0.0017600000137463212
        kl: 0.009792152792215347
        model: {}
        policy_loss: -0.020062146708369255
        total_loss: -0.01825576461851597
        vf_explained_var: 0.06722842156887054
        vf_loss: 25.991727828979492
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005291712004691362
        entropy: 0.5932179689407349
        entropy_coeff: 0.0017600000137463212
        kl: 0.00995525624603033
        model: {}
        policy_loss: -0.021991288289427757
        total_loss: -0.018967052921652794
        vf_explained_var: 0.11015576124191284
        vf_loss: 35.705352783203125
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005291712004691362
        entropy: 0.8990870714187622
        entropy_coeff: 0.0017600000137463212
        kl: 0.012826718389987946
        model: {}
        policy_loss: -0.02300550416111946
        total_loss: -0.020124269649386406
        vf_explained_var: 0.010825753211975098
        vf_loss: 39.826255798339844
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005291712004691362
        entropy: 0.8589853048324585
        entropy_coeff: 0.0017600000137463212
        kl: 0.011767336167395115
        model: {}
        policy_loss: -0.01969613879919052
        total_loss: -0.01810556650161743
        vf_explained_var: -0.007246062159538269
        vf_loss: 29.976736068725586
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005291712004691362
        entropy: 0.46868589520454407
        entropy_coeff: 0.0017600000137463212
        kl: 0.010392017662525177
        model: {}
        policy_loss: -0.01891910657286644
        total_loss: -0.016866624355316162
        vf_explained_var: 0.1636454313993454
        vf_loss: 24.876667022705078
    load_time_ms: 13282.617
    num_steps_sampled: 11808000
    num_steps_trained: 11808000
    sample_time_ms: 106866.221
    update_time_ms: 16.848
  iterations_since_restore: 123
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.453723404255314
    ram_util_percent: 11.595744680851066
  pid: 28570
  policy_reward_max:
    agent-0: 194.0
    agent-1: 194.0
    agent-2: 240.0
    agent-3: 240.0
    agent-4: 191.0
    agent-5: 191.0
  policy_reward_mean:
    agent-0: 136.16
    agent-1: 136.16
    agent-2: 166.0
    agent-3: 166.0
    agent-4: 141.03
    agent-5: 141.03
  policy_reward_min:
    agent-0: 27.0
    agent-1: 27.0
    agent-2: 39.5
    agent-3: 39.5
    agent-4: 19.0
    agent-5: 19.0
  sampler_perf:
    mean_env_wait_ms: 27.22520410584116
    mean_inference_ms: 13.098101432965834
    mean_processing_ms: 58.67334354609407
  time_since_restore: 16280.127196788788
  time_this_iter_s: 132.7724266052246
  time_total_s: 16280.127196788788
  timestamp: 1637524818
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 11808000
  training_iteration: 123
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    123 |          16280.1 | 11808000 |   886.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.61
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 26.42
    apples_agent-1_min: 7
    apples_agent-2_max: 337
    apples_agent-2_mean: 224.44
    apples_agent-2_min: 91
    apples_agent-3_max: 114
    apples_agent-3_mean: 26.31
    apples_agent-3_min: 0
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 357
    apples_agent-5_mean: 243.43
    apples_agent-5_min: 85
    cleaning_beam_agent-0_max: 742
    cleaning_beam_agent-0_mean: 591.96
    cleaning_beam_agent-0_min: 295
    cleaning_beam_agent-1_max: 105
    cleaning_beam_agent-1_mean: 16.95
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 203
    cleaning_beam_agent-2_mean: 32.5
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 365
    cleaning_beam_agent-3_mean: 200.54
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 759
    cleaning_beam_agent-4_mean: 618.61
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 52.25
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.2
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-02-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1155.0
  episode_reward_mean: 870.59
  episode_reward_min: 304.0
  episodes_this_iter: 96
  episodes_total: 11904
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12632.229
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005231808172538877
        entropy: 0.47863659262657166
        entropy_coeff: 0.0017600000137463212
        kl: 0.009036905132234097
        model: {}
        policy_loss: -0.014231341890990734
        total_loss: -0.01222884189337492
        vf_explained_var: 0.06810082495212555
        vf_loss: 26.18975257873535
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005231808172538877
        entropy: 0.5733119249343872
        entropy_coeff: 0.0017600000137463212
        kl: 0.009972081519663334
        model: {}
        policy_loss: -0.020150670781731606
        total_loss: -0.018369819968938828
        vf_explained_var: 0.07384675741195679
        vf_loss: 26.02901840209961
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005231808172538877
        entropy: 0.6199105381965637
        entropy_coeff: 0.0017600000137463212
        kl: 0.009751303121447563
        model: {}
        policy_loss: -0.02217024378478527
        total_loss: -0.019205141812562943
        vf_explained_var: 0.11587926745414734
        vf_loss: 35.685768127441406
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005231808172538877
        entropy: 0.8903346657752991
        entropy_coeff: 0.0017600000137463212
        kl: 0.01174994744360447
        model: {}
        policy_loss: -0.02223506011068821
        total_loss: -0.019175676628947258
        vf_explained_var: -0.028747230768203735
        vf_loss: 41.857460021972656
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005231808172538877
        entropy: 0.7993638515472412
        entropy_coeff: 0.0017600000137463212
        kl: 0.011222869157791138
        model: {}
        policy_loss: -0.018912609666585922
        total_loss: -0.01713491976261139
        vf_explained_var: -0.018957465887069702
        vf_loss: 30.84694480895996
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005231808172538877
        entropy: 0.464927077293396
        entropy_coeff: 0.0017600000137463212
        kl: 0.009264251217246056
        model: {}
        policy_loss: -0.01936253346502781
        total_loss: -0.017209604382514954
        vf_explained_var: 0.13446809351444244
        vf_loss: 26.237926483154297
    load_time_ms: 13277.559
    num_steps_sampled: 11904000
    num_steps_trained: 11904000
    sample_time_ms: 106788.097
    update_time_ms: 16.482
  iterations_since_restore: 124
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.342857142857145
    ram_util_percent: 11.685714285714287
  pid: 28570
  policy_reward_max:
    agent-0: 184.5
    agent-1: 184.5
    agent-2: 241.0
    agent-3: 241.0
    agent-4: 200.0
    agent-5: 200.0
  policy_reward_mean:
    agent-0: 133.36
    agent-1: 133.36
    agent-2: 162.16
    agent-3: 162.16
    agent-4: 139.775
    agent-5: 139.775
  policy_reward_min:
    agent-0: 46.0
    agent-1: 46.0
    agent-2: 58.5
    agent-3: 58.5
    agent-4: 38.5
    agent-5: 38.5
  sampler_perf:
    mean_env_wait_ms: 27.237299729390497
    mean_inference_ms: 13.097014688633141
    mean_processing_ms: 58.66819055744723
  time_since_restore: 16412.663457870483
  time_this_iter_s: 132.53626108169556
  time_total_s: 16412.663457870483
  timestamp: 1637524950
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 11904000
  training_iteration: 124
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    124 |          16412.7 | 11904000 |   870.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.45
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 25.63
    apples_agent-1_min: 5
    apples_agent-2_max: 341
    apples_agent-2_mean: 240.76
    apples_agent-2_min: 127
    apples_agent-3_max: 107
    apples_agent-3_mean: 33.36
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 352
    apples_agent-5_mean: 253.02
    apples_agent-5_min: 154
    cleaning_beam_agent-0_max: 705
    cleaning_beam_agent-0_mean: 575.22
    cleaning_beam_agent-0_min: 416
    cleaning_beam_agent-1_max: 115
    cleaning_beam_agent-1_mean: 19.38
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 127
    cleaning_beam_agent-2_mean: 26.77
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 352
    cleaning_beam_agent-3_mean: 207.51
    cleaning_beam_agent-3_min: 54
    cleaning_beam_agent-4_max: 778
    cleaning_beam_agent-4_mean: 613.74
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 149
    cleaning_beam_agent-5_mean: 50.24
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.23
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-04-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1182.0
  episode_reward_mean: 921.95
  episode_reward_min: 469.0
  episodes_this_iter: 96
  episodes_total: 12000
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12629.439
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005171903758309782
        entropy: 0.48462438583374023
        entropy_coeff: 0.0017600000137463212
        kl: 0.008025427348911762
        model: {}
        policy_loss: -0.014016206376254559
        total_loss: -0.011905346065759659
        vf_explained_var: 0.07584720849990845
        vf_loss: 27.63166046142578
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005171903758309782
        entropy: 0.5618571639060974
        entropy_coeff: 0.0017600000137463212
        kl: 0.009066987782716751
        model: {}
        policy_loss: -0.01921059750020504
        total_loss: -0.017314327880740166
        vf_explained_var: 0.09594519436359406
        vf_loss: 27.15129852294922
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005171903758309782
        entropy: 0.5965068936347961
        entropy_coeff: 0.0017600000137463212
        kl: 0.01047230139374733
        model: {}
        policy_loss: -0.021349458023905754
        total_loss: -0.01829826831817627
        vf_explained_var: 0.09965398907661438
        vf_loss: 35.77425003051758
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005171903758309782
        entropy: 0.8743479251861572
        entropy_coeff: 0.0017600000137463212
        kl: 0.012688344344496727
        model: {}
        policy_loss: -0.023093905299901962
        total_loss: -0.020153574645519257
        vf_explained_var: 0.008473753929138184
        vf_loss: 40.03369140625
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005171903758309782
        entropy: 0.8041189908981323
        entropy_coeff: 0.0017600000137463212
        kl: 0.012012729421257973
        model: {}
        policy_loss: -0.018776677548885345
        total_loss: -0.01707278937101364
        vf_explained_var: -0.026234716176986694
        vf_loss: 30.122386932373047
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005171903758309782
        entropy: 0.45079663395881653
        entropy_coeff: 0.0017600000137463212
        kl: 0.008911637589335442
        model: {}
        policy_loss: -0.019306056201457977
        total_loss: -0.017245307564735413
        vf_explained_var: 0.14811640977859497
        vf_loss: 25.19963836669922
    load_time_ms: 13281.728
    num_steps_sampled: 12000000
    num_steps_trained: 12000000
    sample_time_ms: 106807.811
    update_time_ms: 17.192
  iterations_since_restore: 125
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.347894736842107
    ram_util_percent: 11.523684210526316
  pid: 28570
  policy_reward_max:
    agent-0: 200.5
    agent-1: 200.5
    agent-2: 247.5
    agent-3: 247.5
    agent-4: 205.0
    agent-5: 205.0
  policy_reward_mean:
    agent-0: 142.23
    agent-1: 142.23
    agent-2: 170.645
    agent-3: 170.645
    agent-4: 148.1
    agent-5: 148.1
  policy_reward_min:
    agent-0: 54.0
    agent-1: 54.0
    agent-2: 90.5
    agent-3: 90.5
    agent-4: 90.0
    agent-5: 90.0
  sampler_perf:
    mean_env_wait_ms: 27.250377215393286
    mean_inference_ms: 13.096110966998722
    mean_processing_ms: 58.66483992130984
  time_since_restore: 16545.37228679657
  time_this_iter_s: 132.70882892608643
  time_total_s: 16545.37228679657
  timestamp: 1637525083
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 12000000
  training_iteration: 125
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    125 |          16545.4 | 12000000 |   921.95 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.22
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 26.2
    apples_agent-1_min: 9
    apples_agent-2_max: 347
    apples_agent-2_mean: 232.34
    apples_agent-2_min: 87
    apples_agent-3_max: 89
    apples_agent-3_mean: 31.97
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 355
    apples_agent-5_mean: 254.33
    apples_agent-5_min: 94
    cleaning_beam_agent-0_max: 706
    cleaning_beam_agent-0_mean: 577.33
    cleaning_beam_agent-0_min: 369
    cleaning_beam_agent-1_max: 167
    cleaning_beam_agent-1_mean: 15.64
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 28.27
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 307
    cleaning_beam_agent-3_mean: 188.35
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 752
    cleaning_beam_agent-4_mean: 594.52
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 109
    cleaning_beam_agent-5_mean: 45.57
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.1
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.15
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.24
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-06-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1234.0
  episode_reward_mean: 907.58
  episode_reward_min: 475.0
  episodes_this_iter: 96
  episodes_total: 12096
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12619.12
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0005111999926157296
        entropy: 0.49228808283805847
        entropy_coeff: 0.0017600000137463212
        kl: 0.006721612066030502
        model: {}
        policy_loss: -0.013415796682238579
        total_loss: -0.011169795878231525
        vf_explained_var: 0.03669808804988861
        vf_loss: 29.44390296936035
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0005111999926157296
        entropy: 0.5775254964828491
        entropy_coeff: 0.0017600000137463212
        kl: 0.010268704034388065
        model: {}
        policy_loss: -0.020310528576374054
        total_loss: -0.018367039039731026
        vf_explained_var: 0.09629961848258972
        vf_loss: 27.6739501953125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0005111999926157296
        entropy: 0.6118912696838379
        entropy_coeff: 0.0017600000137463212
        kl: 0.01137540303170681
        model: {}
        policy_loss: -0.022313648834824562
        total_loss: -0.01918541081249714
        vf_explained_var: 0.14585228264331818
        vf_loss: 36.363948822021484
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005111999926157296
        entropy: 0.8622074127197266
        entropy_coeff: 0.0017600000137463212
        kl: 0.011583096347749233
        model: {}
        policy_loss: -0.022355707362294197
        total_loss: -0.018953068181872368
        vf_explained_var: -0.032032161951065063
        vf_loss: 44.857608795166016
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0005111999926157296
        entropy: 0.828668475151062
        entropy_coeff: 0.0017600000137463212
        kl: 0.011789667420089245
        model: {}
        policy_loss: -0.01925060898065567
        total_loss: -0.01756569929420948
        vf_explained_var: -0.009242072701454163
        vf_loss: 30.384510040283203
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0005111999926157296
        entropy: 0.4252893030643463
        entropy_coeff: 0.0017600000137463212
        kl: 0.00967179611325264
        model: {}
        policy_loss: -0.017615441232919693
        total_loss: -0.015516729094088078
        vf_explained_var: 0.17501625418663025
        vf_loss: 24.84527587890625
    load_time_ms: 13279.481
    num_steps_sampled: 12096000
    num_steps_trained: 12096000
    sample_time_ms: 106809.206
    update_time_ms: 17.642
  iterations_since_restore: 126
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.55744680851064
    ram_util_percent: 11.616489361702131
  pid: 28570
  policy_reward_max:
    agent-0: 196.0
    agent-1: 196.0
    agent-2: 238.0
    agent-3: 238.0
    agent-4: 204.0
    agent-5: 204.0
  policy_reward_mean:
    agent-0: 139.75
    agent-1: 139.75
    agent-2: 167.915
    agent-3: 167.915
    agent-4: 146.125
    agent-5: 146.125
  policy_reward_min:
    agent-0: 66.0
    agent-1: 66.0
    agent-2: 70.0
    agent-3: 70.0
    agent-4: 50.0
    agent-5: 50.0
  sampler_perf:
    mean_env_wait_ms: 27.26188594568574
    mean_inference_ms: 13.095752788811481
    mean_processing_ms: 58.66043356018438
  time_since_restore: 16678.311905145645
  time_this_iter_s: 132.93961834907532
  time_total_s: 16678.311905145645
  timestamp: 1637525216
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 12096000
  training_iteration: 126
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    126 |          16678.3 | 12096000 |   907.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.13
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 25.36
    apples_agent-1_min: 9
    apples_agent-2_max: 322
    apples_agent-2_mean: 243.59
    apples_agent-2_min: 0
    apples_agent-3_max: 226
    apples_agent-3_mean: 35.4
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.81
    apples_agent-4_min: 0
    apples_agent-5_max: 364
    apples_agent-5_mean: 262.28
    apples_agent-5_min: 147
    cleaning_beam_agent-0_max: 696
    cleaning_beam_agent-0_mean: 571.65
    cleaning_beam_agent-0_min: 379
    cleaning_beam_agent-1_max: 185
    cleaning_beam_agent-1_mean: 18.16
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 135
    cleaning_beam_agent-2_mean: 29.69
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 343
    cleaning_beam_agent-3_mean: 217.0
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 718
    cleaning_beam_agent-4_mean: 576.99
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 52.81
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.11
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.14
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.35
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-09-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1194.0
  episode_reward_mean: 946.74
  episode_reward_min: 668.0
  episodes_this_iter: 96
  episodes_total: 12192
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12612.868
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000505209609400481
        entropy: 0.48160332441329956
        entropy_coeff: 0.0017600000137463212
        kl: 0.010519348084926605
        model: {}
        policy_loss: -0.011155138723552227
        total_loss: -0.008874576538801193
        vf_explained_var: 0.04935300350189209
        vf_loss: 28.652027130126953
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000505209609400481
        entropy: 0.5607061982154846
        entropy_coeff: 0.0017600000137463212
        kl: 0.00942995399236679
        model: {}
        policy_loss: -0.018540706485509872
        total_loss: -0.016514118760824203
        vf_explained_var: 0.06464417278766632
        vf_loss: 28.366182327270508
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000505209609400481
        entropy: 0.5974016189575195
        entropy_coeff: 0.0017600000137463212
        kl: 0.010223787277936935
        model: {}
        policy_loss: -0.0215337835252285
        total_loss: -0.018535908311605453
        vf_explained_var: 0.13671912252902985
        vf_loss: 35.38114547729492
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000505209609400481
        entropy: 0.90218186378479
        entropy_coeff: 0.0017600000137463212
        kl: 0.012439681217074394
        model: {}
        policy_loss: -0.023158550262451172
        total_loss: -0.019900169223546982
        vf_explained_var: -0.06092455983161926
        vf_loss: 43.797325134277344
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000505209609400481
        entropy: 0.8747203946113586
        entropy_coeff: 0.0017600000137463212
        kl: 0.010486140847206116
        model: {}
        policy_loss: -0.019615232944488525
        total_loss: -0.01785883866250515
        vf_explained_var: 0.004136204719543457
        vf_loss: 32.02587127685547
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000505209609400481
        entropy: 0.4416731595993042
        entropy_coeff: 0.0017600000137463212
        kl: 0.009495467878878117
        model: {}
        policy_loss: -0.019001170992851257
        total_loss: -0.016682207584381104
        vf_explained_var: 0.15756575763225555
        vf_loss: 27.40229034423828
    load_time_ms: 13280.188
    num_steps_sampled: 12192000
    num_steps_trained: 12192000
    sample_time_ms: 106872.576
    update_time_ms: 18.044
  iterations_since_restore: 127
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.32684210526316
    ram_util_percent: 11.595263157894742
  pid: 28570
  policy_reward_max:
    agent-0: 205.5
    agent-1: 205.5
    agent-2: 237.0
    agent-3: 237.0
    agent-4: 216.5
    agent-5: 216.5
  policy_reward_mean:
    agent-0: 147.05
    agent-1: 147.05
    agent-2: 175.19
    agent-3: 175.19
    agent-4: 151.13
    agent-5: 151.13
  policy_reward_min:
    agent-0: 76.5
    agent-1: 76.5
    agent-2: 19.5
    agent-3: 19.5
    agent-4: 96.0
    agent-5: 96.0
  sampler_perf:
    mean_env_wait_ms: 27.274047707635486
    mean_inference_ms: 13.094680889507195
    mean_processing_ms: 58.658271112041284
  time_since_restore: 16811.411237716675
  time_this_iter_s: 133.09933257102966
  time_total_s: 16811.411237716675
  timestamp: 1637525349
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 12192000
  training_iteration: 127
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    127 |          16811.4 | 12192000 |   946.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 0.56
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 24.97
    apples_agent-1_min: 6
    apples_agent-2_max: 326
    apples_agent-2_mean: 239.52
    apples_agent-2_min: 68
    apples_agent-3_max: 129
    apples_agent-3_mean: 30.6
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.57
    apples_agent-4_min: 0
    apples_agent-5_max: 359
    apples_agent-5_mean: 259.35
    apples_agent-5_min: 99
    cleaning_beam_agent-0_max: 699
    cleaning_beam_agent-0_mean: 579.26
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 102
    cleaning_beam_agent-1_mean: 18.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 27.34
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 319
    cleaning_beam_agent-3_mean: 199.72
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 747
    cleaning_beam_agent-4_mean: 569.91
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 53.19
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.12
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-11-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1260.0
  episode_reward_mean: 941.63
  episode_reward_min: 434.0
  episodes_this_iter: 96
  episodes_total: 12288
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12609.305
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004992192261852324
        entropy: 0.5021339654922485
        entropy_coeff: 0.0017600000137463212
        kl: 0.009344278834760189
        model: {}
        policy_loss: -0.014765497297048569
        total_loss: -0.012649926356971264
        vf_explained_var: 0.07060785591602325
        vf_loss: 27.657207489013672
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004992192261852324
        entropy: 0.536467969417572
        entropy_coeff: 0.0017600000137463212
        kl: 0.009506483562290668
        model: {}
        policy_loss: -0.018798314034938812
        total_loss: -0.016863856464624405
        vf_explained_var: 0.09542256593704224
        vf_loss: 27.00395965576172
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004992192261852324
        entropy: 0.5707686543464661
        entropy_coeff: 0.0017600000137463212
        kl: 0.011139166541397572
        model: {}
        policy_loss: -0.021716464310884476
        total_loss: -0.018834300339221954
        vf_explained_var: 0.12626399099826813
        vf_loss: 33.29761505126953
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004992192261852324
        entropy: 0.9138821363449097
        entropy_coeff: 0.0017600000137463212
        kl: 0.012281280010938644
        model: {}
        policy_loss: -0.023184996098279953
        total_loss: -0.020511528477072716
        vf_explained_var: 0.002200096845626831
        vf_loss: 38.213497161865234
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004992192261852324
        entropy: 0.8669813871383667
        entropy_coeff: 0.0017600000137463212
        kl: 0.010559210553765297
        model: {}
        policy_loss: -0.019426733255386353
        total_loss: -0.017787065356969833
        vf_explained_var: 0.0017307549715042114
        vf_loss: 30.71588706970215
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004992192261852324
        entropy: 0.45873701572418213
        entropy_coeff: 0.0017600000137463212
        kl: 0.009284780360758305
        model: {}
        policy_loss: -0.019052991643548012
        total_loss: -0.017078494653105736
        vf_explained_var: 0.21132239699363708
        vf_loss: 24.336997985839844
    load_time_ms: 13286.268
    num_steps_sampled: 12288000
    num_steps_trained: 12288000
    sample_time_ms: 106906.86
    update_time_ms: 17.78
  iterations_since_restore: 128
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.3015873015873
    ram_util_percent: 11.606349206349208
  pid: 28570
  policy_reward_max:
    agent-0: 215.5
    agent-1: 215.5
    agent-2: 225.5
    agent-3: 225.5
    agent-4: 199.0
    agent-5: 199.0
  policy_reward_mean:
    agent-0: 148.175
    agent-1: 148.175
    agent-2: 174.465
    agent-3: 174.465
    agent-4: 148.175
    agent-5: 148.175
  policy_reward_min:
    agent-0: 68.5
    agent-1: 68.5
    agent-2: 64.0
    agent-3: 64.0
    agent-4: 61.0
    agent-5: 61.0
  sampler_perf:
    mean_env_wait_ms: 27.284417366451677
    mean_inference_ms: 13.093819752998193
    mean_processing_ms: 58.65366543358923
  time_since_restore: 16944.56940817833
  time_this_iter_s: 133.15817046165466
  time_total_s: 16944.56940817833
  timestamp: 1637525482
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 12288000
  training_iteration: 128
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    128 |          16944.6 | 12288000 |   941.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 0.33
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 25.27
    apples_agent-1_min: 10
    apples_agent-2_max: 330
    apples_agent-2_mean: 240.8
    apples_agent-2_min: 124
    apples_agent-3_max: 395
    apples_agent-3_mean: 36.01
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.32
    apples_agent-4_min: 0
    apples_agent-5_max: 344
    apples_agent-5_mean: 249.41
    apples_agent-5_min: 96
    cleaning_beam_agent-0_max: 676
    cleaning_beam_agent-0_mean: 559.13
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 230
    cleaning_beam_agent-1_mean: 25.4
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 28.25
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 365
    cleaning_beam_agent-3_mean: 183.31
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 750
    cleaning_beam_agent-4_mean: 558.61
    cleaning_beam_agent-4_min: 338
    cleaning_beam_agent-5_max: 221
    cleaning_beam_agent-5_mean: 64.08
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.09
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.21
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.33
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-13-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1227.0
  episode_reward_mean: 916.62
  episode_reward_min: 417.0
  episodes_this_iter: 96
  episodes_total: 12384
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12639.237
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004932287847623229
        entropy: 0.508587121963501
        entropy_coeff: 0.0017600000137463212
        kl: 0.008193331770598888
        model: {}
        policy_loss: -0.013817811384797096
        total_loss: -0.011682383716106415
        vf_explained_var: 0.09232234954833984
        vf_loss: 28.25710678100586
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004932287847623229
        entropy: 0.5518583059310913
        entropy_coeff: 0.0017600000137463212
        kl: 0.009553216397762299
        model: {}
        policy_loss: -0.01894497685134411
        total_loss: -0.01691741868853569
        vf_explained_var: 0.09481848776340485
        vf_loss: 28.19708251953125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004932287847623229
        entropy: 0.577496349811554
        entropy_coeff: 0.0017600000137463212
        kl: 0.009851617738604546
        model: {}
        policy_loss: -0.021964779123663902
        total_loss: -0.018841758370399475
        vf_explained_var: 0.10610321164131165
        vf_loss: 36.46836853027344
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004932287847623229
        entropy: 0.8820382952690125
        entropy_coeff: 0.0017600000137463212
        kl: 0.012265708297491074
        model: {}
        policy_loss: -0.022099902853369713
        total_loss: -0.019068047404289246
        vf_explained_var: -0.007684424519538879
        vf_loss: 41.242774963378906
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004932287847623229
        entropy: 0.8806280493736267
        entropy_coeff: 0.0017600000137463212
        kl: 0.012105709873139858
        model: {}
        policy_loss: -0.02079225331544876
        total_loss: -0.019213389605283737
        vf_explained_var: 0.028231069445610046
        vf_loss: 30.210391998291016
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004932287847623229
        entropy: 0.5063295364379883
        entropy_coeff: 0.0017600000137463212
        kl: 0.010061318054795265
        model: {}
        policy_loss: -0.021119091659784317
        total_loss: -0.019052919000387192
        vf_explained_var: 0.17156368494033813
        vf_loss: 25.800100326538086
    load_time_ms: 13283.005
    num_steps_sampled: 12384000
    num_steps_trained: 12384000
    sample_time_ms: 106824.084
    update_time_ms: 17.549
  iterations_since_restore: 129
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.367553191489364
    ram_util_percent: 11.60053191489362
  pid: 28570
  policy_reward_max:
    agent-0: 199.5
    agent-1: 199.5
    agent-2: 234.5
    agent-3: 234.5
    agent-4: 200.5
    agent-5: 200.5
  policy_reward_mean:
    agent-0: 143.255
    agent-1: 143.255
    agent-2: 170.955
    agent-3: 170.955
    agent-4: 144.1
    agent-5: 144.1
  policy_reward_min:
    agent-0: 50.5
    agent-1: 50.5
    agent-2: 81.5
    agent-3: 81.5
    agent-4: 62.0
    agent-5: 62.0
  sampler_perf:
    mean_env_wait_ms: 27.293250177203888
    mean_inference_ms: 13.092737639351869
    mean_processing_ms: 58.649438707012486
  time_since_restore: 17076.38569498062
  time_this_iter_s: 131.81628680229187
  time_total_s: 17076.38569498062
  timestamp: 1637525614
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 12384000
  training_iteration: 129
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    129 |          17076.4 | 12384000 |   916.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 0.63
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 24.89
    apples_agent-1_min: 9
    apples_agent-2_max: 367
    apples_agent-2_mean: 248.81
    apples_agent-2_min: 101
    apples_agent-3_max: 80
    apples_agent-3_mean: 31.09
    apples_agent-3_min: 1
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 359
    apples_agent-5_mean: 248.58
    apples_agent-5_min: 105
    cleaning_beam_agent-0_max: 720
    cleaning_beam_agent-0_mean: 584.46
    cleaning_beam_agent-0_min: 328
    cleaning_beam_agent-1_max: 178
    cleaning_beam_agent-1_mean: 21.11
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 26.62
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 335
    cleaning_beam_agent-3_mean: 184.41
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 851
    cleaning_beam_agent-4_mean: 580.69
    cleaning_beam_agent-4_min: 319
    cleaning_beam_agent-5_max: 190
    cleaning_beam_agent-5_mean: 61.04
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.18
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.26
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-15-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1241.0
  episode_reward_mean: 935.25
  episode_reward_min: 409.0
  episodes_this_iter: 96
  episodes_total: 12480
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12641.784
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004872384015470743
        entropy: 0.4681687653064728
        entropy_coeff: 0.0017600000137463212
        kl: 0.00748199550434947
        model: {}
        policy_loss: -0.012873271480202675
        total_loss: -0.010663352906703949
        vf_explained_var: 0.07891151309013367
        vf_loss: 28.468456268310547
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004872384015470743
        entropy: 0.5430044531822205
        entropy_coeff: 0.0017600000137463212
        kl: 0.00906457006931305
        model: {}
        policy_loss: -0.018682612106204033
        total_loss: -0.016630997881293297
        vf_explained_var: 0.0855066329240799
        vf_loss: 28.3734073638916
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004872384015470743
        entropy: 0.5581268072128296
        entropy_coeff: 0.0017600000137463212
        kl: 0.00965962279587984
        model: {}
        policy_loss: -0.0219751987606287
        total_loss: -0.018599918112158775
        vf_explained_var: 0.07529325783252716
        vf_loss: 38.74602127075195
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004872384015470743
        entropy: 0.8941434621810913
        entropy_coeff: 0.0017600000137463212
        kl: 0.012065221555531025
        model: {}
        policy_loss: -0.022056816145777702
        total_loss: -0.018911149352788925
        vf_explained_var: -0.029287099838256836
        vf_loss: 42.669105529785156
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004872384015470743
        entropy: 0.8517175316810608
        entropy_coeff: 0.0017600000137463212
        kl: 0.011132211424410343
        model: {}
        policy_loss: -0.021114032715559006
        total_loss: -0.019561849534511566
        vf_explained_var: 0.040326446294784546
        vf_loss: 29.52145004272461
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004872384015470743
        entropy: 0.4901082515716553
        entropy_coeff: 0.0017600000137463212
        kl: 0.009851297363638878
        model: {}
        policy_loss: -0.020668787881731987
        total_loss: -0.01862456277012825
        vf_explained_var: 0.1748306006193161
        vf_loss: 25.3739070892334
    load_time_ms: 13277.037
    num_steps_sampled: 12480000
    num_steps_trained: 12480000
    sample_time_ms: 106788.115
    update_time_ms: 17.122
  iterations_since_restore: 130
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.413227513227515
    ram_util_percent: 11.599470899470903
  pid: 28570
  policy_reward_max:
    agent-0: 208.0
    agent-1: 208.0
    agent-2: 236.0
    agent-3: 236.0
    agent-4: 211.0
    agent-5: 211.0
  policy_reward_mean:
    agent-0: 146.63
    agent-1: 146.63
    agent-2: 176.48
    agent-3: 176.48
    agent-4: 144.515
    agent-5: 144.515
  policy_reward_min:
    agent-0: 61.0
    agent-1: 61.0
    agent-2: 83.0
    agent-3: 83.0
    agent-4: 60.5
    agent-5: 60.5
  sampler_perf:
    mean_env_wait_ms: 27.30571879591069
    mean_inference_ms: 13.091782935487783
    mean_processing_ms: 58.64698734591514
  time_since_restore: 17209.918695926666
  time_this_iter_s: 133.53300094604492
  time_total_s: 17209.918695926666
  timestamp: 1637525748
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 12480000
  training_iteration: 130
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    130 |          17209.9 | 12480000 |   935.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 0.29
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 25.07
    apples_agent-1_min: 11
    apples_agent-2_max: 329
    apples_agent-2_mean: 243.97
    apples_agent-2_min: 123
    apples_agent-3_max: 216
    apples_agent-3_mean: 35.55
    apples_agent-3_min: 1
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 354
    apples_agent-5_mean: 252.57
    apples_agent-5_min: 112
    cleaning_beam_agent-0_max: 690
    cleaning_beam_agent-0_mean: 580.96
    cleaning_beam_agent-0_min: 395
    cleaning_beam_agent-1_max: 170
    cleaning_beam_agent-1_mean: 24.62
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 68
    cleaning_beam_agent-2_mean: 26.22
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 360
    cleaning_beam_agent-3_mean: 210.09
    cleaning_beam_agent-3_min: 83
    cleaning_beam_agent-4_max: 819
    cleaning_beam_agent-4_mean: 576.02
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 59.76
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.15
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-18-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1205.0
  episode_reward_mean: 945.01
  episode_reward_min: 543.0
  episodes_this_iter: 96
  episodes_total: 12576
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12651.33
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004812479892279953
        entropy: 0.4554535746574402
        entropy_coeff: 0.0017600000137463212
        kl: 0.007051854860037565
        model: {}
        policy_loss: -0.01205473579466343
        total_loss: -0.0097409188747406
        vf_explained_var: 0.039786532521247864
        vf_loss: 29.391149520874023
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004812479892279953
        entropy: 0.5404794812202454
        entropy_coeff: 0.0017600000137463212
        kl: 0.00898420624434948
        model: {}
        policy_loss: -0.01924658939242363
        total_loss: -0.017280951142311096
        vf_explained_var: 0.10714252293109894
        vf_loss: 27.48428726196289
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004812479892279953
        entropy: 0.5582813024520874
        entropy_coeff: 0.0017600000137463212
        kl: 0.01039158646017313
        model: {}
        policy_loss: -0.020620327442884445
        total_loss: -0.01776985079050064
        vf_explained_var: 0.0720166563987732
        vf_loss: 33.13473129272461
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004812479892279953
        entropy: 0.9349950551986694
        entropy_coeff: 0.0017600000137463212
        kl: 0.012309323996305466
        model: {}
        policy_loss: -0.02209937945008278
        total_loss: -0.019710954278707504
        vf_explained_var: -0.006834179162979126
        vf_loss: 35.724159240722656
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004812479892279953
        entropy: 0.8554301261901855
        entropy_coeff: 0.0017600000137463212
        kl: 0.01177481934428215
        model: {}
        policy_loss: -0.020827043801546097
        total_loss: -0.019349580630660057
        vf_explained_var: 0.04915912449359894
        vf_loss: 28.782352447509766
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004812479892279953
        entropy: 0.4699040651321411
        entropy_coeff: 0.0017600000137463212
        kl: 0.009769636206328869
        model: {}
        policy_loss: -0.018864836543798447
        total_loss: -0.0168867576867342
        vf_explained_var: 0.18940098583698273
        vf_loss: 24.387470245361328
    load_time_ms: 13284.35
    num_steps_sampled: 12576000
    num_steps_trained: 12576000
    sample_time_ms: 106882.594
    update_time_ms: 17.173
  iterations_since_restore: 131
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.23560209424084
    ram_util_percent: 11.600000000000005
  pid: 28570
  policy_reward_max:
    agent-0: 198.0
    agent-1: 198.0
    agent-2: 233.0
    agent-3: 233.0
    agent-4: 206.5
    agent-5: 206.5
  policy_reward_mean:
    agent-0: 148.44
    agent-1: 148.44
    agent-2: 178.45
    agent-3: 178.45
    agent-4: 145.615
    agent-5: 145.615
  policy_reward_min:
    agent-0: 95.0
    agent-1: 95.0
    agent-2: 94.5
    agent-3: 94.5
    agent-4: 77.5
    agent-5: 77.5
  sampler_perf:
    mean_env_wait_ms: 27.317557621080446
    mean_inference_ms: 13.090896379282162
    mean_processing_ms: 58.64528953373542
  time_since_restore: 17343.177134752274
  time_this_iter_s: 133.2584388256073
  time_total_s: 17343.177134752274
  timestamp: 1637525881
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 12576000
  training_iteration: 131
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    131 |          17343.2 | 12576000 |   945.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.13
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 25.04
    apples_agent-1_min: 8
    apples_agent-2_max: 308
    apples_agent-2_mean: 243.54
    apples_agent-2_min: 166
    apples_agent-3_max: 94
    apples_agent-3_mean: 35.63
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 346
    apples_agent-5_mean: 249.94
    apples_agent-5_min: 159
    cleaning_beam_agent-0_max: 786
    cleaning_beam_agent-0_mean: 579.14
    cleaning_beam_agent-0_min: 427
    cleaning_beam_agent-1_max: 170
    cleaning_beam_agent-1_mean: 16.81
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 27.29
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 328
    cleaning_beam_agent-3_mean: 196.68
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 776
    cleaning_beam_agent-4_mean: 570.41
    cleaning_beam_agent-4_min: 211
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 61.72
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.06
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.18
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.22
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-20-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1180.0
  episode_reward_mean: 949.33
  episode_reward_min: 646.0
  episodes_this_iter: 96
  episodes_total: 12672
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12658.082
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004752576060127467
        entropy: 0.4690302908420563
        entropy_coeff: 0.0017600000137463212
        kl: 0.007480466738343239
        model: {}
        policy_loss: -0.012671999633312225
        total_loss: -0.010530447587370872
        vf_explained_var: 0.05279587209224701
        vf_loss: 27.80031967163086
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004752576060127467
        entropy: 0.5444446802139282
        entropy_coeff: 0.0017600000137463212
        kl: 0.010496506467461586
        model: {}
        policy_loss: -0.01939515396952629
        total_loss: -0.017413394525647163
        vf_explained_var: 0.0654386579990387
        vf_loss: 27.431745529174805
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004752576060127467
        entropy: 0.5446087121963501
        entropy_coeff: 0.0017600000137463212
        kl: 0.01025359332561493
        model: {}
        policy_loss: -0.019545234739780426
        total_loss: -0.01629192754626274
        vf_explained_var: 0.09104114770889282
        vf_loss: 36.99140930175781
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004752576060127467
        entropy: 0.9267280101776123
        entropy_coeff: 0.0017600000137463212
        kl: 0.011612243950366974
        model: {}
        policy_loss: -0.022981371730566025
        total_loss: -0.020114202052354813
        vf_explained_var: 0.00629781186580658
        vf_loss: 40.62751770019531
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004752576060127467
        entropy: 0.8648813962936401
        entropy_coeff: 0.0017600000137463212
        kl: 0.01049668900668621
        model: {}
        policy_loss: -0.01980915479362011
        total_loss: -0.018436521291732788
        vf_explained_var: 0.0281171053647995
        vf_loss: 28.014198303222656
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004752576060127467
        entropy: 0.4983484447002411
        entropy_coeff: 0.0017600000137463212
        kl: 0.00972900353372097
        model: {}
        policy_loss: -0.019890356808900833
        total_loss: -0.018030736595392227
        vf_explained_var: 0.18088188767433167
        vf_loss: 23.71875
    load_time_ms: 13280.23
    num_steps_sampled: 12672000
    num_steps_trained: 12672000
    sample_time_ms: 106892.307
    update_time_ms: 17.266
  iterations_since_restore: 132
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.27052631578947
    ram_util_percent: 11.606315789473689
  pid: 28570
  policy_reward_max:
    agent-0: 201.0
    agent-1: 201.0
    agent-2: 240.5
    agent-3: 240.5
    agent-4: 196.0
    agent-5: 196.0
  policy_reward_mean:
    agent-0: 147.795
    agent-1: 147.795
    agent-2: 180.98
    agent-3: 180.98
    agent-4: 145.89
    agent-5: 145.89
  policy_reward_min:
    agent-0: 107.0
    agent-1: 107.0
    agent-2: 113.0
    agent-3: 113.0
    agent-4: 103.0
    agent-5: 103.0
  sampler_perf:
    mean_env_wait_ms: 27.328243466402945
    mean_inference_ms: 13.090215407934448
    mean_processing_ms: 58.64252200778992
  time_since_restore: 17476.722725868225
  time_this_iter_s: 133.54559111595154
  time_total_s: 17476.722725868225
  timestamp: 1637526015
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 12672000
  training_iteration: 132
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    132 |          17476.7 | 12672000 |   949.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.1
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 24.7
    apples_agent-1_min: 10
    apples_agent-2_max: 325
    apples_agent-2_mean: 250.09
    apples_agent-2_min: 117
    apples_agent-3_max: 177
    apples_agent-3_mean: 35.94
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 370
    apples_agent-5_mean: 254.62
    apples_agent-5_min: 138
    cleaning_beam_agent-0_max: 701
    cleaning_beam_agent-0_mean: 569.31
    cleaning_beam_agent-0_min: 357
    cleaning_beam_agent-1_max: 184
    cleaning_beam_agent-1_mean: 17.46
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 27.62
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 320
    cleaning_beam_agent-3_mean: 217.44
    cleaning_beam_agent-3_min: 65
    cleaning_beam_agent-4_max: 754
    cleaning_beam_agent-4_mean: 545.41
    cleaning_beam_agent-4_min: 303
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 62.96
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.23
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.26
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-22-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1219.0
  episode_reward_mean: 969.67
  episode_reward_min: 546.0
  episodes_this_iter: 96
  episodes_total: 12768
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12675.175
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00046926719369366765
        entropy: 0.4674370586872101
        entropy_coeff: 0.0017600000137463212
        kl: 0.00722230039536953
        model: {}
        policy_loss: -0.01267101801931858
        total_loss: -0.010245509445667267
        vf_explained_var: 0.0870947390794754
        vf_loss: 30.676410675048828
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00046926719369366765
        entropy: 0.5491334795951843
        entropy_coeff: 0.0017600000137463212
        kl: 0.009691477753221989
        model: {}
        policy_loss: -0.020014705136418343
        total_loss: -0.017834240570664406
        vf_explained_var: 0.1210319995880127
        vf_loss: 29.65223503112793
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00046926719369366765
        entropy: 0.541257381439209
        entropy_coeff: 0.0017600000137463212
        kl: 0.009770480915904045
        model: {}
        policy_loss: -0.020890846848487854
        total_loss: -0.017555415630340576
        vf_explained_var: 0.14047758281230927
        vf_loss: 37.995201110839844
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00046926719369366765
        entropy: 0.923936128616333
        entropy_coeff: 0.0017600000137463212
        kl: 0.012502603232860565
        model: {}
        policy_loss: -0.022615298628807068
        total_loss: -0.019348695874214172
        vf_explained_var: -0.00394013524055481
        vf_loss: 44.238773345947266
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00046926719369366765
        entropy: 0.8977473974227905
        entropy_coeff: 0.0017600000137463212
        kl: 0.011685676872730255
        model: {}
        policy_loss: -0.020084327086806297
        total_loss: -0.01847291737794876
        vf_explained_var: 0.0287628173828125
        vf_loss: 30.87454605102539
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00046926719369366765
        entropy: 0.49149957299232483
        entropy_coeff: 0.0017600000137463212
        kl: 0.009783083572983742
        model: {}
        policy_loss: -0.020291678607463837
        total_loss: -0.01820381172001362
        vf_explained_var: 0.189847931265831
        vf_loss: 25.86040496826172
    load_time_ms: 13279.155
    num_steps_sampled: 12768000
    num_steps_trained: 12768000
    sample_time_ms: 106835.267
    update_time_ms: 17.437
  iterations_since_restore: 133
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.415957446808516
    ram_util_percent: 11.588829787234044
  pid: 28570
  policy_reward_max:
    agent-0: 196.5
    agent-1: 196.5
    agent-2: 235.5
    agent-3: 235.5
    agent-4: 206.0
    agent-5: 206.0
  policy_reward_mean:
    agent-0: 149.805
    agent-1: 149.805
    agent-2: 185.635
    agent-3: 185.635
    agent-4: 149.395
    agent-5: 149.395
  policy_reward_min:
    agent-0: 81.5
    agent-1: 81.5
    agent-2: 108.0
    agent-3: 108.0
    agent-4: 83.5
    agent-5: 83.5
  sampler_perf:
    mean_env_wait_ms: 27.33695783342919
    mean_inference_ms: 13.089604464865465
    mean_processing_ms: 58.63992689293186
  time_since_restore: 17609.070065259933
  time_this_iter_s: 132.34733939170837
  time_total_s: 17609.070065259933
  timestamp: 1637526147
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 12768000
  training_iteration: 133
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    133 |          17609.1 | 12768000 |   969.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.17
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 25.96
    apples_agent-1_min: 10
    apples_agent-2_max: 333
    apples_agent-2_mean: 250.32
    apples_agent-2_min: 135
    apples_agent-3_max: 106
    apples_agent-3_mean: 32.72
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 2.02
    apples_agent-4_min: 0
    apples_agent-5_max: 358
    apples_agent-5_mean: 249.75
    apples_agent-5_min: 153
    cleaning_beam_agent-0_max: 664
    cleaning_beam_agent-0_mean: 560.83
    cleaning_beam_agent-0_min: 395
    cleaning_beam_agent-1_max: 195
    cleaning_beam_agent-1_mean: 18.44
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 26.2
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 308
    cleaning_beam_agent-3_mean: 203.61
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 790
    cleaning_beam_agent-4_mean: 541.9
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 67.96
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.31
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.25
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-24-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1195.0
  episode_reward_mean: 968.1
  episode_reward_min: 546.0
  episodes_this_iter: 96
  episodes_total: 12864
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12673.051
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00046327681047841907
        entropy: 0.4708276391029358
        entropy_coeff: 0.0017600000137463212
        kl: 0.007743228226900101
        model: {}
        policy_loss: -0.012092710472643375
        total_loss: -0.00965900532901287
        vf_explained_var: 0.037914350628852844
        vf_loss: 30.68779182434082
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00046327681047841907
        entropy: 0.5386732816696167
        entropy_coeff: 0.0017600000137463212
        kl: 0.010979734361171722
        model: {}
        policy_loss: -0.019185684621334076
        total_loss: -0.016935260966420174
        vf_explained_var: 0.06890152394771576
        vf_loss: 29.92621612548828
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00046327681047841907
        entropy: 0.5427289605140686
        entropy_coeff: 0.0017600000137463212
        kl: 0.009088589809834957
        model: {}
        policy_loss: -0.020882338285446167
        total_loss: -0.01769690215587616
        vf_explained_var: 0.07737143337726593
        vf_loss: 36.86206817626953
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00046327681047841907
        entropy: 0.915002703666687
        entropy_coeff: 0.0017600000137463212
        kl: 0.01240182388573885
        model: {}
        policy_loss: -0.022522155195474625
        total_loss: -0.019519422203302383
        vf_explained_var: -0.035037606954574585
        vf_loss: 41.480690002441406
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00046327681047841907
        entropy: 0.8828283548355103
        entropy_coeff: 0.0017600000137463212
        kl: 0.010724355466663837
        model: {}
        policy_loss: -0.020211927592754364
        total_loss: -0.018611226230859756
        vf_explained_var: 0.0023713111877441406
        vf_loss: 30.590431213378906
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00046327681047841907
        entropy: 0.4970022439956665
        entropy_coeff: 0.0017600000137463212
        kl: 0.00996272824704647
        model: {}
        policy_loss: -0.02058921381831169
        total_loss: -0.018435170873999596
        vf_explained_var: 0.13324490189552307
        vf_loss: 26.551637649536133
    load_time_ms: 13276.943
    num_steps_sampled: 12864000
    num_steps_trained: 12864000
    sample_time_ms: 106971.405
    update_time_ms: 17.659
  iterations_since_restore: 134
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.29736842105263
    ram_util_percent: 11.607368421052634
  pid: 28570
  policy_reward_max:
    agent-0: 202.5
    agent-1: 202.5
    agent-2: 231.0
    agent-3: 231.0
    agent-4: 192.5
    agent-5: 192.5
  policy_reward_mean:
    agent-0: 150.555
    agent-1: 150.555
    agent-2: 186.505
    agent-3: 186.505
    agent-4: 146.99
    agent-5: 146.99
  policy_reward_min:
    agent-0: 77.5
    agent-1: 77.5
    agent-2: 92.5
    agent-3: 92.5
    agent-4: 81.0
    agent-5: 81.0
  sampler_perf:
    mean_env_wait_ms: 27.346624299066164
    mean_inference_ms: 13.089190756728783
    mean_processing_ms: 58.63929359491459
  time_since_restore: 17742.923622846603
  time_this_iter_s: 133.85355758666992
  time_total_s: 17742.923622846603
  timestamp: 1637526281
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 12864000
  training_iteration: 134
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    134 |          17742.9 | 12864000 |    968.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.23
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 24.95
    apples_agent-1_min: 12
    apples_agent-2_max: 346
    apples_agent-2_mean: 250.73
    apples_agent-2_min: 155
    apples_agent-3_max: 117
    apples_agent-3_mean: 36.55
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 344
    apples_agent-5_mean: 255.75
    apples_agent-5_min: 158
    cleaning_beam_agent-0_max: 697
    cleaning_beam_agent-0_mean: 579.1
    cleaning_beam_agent-0_min: 365
    cleaning_beam_agent-1_max: 195
    cleaning_beam_agent-1_mean: 15.0
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 22.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 339
    cleaning_beam_agent-3_mean: 204.37
    cleaning_beam_agent-3_min: 89
    cleaning_beam_agent-4_max: 736
    cleaning_beam_agent-4_mean: 512.12
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 192
    cleaning_beam_agent-5_mean: 61.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.19
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-26-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1235.0
  episode_reward_mean: 985.29
  episode_reward_min: 646.0
  episodes_this_iter: 96
  episodes_total: 12960
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12662.847
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00045728639815934
        entropy: 0.4671071171760559
        entropy_coeff: 0.0017600000137463212
        kl: 0.007501309737563133
        model: {}
        policy_loss: -0.0120330099016428
        total_loss: -0.00980465393513441
        vf_explained_var: 0.05363595485687256
        vf_loss: 28.629343032836914
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00045728639815934
        entropy: 0.5265158414840698
        entropy_coeff: 0.0017600000137463212
        kl: 0.009435072541236877
        model: {}
        policy_loss: -0.018724173307418823
        total_loss: -0.016600364819169044
        vf_explained_var: 0.053459182381629944
        vf_loss: 28.735713958740234
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00045728639815934
        entropy: 0.5362370610237122
        entropy_coeff: 0.0017600000137463212
        kl: 0.009689504280686378
        model: {}
        policy_loss: -0.020028669387102127
        total_loss: -0.016931215301156044
        vf_explained_var: 0.10458265244960785
        vf_loss: 35.56755065917969
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00045728639815934
        entropy: 0.9060755968093872
        entropy_coeff: 0.0017600000137463212
        kl: 0.011677577160298824
        model: {}
        policy_loss: -0.022346263751387596
        total_loss: -0.019400162622332573
        vf_explained_var: -0.05280959606170654
        vf_loss: 41.02886962890625
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00045728639815934
        entropy: 0.937487781047821
        entropy_coeff: 0.0017600000137463212
        kl: 0.010397481732070446
        model: {}
        policy_loss: -0.019688492640852928
        total_loss: -0.018201587721705437
        vf_explained_var: -0.006009802222251892
        vf_loss: 30.443561553955078
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00045728639815934
        entropy: 0.49325624108314514
        entropy_coeff: 0.0017600000137463212
        kl: 0.010262897238135338
        model: {}
        policy_loss: -0.02020151913166046
        total_loss: -0.01816728711128235
        vf_explained_var: 0.17382210493087769
        vf_loss: 25.175064086914062
    load_time_ms: 13276.218
    num_steps_sampled: 12960000
    num_steps_trained: 12960000
    sample_time_ms: 106888.177
    update_time_ms: 16.942
  iterations_since_restore: 135
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.46276595744681
    ram_util_percent: 11.60053191489362
  pid: 28570
  policy_reward_max:
    agent-0: 208.5
    agent-1: 208.5
    agent-2: 250.5
    agent-3: 250.5
    agent-4: 196.0
    agent-5: 196.0
  policy_reward_mean:
    agent-0: 154.7
    agent-1: 154.7
    agent-2: 188.475
    agent-3: 188.475
    agent-4: 149.47
    agent-5: 149.47
  policy_reward_min:
    agent-0: 94.0
    agent-1: 94.0
    agent-2: 116.5
    agent-3: 116.5
    agent-4: 86.5
    agent-5: 86.5
  sampler_perf:
    mean_env_wait_ms: 27.354805219729336
    mean_inference_ms: 13.088366033409923
    mean_processing_ms: 58.636511204556975
  time_since_restore: 17874.670142412186
  time_this_iter_s: 131.74651956558228
  time_total_s: 17874.670142412186
  timestamp: 1637526413
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 12960000
  training_iteration: 135
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    135 |          17874.7 | 12960000 |   985.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.3
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 24.92
    apples_agent-1_min: 12
    apples_agent-2_max: 323
    apples_agent-2_mean: 251.7
    apples_agent-2_min: 97
    apples_agent-3_max: 115
    apples_agent-3_mean: 36.76
    apples_agent-3_min: 4
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.35
    apples_agent-4_min: 0
    apples_agent-5_max: 362
    apples_agent-5_mean: 261.11
    apples_agent-5_min: 102
    cleaning_beam_agent-0_max: 693
    cleaning_beam_agent-0_mean: 580.86
    cleaning_beam_agent-0_min: 259
    cleaning_beam_agent-1_max: 131
    cleaning_beam_agent-1_mean: 14.54
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 186
    cleaning_beam_agent-2_mean: 22.4
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 344
    cleaning_beam_agent-3_mean: 213.07
    cleaning_beam_agent-3_min: 83
    cleaning_beam_agent-4_max: 708
    cleaning_beam_agent-4_mean: 504.8
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 185
    cleaning_beam_agent-5_mean: 58.12
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.13
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-29-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1269.0
  episode_reward_mean: 998.9
  episode_reward_min: 463.0
  episodes_this_iter: 96
  episodes_total: 13056
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12663.824
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000451295985840261
        entropy: 0.4501614272594452
        entropy_coeff: 0.0017600000137463212
        kl: 0.007509240880608559
        model: {}
        policy_loss: -0.012458600103855133
        total_loss: -0.010239209979772568
        vf_explained_var: 0.08294624090194702
        vf_loss: 28.23939323425293
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.000451295985840261
        entropy: 0.5161941051483154
        entropy_coeff: 0.0017600000137463212
        kl: 0.009990711696445942
        model: {}
        policy_loss: -0.018886368721723557
        total_loss: -0.016782909631729126
        vf_explained_var: 0.08957208693027496
        vf_loss: 28.246334075927734
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000451295985840261
        entropy: 0.5410175323486328
        entropy_coeff: 0.0017600000137463212
        kl: 0.010115088894963264
        model: {}
        policy_loss: -0.020948108285665512
        total_loss: -0.01773550547659397
        vf_explained_var: 0.11387600004673004
        vf_loss: 36.590423583984375
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000451295985840261
        entropy: 0.9183855652809143
        entropy_coeff: 0.0017600000137463212
        kl: 0.011945907026529312
        model: {}
        policy_loss: -0.022188954055309296
        total_loss: -0.019146908074617386
        vf_explained_var: -0.03114721179008484
        vf_loss: 42.10429382324219
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.000451295985840261
        entropy: 0.9088606834411621
        entropy_coeff: 0.0017600000137463212
        kl: 0.012145881541073322
        model: {}
        policy_loss: -0.01755010336637497
        total_loss: -0.016217589378356934
        vf_explained_var: 0.023141667246818542
        vf_loss: 28.240253448486328
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.000451295985840261
        entropy: 0.5005444288253784
        entropy_coeff: 0.0017600000137463212
        kl: 0.009395473636686802
        model: {}
        policy_loss: -0.01993367448449135
        total_loss: -0.017995648086071014
        vf_explained_var: 0.1527893990278244
        vf_loss: 24.66654396057129
    load_time_ms: 13281.994
    num_steps_sampled: 13056000
    num_steps_trained: 13056000
    sample_time_ms: 106842.219
    update_time_ms: 16.328
  iterations_since_restore: 136
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.327127659574472
    ram_util_percent: 11.528191489361703
  pid: 28570
  policy_reward_max:
    agent-0: 205.5
    agent-1: 205.5
    agent-2: 248.0
    agent-3: 248.0
    agent-4: 203.5
    agent-5: 203.5
  policy_reward_mean:
    agent-0: 157.315
    agent-1: 157.315
    agent-2: 189.8
    agent-3: 189.8
    agent-4: 152.335
    agent-5: 152.335
  policy_reward_min:
    agent-0: 81.5
    agent-1: 81.5
    agent-2: 72.5
    agent-3: 72.5
    agent-4: 66.0
    agent-5: 66.0
  sampler_perf:
    mean_env_wait_ms: 27.36229164891077
    mean_inference_ms: 13.087312487246276
    mean_processing_ms: 58.6322539318477
  time_since_restore: 18007.245933532715
  time_this_iter_s: 132.57579112052917
  time_total_s: 18007.245933532715
  timestamp: 1637526546
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 13056000
  training_iteration: 136
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    136 |          18007.2 | 13056000 |    998.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 25.17
    apples_agent-1_min: 11
    apples_agent-2_max: 369
    apples_agent-2_mean: 251.72
    apples_agent-2_min: 0
    apples_agent-3_max: 101
    apples_agent-3_mean: 30.86
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 252.43
    apples_agent-5_min: 138
    cleaning_beam_agent-0_max: 679
    cleaning_beam_agent-0_mean: 583.14
    cleaning_beam_agent-0_min: 393
    cleaning_beam_agent-1_max: 77
    cleaning_beam_agent-1_mean: 14.32
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 194
    cleaning_beam_agent-2_mean: 22.12
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 302
    cleaning_beam_agent-3_mean: 201.82
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 759
    cleaning_beam_agent-4_mean: 498.12
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 63.62
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.08
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.29
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.23
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-31-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1274.0
  episode_reward_mean: 987.04
  episode_reward_min: 572.0
  episodes_this_iter: 96
  episodes_total: 13152
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12656.51
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004453056026250124
        entropy: 0.4474055767059326
        entropy_coeff: 0.0017600000137463212
        kl: 0.006510438397526741
        model: {}
        policy_loss: -0.012036065571010113
        total_loss: -0.009539332240819931
        vf_explained_var: 0.05767601728439331
        vf_loss: 31.21405029296875
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004453056026250124
        entropy: 0.5074746012687683
        entropy_coeff: 0.0017600000137463212
        kl: 0.009028238244354725
        model: {}
        policy_loss: -0.01776151731610298
        total_loss: -0.01535782590508461
        vf_explained_var: 0.06427149474620819
        vf_loss: 31.27567481994629
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004453056026250124
        entropy: 0.5457177758216858
        entropy_coeff: 0.0017600000137463212
        kl: 0.009502975270152092
        model: {}
        policy_loss: -0.020225297659635544
        total_loss: -0.01727711595594883
        vf_explained_var: 0.1499680131673813
        vf_loss: 34.334999084472656
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004453056026250124
        entropy: 0.9295642971992493
        entropy_coeff: 0.0017600000137463212
        kl: 0.012466922402381897
        model: {}
        policy_loss: -0.021023640409111977
        total_loss: -0.018031582236289978
        vf_explained_var: -0.033935219049453735
        vf_loss: 41.605838775634766
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004453056026250124
        entropy: 0.9582139849662781
        entropy_coeff: 0.0017600000137463212
        kl: 0.010479509830474854
        model: {}
        policy_loss: -0.01917601004242897
        total_loss: -0.017665179446339607
        vf_explained_var: 0.019704625010490417
        vf_loss: 31.040319442749023
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004453056026250124
        entropy: 0.534771203994751
        entropy_coeff: 0.0017600000137463212
        kl: 0.009658192284405231
        model: {}
        policy_loss: -0.020011987537145615
        total_loss: -0.017928270623087883
        vf_explained_var: 0.15974344313144684
        vf_loss: 26.627338409423828
    load_time_ms: 13274.851
    num_steps_sampled: 13152000
    num_steps_trained: 13152000
    sample_time_ms: 106828.694
    update_time_ms: 16.082
  iterations_since_restore: 137
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.345502645502652
    ram_util_percent: 11.603174603174606
  pid: 28570
  policy_reward_max:
    agent-0: 255.5
    agent-1: 255.5
    agent-2: 253.5
    agent-3: 253.5
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 159.38
    agent-1: 159.38
    agent-2: 186.83
    agent-3: 186.83
    agent-4: 147.31
    agent-5: 147.31
  policy_reward_min:
    agent-0: 86.0
    agent-1: 86.0
    agent-2: 16.5
    agent-3: 16.5
    agent-4: 78.0
    agent-5: 78.0
  sampler_perf:
    mean_env_wait_ms: 27.370489349072756
    mean_inference_ms: 13.086501334831352
    mean_processing_ms: 58.63119896187416
  time_since_restore: 18140.009752750397
  time_this_iter_s: 132.76381921768188
  time_total_s: 18140.009752750397
  timestamp: 1637526679
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 13152000
  training_iteration: 137
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    137 |            18140 | 13152000 |   987.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.2
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 24.42
    apples_agent-1_min: 12
    apples_agent-2_max: 369
    apples_agent-2_mean: 251.57
    apples_agent-2_min: 150
    apples_agent-3_max: 98
    apples_agent-3_mean: 34.31
    apples_agent-3_min: 4
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 361
    apples_agent-5_mean: 259.6
    apples_agent-5_min: 157
    cleaning_beam_agent-0_max: 701
    cleaning_beam_agent-0_mean: 596.84
    cleaning_beam_agent-0_min: 383
    cleaning_beam_agent-1_max: 88
    cleaning_beam_agent-1_mean: 15.37
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 21.55
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 319
    cleaning_beam_agent-3_mean: 205.77
    cleaning_beam_agent-3_min: 96
    cleaning_beam_agent-4_max: 742
    cleaning_beam_agent-4_mean: 513.94
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 58.99
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.23
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.18
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-33-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1280.0
  episode_reward_mean: 1013.94
  episode_reward_min: 702.0
  episodes_this_iter: 96
  episodes_total: 13248
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12652.804
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00043931519030593336
        entropy: 0.4321751296520233
        entropy_coeff: 0.0017600000137463212
        kl: 0.006826122757047415
        model: {}
        policy_loss: -0.011399492621421814
        total_loss: -0.009084205143153667
        vf_explained_var: 0.06953269243240356
        vf_loss: 29.0526123046875
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00043931519030593336
        entropy: 0.49733930826187134
        entropy_coeff: 0.0017600000137463212
        kl: 0.009452185593545437
        model: {}
        policy_loss: -0.016711711883544922
        total_loss: -0.014542430639266968
        vf_explained_var: 0.09042754769325256
        vf_loss: 28.673681259155273
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00043931519030593336
        entropy: 0.5515823364257812
        entropy_coeff: 0.0017600000137463212
        kl: 0.009293504059314728
        model: {}
        policy_loss: -0.020733358338475227
        total_loss: -0.017779186367988586
        vf_explained_var: 0.13837690651416779
        vf_loss: 34.60281753540039
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00043931519030593336
        entropy: 0.9267828464508057
        entropy_coeff: 0.0017600000137463212
        kl: 0.01236700639128685
        model: {}
        policy_loss: -0.022435227409005165
        total_loss: -0.01942078210413456
        vf_explained_var: -0.03179159760475159
        vf_loss: 41.81821823120117
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00043931519030593336
        entropy: 0.9389733076095581
        entropy_coeff: 0.0017600000137463212
        kl: 0.011269616894423962
        model: {}
        policy_loss: -0.019252412021160126
        total_loss: -0.017823444679379463
        vf_explained_var: 0.0070188045501708984
        vf_loss: 29.81273651123047
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00043931519030593336
        entropy: 0.5352384448051453
        entropy_coeff: 0.0017600000137463212
        kl: 0.00989445112645626
        model: {}
        policy_loss: -0.019997531548142433
        total_loss: -0.01799585111439228
        vf_explained_var: 0.14893202483654022
        vf_loss: 25.726634979248047
    load_time_ms: 13264.055
    num_steps_sampled: 13248000
    num_steps_trained: 13248000
    sample_time_ms: 106823.465
    update_time_ms: 15.742
  iterations_since_restore: 138
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.331216931216932
    ram_util_percent: 11.667724867724871
  pid: 28570
  policy_reward_max:
    agent-0: 208.0
    agent-1: 208.0
    agent-2: 253.5
    agent-3: 253.5
    agent-4: 205.0
    agent-5: 205.0
  policy_reward_mean:
    agent-0: 165.34
    agent-1: 165.34
    agent-2: 189.995
    agent-3: 189.995
    agent-4: 151.635
    agent-5: 151.635
  policy_reward_min:
    agent-0: 99.0
    agent-1: 99.0
    agent-2: 141.5
    agent-3: 141.5
    agent-4: 76.5
    agent-5: 76.5
  sampler_perf:
    mean_env_wait_ms: 27.37816915483495
    mean_inference_ms: 13.085659465485326
    mean_processing_ms: 58.630284754447075
  time_since_restore: 18272.965131521225
  time_this_iter_s: 132.95537877082825
  time_total_s: 18272.965131521225
  timestamp: 1637526812
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 13248000
  training_iteration: 138
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    138 |            18273 | 13248000 |  1013.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 152
    apples_agent-1_mean: 26.85
    apples_agent-1_min: 12
    apples_agent-2_max: 343
    apples_agent-2_mean: 259.22
    apples_agent-2_min: 154
    apples_agent-3_max: 123
    apples_agent-3_mean: 37.4
    apples_agent-3_min: 5
    apples_agent-4_max: 37
    apples_agent-4_mean: 2.03
    apples_agent-4_min: 0
    apples_agent-5_max: 361
    apples_agent-5_mean: 261.59
    apples_agent-5_min: 182
    cleaning_beam_agent-0_max: 690
    cleaning_beam_agent-0_mean: 595.38
    cleaning_beam_agent-0_min: 476
    cleaning_beam_agent-1_max: 130
    cleaning_beam_agent-1_mean: 17.51
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 138
    cleaning_beam_agent-2_mean: 25.26
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 298
    cleaning_beam_agent-3_mean: 203.83
    cleaning_beam_agent-3_min: 56
    cleaning_beam_agent-4_max: 786
    cleaning_beam_agent-4_mean: 528.38
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 151
    cleaning_beam_agent-5_mean: 61.01
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.39
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.21
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-35-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1280.0
  episode_reward_mean: 1035.32
  episode_reward_min: 673.0
  episodes_this_iter: 96
  episodes_total: 13344
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12640.441
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00043332480709068477
        entropy: 0.41920679807662964
        entropy_coeff: 0.0017600000137463212
        kl: 0.006120649166405201
        model: {}
        policy_loss: -0.010115236043930054
        total_loss: -0.007724355906248093
        vf_explained_var: 0.03174304962158203
        vf_loss: 29.756681442260742
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00043332480709068477
        entropy: 0.5040605068206787
        entropy_coeff: 0.0017600000137463212
        kl: 0.008580053225159645
        model: {}
        policy_loss: -0.01730009913444519
        total_loss: -0.015051556751132011
        vf_explained_var: 0.04973839223384857
        vf_loss: 29.74814796447754
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00043332480709068477
        entropy: 0.5298508405685425
        entropy_coeff: 0.0017600000137463212
        kl: 0.008727312088012695
        model: {}
        policy_loss: -0.018838243559002876
        total_loss: -0.015708399936556816
        vf_explained_var: 0.09467291831970215
        vf_loss: 36.26013946533203
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00043332480709068477
        entropy: 0.9275738596916199
        entropy_coeff: 0.0017600000137463212
        kl: 0.011848239228129387
        model: {}
        policy_loss: -0.022723719477653503
        total_loss: -0.019660601392388344
        vf_explained_var: -0.04530459642410278
        vf_loss: 42.51338577270508
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00043332480709068477
        entropy: 0.9294838905334473
        entropy_coeff: 0.0017600000137463212
        kl: 0.011294633150100708
        model: {}
        policy_loss: -0.019120238721370697
        total_loss: -0.018057215958833694
        vf_explained_var: 0.019499272108078003
        vf_loss: 25.98406219482422
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00043332480709068477
        entropy: 0.559069037437439
        entropy_coeff: 0.0017600000137463212
        kl: 0.009730454534292221
        model: {}
        policy_loss: -0.019608620554208755
        total_loss: -0.0178360752761364
        vf_explained_var: 0.12002922594547272
        vf_loss: 23.916162490844727
    load_time_ms: 13265.868
    num_steps_sampled: 13344000
    num_steps_trained: 13344000
    sample_time_ms: 106975.702
    update_time_ms: 15.721
  iterations_since_restore: 139
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.276315789473685
    ram_util_percent: 11.522631578947369
  pid: 28570
  policy_reward_max:
    agent-0: 207.5
    agent-1: 207.5
    agent-2: 264.5
    agent-3: 264.5
    agent-4: 205.0
    agent-5: 205.0
  policy_reward_mean:
    agent-0: 167.945
    agent-1: 167.945
    agent-2: 195.61
    agent-3: 195.61
    agent-4: 154.105
    agent-5: 154.105
  policy_reward_min:
    agent-0: 94.0
    agent-1: 94.0
    agent-2: 124.0
    agent-3: 124.0
    agent-4: 109.5
    agent-5: 109.5
  sampler_perf:
    mean_env_wait_ms: 27.38791370913008
    mean_inference_ms: 13.084930297735282
    mean_processing_ms: 58.62950313231902
  time_since_restore: 18406.217950344086
  time_this_iter_s: 133.25281882286072
  time_total_s: 18406.217950344086
  timestamp: 1637526945
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 13344000
  training_iteration: 139
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    139 |          18406.2 | 13344000 |  1035.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.14
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 27.24
    apples_agent-1_min: 12
    apples_agent-2_max: 377
    apples_agent-2_mean: 259.73
    apples_agent-2_min: 161
    apples_agent-3_max: 150
    apples_agent-3_mean: 35.84
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.37
    apples_agent-4_min: 0
    apples_agent-5_max: 377
    apples_agent-5_mean: 260.74
    apples_agent-5_min: 133
    cleaning_beam_agent-0_max: 675
    cleaning_beam_agent-0_mean: 578.74
    cleaning_beam_agent-0_min: 426
    cleaning_beam_agent-1_max: 127
    cleaning_beam_agent-1_mean: 12.23
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 95
    cleaning_beam_agent-2_mean: 25.05
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 356
    cleaning_beam_agent-3_mean: 188.14
    cleaning_beam_agent-3_min: 84
    cleaning_beam_agent-4_max: 746
    cleaning_beam_agent-4_mean: 524.71
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 69.86
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.09
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.35
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.27
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-37-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1319.0
  episode_reward_mean: 1019.97
  episode_reward_min: 599.0
  episodes_this_iter: 96
  episodes_total: 13440
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12638.16
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00042733439477160573
        entropy: 0.43464964628219604
        entropy_coeff: 0.0017600000137463212
        kl: 0.006780095864087343
        model: {}
        policy_loss: -0.01268763281404972
        total_loss: -0.010522155091166496
        vf_explained_var: 0.0768546313047409
        vf_loss: 27.609567642211914
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00042733439477160573
        entropy: 0.49149835109710693
        entropy_coeff: 0.0017600000137463212
        kl: 0.008779003284871578
        model: {}
        policy_loss: -0.018268009647727013
        total_loss: -0.01620742678642273
        vf_explained_var: 0.08349879086017609
        vf_loss: 27.610122680664062
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00042733439477160573
        entropy: 0.521438717842102
        entropy_coeff: 0.0017600000137463212
        kl: 0.009136409498751163
        model: {}
        policy_loss: -0.019969098269939423
        total_loss: -0.016636401414871216
        vf_explained_var: 0.08120200037956238
        vf_loss: 37.93614959716797
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00042733439477160573
        entropy: 0.9229275584220886
        entropy_coeff: 0.0017600000137463212
        kl: 0.01129875611513853
        model: {}
        policy_loss: -0.02250298112630844
        total_loss: -0.01943298429250717
        vf_explained_var: -0.025380074977874756
        vf_loss: 42.706485748291016
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00042733439477160573
        entropy: 0.9337119460105896
        entropy_coeff: 0.0017600000137463212
        kl: 0.00996476225554943
        model: {}
        policy_loss: -0.018289564177393913
        total_loss: -0.017009466886520386
        vf_explained_var: 0.008692830801010132
        vf_loss: 28.347549438476562
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00042733439477160573
        entropy: 0.544439971446991
        entropy_coeff: 0.0017600000137463212
        kl: 0.009941663593053818
        model: {}
        policy_loss: -0.020588593557476997
        total_loss: -0.01865699701011181
        vf_explained_var: 0.12985944747924805
        vf_loss: 25.16996192932129
    load_time_ms: 13266.597
    num_steps_sampled: 13440000
    num_steps_trained: 13440000
    sample_time_ms: 106992.957
    update_time_ms: 15.777
  iterations_since_restore: 140
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.24263157894737
    ram_util_percent: 11.622631578947372
  pid: 28570
  policy_reward_max:
    agent-0: 215.0
    agent-1: 215.0
    agent-2: 248.0
    agent-3: 248.0
    agent-4: 222.0
    agent-5: 222.0
  policy_reward_mean:
    agent-0: 165.575
    agent-1: 165.575
    agent-2: 191.795
    agent-3: 191.795
    agent-4: 152.615
    agent-5: 152.615
  policy_reward_min:
    agent-0: 88.0
    agent-1: 88.0
    agent-2: 94.0
    agent-3: 94.0
    agent-4: 81.0
    agent-5: 81.0
  sampler_perf:
    mean_env_wait_ms: 27.396044372195423
    mean_inference_ms: 13.084343300712057
    mean_processing_ms: 58.628569995489244
  time_since_restore: 18539.903253555298
  time_this_iter_s: 133.68530321121216
  time_total_s: 18539.903253555298
  timestamp: 1637527079
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 13440000
  training_iteration: 140
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    140 |          18539.9 | 13440000 |  1019.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 26.6
    apples_agent-1_min: 10
    apples_agent-2_max: 369
    apples_agent-2_mean: 263.62
    apples_agent-2_min: 140
    apples_agent-3_max: 123
    apples_agent-3_mean: 38.7
    apples_agent-3_min: 2
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 373
    apples_agent-5_mean: 264.84
    apples_agent-5_min: 113
    cleaning_beam_agent-0_max: 663
    cleaning_beam_agent-0_mean: 555.02
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 102
    cleaning_beam_agent-1_mean: 11.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 29.35
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 320
    cleaning_beam_agent-3_mean: 194.48
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 720
    cleaning_beam_agent-4_mean: 554.68
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 221
    cleaning_beam_agent-5_mean: 66.28
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.13
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.28
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-40-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1365.0
  episode_reward_mean: 1036.22
  episode_reward_min: 519.0
  episodes_this_iter: 96
  episodes_total: 13536
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12629.924
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00042134401155635715
        entropy: 0.44958287477493286
        entropy_coeff: 0.0017600000137463212
        kl: 0.00705545861274004
        model: {}
        policy_loss: -0.012630597688257694
        total_loss: -0.010172025300562382
        vf_explained_var: 0.08886121213436127
        vf_loss: 30.73455047607422
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00042134401155635715
        entropy: 0.4711865782737732
        entropy_coeff: 0.0017600000137463212
        kl: 0.008228722028434277
        model: {}
        policy_loss: -0.01754099689424038
        total_loss: -0.015143654309213161
        vf_explained_var: 0.0914297103881836
        vf_loss: 30.72342872619629
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00042134401155635715
        entropy: 0.5425798892974854
        entropy_coeff: 0.0017600000137463212
        kl: 0.009136014617979527
        model: {}
        policy_loss: -0.01993389055132866
        total_loss: -0.016437677666544914
        vf_explained_var: 0.13242854177951813
        vf_loss: 39.94350814819336
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00042134401155635715
        entropy: 0.9178490042686462
        entropy_coeff: 0.0017600000137463212
        kl: 0.011461453512310982
        model: {}
        policy_loss: -0.02196144312620163
        total_loss: -0.018521975725889206
        vf_explained_var: -0.005696624517440796
        vf_loss: 46.25079345703125
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00042134401155635715
        entropy: 0.900844395160675
        entropy_coeff: 0.0017600000137463212
        kl: 0.011386655271053314
        model: {}
        policy_loss: -0.019533922895789146
        total_loss: -0.018043972551822662
        vf_explained_var: 0.06806077063083649
        vf_loss: 29.741052627563477
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00042134401155635715
        entropy: 0.5398080348968506
        entropy_coeff: 0.0017600000137463212
        kl: 0.010526735335588455
        model: {}
        policy_loss: -0.020285360515117645
        total_loss: -0.01817668229341507
        vf_explained_var: 0.17274387180805206
        vf_loss: 26.639860153198242
    load_time_ms: 13264.232
    num_steps_sampled: 13536000
    num_steps_trained: 13536000
    sample_time_ms: 106897.461
    update_time_ms: 15.75
  iterations_since_restore: 141
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.362433862433864
    ram_util_percent: 11.676719576719579
  pid: 28570
  policy_reward_max:
    agent-0: 222.0
    agent-1: 222.0
    agent-2: 257.5
    agent-3: 257.5
    agent-4: 222.5
    agent-5: 222.5
  policy_reward_mean:
    agent-0: 169.18
    agent-1: 169.18
    agent-2: 194.1
    agent-3: 194.1
    agent-4: 154.83
    agent-5: 154.83
  policy_reward_min:
    agent-0: 76.0
    agent-1: 76.0
    agent-2: 107.5
    agent-3: 107.5
    agent-4: 69.5
    agent-5: 69.5
  sampler_perf:
    mean_env_wait_ms: 27.403469171772475
    mean_inference_ms: 13.08349741448337
    mean_processing_ms: 58.626522213766975
  time_since_restore: 18672.086234807968
  time_this_iter_s: 132.1829812526703
  time_total_s: 18672.086234807968
  timestamp: 1637527211
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 13536000
  training_iteration: 141
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    141 |          18672.1 | 13536000 |  1036.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.42
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 26.44
    apples_agent-1_min: 11
    apples_agent-2_max: 357
    apples_agent-2_mean: 262.5
    apples_agent-2_min: 67
    apples_agent-3_max: 137
    apples_agent-3_mean: 40.12
    apples_agent-3_min: 1
    apples_agent-4_max: 32
    apples_agent-4_mean: 2.5
    apples_agent-4_min: 0
    apples_agent-5_max: 370
    apples_agent-5_mean: 259.53
    apples_agent-5_min: 75
    cleaning_beam_agent-0_max: 648
    cleaning_beam_agent-0_mean: 542.94
    cleaning_beam_agent-0_min: 351
    cleaning_beam_agent-1_max: 73
    cleaning_beam_agent-1_mean: 13.53
    cleaning_beam_agent-1_min: 2
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 24.54
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 422
    cleaning_beam_agent-3_mean: 199.93
    cleaning_beam_agent-3_min: 50
    cleaning_beam_agent-4_max: 720
    cleaning_beam_agent-4_mean: 534.29
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 187
    cleaning_beam_agent-5_mean: 63.25
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.34
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-42-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1340.0
  episode_reward_mean: 1035.41
  episode_reward_min: 262.0
  episodes_this_iter: 96
  episodes_total: 13632
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12621.057
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004153535992372781
        entropy: 0.44328805804252625
        entropy_coeff: 0.0017600000137463212
        kl: 0.006969247478991747
        model: {}
        policy_loss: -0.012913426384329796
        total_loss: -0.01055845245718956
        vf_explained_var: 0.08745449781417847
        vf_loss: 29.609315872192383
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004153535992372781
        entropy: 0.492422491312027
        entropy_coeff: 0.0017600000137463212
        kl: 0.007939742878079414
        model: {}
        policy_loss: -0.017552398145198822
        total_loss: -0.015228538773953915
        vf_explained_var: 0.06341809034347534
        vf_loss: 30.416522979736328
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004153535992372781
        entropy: 0.5307564735412598
        entropy_coeff: 0.0017600000137463212
        kl: 0.00928293727338314
        model: {}
        policy_loss: -0.019764021039009094
        total_loss: -0.015985626727342606
        vf_explained_var: 0.14026381075382233
        vf_loss: 42.48377227783203
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004153535992372781
        entropy: 0.898853600025177
        entropy_coeff: 0.0017600000137463212
        kl: 0.011095532216131687
        model: {}
        policy_loss: -0.021580982953310013
        total_loss: -0.01774955913424492
        vf_explained_var: -0.013875052332878113
        vf_loss: 49.973236083984375
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004153535992372781
        entropy: 0.9124938249588013
        entropy_coeff: 0.0017600000137463212
        kl: 0.011163144372403622
        model: {}
        policy_loss: -0.020130066201090813
        total_loss: -0.01856371760368347
        vf_explained_var: 0.016902729868888855
        vf_loss: 30.729961395263672
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004153535992372781
        entropy: 0.5504702925682068
        entropy_coeff: 0.0017600000137463212
        kl: 0.010941119864583015
        model: {}
        policy_loss: -0.020499376580119133
        total_loss: -0.01836814358830452
        vf_explained_var: 0.13838568329811096
        vf_loss: 26.89771842956543
    load_time_ms: 13262.864
    num_steps_sampled: 13632000
    num_steps_trained: 13632000
    sample_time_ms: 107027.662
    update_time_ms: 15.618
  iterations_since_restore: 142
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.16439790575916
    ram_util_percent: 11.51780104712042
  pid: 28570
  policy_reward_max:
    agent-0: 223.0
    agent-1: 223.0
    agent-2: 259.5
    agent-3: 259.5
    agent-4: 209.5
    agent-5: 209.5
  policy_reward_mean:
    agent-0: 168.18
    agent-1: 168.18
    agent-2: 196.425
    agent-3: 196.425
    agent-4: 153.1
    agent-5: 153.1
  policy_reward_min:
    agent-0: 58.0
    agent-1: 58.0
    agent-2: 34.0
    agent-3: 34.0
    agent-4: 39.0
    agent-5: 39.0
  sampler_perf:
    mean_env_wait_ms: 27.411703127616367
    mean_inference_ms: 13.083001119479757
    mean_processing_ms: 58.62521980200184
  time_since_restore: 18806.837547540665
  time_this_iter_s: 134.75131273269653
  time_total_s: 18806.837547540665
  timestamp: 1637527346
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 13632000
  training_iteration: 142
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    142 |          18806.8 | 13632000 |  1035.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.1
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 27.24
    apples_agent-1_min: 11
    apples_agent-2_max: 346
    apples_agent-2_mean: 256.18
    apples_agent-2_min: 95
    apples_agent-3_max: 110
    apples_agent-3_mean: 40.53
    apples_agent-3_min: 5
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.33
    apples_agent-4_min: 0
    apples_agent-5_max: 382
    apples_agent-5_mean: 264.13
    apples_agent-5_min: 160
    cleaning_beam_agent-0_max: 642
    cleaning_beam_agent-0_mean: 557.09
    cleaning_beam_agent-0_min: 404
    cleaning_beam_agent-1_max: 152
    cleaning_beam_agent-1_mean: 12.92
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 213
    cleaning_beam_agent-2_mean: 29.91
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 330
    cleaning_beam_agent-3_mean: 194.95
    cleaning_beam_agent-3_min: 85
    cleaning_beam_agent-4_max: 777
    cleaning_beam_agent-4_mean: 506.78
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 168
    cleaning_beam_agent-5_mean: 61.42
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.26
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-44-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1319.0
  episode_reward_mean: 1041.88
  episode_reward_min: 591.0
  episodes_this_iter: 96
  episodes_total: 13728
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12607.373
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00040936318691819906
        entropy: 0.43608275055885315
        entropy_coeff: 0.0017600000137463212
        kl: 0.006965401582419872
        model: {}
        policy_loss: -0.012368245981633663
        total_loss: -0.009632335975766182
        vf_explained_var: 0.03525453805923462
        vf_loss: 33.29283905029297
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00040936318691819906
        entropy: 0.47343456745147705
        entropy_coeff: 0.0017600000137463212
        kl: 0.007628241553902626
        model: {}
        policy_loss: -0.017184652388095856
        total_loss: -0.014701396226882935
        vf_explained_var: 0.09016650915145874
        vf_loss: 31.734745025634766
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00040936318691819906
        entropy: 0.5635347366333008
        entropy_coeff: 0.0017600000137463212
        kl: 0.009255853481590748
        model: {}
        policy_loss: -0.021289313212037086
        total_loss: -0.01785087399184704
        vf_explained_var: 0.12314526736736298
        vf_loss: 39.67467498779297
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00040936318691819906
        entropy: 0.9329896569252014
        entropy_coeff: 0.0017600000137463212
        kl: 0.011244354769587517
        model: {}
        policy_loss: -0.021236281841993332
        total_loss: -0.017719006165862083
        vf_explained_var: -0.0493791401386261
        vf_loss: 47.376766204833984
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00040936318691819906
        entropy: 0.9714763760566711
        entropy_coeff: 0.0017600000137463212
        kl: 0.010746754705905914
        model: {}
        policy_loss: -0.01974385417997837
        total_loss: -0.018246537074446678
        vf_explained_var: 0.04150480031967163
        vf_loss: 31.1148624420166
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00040936318691819906
        entropy: 0.5328694581985474
        entropy_coeff: 0.0017600000137463212
        kl: 0.009766120463609695
        model: {}
        policy_loss: -0.019655350595712662
        total_loss: -0.017627893015742302
        vf_explained_var: 0.19592121243476868
        vf_loss: 25.990787506103516
    load_time_ms: 13271.348
    num_steps_sampled: 13728000
    num_steps_trained: 13728000
    sample_time_ms: 106924.712
    update_time_ms: 15.428
  iterations_since_restore: 143
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.398930481283426
    ram_util_percent: 11.682352941176472
  pid: 28570
  policy_reward_max:
    agent-0: 233.0
    agent-1: 233.0
    agent-2: 252.5
    agent-3: 252.5
    agent-4: 216.5
    agent-5: 216.5
  policy_reward_mean:
    agent-0: 171.725
    agent-1: 171.725
    agent-2: 193.76
    agent-3: 193.76
    agent-4: 155.455
    agent-5: 155.455
  policy_reward_min:
    agent-0: 107.5
    agent-1: 107.5
    agent-2: 89.0
    agent-3: 89.0
    agent-4: 92.5
    agent-5: 92.5
  sampler_perf:
    mean_env_wait_ms: 27.417355532342263
    mean_inference_ms: 13.082040609252509
    mean_processing_ms: 58.62117317567823
  time_since_restore: 18938.067183732986
  time_this_iter_s: 131.22963619232178
  time_total_s: 18938.067183732986
  timestamp: 1637527477
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 13728000
  training_iteration: 143
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    143 |          18938.1 | 13728000 |  1041.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.29
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 27.4
    apples_agent-1_min: 10
    apples_agent-2_max: 344
    apples_agent-2_mean: 260.35
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 41.38
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 2.5
    apples_agent-4_min: 0
    apples_agent-5_max: 414
    apples_agent-5_mean: 273.38
    apples_agent-5_min: 84
    cleaning_beam_agent-0_max: 645
    cleaning_beam_agent-0_mean: 541.64
    cleaning_beam_agent-0_min: 274
    cleaning_beam_agent-1_max: 68
    cleaning_beam_agent-1_mean: 12.95
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 411
    cleaning_beam_agent-2_mean: 32.26
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 389
    cleaning_beam_agent-3_mean: 203.69
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 730
    cleaning_beam_agent-4_mean: 523.93
    cleaning_beam_agent-4_min: 200
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 52.45
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.36
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-46-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1309.0
  episode_reward_mean: 1056.66
  episode_reward_min: 437.0
  episodes_this_iter: 96
  episodes_total: 13824
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12612.989
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0004033728037029505
        entropy: 0.46100905537605286
        entropy_coeff: 0.0017600000137463212
        kl: 0.007534044794738293
        model: {}
        policy_loss: -0.013464205898344517
        total_loss: -0.010406078770756721
        vf_explained_var: 0.08303040266036987
        vf_loss: 36.811500549316406
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0004033728037029505
        entropy: 0.4796229898929596
        entropy_coeff: 0.0017600000137463212
        kl: 0.008218694478273392
        model: {}
        policy_loss: -0.018161699175834656
        total_loss: -0.015417538583278656
        vf_explained_var: 0.1482377052307129
        vf_loss: 34.34196090698242
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0004033728037029505
        entropy: 0.5581575632095337
        entropy_coeff: 0.0017600000137463212
        kl: 0.009001325815916061
        model: {}
        policy_loss: -0.020862603560090065
        total_loss: -0.01733207516372204
        vf_explained_var: 0.19792166352272034
        vf_loss: 40.62816619873047
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004033728037029505
        entropy: 0.9386905431747437
        entropy_coeff: 0.0017600000137463212
        kl: 0.010547887533903122
        model: {}
        policy_loss: -0.02100430615246296
        total_loss: -0.016947949305176735
        vf_explained_var: -0.05056557059288025
        vf_loss: 53.12908172607422
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0004033728037029505
        entropy: 0.9331766963005066
        entropy_coeff: 0.0017600000137463212
        kl: 0.010072870180010796
        model: {}
        policy_loss: -0.018301604315638542
        total_loss: -0.016690080985426903
        vf_explained_var: 0.07326297461986542
        vf_loss: 31.642763137817383
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0004033728037029505
        entropy: 0.5152164697647095
        entropy_coeff: 0.0017600000137463212
        kl: 0.009595200419425964
        model: {}
        policy_loss: -0.01977849006652832
        total_loss: -0.017631229013204575
        vf_explained_var: 0.20571376383304596
        vf_loss: 26.942232131958008
    load_time_ms: 13287.849
    num_steps_sampled: 13824000
    num_steps_trained: 13824000
    sample_time_ms: 106823.517
    update_time_ms: 15.28
  iterations_since_restore: 144
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.17315789473684
    ram_util_percent: 11.608947368421056
  pid: 28570
  policy_reward_max:
    agent-0: 234.5
    agent-1: 234.5
    agent-2: 262.0
    agent-3: 262.0
    agent-4: 232.0
    agent-5: 232.0
  policy_reward_mean:
    agent-0: 171.04
    agent-1: 171.04
    agent-2: 195.545
    agent-3: 195.545
    agent-4: 161.745
    agent-5: 161.745
  policy_reward_min:
    agent-0: 55.5
    agent-1: 55.5
    agent-2: 38.0
    agent-3: 38.0
    agent-4: 59.0
    agent-5: 59.0
  sampler_perf:
    mean_env_wait_ms: 27.42316953381396
    mean_inference_ms: 13.081449228303189
    mean_processing_ms: 58.61844306501777
  time_since_restore: 19071.162722826004
  time_this_iter_s: 133.09553909301758
  time_total_s: 19071.162722826004
  timestamp: 1637527611
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 13824000
  training_iteration: 144
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    144 |          19071.2 | 13824000 |  1056.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 26.58
    apples_agent-1_min: 14
    apples_agent-2_max: 356
    apples_agent-2_mean: 257.54
    apples_agent-2_min: 0
    apples_agent-3_max: 104
    apples_agent-3_mean: 41.0
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.79
    apples_agent-4_min: 0
    apples_agent-5_max: 383
    apples_agent-5_mean: 274.36
    apples_agent-5_min: 192
    cleaning_beam_agent-0_max: 628
    cleaning_beam_agent-0_mean: 555.28
    cleaning_beam_agent-0_min: 446
    cleaning_beam_agent-1_max: 73
    cleaning_beam_agent-1_mean: 14.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 173
    cleaning_beam_agent-2_mean: 29.03
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 379
    cleaning_beam_agent-3_mean: 211.14
    cleaning_beam_agent-3_min: 70
    cleaning_beam_agent-4_max: 710
    cleaning_beam_agent-4_mean: 492.92
    cleaning_beam_agent-4_min: 185
    cleaning_beam_agent-5_max: 140
    cleaning_beam_agent-5_mean: 57.97
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.31
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-49-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1292.0
  episode_reward_mean: 1057.13
  episode_reward_min: 753.0
  episodes_this_iter: 96
  episodes_total: 13920
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12613.178
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00039738239138387144
        entropy: 0.44245052337646484
        entropy_coeff: 0.0017600000137463212
        kl: 0.006628614384680986
        model: {}
        policy_loss: -0.010796699672937393
        total_loss: -0.008098674938082695
        vf_explained_var: 0.040701717138290405
        vf_loss: 33.11021423339844
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00039738239138387144
        entropy: 0.47166135907173157
        entropy_coeff: 0.0017600000137463212
        kl: 0.00855537410825491
        model: {}
        policy_loss: -0.016168713569641113
        total_loss: -0.013835979625582695
        vf_explained_var: 0.13523434102535248
        vf_loss: 30.024463653564453
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00039738239138387144
        entropy: 0.5598392486572266
        entropy_coeff: 0.0017600000137463212
        kl: 0.00849519670009613
        model: {}
        policy_loss: -0.020639438182115555
        total_loss: -0.017209554091095924
        vf_explained_var: 0.18527016043663025
        vf_loss: 39.90441131591797
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00039738239138387144
        entropy: 0.9522180557250977
        entropy_coeff: 0.0017600000137463212
        kl: 0.013507243245840073
        model: {}
        policy_loss: -0.017861582338809967
        total_loss: -0.014151135459542274
        vf_explained_var: 0.0002469271421432495
        vf_loss: 48.798336029052734
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00039738239138387144
        entropy: 0.9998117089271545
        entropy_coeff: 0.0017600000137463212
        kl: 0.01054154522716999
        model: {}
        policy_loss: -0.018825862556695938
        total_loss: -0.01730947755277157
        vf_explained_var: 0.06979256868362427
        vf_loss: 31.822460174560547
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00039738239138387144
        entropy: 0.5027391910552979
        entropy_coeff: 0.0017600000137463212
        kl: 0.009452110156416893
        model: {}
        policy_loss: -0.018110530450940132
        total_loss: -0.015917779877781868
        vf_explained_var: 0.19949737191200256
        vf_loss: 27.231168746948242
    load_time_ms: 13282.322
    num_steps_sampled: 13920000
    num_steps_trained: 13920000
    sample_time_ms: 106941.776
    update_time_ms: 15.241
  iterations_since_restore: 145
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.406878306878305
    ram_util_percent: 11.610052910052913
  pid: 28570
  policy_reward_max:
    agent-0: 233.0
    agent-1: 233.0
    agent-2: 254.0
    agent-3: 254.0
    agent-4: 217.0
    agent-5: 217.0
  policy_reward_mean:
    agent-0: 174.36
    agent-1: 174.36
    agent-2: 192.265
    agent-3: 192.265
    agent-4: 161.94
    agent-5: 161.94
  policy_reward_min:
    agent-0: 120.5
    agent-1: 120.5
    agent-2: 39.5
    agent-3: 39.5
    agent-4: 106.5
    agent-5: 106.5
  sampler_perf:
    mean_env_wait_ms: 27.43090390781781
    mean_inference_ms: 13.080921410413858
    mean_processing_ms: 58.61964132182765
  time_since_restore: 19204.002409934998
  time_this_iter_s: 132.83968710899353
  time_total_s: 19204.002409934998
  timestamp: 1637527744
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 13920000
  training_iteration: 145
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    145 |            19204 | 13920000 |  1057.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.13
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 26.78
    apples_agent-1_min: 12
    apples_agent-2_max: 399
    apples_agent-2_mean: 272.4
    apples_agent-2_min: 129
    apples_agent-3_max: 98
    apples_agent-3_mean: 40.6
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 376
    apples_agent-5_mean: 287.51
    apples_agent-5_min: 169
    cleaning_beam_agent-0_max: 632
    cleaning_beam_agent-0_mean: 557.01
    cleaning_beam_agent-0_min: 428
    cleaning_beam_agent-1_max: 58
    cleaning_beam_agent-1_mean: 10.35
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 78
    cleaning_beam_agent-2_mean: 23.44
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 291
    cleaning_beam_agent-3_mean: 198.59
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 749
    cleaning_beam_agent-4_mean: 511.14
    cleaning_beam_agent-4_min: 202
    cleaning_beam_agent-5_max: 111
    cleaning_beam_agent-5_mean: 41.98
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.07
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.23
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-51-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1340.0
  episode_reward_mean: 1102.35
  episode_reward_min: 650.0
  episodes_this_iter: 96
  episodes_total: 14016
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12602.847
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00039139200816862285
        entropy: 0.44045788049697876
        entropy_coeff: 0.0017600000137463212
        kl: 0.006580786779522896
        model: {}
        policy_loss: -0.011751489713788033
        total_loss: -0.009141959249973297
        vf_explained_var: 0.03788769245147705
        vf_loss: 32.202171325683594
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00039139200816862285
        entropy: 0.46141043305397034
        entropy_coeff: 0.0017600000137463212
        kl: 0.007693678140640259
        model: {}
        policy_loss: -0.015784647315740585
        total_loss: -0.013405438512563705
        vf_explained_var: 0.0924142450094223
        vf_loss: 30.470354080200195
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00039139200816862285
        entropy: 0.5518013834953308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0082957549020648
        model: {}
        policy_loss: -0.019595053046941757
        total_loss: -0.016077812761068344
        vf_explained_var: 0.10516710579395294
        vf_loss: 40.736236572265625
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00039139200816862285
        entropy: 0.9430723786354065
        entropy_coeff: 0.0017600000137463212
        kl: 0.011154977604746819
        model: {}
        policy_loss: -0.0211123526096344
        total_loss: -0.01772928610444069
        vf_explained_var: 0.015419572591781616
        vf_loss: 46.24563980102539
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00039139200816862285
        entropy: 0.9702152609825134
        entropy_coeff: 0.0017600000137463212
        kl: 0.010356937535107136
        model: {}
        policy_loss: -0.019194886088371277
        total_loss: -0.017760014161467552
        vf_explained_var: 0.06938111782073975
        vf_loss: 30.50285530090332
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00039139200816862285
        entropy: 0.44598260521888733
        entropy_coeff: 0.0017600000137463212
        kl: 0.008486662060022354
        model: {}
        policy_loss: -0.017874643206596375
        total_loss: -0.015618328005075455
        vf_explained_var: 0.16658726334571838
        vf_loss: 27.229936599731445
    load_time_ms: 13268.659
    num_steps_sampled: 14016000
    num_steps_trained: 14016000
    sample_time_ms: 106934.564
    update_time_ms: 15.924
  iterations_since_restore: 146
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.392553191489366
    ram_util_percent: 11.593085106382981
  pid: 28570
  policy_reward_max:
    agent-0: 236.5
    agent-1: 236.5
    agent-2: 260.0
    agent-3: 260.0
    agent-4: 214.5
    agent-5: 214.5
  policy_reward_mean:
    agent-0: 178.86
    agent-1: 178.86
    agent-2: 203.855
    agent-3: 203.855
    agent-4: 168.46
    agent-5: 168.46
  policy_reward_min:
    agent-0: 106.5
    agent-1: 106.5
    agent-2: 118.0
    agent-3: 118.0
    agent-4: 100.5
    agent-5: 100.5
  sampler_perf:
    mean_env_wait_ms: 27.436676695198603
    mean_inference_ms: 13.080296775053025
    mean_processing_ms: 58.61676433356931
  time_since_restore: 19336.238073349
  time_this_iter_s: 132.23566341400146
  time_total_s: 19336.238073349
  timestamp: 1637527876
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 14016000
  training_iteration: 146
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    146 |          19336.2 | 14016000 |  1102.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 26.97
    apples_agent-1_min: 9
    apples_agent-2_max: 353
    apples_agent-2_mean: 265.84
    apples_agent-2_min: 166
    apples_agent-3_max: 156
    apples_agent-3_mean: 47.33
    apples_agent-3_min: 2
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.9
    apples_agent-4_min: 0
    apples_agent-5_max: 383
    apples_agent-5_mean: 284.67
    apples_agent-5_min: 186
    cleaning_beam_agent-0_max: 625
    cleaning_beam_agent-0_mean: 552.66
    cleaning_beam_agent-0_min: 412
    cleaning_beam_agent-1_max: 46
    cleaning_beam_agent-1_mean: 10.24
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 242
    cleaning_beam_agent-2_mean: 24.33
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 299
    cleaning_beam_agent-3_mean: 181.92
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 721
    cleaning_beam_agent-4_mean: 486.63
    cleaning_beam_agent-4_min: 270
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 42.72
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 5
    fire_beam_agent-4_mean: 0.26
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-53-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1378.0
  episode_reward_mean: 1090.0
  episode_reward_min: 730.0
  episodes_this_iter: 96
  episodes_total: 14112
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12593.389
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0003854015958495438
        entropy: 0.4461834728717804
        entropy_coeff: 0.0017600000137463212
        kl: 0.007258465047925711
        model: {}
        policy_loss: -0.011305060237646103
        total_loss: -0.008838833309710026
        vf_explained_var: 0.05359886586666107
        vf_loss: 30.700519561767578
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0003854015958495438
        entropy: 0.460562527179718
        entropy_coeff: 0.0017600000137463212
        kl: 0.007742790505290031
        model: {}
        policy_loss: -0.016084235161542892
        total_loss: -0.013863424770534039
        vf_explained_var: 0.11225567758083344
        vf_loss: 28.862224578857422
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0003854015958495438
        entropy: 0.5493449568748474
        entropy_coeff: 0.0017600000137463212
        kl: 0.008716174401342869
        model: {}
        policy_loss: -0.02004990540444851
        total_loss: -0.016846152022480965
        vf_explained_var: 0.12675979733467102
        vf_loss: 37.347923278808594
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003854015958495438
        entropy: 0.9092767238616943
        entropy_coeff: 0.0017600000137463212
        kl: 0.011129237711429596
        model: {}
        policy_loss: -0.021222975105047226
        total_loss: -0.01801575906574726
        vf_explained_var: -0.01888132095336914
        vf_loss: 43.90194320678711
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0003854015958495438
        entropy: 1.0136139392852783
        entropy_coeff: 0.0017600000137463212
        kl: 0.01009127963334322
        model: {}
        policy_loss: -0.019350923597812653
        total_loss: -0.01807016134262085
        vf_explained_var: 0.060662463307380676
        vf_loss: 29.74921417236328
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003854015958495438
        entropy: 0.4561190903186798
        entropy_coeff: 0.0017600000137463212
        kl: 0.008697491139173508
        model: {}
        policy_loss: -0.017994210124015808
        total_loss: -0.015854056924581528
        vf_explained_var: 0.17347608506679535
        vf_loss: 26.167652130126953
    load_time_ms: 13277.89
    num_steps_sampled: 14112000
    num_steps_trained: 14112000
    sample_time_ms: 106869.182
    update_time_ms: 16.028
  iterations_since_restore: 147
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.323936170212765
    ram_util_percent: 11.597340425531918
  pid: 28570
  policy_reward_max:
    agent-0: 224.5
    agent-1: 224.5
    agent-2: 256.0
    agent-3: 256.0
    agent-4: 222.0
    agent-5: 222.0
  policy_reward_mean:
    agent-0: 176.24
    agent-1: 176.24
    agent-2: 200.69
    agent-3: 200.69
    agent-4: 168.07
    agent-5: 168.07
  policy_reward_min:
    agent-0: 114.5
    agent-1: 114.5
    agent-2: 124.5
    agent-3: 124.5
    agent-4: 118.5
    agent-5: 118.5
  sampler_perf:
    mean_env_wait_ms: 27.440794515597194
    mean_inference_ms: 13.07993753387665
    mean_processing_ms: 58.61394278147776
  time_since_restore: 19468.386765241623
  time_this_iter_s: 132.1486918926239
  time_total_s: 19468.386765241623
  timestamp: 1637528008
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 14112000
  training_iteration: 147
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    147 |          19468.4 | 14112000 |     1090 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 29.98
    apples_agent-1_min: 17
    apples_agent-2_max: 346
    apples_agent-2_mean: 276.52
    apples_agent-2_min: 64
    apples_agent-3_max: 156
    apples_agent-3_mean: 47.11
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 398
    apples_agent-5_mean: 292.31
    apples_agent-5_min: 158
    cleaning_beam_agent-0_max: 646
    cleaning_beam_agent-0_mean: 542.28
    cleaning_beam_agent-0_min: 424
    cleaning_beam_agent-1_max: 78
    cleaning_beam_agent-1_mean: 12.45
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 167
    cleaning_beam_agent-2_mean: 21.36
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 313
    cleaning_beam_agent-3_mean: 189.79
    cleaning_beam_agent-3_min: 58
    cleaning_beam_agent-4_max: 812
    cleaning_beam_agent-4_mean: 500.51
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 41.41
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-55-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1349.0
  episode_reward_mean: 1113.98
  episode_reward_min: 671.0
  episodes_this_iter: 96
  episodes_total: 14208
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12590.925
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0003794112126342952
        entropy: 0.4574425220489502
        entropy_coeff: 0.0017600000137463212
        kl: 0.00716202100738883
        model: {}
        policy_loss: -0.012719698250293732
        total_loss: -0.010285081341862679
        vf_explained_var: 0.08457185328006744
        vf_loss: 30.606639862060547
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0003794112126342952
        entropy: 0.4515492022037506
        entropy_coeff: 0.0017600000137463212
        kl: 0.007427236996591091
        model: {}
        policy_loss: -0.015914281830191612
        total_loss: -0.013706682249903679
        vf_explained_var: 0.13882409036159515
        vf_loss: 28.630672454833984
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0003794112126342952
        entropy: 0.5411109924316406
        entropy_coeff: 0.0017600000137463212
        kl: 0.008506548590958118
        model: {}
        policy_loss: -0.019711896777153015
        total_loss: -0.01601269654929638
        vf_explained_var: 0.0999096930027008
        vf_loss: 42.262306213378906
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003794112126342952
        entropy: 0.935814380645752
        entropy_coeff: 0.0017600000137463212
        kl: 0.011276578530669212
        model: {}
        policy_loss: -0.02131161093711853
        total_loss: -0.01778745837509632
        vf_explained_var: -0.012914642691612244
        vf_loss: 47.48314666748047
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0003794112126342952
        entropy: 0.9777436256408691
        entropy_coeff: 0.0017600000137463212
        kl: 0.010311374440789223
        model: {}
        policy_loss: -0.018169276416301727
        total_loss: -0.016547493636608124
        vf_explained_var: 0.07317818701267242
        vf_loss: 32.50851821899414
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003794112126342952
        entropy: 0.43872228264808655
        entropy_coeff: 0.0017600000137463212
        kl: 0.008716298267245293
        model: {}
        policy_loss: -0.017340360209345818
        total_loss: -0.014842749573290348
        vf_explained_var: 0.1645279973745346
        vf_loss: 29.42903709411621
    load_time_ms: 13275.842
    num_steps_sampled: 14208000
    num_steps_trained: 14208000
    sample_time_ms: 106758.014
    update_time_ms: 16.39
  iterations_since_restore: 148
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.32287234042553
    ram_util_percent: 11.685638297872343
  pid: 28570
  policy_reward_max:
    agent-0: 231.5
    agent-1: 231.5
    agent-2: 265.0
    agent-3: 265.0
    agent-4: 230.5
    agent-5: 230.5
  policy_reward_mean:
    agent-0: 178.325
    agent-1: 178.325
    agent-2: 206.595
    agent-3: 206.595
    agent-4: 172.07
    agent-5: 172.07
  policy_reward_min:
    agent-0: 94.5
    agent-1: 94.5
    agent-2: 70.0
    agent-3: 70.0
    agent-4: 93.5
    agent-5: 93.5
  sampler_perf:
    mean_env_wait_ms: 27.444136915183417
    mean_inference_ms: 13.079250050929234
    mean_processing_ms: 58.61086892319807
  time_since_restore: 19600.189930200577
  time_this_iter_s: 131.80316495895386
  time_total_s: 19600.189930200577
  timestamp: 1637528140
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 14208000
  training_iteration: 148
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    148 |          19600.2 | 14208000 |  1113.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.23
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 28.24
    apples_agent-1_min: 13
    apples_agent-2_max: 369
    apples_agent-2_mean: 277.39
    apples_agent-2_min: 158
    apples_agent-3_max: 121
    apples_agent-3_mean: 43.65
    apples_agent-3_min: 3
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 365
    apples_agent-5_mean: 282.57
    apples_agent-5_min: 158
    cleaning_beam_agent-0_max: 602
    cleaning_beam_agent-0_mean: 530.23
    cleaning_beam_agent-0_min: 394
    cleaning_beam_agent-1_max: 64
    cleaning_beam_agent-1_mean: 13.9
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 99
    cleaning_beam_agent-2_mean: 22.06
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 295
    cleaning_beam_agent-3_mean: 180.03
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 748
    cleaning_beam_agent-4_mean: 477.8
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 44.5
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.07
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.22
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_15-57-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1331.0
  episode_reward_mean: 1103.75
  episode_reward_min: 671.0
  episodes_this_iter: 96
  episodes_total: 14304
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12586.002
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0003734208003152162
        entropy: 0.46372687816619873
        entropy_coeff: 0.0017600000137463212
        kl: 0.006799032911658287
        model: {}
        policy_loss: -0.011563273146748543
        total_loss: -0.008897572755813599
        vf_explained_var: 0.06491734087467194
        vf_loss: 33.11887741088867
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0003734208003152162
        entropy: 0.44428470730781555
        entropy_coeff: 0.0017600000137463212
        kl: 0.008210411295294762
        model: {}
        policy_loss: -0.016380079090595245
        total_loss: -0.013999796472489834
        vf_explained_var: 0.14960308372974396
        vf_loss: 30.082813262939453
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0003734208003152162
        entropy: 0.5304567813873291
        entropy_coeff: 0.0017600000137463212
        kl: 0.00857590138912201
        model: {}
        policy_loss: -0.019632354378700256
        total_loss: -0.015880826860666275
        vf_explained_var: 0.12612725794315338
        vf_loss: 42.56334686279297
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003734208003152162
        entropy: 0.9158946871757507
        entropy_coeff: 0.0017600000137463212
        kl: 0.011310522444546223
        model: {}
        policy_loss: -0.021547120064496994
        total_loss: -0.017854133620858192
        vf_explained_var: 0.01035599410533905
        vf_loss: 48.80812072753906
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0003734208003152162
        entropy: 1.0208170413970947
        entropy_coeff: 0.0017600000137463212
        kl: 0.010197419673204422
        model: {}
        policy_loss: -0.018307190388441086
        total_loss: -0.016947828233242035
        vf_explained_var: 0.06258028745651245
        vf_loss: 30.65255355834961
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003734208003152162
        entropy: 0.46049225330352783
        entropy_coeff: 0.0017600000137463212
        kl: 0.008746449835598469
        model: {}
        policy_loss: -0.016969896852970123
        total_loss: -0.014751606620848179
        vf_explained_var: 0.17320601642131805
        vf_loss: 27.007619857788086
    load_time_ms: 13274.443
    num_steps_sampled: 14304000
    num_steps_trained: 14304000
    sample_time_ms: 106716.308
    update_time_ms: 16.672
  iterations_since_restore: 149
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.269312169312173
    ram_util_percent: 11.600529100529103
  pid: 28570
  policy_reward_max:
    agent-0: 228.0
    agent-1: 228.0
    agent-2: 264.0
    agent-3: 264.0
    agent-4: 208.0
    agent-5: 208.0
  policy_reward_mean:
    agent-0: 177.75
    agent-1: 177.75
    agent-2: 207.565
    agent-3: 207.565
    agent-4: 166.56
    agent-5: 166.56
  policy_reward_min:
    agent-0: 91.0
    agent-1: 91.0
    agent-2: 127.5
    agent-3: 127.5
    agent-4: 93.5
    agent-5: 93.5
  sampler_perf:
    mean_env_wait_ms: 27.44773820614523
    mean_inference_ms: 13.079178179726965
    mean_processing_ms: 58.61001257325614
  time_since_restore: 19732.942435503006
  time_this_iter_s: 132.7525053024292
  time_total_s: 19732.942435503006
  timestamp: 1637528273
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 14304000
  training_iteration: 149
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    149 |          19732.9 | 14304000 |  1103.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.16
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 26.01
    apples_agent-1_min: 10
    apples_agent-2_max: 387
    apples_agent-2_mean: 272.66
    apples_agent-2_min: 71
    apples_agent-3_max: 253
    apples_agent-3_mean: 46.91
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 390
    apples_agent-5_mean: 291.16
    apples_agent-5_min: 64
    cleaning_beam_agent-0_max: 608
    cleaning_beam_agent-0_mean: 529.61
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 114
    cleaning_beam_agent-1_mean: 15.55
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 123
    cleaning_beam_agent-2_mean: 25.51
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 335
    cleaning_beam_agent-3_mean: 181.76
    cleaning_beam_agent-3_min: 37
    cleaning_beam_agent-4_max: 697
    cleaning_beam_agent-4_mean: 479.08
    cleaning_beam_agent-4_min: 286
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 41.32
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.2
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_16-00-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1428.0
  episode_reward_mean: 1111.29
  episode_reward_min: 287.0
  episodes_this_iter: 96
  episodes_total: 14400
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12585.096
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00036743038799613714
        entropy: 0.46861571073532104
        entropy_coeff: 0.0017600000137463212
        kl: 0.009013091214001179
        model: {}
        policy_loss: -0.011297483928501606
        total_loss: -0.008417874574661255
        vf_explained_var: 0.091133713722229
        vf_loss: 34.79045486450195
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00036743038799613714
        entropy: 0.4321880340576172
        entropy_coeff: 0.0017600000137463212
        kl: 0.00823539961129427
        model: {}
        policy_loss: -0.01639443449676037
        total_loss: -0.013738302513957024
        vf_explained_var: 0.1417035162448883
        vf_loss: 32.62374496459961
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00036743038799613714
        entropy: 0.5402929186820984
        entropy_coeff: 0.0017600000137463212
        kl: 0.008438482880592346
        model: {}
        policy_loss: -0.019585032016038895
        total_loss: -0.015958789736032486
        vf_explained_var: 0.14044763147830963
        vf_loss: 41.55235290527344
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00036743038799613714
        entropy: 0.9599437117576599
        entropy_coeff: 0.0017600000137463212
        kl: 0.01064458116889
        model: {}
        policy_loss: -0.021397149190306664
        total_loss: -0.017639119178056717
        vf_explained_var: -0.04401865601539612
        vf_loss: 50.483558654785156
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00036743038799613714
        entropy: 1.0283418893814087
        entropy_coeff: 0.0017600000137463212
        kl: 0.010916334576904774
        model: {}
        policy_loss: -0.01862473227083683
        total_loss: -0.016990164294838905
        vf_explained_var: 0.057944223284721375
        vf_loss: 33.473045349121094
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00036743038799613714
        entropy: 0.42866289615631104
        entropy_coeff: 0.0017600000137463212
        kl: 0.008510207757353783
        model: {}
        policy_loss: -0.016556913033127785
        total_loss: -0.014025754295289516
        vf_explained_var: 0.1673298329114914
        vf_loss: 29.664722442626953
    load_time_ms: 13272.431
    num_steps_sampled: 14400000
    num_steps_trained: 14400000
    sample_time_ms: 106663.664
    update_time_ms: 16.793
  iterations_since_restore: 150
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.236842105263154
    ram_util_percent: 11.60789473684211
  pid: 28570
  policy_reward_max:
    agent-0: 254.0
    agent-1: 254.0
    agent-2: 261.5
    agent-3: 261.5
    agent-4: 228.5
    agent-5: 228.5
  policy_reward_mean:
    agent-0: 181.39
    agent-1: 181.39
    agent-2: 203.73
    agent-3: 203.73
    agent-4: 170.525
    agent-5: 170.525
  policy_reward_min:
    agent-0: 48.0
    agent-1: 48.0
    agent-2: 55.0
    agent-3: 55.0
    agent-4: 40.5
    agent-5: 40.5
  sampler_perf:
    mean_env_wait_ms: 27.45079589990365
    mean_inference_ms: 13.078445900850054
    mean_processing_ms: 58.61020987175117
  time_since_restore: 19866.074962615967
  time_this_iter_s: 133.13252711296082
  time_total_s: 19866.074962615967
  timestamp: 1637528406
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 14400000
  training_iteration: 150
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    150 |          19866.1 | 14400000 |  1111.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 0.6
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 25.88
    apples_agent-1_min: 12
    apples_agent-2_max: 342
    apples_agent-2_mean: 267.89
    apples_agent-2_min: 11
    apples_agent-3_max: 107
    apples_agent-3_mean: 47.81
    apples_agent-3_min: 0
    apples_agent-4_max: 104
    apples_agent-4_mean: 1.32
    apples_agent-4_min: 0
    apples_agent-5_max: 390
    apples_agent-5_mean: 289.76
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 604
    cleaning_beam_agent-0_mean: 521.88
    cleaning_beam_agent-0_min: 315
    cleaning_beam_agent-1_max: 84
    cleaning_beam_agent-1_mean: 14.15
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 159
    cleaning_beam_agent-2_mean: 28.85
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 332
    cleaning_beam_agent-3_mean: 185.04
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 775
    cleaning_beam_agent-4_mean: 508.8
    cleaning_beam_agent-4_min: 260
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 39.39
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.15
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_16-02-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1365.0
  episode_reward_mean: 1104.26
  episode_reward_min: 331.0
  episodes_this_iter: 96
  episodes_total: 14496
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12585.74
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00036144000478088856
        entropy: 0.48134055733680725
        entropy_coeff: 0.0017600000137463212
        kl: 0.006194515619426966
        model: {}
        policy_loss: -0.012566324323415756
        total_loss: -0.009953362867236137
        vf_explained_var: 0.0869530588388443
        vf_loss: 33.05255889892578
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00036144000478088856
        entropy: 0.41512641310691833
        entropy_coeff: 0.0017600000137463212
        kl: 0.007457643747329712
        model: {}
        policy_loss: -0.014698401093482971
        total_loss: -0.01216469518840313
        vf_explained_var: 0.14293673634529114
        vf_loss: 31.24496841430664
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00036144000478088856
        entropy: 0.5466431975364685
        entropy_coeff: 0.0017600000137463212
        kl: 0.008608955889940262
        model: {}
        policy_loss: -0.019525624811649323
        total_loss: -0.015808304771780968
        vf_explained_var: 0.1434558928012848
        vf_loss: 42.48963928222656
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00036144000478088856
        entropy: 0.9431476593017578
        entropy_coeff: 0.0017600000137463212
        kl: 0.011218464002013206
        model: {}
        policy_loss: -0.020546473562717438
        total_loss: -0.01674787327647209
        vf_explained_var: -0.015536800026893616
        vf_loss: 50.37846374511719
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00036144000478088856
        entropy: 1.003973126411438
        entropy_coeff: 0.0017600000137463212
        kl: 0.009860390797257423
        model: {}
        policy_loss: -0.018579352647066116
        total_loss: -0.016786200925707817
        vf_explained_var: 0.04366232454776764
        vf_loss: 34.7239875793457
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00036144000478088856
        entropy: 0.418933629989624
        entropy_coeff: 0.0017600000137463212
        kl: 0.007782121654599905
        model: {}
        policy_loss: -0.017583023756742477
        total_loss: -0.015028970316052437
        vf_explained_var: 0.17554336786270142
        vf_loss: 29.9954891204834
    load_time_ms: 13267.66
    num_steps_sampled: 14496000
    num_steps_trained: 14496000
    sample_time_ms: 106602.109
    update_time_ms: 16.781
  iterations_since_restore: 151
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.33351063829787
    ram_util_percent: 11.590957446808513
  pid: 28570
  policy_reward_max:
    agent-0: 235.0
    agent-1: 235.0
    agent-2: 260.0
    agent-3: 260.0
    agent-4: 217.5
    agent-5: 217.5
  policy_reward_mean:
    agent-0: 179.49
    agent-1: 179.49
    agent-2: 203.005
    agent-3: 203.005
    agent-4: 169.635
    agent-5: 169.635
  policy_reward_min:
    agent-0: 67.5
    agent-1: 67.5
    agent-2: 61.5
    agent-3: 61.5
    agent-4: 36.5
    agent-5: 36.5
  sampler_perf:
    mean_env_wait_ms: 27.45452269982851
    mean_inference_ms: 13.077752729738126
    mean_processing_ms: 58.6082868013772
  time_since_restore: 19997.609106063843
  time_this_iter_s: 131.53414344787598
  time_total_s: 19997.609106063843
  timestamp: 1637528538
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 14496000
  training_iteration: 151
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    151 |          19997.6 | 14496000 |  1104.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.1
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 27.61
    apples_agent-1_min: 13
    apples_agent-2_max: 401
    apples_agent-2_mean: 278.93
    apples_agent-2_min: 191
    apples_agent-3_max: 112
    apples_agent-3_mean: 47.13
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 294.78
    apples_agent-5_min: 189
    cleaning_beam_agent-0_max: 607
    cleaning_beam_agent-0_mean: 532.81
    cleaning_beam_agent-0_min: 445
    cleaning_beam_agent-1_max: 182
    cleaning_beam_agent-1_mean: 11.67
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 22.25
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 294
    cleaning_beam_agent-3_mean: 186.65
    cleaning_beam_agent-3_min: 104
    cleaning_beam_agent-4_max: 773
    cleaning_beam_agent-4_mean: 534.25
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 34.02
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.15
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_16-04-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1440.0
  episode_reward_mean: 1143.74
  episode_reward_min: 793.0
  episodes_this_iter: 96
  episodes_total: 14592
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12594.338
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0003554495924618095
        entropy: 0.4511786103248596
        entropy_coeff: 0.0017600000137463212
        kl: 0.005955779924988747
        model: {}
        policy_loss: -0.010987759567797184
        total_loss: -0.008482038043439388
        vf_explained_var: 0.07599951326847076
        vf_loss: 31.508995056152344
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.0003554495924618095
        entropy: 0.37800171971321106
        entropy_coeff: 0.0017600000137463212
        kl: 0.00667761592194438
        model: {}
        policy_loss: -0.014657551422715187
        total_loss: -0.012249992229044437
        vf_explained_var: 0.14189456403255463
        vf_loss: 29.476375579833984
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0003554495924618095
        entropy: 0.5265471935272217
        entropy_coeff: 0.0017600000137463212
        kl: 0.007773368153721094
        model: {}
        policy_loss: -0.018191605806350708
        total_loss: -0.014708815142512321
        vf_explained_var: 0.0942232757806778
        vf_loss: 40.208457946777344
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003554495924618095
        entropy: 0.9795379638671875
        entropy_coeff: 0.0017600000137463212
        kl: 0.01096413005143404
        model: {}
        policy_loss: -0.0202031210064888
        total_loss: -0.01711873710155487
        vf_explained_var: 0.007967829704284668
        vf_loss: 43.97210693359375
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.0003554495924618095
        entropy: 0.9547680020332336
        entropy_coeff: 0.0017600000137463212
        kl: 0.00997091829776764
        model: {}
        policy_loss: -0.018203262239694595
        total_loss: -0.016650529578328133
        vf_explained_var: 0.07949039340019226
        vf_loss: 31.44396209716797
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.0003554495924618095
        entropy: 0.4199649691581726
        entropy_coeff: 0.0017600000137463212
        kl: 0.007712729275226593
        model: {}
        policy_loss: -0.016265029087662697
        total_loss: -0.01398313045501709
        vf_explained_var: 0.20232397317886353
        vf_loss: 27.318092346191406
    load_time_ms: 13257.726
    num_steps_sampled: 14592000
    num_steps_trained: 14592000
    sample_time_ms: 106383.411
    update_time_ms: 16.81
  iterations_since_restore: 152
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.310582010582014
    ram_util_percent: 11.604761904761908
  pid: 28570
  policy_reward_max:
    agent-0: 243.5
    agent-1: 243.5
    agent-2: 289.0
    agent-3: 289.0
    agent-4: 241.0
    agent-5: 241.0
  policy_reward_mean:
    agent-0: 185.465
    agent-1: 185.465
    agent-2: 213.36
    agent-3: 213.36
    agent-4: 173.045
    agent-5: 173.045
  policy_reward_min:
    agent-0: 105.5
    agent-1: 105.5
    agent-2: 161.0
    agent-3: 161.0
    agent-4: 116.5
    agent-5: 116.5
  sampler_perf:
    mean_env_wait_ms: 27.45888650434693
    mean_inference_ms: 13.077297619434594
    mean_processing_ms: 58.60674391502499
  time_since_restore: 20130.155556440353
  time_this_iter_s: 132.54645037651062
  time_total_s: 20130.155556440353
  timestamp: 1637528670
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 14592000
  training_iteration: 152
  trial_id: '00000'
  
[2m[36m(pid=28570)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7ff595306518> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.74 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.15:28570 |    152 |          20130.2 | 14592000 |  1143.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 29.75
    apples_agent-1_min: 14
    apples_agent-2_max: 374
    apples_agent-2_mean: 280.45
    apples_agent-2_min: 163
    apples_agent-3_max: 287
    apples_agent-3_mean: 54.14
    apples_agent-3_min: 6
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.7
    apples_agent-4_min: 0
    apples_agent-5_max: 405
    apples_agent-5_mean: 299.95
    apples_agent-5_min: 159
    cleaning_beam_agent-0_max: 630
    cleaning_beam_agent-0_mean: 545.47
    cleaning_beam_agent-0_min: 455
    cleaning_beam_agent-1_max: 87
    cleaning_beam_agent-1_mean: 11.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 109
    cleaning_beam_agent-2_mean: 24.47
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 295
    cleaning_beam_agent-3_mean: 189.92
    cleaning_beam_agent-3_min: 50
    cleaning_beam_agent-4_max: 783
    cleaning_beam_agent-4_mean: 561.13
    cleaning_beam_agent-4_min: 308
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 34.23
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.14
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-21_16-06-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1376.0
  episode_reward_mean: 1141.68
  episode_reward_min: 708.0
  episodes_this_iter: 96
  episodes_total: 14688
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu015
  info:
    grad_time_ms: 12591.635
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00034945920924656093
        entropy: 0.43706589937210083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062507810071110725
        model: {}
        policy_loss: -0.011001232080161572
        total_loss: -0.008557967841625214
        vf_explained_var: 0.03160378336906433
        vf_loss: 30.56230926513672
      agent-1:
        cur_kl_coeff: 0.01875000074505806
        cur_lr: 0.00034945920924656093
        entropy: 0.3864641487598419
        entropy_coeff: 0.0017600000137463212
        kl: 0.007117978297173977
        model: {}
        policy_loss: -0.013559157960116863
        total_loss: -0.011257159523665905
        vf_explained_var: 0.10147625207901001
        vf_loss: 28.487133026123047
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00034945920924656093
        entropy: 0.5215854644775391
        entropy_coeff: 0.0017600000137463212
        kl: 0.007858900353312492
        model: {}
        policy_loss: -0.018551765009760857
        total_loss: -0.015099359676241875
        vf_explained_var: 0.11705152690410614
        vf_loss: 39.7745361328125
      agent-3:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00034945920924656093
        entropy: 0.9664443731307983
        entropy_coeff: 0.0017600000137463212
        kl: 0.010833966545760632
        model: {}
        policy_loss: -0.019964367151260376
        total_loss: -0.016835205256938934
        vf_explained_var: 0.02018551528453827
        vf_loss: 44.23832702636719
      agent-4:
        cur_kl_coeff: 0.008898925967514515
        cur_lr: 0.00034945920924656093
        entropy: 0.9415112733840942
        entropy_coeff: 0.0017600000137463212
        kl: 0.010036466643214226
        model: {}
        policy_loss: -0.017662441357970238
        total_loss: -0.01608915627002716
        vf_explained_var: 0.00783894956111908
        vf_loss: 31.41035270690918
      agent-5:
        cur_kl_coeff: 0.03750000149011612
        cur_lr: 0.00034945920924656093
        entropy: 0.405705988407135
        entropy_coeff: 0.0017600000137463212
        kl: 0.007341735064983368
        model: {}
        policy_loss: -0.016246389597654343
        total_loss: -0.013883128762245178
        vf_explained_var: 0.12785808742046356
        vf_loss: 28.01987075805664
    load_time_ms: 13252.371
    num_steps_sampled: 14688000
    num_steps_trained: 14688000
    sample_time_ms: 106374.688
    update_time_ms: 16.799
  iterations_since_restore: 153
  node_ip: 172.17.8.15
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.494086021505378
    ram_util_percent: 11.608602150537639
  pid: 28570
  policy_reward_max:
    agent-0: 232.5
    agent-1: 232.5
    agent-2: 260.0
    agent-3: 260.0
    agent-4: 230.5
    agent-5: 230.5
  policy_reward_mean:
    agent-0: 186.08
    agent-1: 186.08
    agent-2: 210.235
    agent-3: 210.235
    agent-4: 174.525
    agent-5: 174.525
  policy_reward_min:
    agent-0: 118.0
    agent-1: 118.0
    agent-2: 139.0
    agent-3: 139.0
    agent-4: 94.0
    agent-5: 94.0
  sampler_perf:
    mean_env_wait_ms: 27.464355409714987
    mean_inference_ms: 13.07663489438749
    mean_processing_ms: 58.60247065024059
  time_since_restore: 20261.256773471832
  time_this_iter_s: 131.10121703147888
  time_total_s: 20261.256773471832
  timestamp: 1637528802
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 14688000
  training_iteration: 153
  trial_id: '00000'
  >>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-21_10-26-43brn7k2yg/checkpoint_370
== Status ==
Memory usage on this node: 7.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    370 |          47081.5 | 35520000 |  1541.87 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m 2021-11-22 00:13:18,698	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=21723)[0m 2021-11-22 00:13:18,712	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=21723)[0m 2021-11-22 00:15:00,542	INFO trainable.py:180 -- _setup took 101.844 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=21723)[0m 2021-11-22 00:15:00,542	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=21723)[0m 2021-11-22 00:15:00,542	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=21723)[0m 2021-11-22 00:15:03,561	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=21723)[0m 2021-11-22 00:15:03,562	INFO trainable.py:423 -- Restored on 172.17.8.139 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-21_10-26-43brn7k2yg/tmp8i9lhs1drestore_from_object/checkpoint-370
[2m[36m(pid=21723)[0m 2021-11-22 00:15:03,562	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 370, '_timesteps_total': 35520000, '_time_total': 47081.545552015305, '_episodes_total': 35520}
== Status ==
Memory usage on this node: 16.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    370 |          47081.5 | 35520000 |  1541.87 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 0.3645833333333333
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 29.802083333333332
    apples_agent-1_min: 12
    apples_agent-2_max: 431
    apples_agent-2_mean: 356.71875
    apples_agent-2_min: 63
    apples_agent-3_max: 152
    apples_agent-3_mean: 101.95833333333333
    apples_agent-3_min: 3
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.07291666666666667
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 382.0416666666667
    apples_agent-5_min: 66
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 432.3229166666667
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 35
    cleaning_beam_agent-1_mean: 3.375
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 5.479166666666667
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 112.76041666666667
    cleaning_beam_agent-3_min: 59
    cleaning_beam_agent-4_max: 649
    cleaning_beam_agent-4_mean: 544.6041666666666
    cleaning_beam_agent-4_min: 437
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 3.96875
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.020833333333333332
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.07291666666666667
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07291666666666667
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-18-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1696.0
  episode_reward_mean: 1543.5833333333333
  episode_reward_min: 255.0
  episodes_this_iter: 96
  episodes_total: 35616
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 29708.22
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39401474595069885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010732951341196895
        model: {}
        policy_loss: -0.0016862398479133844
        total_loss: 0.001456652069464326
        vf_explained_var: 0.07266263663768768
        vf_loss: 36.21697235107422
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27592089772224426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007193662459030747
        model: {}
        policy_loss: -0.002143300138413906
        total_loss: 0.0010114640463143587
        vf_explained_var: 0.10524505376815796
        vf_loss: 34.965118408203125
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3200174868106842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007419279427267611
        model: {}
        policy_loss: -0.0024434993974864483
        total_loss: 0.0035369202960282564
        vf_explained_var: 0.09248185157775879
        vf_loss: 63.95265579223633
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9136990308761597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001050598919391632
        model: {}
        policy_loss: -0.0028344017919152975
        total_loss: 0.0029502823017537594
        vf_explained_var: -0.02357041835784912
        vf_loss: 71.82675170898438
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.887591540813446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013955167960375547
        model: {}
        policy_loss: -0.0024172451812773943
        total_loss: -0.0002838738728314638
        vf_explained_var: 0.03632429242134094
        vf_loss: 34.16427230834961
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2686863839626312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006364611908793449
        model: {}
        policy_loss: -0.0024087303318083286
        total_loss: 0.00022536935284733772
        vf_explained_var: 0.16166722774505615
        vf_loss: 29.796953201293945
    load_time_ms: 41404.087
    num_steps_sampled: 35616000
    num_steps_trained: 35616000
    sample_time_ms: 96260.189
    update_time_ms: 3141.724
  iterations_since_restore: 1
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 37.227586206896554
    ram_util_percent: 11.819540229885058
  pid: 21723
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 361.0
    agent-3: 361.0
    agent-4: 248.5
    agent-5: 248.5
  policy_reward_mean:
    agent-0: 238.91145833333334
    agent-1: 238.91145833333334
    agent-2: 308.8229166666667
    agent-3: 308.8229166666667
    agent-4: 224.05729166666666
    agent-5: 224.05729166666666
  policy_reward_min:
    agent-0: 43.0
    agent-1: 43.0
    agent-2: 45.5
    agent-3: 45.5
    agent-4: 39.0
    agent-5: 39.0
  sampler_perf:
    mean_env_wait_ms: 24.744724496936072
    mean_inference_ms: 13.341420775765066
    mean_processing_ms: 53.45584260119306
  time_since_restore: 173.86383938789368
  time_this_iter_s: 173.86383938789368
  time_total_s: 47255.4093914032
  timestamp: 1637558283
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 35616000
  training_iteration: 371
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    371 |          47255.4 | 35616000 |  1543.58 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 31.84
    apples_agent-1_min: 18
    apples_agent-2_max: 423
    apples_agent-2_mean: 356.29
    apples_agent-2_min: 288
    apples_agent-3_max: 185
    apples_agent-3_mean: 106.86
    apples_agent-3_min: 44
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 387.58
    apples_agent-5_min: 296
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 430.08
    cleaning_beam_agent-0_min: 374
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 2.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 4.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 114.42
    cleaning_beam_agent-3_min: 49
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 537.23
    cleaning_beam_agent-4_min: 365
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-20-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1717.0
  episode_reward_mean: 1563.37
  episode_reward_min: 1278.0
  episodes_this_iter: 96
  episodes_total: 35712
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 24270.544
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3859720528125763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013667811872437596
        model: {}
        policy_loss: -0.0015148004749789834
        total_loss: 0.0015185715164989233
        vf_explained_var: 0.005451962351799011
        vf_loss: 35.76007843017578
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2631600499153137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007393612759187818
        model: {}
        policy_loss: -0.0016939220950007439
        total_loss: 0.0012991103576496243
        vf_explained_var: 0.059905797243118286
        vf_loss: 33.82257080078125
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31553688645362854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006709893932566047
        model: {}
        policy_loss: -0.0019339784048497677
        total_loss: 0.0033744992688298225
        vf_explained_var: 0.034490495920181274
        vf_loss: 57.967247009277344
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.914789617061615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011312981368973851
        model: {}
        policy_loss: -0.0025082160718739033
        total_loss: 0.0021511949598789215
        vf_explained_var: -0.030519187450408936
        vf_loss: 61.56315612792969
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8775621056556702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018179455073550344
        model: {}
        policy_loss: -0.002024988643825054
        total_loss: -0.0003552976995706558
        vf_explained_var: 0.04143482446670532
        vf_loss: 30.324058532714844
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2510829567909241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005342833464965224
        model: {}
        policy_loss: -0.001825536135584116
        total_loss: 0.0005973419174551964
        vf_explained_var: 0.11441940069198608
        vf_loss: 28.113563537597656
    load_time_ms: 30935.69
    num_steps_sampled: 35712000
    num_steps_trained: 35712000
    sample_time_ms: 95544.815
    update_time_ms: 1580.042
  iterations_since_restore: 2
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.615625
    ram_util_percent: 13.904687500000001
  pid: 21723
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 354.5
    agent-3: 354.5
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 243.33
    agent-1: 243.33
    agent-2: 310.95
    agent-3: 310.95
    agent-4: 227.405
    agent-5: 227.405
  policy_reward_min:
    agent-0: 190.0
    agent-1: 190.0
    agent-2: 252.5
    agent-3: 252.5
    agent-4: 186.0
    agent-5: 186.0
  sampler_perf:
    mean_env_wait_ms: 25.07954789208722
    mean_inference_ms: 12.93519562304374
    mean_processing_ms: 54.19847384133394
  time_since_restore: 308.12867522239685
  time_this_iter_s: 134.26483583450317
  time_total_s: 47389.6742272377
  timestamp: 1637558417
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 35712000
  training_iteration: 372
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    372 |          47389.7 | 35712000 |  1563.37 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.44
    apples_agent-1_min: 15
    apples_agent-2_max: 412
    apples_agent-2_mean: 355.21
    apples_agent-2_min: 288
    apples_agent-3_max: 194
    apples_agent-3_mean: 110.72
    apples_agent-3_min: 37
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 389.09
    apples_agent-5_min: 296
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 424.8
    cleaning_beam_agent-0_min: 373
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.9
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 4.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 213
    cleaning_beam_agent-3_mean: 103.26
    cleaning_beam_agent-3_min: 48
    cleaning_beam_agent-4_max: 614
    cleaning_beam_agent-4_mean: 545.89
    cleaning_beam_agent-4_min: 365
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-22-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1674.0
  episode_reward_mean: 1561.21
  episode_reward_min: 1283.0
  episodes_this_iter: 96
  episodes_total: 35808
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 22481.684
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38284412026405334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010576988570392132
        model: {}
        policy_loss: -0.0013800081796944141
        total_loss: 0.0015144299250096083
        vf_explained_var: 0.007982969284057617
        vf_loss: 35.15355682373047
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2573595941066742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005843430990353227
        model: {}
        policy_loss: -0.0015291362069547176
        total_loss: 0.001461029052734375
        vf_explained_var: 0.036581188440322876
        vf_loss: 34.13901138305664
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31412196159362793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009343705605715513
        model: {}
        policy_loss: -0.002000517211854458
        total_loss: 0.0029646973125636578
        vf_explained_var: 0.03494542837142944
        vf_loss: 54.713565826416016
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8948276042938232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013059366028755903
        model: {}
        policy_loss: -0.0027122527826577425
        total_loss: 0.0015553247649222612
        vf_explained_var: -0.022098898887634277
        vf_loss: 57.77177810668945
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8779946565628052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014888698933646083
        model: {}
        policy_loss: -0.0020961062982678413
        total_loss: -0.0006201365031301975
        vf_explained_var: 0.026872843503952026
        vf_loss: 29.467927932739258
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24056130647659302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006429468048736453
        model: {}
        policy_loss: -0.0017299683531746268
        total_loss: 0.000584861496463418
        vf_explained_var: 0.10942511260509491
        vf_loss: 27.060741424560547
    load_time_ms: 25797.666
    num_steps_sampled: 35808000
    num_steps_trained: 35808000
    sample_time_ms: 95269.976
    update_time_ms: 1059.234
  iterations_since_restore: 3
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.66467391304347
    ram_util_percent: 13.912500000000003
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 349.5
    agent-3: 349.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 241.13
    agent-1: 241.13
    agent-2: 311.14
    agent-3: 311.14
    agent-4: 228.335
    agent-5: 228.335
  policy_reward_min:
    agent-0: 194.5
    agent-1: 194.5
    agent-2: 252.5
    agent-3: 252.5
    agent-4: 186.0
    agent-5: 186.0
  sampler_perf:
    mean_env_wait_ms: 25.182472462380762
    mean_inference_ms: 12.74871293599728
    mean_processing_ms: 54.32690363532719
  time_since_restore: 437.365496635437
  time_this_iter_s: 129.23682141304016
  time_total_s: 47518.91104865074
  timestamp: 1637558547
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 35808000
  training_iteration: 373
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    373 |          47518.9 | 35808000 |  1561.21 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.23
    apples_agent-1_min: 12
    apples_agent-2_max: 413
    apples_agent-2_mean: 358.07
    apples_agent-2_min: 112
    apples_agent-3_max: 170
    apples_agent-3_mean: 109.17
    apples_agent-3_min: 26
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 385.87
    apples_agent-5_min: 113
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 431.0
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 2.72
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 5.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 101.25
    cleaning_beam_agent-3_min: 52
    cleaning_beam_agent-4_max: 615
    cleaning_beam_agent-4_mean: 540.37
    cleaning_beam_agent-4_min: 441
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 4.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-24-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1717.0
  episode_reward_mean: 1553.16
  episode_reward_min: 474.0
  episodes_this_iter: 96
  episodes_total: 35904
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 21556.803
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3848993182182312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012464375467970967
        model: {}
        policy_loss: -0.001645201351493597
        total_loss: 0.0016530007123947144
        vf_explained_var: 0.03730715811252594
        vf_loss: 39.44465637207031
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2685794234275818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015355213545262814
        model: {}
        policy_loss: -0.002281980589032173
        total_loss: 0.0009605600498616695
        vf_explained_var: 0.10355401039123535
        vf_loss: 36.76850891113281
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3095755875110626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011913222260773182
        model: {}
        policy_loss: -0.002148851752281189
        total_loss: 0.0035745156928896904
        vf_explained_var: 0.09903189539909363
        vf_loss: 62.38433837890625
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9188055992126465
        entropy_coeff: 0.0017600000137463212
        kl: 0.000727377540897578
        model: {}
        policy_loss: -0.002329760231077671
        total_loss: 0.0030152802355587482
        vf_explained_var: -0.0022195130586624146
        vf_loss: 69.43958282470703
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9128115177154541
        entropy_coeff: 0.0017600000137463212
        kl: 0.002992551075294614
        model: {}
        policy_loss: -0.002488322090357542
        total_loss: -0.0005821669474244118
        vf_explained_var: 0.02638290822505951
        vf_loss: 34.37889862060547
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25454023480415344
        entropy_coeff: 0.0017600000137463212
        kl: 0.000620252569206059
        model: {}
        policy_loss: -0.002050959039479494
        total_loss: 0.0005681072361767292
        vf_explained_var: 0.13459020853042603
        vf_loss: 30.51551055908203
    load_time_ms: 23218.599
    num_steps_sampled: 35904000
    num_steps_trained: 35904000
    sample_time_ms: 94890.001
    update_time_ms: 798.669
  iterations_since_restore: 4
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.37362637362637
    ram_util_percent: 13.909340659340662
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 347.5
    agent-3: 347.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 239.465
    agent-1: 239.465
    agent-2: 311.165
    agent-3: 311.165
    agent-4: 225.95
    agent-5: 225.95
  policy_reward_min:
    agent-0: 72.0
    agent-1: 72.0
    agent-2: 99.0
    agent-3: 99.0
    agent-4: 66.0
    agent-5: 66.0
  sampler_perf:
    mean_env_wait_ms: 25.184583847521324
    mean_inference_ms: 12.649394221537282
    mean_processing_ms: 54.32547194407298
  time_since_restore: 565.4737074375153
  time_this_iter_s: 128.10821080207825
  time_total_s: 47647.01925945282
  timestamp: 1637558675
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 35904000
  training_iteration: 374
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    374 |            47647 | 35904000 |  1553.16 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.29
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 28.71
    apples_agent-1_min: 5
    apples_agent-2_max: 444
    apples_agent-2_mean: 343.98
    apples_agent-2_min: 62
    apples_agent-3_max: 177
    apples_agent-3_mean: 107.88
    apples_agent-3_min: 14
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 366.71
    apples_agent-5_min: 65
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 426.54
    cleaning_beam_agent-0_min: 309
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 2.37
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 5.65
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 199
    cleaning_beam_agent-3_mean: 93.38
    cleaning_beam_agent-3_min: 41
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 514.12
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 4.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-26-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1701.0
  episode_reward_mean: 1499.79
  episode_reward_min: 236.0
  episodes_this_iter: 96
  episodes_total: 36000
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 20942.092
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39122793078422546
        entropy_coeff: 0.0017600000137463212
        kl: 0.001197431469336152
        model: {}
        policy_loss: -0.0017739641480147839
        total_loss: 0.0015215768944472075
        vf_explained_var: 0.12105345726013184
        vf_loss: 39.69139862060547
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2830120623111725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006746042054146528
        model: {}
        policy_loss: -0.0018678405322134495
        total_loss: 0.0015059472061693668
        vf_explained_var: 0.1447068452835083
        vf_loss: 38.63460159301758
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3288443982601166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007605556747876108
        model: {}
        policy_loss: -0.0021330593153834343
        total_loss: 0.004074889235198498
        vf_explained_var: 0.14880304038524628
        vf_loss: 67.77207946777344
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9008544683456421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017430316656827927
        model: {}
        policy_loss: -0.002649847185239196
        total_loss: 0.0037077434826642275
        vf_explained_var: 0.006148472428321838
        vf_loss: 79.21307373046875
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9223494529724121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020406688563525677
        model: {}
        policy_loss: -0.0019836798310279846
        total_loss: 0.00034712767228484154
        vf_explained_var: 0.047434210777282715
        vf_loss: 39.28633117675781
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26922446489334106
        entropy_coeff: 0.0017600000137463212
        kl: 0.00039732735604047775
        model: {}
        policy_loss: -0.002010113326832652
        total_loss: 0.0007353241089731455
        vf_explained_var: 0.218196839094162
        vf_loss: 32.14309310913086
    load_time_ms: 21964.667
    num_steps_sampled: 36000000
    num_steps_trained: 36000000
    sample_time_ms: 94516.122
    update_time_ms: 642.959
  iterations_since_restore: 5
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.32677595628415
    ram_util_percent: 13.926775956284153
  pid: 21723
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 357.0
    agent-3: 357.0
    agent-4: 249.5
    agent-5: 249.5
  policy_reward_mean:
    agent-0: 229.98
    agent-1: 229.98
    agent-2: 302.575
    agent-3: 302.575
    agent-4: 217.34
    agent-5: 217.34
  policy_reward_min:
    agent-0: 36.0
    agent-1: 36.0
    agent-2: 42.5
    agent-3: 42.5
    agent-4: 39.5
    agent-5: 39.5
  sampler_perf:
    mean_env_wait_ms: 25.12157990031262
    mean_inference_ms: 12.583283337314736
    mean_processing_ms: 54.24030054238029
  time_since_restore: 694.0545055866241
  time_this_iter_s: 128.5807981491089
  time_total_s: 47775.60005760193
  timestamp: 1637558804
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 36000000
  training_iteration: 375
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    375 |          47775.6 | 36000000 |  1499.79 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.2
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 30.9
    apples_agent-1_min: 8
    apples_agent-2_max: 405
    apples_agent-2_mean: 350.63
    apples_agent-2_min: 62
    apples_agent-3_max: 176
    apples_agent-3_mean: 111.62
    apples_agent-3_min: 15
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.38
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 378.31
    apples_agent-5_min: 65
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 433.12
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.15
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 5.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 96.56
    cleaning_beam_agent-3_min: 47
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 505.1
    cleaning_beam_agent-4_min: 356
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-28-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1684.0
  episode_reward_mean: 1535.06
  episode_reward_min: 236.0
  episodes_this_iter: 96
  episodes_total: 36096
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 20599.011
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.384878009557724
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014019433874636889
        model: {}
        policy_loss: -0.0015138762537389994
        total_loss: 0.0012388804461807013
        vf_explained_var: 0.044756799936294556
        vf_loss: 34.21380615234375
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27413010597229004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011500646360218525
        model: {}
        policy_loss: -0.00213660579174757
        total_loss: 0.0006956066936254501
        vf_explained_var: 0.07686705887317657
        vf_loss: 33.07495880126953
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31766703724861145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006349412724375725
        model: {}
        policy_loss: -0.001872741850093007
        total_loss: 0.0035229865461587906
        vf_explained_var: 0.0791044682264328
        vf_loss: 59.50856018066406
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8963570594787598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007588685839436948
        model: {}
        policy_loss: -0.002353181131184101
        total_loss: 0.0024656755849719048
        vf_explained_var: 0.011177986860275269
        vf_loss: 63.916969299316406
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.941203236579895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026716331485658884
        model: {}
        policy_loss: -0.002010634168982506
        total_loss: -0.00045825913548469543
        vf_explained_var: 0.040427714586257935
        vf_loss: 31.921960830688477
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25500205159187317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006008585914969444
        model: {}
        policy_loss: -0.001748630777001381
        total_loss: 0.0006373915821313858
        vf_explained_var: 0.14917106926441193
        vf_loss: 28.310726165771484
    load_time_ms: 20596.82
    num_steps_sampled: 36096000
    num_steps_trained: 36096000
    sample_time_ms: 94493.625
    update_time_ms: 539.023
  iterations_since_restore: 6
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.19613259668508
    ram_util_percent: 13.782872928176797
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 359.0
    agent-3: 359.0
    agent-4: 253.5
    agent-5: 253.5
  policy_reward_mean:
    agent-0: 235.905
    agent-1: 235.905
    agent-2: 310.075
    agent-3: 310.075
    agent-4: 221.55
    agent-5: 221.55
  policy_reward_min:
    agent-0: 36.0
    agent-1: 36.0
    agent-2: 42.5
    agent-3: 42.5
    agent-4: 39.5
    agent-5: 39.5
  sampler_perf:
    mean_env_wait_ms: 25.104007303149846
    mean_inference_ms: 12.54412438285651
    mean_processing_ms: 54.210500417079665
  time_since_restore: 821.175235748291
  time_this_iter_s: 127.12073016166687
  time_total_s: 47902.720787763596
  timestamp: 1637558931
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 36096000
  training_iteration: 376
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    376 |          47902.7 | 36096000 |  1535.06 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 0.26
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 30.73
    apples_agent-1_min: 14
    apples_agent-2_max: 416
    apples_agent-2_mean: 359.51
    apples_agent-2_min: 280
    apples_agent-3_max: 185
    apples_agent-3_mean: 107.73
    apples_agent-3_min: 40
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 384.92
    apples_agent-5_min: 164
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 423.62
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 33
    cleaning_beam_agent-1_mean: 2.75
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 4.81
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 218
    cleaning_beam_agent-3_mean: 101.34
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 498.04
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-30-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1691.0
  episode_reward_mean: 1562.89
  episode_reward_min: 1395.0
  episodes_this_iter: 96
  episodes_total: 36192
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 20350.609
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37991073727607727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017592236399650574
        model: {}
        policy_loss: -0.001388240372762084
        total_loss: 0.0014703671913594007
        vf_explained_var: 0.015151381492614746
        vf_loss: 35.217552185058594
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2729678750038147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008844129042699933
        model: {}
        policy_loss: -0.0016828083898872137
        total_loss: 0.0013185462448745966
        vf_explained_var: 0.02830834686756134
        vf_loss: 34.79011917114258
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31192851066589355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017103198915719986
        model: {}
        policy_loss: -0.0020655347034335136
        total_loss: 0.0031518181785941124
        vf_explained_var: 0.05334937572479248
        vf_loss: 57.610050201416016
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9147933125495911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014797013718634844
        model: {}
        policy_loss: -0.0023295136634260416
        total_loss: 0.002182372845709324
        vf_explained_var: -8.80211591720581e-05
        vf_loss: 61.17298126220703
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9335217475891113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023039320949465036
        model: {}
        policy_loss: -0.0018475791439414024
        total_loss: -0.00019749393686652184
        vf_explained_var: 0.04324887692928314
        vf_loss: 32.858829498291016
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24224497377872467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005957857938483357
        model: {}
        policy_loss: -0.0017690700478851795
        total_loss: 0.0008322556386701763
        vf_explained_var: 0.11962255835533142
        vf_loss: 30.258188247680664
    load_time_ms: 19616.336
    num_steps_sampled: 36192000
    num_steps_trained: 36192000
    sample_time_ms: 94374.095
    update_time_ms: 464.623
  iterations_since_restore: 7
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 52.21833333333333
    ram_util_percent: 14.095555555555555
  pid: 21723
  policy_reward_max:
    agent-0: 267.0
    agent-1: 267.0
    agent-2: 376.0
    agent-3: 376.0
    agent-4: 251.0
    agent-5: 251.0
  policy_reward_mean:
    agent-0: 240.095
    agent-1: 240.095
    agent-2: 313.88
    agent-3: 313.88
    agent-4: 227.47
    agent-5: 227.47
  policy_reward_min:
    agent-0: 188.0
    agent-1: 188.0
    agent-2: 255.5
    agent-3: 255.5
    agent-4: 101.0
    agent-5: 101.0
  sampler_perf:
    mean_env_wait_ms: 25.08338082591263
    mean_inference_ms: 12.514051414771172
    mean_processing_ms: 54.265271172571055
  time_since_restore: 947.5202922821045
  time_this_iter_s: 126.34505653381348
  time_total_s: 48029.06584429741
  timestamp: 1637559057
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 36192000
  training_iteration: 377
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    377 |          48029.1 | 36192000 |  1562.89 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.12
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.81
    apples_agent-1_min: 0
    apples_agent-2_max: 418
    apples_agent-2_mean: 355.86
    apples_agent-2_min: 13
    apples_agent-3_max: 177
    apples_agent-3_mean: 109.76
    apples_agent-3_min: 3
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 460
    apples_agent-5_mean: 383.66
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 414.96
    cleaning_beam_agent-0_min: 242
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 2.4
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 4.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 101.39
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 511.97
    cleaning_beam_agent-4_min: 412
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 4.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-33-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1680.0
  episode_reward_mean: 1548.51
  episode_reward_min: 40.0
  episodes_this_iter: 96
  episodes_total: 36288
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 20142.187
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3829818367958069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013251975178718567
        model: {}
        policy_loss: -0.0016226377338171005
        total_loss: 0.0013424130156636238
        vf_explained_var: 0.10262060165405273
        vf_loss: 36.370323181152344
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27456173300743103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015243346570059657
        model: {}
        policy_loss: -0.002152120228856802
        total_loss: 0.0010465222876518965
        vf_explained_var: 0.09491382539272308
        vf_loss: 36.79491424560547
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32321903109550476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008812573505565524
        model: {}
        policy_loss: -0.0018479153513908386
        total_loss: 0.0037282011471688747
        vf_explained_var: 0.11226701736450195
        vf_loss: 61.43611145019531
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9052781462669373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011567925103008747
        model: {}
        policy_loss: -0.002459505572915077
        total_loss: 0.0028753094375133514
        vf_explained_var: -0.0010835975408554077
        vf_loss: 69.26298522949219
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9246633052825928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008892879704944789
        model: {}
        policy_loss: -0.0017957263626158237
        total_loss: 0.0002542356960475445
        vf_explained_var: 0.034908607602119446
        vf_loss: 36.759822845458984
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2560432553291321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008012712933123112
        model: {}
        policy_loss: -0.002105340827256441
        total_loss: 0.000642457976937294
        vf_explained_var: 0.16239315271377563
        vf_loss: 31.971878051757812
    load_time_ms: 18892.059
    num_steps_sampled: 36288000
    num_steps_trained: 36288000
    sample_time_ms: 94342.735
    update_time_ms: 409.037
  iterations_since_restore: 8
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.24444444444445
    ram_util_percent: 13.98777777777778
  pid: 21723
  policy_reward_max:
    agent-0: 277.0
    agent-1: 277.0
    agent-2: 345.0
    agent-3: 345.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 240.985
    agent-1: 240.985
    agent-2: 308.23
    agent-3: 308.23
    agent-4: 225.04
    agent-5: 225.04
  policy_reward_min:
    agent-0: 6.5
    agent-1: 6.5
    agent-2: 10.0
    agent-3: 10.0
    agent-4: 3.5
    agent-5: 3.5
  sampler_perf:
    mean_env_wait_ms: 25.091325284973024
    mean_inference_ms: 12.501701817442209
    mean_processing_ms: 54.277863509137276
  time_since_restore: 1074.2636215686798
  time_this_iter_s: 126.74332928657532
  time_total_s: 48155.809173583984
  timestamp: 1637559184
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 36288000
  training_iteration: 378
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    378 |          48155.8 | 36288000 |  1548.51 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.15
    apples_agent-1_min: 15
    apples_agent-2_max: 426
    apples_agent-2_mean: 356.15
    apples_agent-2_min: 247
    apples_agent-3_max: 170
    apples_agent-3_mean: 109.32
    apples_agent-3_min: 46
    apples_agent-4_max: 46
    apples_agent-4_mean: 0.51
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 385.16
    apples_agent-5_min: 283
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 414.04
    cleaning_beam_agent-0_min: 326
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 2.31
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 4.05
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 107.16
    cleaning_beam_agent-3_min: 45
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 501.92
    cleaning_beam_agent-4_min: 405
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-35-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1709.0
  episode_reward_mean: 1556.77
  episode_reward_min: 1199.0
  episodes_this_iter: 96
  episodes_total: 36384
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 19981.089
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3771868050098419
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010423780186101794
        model: {}
        policy_loss: -0.001368420198559761
        total_loss: 0.0015441528521478176
        vf_explained_var: 0.03135627508163452
        vf_loss: 35.75608444213867
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27679532766342163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022664926946163177
        model: {}
        policy_loss: -0.0021972176618874073
        total_loss: 0.0007424700888805091
        vf_explained_var: 0.07237420976161957
        vf_loss: 34.25080871582031
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3186427056789398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010747869964689016
        model: {}
        policy_loss: -0.001969579141587019
        total_loss: 0.003567995736375451
        vf_explained_var: 0.05575966835021973
        vf_loss: 60.975467681884766
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9111893773078918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010369922965765
        model: {}
        policy_loss: -0.002430600579828024
        total_loss: 0.0025971177965402603
        vf_explained_var: -0.028749138116836548
        vf_loss: 66.3060302734375
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9425568580627441
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011481507681310177
        model: {}
        policy_loss: -0.0016387610230594873
        total_loss: -0.00011594919487833977
        vf_explained_var: 0.03687550127506256
        vf_loss: 31.808124542236328
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25176578760147095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005447434959933162
        model: {}
        policy_loss: -0.0017380253411829472
        total_loss: 0.0006802557036280632
        vf_explained_var: 0.1366121917963028
        vf_loss: 28.6096134185791
    load_time_ms: 18269.311
    num_steps_sampled: 36384000
    num_steps_trained: 36384000
    sample_time_ms: 94228.618
    update_time_ms: 365.524
  iterations_since_restore: 9
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.573184357541905
    ram_util_percent: 14.046927374301674
  pid: 21723
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 351.5
    agent-3: 351.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 239.49
    agent-1: 239.49
    agent-2: 312.345
    agent-3: 312.345
    agent-4: 226.55
    agent-5: 226.55
  policy_reward_min:
    agent-0: 178.0
    agent-1: 178.0
    agent-2: 229.5
    agent-3: 229.5
    agent-4: 170.5
    agent-5: 170.5
  sampler_perf:
    mean_env_wait_ms: 25.070926375360532
    mean_inference_ms: 12.478260334329526
    mean_processing_ms: 54.230572492258744
  time_since_restore: 1199.6513893604279
  time_this_iter_s: 125.38776779174805
  time_total_s: 48281.19694137573
  timestamp: 1637559310
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 36384000
  training_iteration: 379
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    379 |          48281.2 | 36384000 |  1556.77 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 32.15
    apples_agent-1_min: 14
    apples_agent-2_max: 415
    apples_agent-2_mean: 356.88
    apples_agent-2_min: 260
    apples_agent-3_max: 172
    apples_agent-3_mean: 107.86
    apples_agent-3_min: 49
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 387.42
    apples_agent-5_min: 290
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 415.51
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 32
    cleaning_beam_agent-1_mean: 2.71
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 4.65
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 246
    cleaning_beam_agent-3_mean: 103.82
    cleaning_beam_agent-3_min: 55
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 484.04
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-37-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1689.0
  episode_reward_mean: 1554.02
  episode_reward_min: 1123.0
  episodes_this_iter: 96
  episodes_total: 36480
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 19859.887
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37624526023864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012887166813015938
        model: {}
        policy_loss: -0.0014164283638820052
        total_loss: 0.001364110386930406
        vf_explained_var: 0.03254535794258118
        vf_loss: 34.422298431396484
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27654391527175903
        entropy_coeff: 0.0017600000137463212
        kl: 0.001596061047166586
        model: {}
        policy_loss: -0.001865140162408352
        total_loss: 0.001007626298815012
        vf_explained_var: 0.05673500895500183
        vf_loss: 33.588645935058594
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31410056352615356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007294506649486721
        model: {}
        policy_loss: -0.001973701873794198
        total_loss: 0.0035201332066208124
        vf_explained_var: 0.05514277517795563
        vf_loss: 60.46370315551758
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9205241799354553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001180713647045195
        model: {}
        policy_loss: -0.002575196325778961
        total_loss: 0.002170679159462452
        vf_explained_var: 0.00685197114944458
        vf_loss: 63.655391693115234
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9510833024978638
        entropy_coeff: 0.0017600000137463212
        kl: 0.002065931214019656
        model: {}
        policy_loss: -0.0018748566508293152
        total_loss: -0.00025900406762957573
        vf_explained_var: 0.01985473930835724
        vf_loss: 32.88953399658203
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2511812746524811
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010434172581881285
        model: {}
        policy_loss: -0.001775343669578433
        total_loss: 0.0006697855424135923
        vf_explained_var: 0.1416371762752533
        vf_loss: 28.868057250976562
    load_time_ms: 17778.67
    num_steps_sampled: 36480000
    num_steps_trained: 36480000
    sample_time_ms: 94126.667
    update_time_ms: 330.926
  iterations_since_restore: 10
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.77765363128492
    ram_util_percent: 14.022346368715084
  pid: 21723
  policy_reward_max:
    agent-0: 282.0
    agent-1: 282.0
    agent-2: 351.5
    agent-3: 351.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 239.7
    agent-1: 239.7
    agent-2: 310.965
    agent-3: 310.965
    agent-4: 226.345
    agent-5: 226.345
  policy_reward_min:
    agent-0: 177.5
    agent-1: 177.5
    agent-2: 195.0
    agent-3: 195.0
    agent-4: 179.5
    agent-5: 179.5
  sampler_perf:
    mean_env_wait_ms: 25.05777436926478
    mean_inference_ms: 12.464125419654028
    mean_processing_ms: 54.22380739814766
  time_since_restore: 1325.0833327770233
  time_this_iter_s: 125.43194341659546
  time_total_s: 48406.62888479233
  timestamp: 1637559436
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 36480000
  training_iteration: 380
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    380 |          48406.6 | 36480000 |  1554.02 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 0.37
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.02
    apples_agent-1_min: 13
    apples_agent-2_max: 419
    apples_agent-2_mean: 344.38
    apples_agent-2_min: 172
    apples_agent-3_max: 181
    apples_agent-3_mean: 107.82
    apples_agent-3_min: 26
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 378.11
    apples_agent-5_min: 170
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 407.31
    cleaning_beam_agent-0_min: 296
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 4.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 95.5
    cleaning_beam_agent-3_min: 44
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 476.26
    cleaning_beam_agent-4_min: 383
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 4.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-39-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1716.0
  episode_reward_mean: 1528.84
  episode_reward_min: 741.0
  episodes_this_iter: 96
  episodes_total: 36576
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18775.564
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38174909353256226
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013711298815906048
        model: {}
        policy_loss: -0.0016114444006234407
        total_loss: 0.0012505457270890474
        vf_explained_var: 0.08869220316410065
        vf_loss: 35.33601379394531
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27543771266937256
        entropy_coeff: 0.0017600000137463212
        kl: 0.001436868915334344
        model: {}
        policy_loss: -0.001853323308750987
        total_loss: 0.001117125153541565
        vf_explained_var: 0.10908325016498566
        vf_loss: 34.54939270019531
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32536283135414124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009618464391678572
        model: {}
        policy_loss: -0.0022453414276242256
        total_loss: 0.003294683527201414
        vf_explained_var: 0.10234956443309784
        vf_loss: 61.12479019165039
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.902342677116394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009395023807883263
        model: {}
        policy_loss: -0.002388902008533478
        total_loss: 0.0029339096508920193
        vf_explained_var: -0.016169577836990356
        vf_loss: 69.1075668334961
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9446730613708496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013958537019789219
        model: {}
        policy_loss: -0.0017268597148358822
        total_loss: 0.0002665584906935692
        vf_explained_var: 0.03425399959087372
        vf_loss: 36.5577278137207
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2580750584602356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0003936671419069171
        model: {}
        policy_loss: -0.0017719417810440063
        total_loss: 0.0009194444282911718
        vf_explained_var: 0.1663980484008789
        vf_loss: 31.45523452758789
    load_time_ms: 15109.558
    num_steps_sampled: 36576000
    num_steps_trained: 36576000
    sample_time_ms: 93859.307
    update_time_ms: 18.568
  iterations_since_restore: 11
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.97857142857142
    ram_util_percent: 14.063186813186814
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 354.5
    agent-3: 354.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 234.825
    agent-1: 234.825
    agent-2: 305.705
    agent-3: 305.705
    agent-4: 223.89
    agent-5: 223.89
  policy_reward_min:
    agent-0: 116.5
    agent-1: 116.5
    agent-2: 148.0
    agent-3: 148.0
    agent-4: 106.0
    agent-5: 106.0
  sampler_perf:
    mean_env_wait_ms: 25.052744287592347
    mean_inference_ms: 12.458124890544024
    mean_processing_ms: 54.24237380216437
  time_since_restore: 1452.3402512073517
  time_this_iter_s: 127.25691843032837
  time_total_s: 48533.885803222656
  timestamp: 1637559564
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 36576000
  training_iteration: 381
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    381 |          48533.9 | 36576000 |  1528.84 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 30.33
    apples_agent-1_min: 16
    apples_agent-2_max: 427
    apples_agent-2_mean: 356.78
    apples_agent-2_min: 285
    apples_agent-3_max: 185
    apples_agent-3_mean: 116.89
    apples_agent-3_min: 45
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 459
    apples_agent-5_mean: 386.38
    apples_agent-5_min: 321
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 412.28
    cleaning_beam_agent-0_min: 343
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.59
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 5.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 94.82
    cleaning_beam_agent-3_min: 45
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 472.46
    cleaning_beam_agent-4_min: 361
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-41-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1704.0
  episode_reward_mean: 1572.63
  episode_reward_min: 1345.0
  episodes_this_iter: 96
  episodes_total: 36672
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18779.951
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37555456161499023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012729441514238715
        model: {}
        policy_loss: -0.001348983496427536
        total_loss: 0.0013461513444781303
        vf_explained_var: 0.020160317420959473
        vf_loss: 33.55989074707031
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2677273750305176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012764926068484783
        model: {}
        policy_loss: -0.0015062445309013128
        total_loss: 0.0013100486248731613
        vf_explained_var: 0.03965774178504944
        vf_loss: 32.87370300292969
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.316489577293396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006462301826104522
        model: {}
        policy_loss: -0.0017725597135722637
        total_loss: 0.0034456998109817505
        vf_explained_var: 0.04974724352359772
        vf_loss: 57.752201080322266
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9242715835571289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006755709182471037
        model: {}
        policy_loss: -0.0021424645092338324
        total_loss: 0.0025270453188568354
        vf_explained_var: -0.029219061136245728
        vf_loss: 62.961666107177734
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9480075836181641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016809443477541208
        model: {}
        policy_loss: -0.0019350929651409388
        total_loss: -0.0005331924767233431
        vf_explained_var: 0.0347820520401001
        vf_loss: 30.702342987060547
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23706302046775818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007198711391538382
        model: {}
        policy_loss: -0.0016181948594748974
        total_loss: 0.0007408177480101585
        vf_explained_var: 0.13088996708393097
        vf_loss: 27.761737823486328
    load_time_ms: 14605.641
    num_steps_sampled: 36672000
    num_steps_trained: 36672000
    sample_time_ms: 93807.183
    update_time_ms: 18.524
  iterations_since_restore: 12
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.082608695652176
    ram_util_percent: 14.03315217391304
  pid: 21723
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 355.5
    agent-3: 355.5
    agent-4: 269.5
    agent-5: 269.5
  policy_reward_mean:
    agent-0: 239.605
    agent-1: 239.605
    agent-2: 317.54
    agent-3: 317.54
    agent-4: 229.17
    agent-5: 229.17
  policy_reward_min:
    agent-0: 202.0
    agent-1: 202.0
    agent-2: 253.0
    agent-3: 253.0
    agent-4: 189.5
    agent-5: 189.5
  sampler_perf:
    mean_env_wait_ms: 25.045101636172944
    mean_inference_ms: 12.463337640997695
    mean_processing_ms: 54.279527809130244
  time_since_restore: 1581.0693264007568
  time_this_iter_s: 128.72907519340515
  time_total_s: 48662.61487841606
  timestamp: 1637559692
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 36672000
  training_iteration: 382
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    382 |          48662.6 | 36672000 |  1572.63 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 0.38
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 29.84
    apples_agent-1_min: 7
    apples_agent-2_max: 409
    apples_agent-2_mean: 348.83
    apples_agent-2_min: 35
    apples_agent-3_max: 214
    apples_agent-3_mean: 114.25
    apples_agent-3_min: 9
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 375.62
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 411.69
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 2.26
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 4.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 89.28
    cleaning_beam_agent-3_min: 37
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 467.53
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-43-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1706.0
  episode_reward_mean: 1535.58
  episode_reward_min: 128.0
  episodes_this_iter: 96
  episodes_total: 36768
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18769.357
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3926704227924347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013421362964436412
        model: {}
        policy_loss: -0.0016556913033127785
        total_loss: 0.0015875508543103933
        vf_explained_var: 0.07339583337306976
        vf_loss: 39.34279251098633
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2714165449142456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011109092738479376
        model: {}
        policy_loss: -0.0018805144354701042
        total_loss: 0.0013939030468463898
        vf_explained_var: 0.11693167686462402
        vf_loss: 37.52058792114258
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3304532766342163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011112513020634651
        model: {}
        policy_loss: -0.0019353041425347328
        total_loss: 0.0036974791437387466
        vf_explained_var: 0.15308314561843872
        vf_loss: 62.14329528808594
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9060633778572083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009466764749959111
        model: {}
        policy_loss: -0.0024468651972711086
        total_loss: 0.0032570073381066322
        vf_explained_var: 0.0046370625495910645
        vf_loss: 72.98497009277344
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9467570781707764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016688905889168382
        model: {}
        policy_loss: -0.001954990206286311
        total_loss: 0.0003523727646097541
        vf_explained_var: 0.027559414505958557
        vf_loss: 39.73576354980469
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25674968957901
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008069486357271671
        model: {}
        policy_loss: -0.002182238968089223
        total_loss: 0.0007058822084218264
        vf_explained_var: 0.18256869912147522
        vf_loss: 33.39963150024414
    load_time_ms: 14591.426
    num_steps_sampled: 36768000
    num_steps_trained: 36768000
    sample_time_ms: 93785.158
    update_time_ms: 18.627
  iterations_since_restore: 13
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.57322404371585
    ram_util_percent: 14.084699453551908
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 348.5
    agent-3: 348.5
    agent-4: 271.5
    agent-5: 271.5
  policy_reward_mean:
    agent-0: 236.64
    agent-1: 236.64
    agent-2: 308.495
    agent-3: 308.495
    agent-4: 222.655
    agent-5: 222.655
  policy_reward_min:
    agent-0: 6.0
    agent-1: 6.0
    agent-2: 31.5
    agent-3: 31.5
    agent-4: 26.5
    agent-5: 26.5
  sampler_perf:
    mean_env_wait_ms: 25.029627135348413
    mean_inference_ms: 12.459148029001465
    mean_processing_ms: 54.31079430729074
  time_since_restore: 1709.8408880233765
  time_this_iter_s: 128.77156162261963
  time_total_s: 48791.38644003868
  timestamp: 1637559821
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 36768000
  training_iteration: 383
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    383 |          48791.4 | 36768000 |  1535.58 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.17
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 29.91
    apples_agent-1_min: 6
    apples_agent-2_max: 409
    apples_agent-2_mean: 347.43
    apples_agent-2_min: 50
    apples_agent-3_max: 178
    apples_agent-3_mean: 112.25
    apples_agent-3_min: 6
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.26
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 376.47
    apples_agent-5_min: 72
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 405.14
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 28
    cleaning_beam_agent-1_mean: 2.75
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 5.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 94.71
    cleaning_beam_agent-3_min: 52
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 460.63
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-45-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1664.0
  episode_reward_mean: 1532.99
  episode_reward_min: 272.0
  episodes_this_iter: 96
  episodes_total: 36864
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18778.267
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3899264931678772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019784346222877502
        model: {}
        policy_loss: -0.0017115231603384018
        total_loss: 0.0012543979100883007
        vf_explained_var: 0.045383796095848083
        vf_loss: 36.52146911621094
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2723314166069031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018514975672587752
        model: {}
        policy_loss: -0.002015594393014908
        total_loss: 0.0010233186185359955
        vf_explained_var: 0.07912471890449524
        vf_loss: 35.18170928955078
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3226495087146759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008937761886045337
        model: {}
        policy_loss: -0.00206947885453701
        total_loss: 0.0033763055689632893
        vf_explained_var: 0.08331635594367981
        vf_loss: 60.13630294799805
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9193598031997681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016234703361988068
        model: {}
        policy_loss: -0.0024611172266304493
        total_loss: 0.0025990516878664494
        vf_explained_var: -0.0174628347158432
        vf_loss: 66.78207397460938
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9524294137954712
        entropy_coeff: 0.0017600000137463212
        kl: 0.002114722738042474
        model: {}
        policy_loss: -0.0018964207265526056
        total_loss: -9.979493916034698e-05
        vf_explained_var: 0.032524511218070984
        vf_loss: 34.72855758666992
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26614928245544434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010189448948949575
        model: {}
        policy_loss: -0.002215971704572439
        total_loss: 0.00030327681452035904
        vf_explained_var: 0.16511915624141693
        vf_loss: 29.87647247314453
    load_time_ms: 14464.389
    num_steps_sampled: 36864000
    num_steps_trained: 36864000
    sample_time_ms: 93819.884
    update_time_ms: 18.8
  iterations_since_restore: 14
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.072527472527476
    ram_util_percent: 14.03076923076923
  pid: 21723
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 357.5
    agent-3: 357.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 236.105
    agent-1: 236.105
    agent-2: 307.6
    agent-3: 307.6
    agent-4: 222.79
    agent-5: 222.79
  policy_reward_min:
    agent-0: 52.5
    agent-1: 52.5
    agent-2: 43.0
    agent-3: 43.0
    agent-4: 40.5
    agent-5: 40.5
  sampler_perf:
    mean_env_wait_ms: 25.020551557616603
    mean_inference_ms: 12.458256075033528
    mean_processing_ms: 54.336789632041516
  time_since_restore: 1837.1118643283844
  time_this_iter_s: 127.27097630500793
  time_total_s: 48918.65741634369
  timestamp: 1637559949
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 36864000
  training_iteration: 384
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    384 |          48918.7 | 36864000 |  1532.99 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 29.82
    apples_agent-1_min: 17
    apples_agent-2_max: 420
    apples_agent-2_mean: 353.1
    apples_agent-2_min: 102
    apples_agent-3_max: 174
    apples_agent-3_mean: 106.53
    apples_agent-3_min: 40
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 461
    apples_agent-5_mean: 383.54
    apples_agent-5_min: 169
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 413.91
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 30
    cleaning_beam_agent-1_mean: 2.09
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 4.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 229
    cleaning_beam_agent-3_mean: 105.54
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 460.47
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 3.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-47-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1710.0
  episode_reward_mean: 1547.75
  episode_reward_min: 608.0
  episodes_this_iter: 96
  episodes_total: 36960
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18824.225
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37951987981796265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012235650792717934
        model: {}
        policy_loss: -0.0013126316480338573
        total_loss: 0.0016021099872887135
        vf_explained_var: 0.030602559447288513
        vf_loss: 35.82683563232422
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.260642945766449
        entropy_coeff: 0.0017600000137463212
        kl: 0.00144616374745965
        model: {}
        policy_loss: -0.001971811056137085
        total_loss: 0.0010615615174174309
        vf_explained_var: 0.057826802134513855
        vf_loss: 34.920875549316406
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3163028955459595
        entropy_coeff: 0.0017600000137463212
        kl: 0.001203279010951519
        model: {}
        policy_loss: -0.002095259726047516
        total_loss: 0.0035913786850869656
        vf_explained_var: 0.0917937159538269
        vf_loss: 62.43320846557617
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.923369288444519
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014683132758364081
        model: {}
        policy_loss: -0.0025025494396686554
        total_loss: 0.0028806535992771387
        vf_explained_var: -0.019507139921188354
        vf_loss: 70.08319854736328
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9816449284553528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018189770635217428
        model: {}
        policy_loss: -0.0021691014990210533
        total_loss: -0.0006447324994951487
        vf_explained_var: 0.03302924335002899
        vf_loss: 32.52048110961914
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.255687952041626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004692826187238097
        model: {}
        policy_loss: -0.001819675206206739
        total_loss: 0.0005784390959888697
        vf_explained_var: 0.15377801656723022
        vf_loss: 28.48120880126953
    load_time_ms: 14131.102
    num_steps_sampled: 36960000
    num_steps_trained: 36960000
    sample_time_ms: 93865.636
    update_time_ms: 18.694
  iterations_since_restore: 15
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 51.83072625698325
    ram_util_percent: 14.200000000000001
  pid: 21723
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 376.5
    agent-3: 376.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 241.27
    agent-1: 241.27
    agent-2: 307.195
    agent-3: 307.195
    agent-4: 225.41
    agent-5: 225.41
  policy_reward_min:
    agent-0: 103.0
    agent-1: 103.0
    agent-2: 102.5
    agent-3: 102.5
    agent-4: 98.5
    agent-5: 98.5
  sampler_perf:
    mean_env_wait_ms: 25.00522107758882
    mean_inference_ms: 12.452348372289196
    mean_processing_ms: 54.330555684621174
  time_since_restore: 1963.2591354846954
  time_this_iter_s: 126.14727115631104
  time_total_s: 49044.8046875
  timestamp: 1637560075
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 36960000
  training_iteration: 385
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    385 |          49044.8 | 36960000 |  1547.75 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.89
    apples_agent-1_min: 16
    apples_agent-2_max: 447
    apples_agent-2_mean: 351.62
    apples_agent-2_min: 102
    apples_agent-3_max: 176
    apples_agent-3_mean: 106.55
    apples_agent-3_min: 33
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 459
    apples_agent-5_mean: 384.82
    apples_agent-5_min: 169
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 410.58
    cleaning_beam_agent-0_min: 294
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 2.73
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 4.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 106.3
    cleaning_beam_agent-3_min: 50
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 438.38
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 3.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-50-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1710.0
  episode_reward_mean: 1543.04
  episode_reward_min: 608.0
  episodes_this_iter: 96
  episodes_total: 37056
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18802.638
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37797868251800537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018450276693329215
        model: {}
        policy_loss: -0.00158771313726902
        total_loss: 0.001189863309264183
        vf_explained_var: 0.05827324092388153
        vf_loss: 34.42808532714844
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26780444383621216
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012669680872932076
        model: {}
        policy_loss: -0.0019856300204992294
        total_loss: 0.0008889082819223404
        vf_explained_var: 0.08614866435527802
        vf_loss: 33.45870590209961
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3190537989139557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010035285959020257
        model: {}
        policy_loss: -0.0019942980725318193
        total_loss: 0.0034699756652116776
        vf_explained_var: 0.09101289510726929
        vf_loss: 60.25804901123047
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9100995063781738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013446931261569262
        model: {}
        policy_loss: -0.002480387454852462
        total_loss: 0.0026301443576812744
        vf_explained_var: -0.011079534888267517
        vf_loss: 67.12300109863281
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9834244847297668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008038377854973078
        model: {}
        policy_loss: -0.0017171257641166449
        total_loss: -7.653748616576195e-05
        vf_explained_var: 0.02280762791633606
        vf_loss: 33.714134216308594
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26191163063049316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009625789243727922
        model: {}
        policy_loss: -0.0021082484163343906
        total_loss: 0.000415610964410007
        vf_explained_var: 0.14406436681747437
        vf_loss: 29.84821319580078
    load_time_ms: 14094.192
    num_steps_sampled: 37056000
    num_steps_trained: 37056000
    sample_time_ms: 93787.521
    update_time_ms: 18.735
  iterations_since_restore: 16
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.62849162011173
    ram_util_percent: 14.202793296089384
  pid: 21723
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 349.0
    agent-3: 349.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 238.395
    agent-1: 238.395
    agent-2: 306.995
    agent-3: 306.995
    agent-4: 226.13
    agent-5: 226.13
  policy_reward_min:
    agent-0: 92.0
    agent-1: 92.0
    agent-2: 102.5
    agent-3: 102.5
    agent-4: 98.5
    agent-5: 98.5
  sampler_perf:
    mean_env_wait_ms: 24.987847344104903
    mean_inference_ms: 12.450830390807841
    mean_processing_ms: 54.34989559339393
  time_since_restore: 2089.007947921753
  time_this_iter_s: 125.7488124370575
  time_total_s: 49170.55349993706
  timestamp: 1637560201
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 37056000
  training_iteration: 386
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    386 |          49170.6 | 37056000 |  1543.04 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.2
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.56
    apples_agent-1_min: 11
    apples_agent-2_max: 414
    apples_agent-2_mean: 354.78
    apples_agent-2_min: 123
    apples_agent-3_max: 180
    apples_agent-3_mean: 107.87
    apples_agent-3_min: 30
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 385.52
    apples_agent-5_min: 134
    cleaning_beam_agent-0_max: 487
    cleaning_beam_agent-0_mean: 419.15
    cleaning_beam_agent-0_min: 353
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.49
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 77
    cleaning_beam_agent-2_mean: 5.48
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 231
    cleaning_beam_agent-3_mean: 111.56
    cleaning_beam_agent-3_min: 59
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 449.62
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-52-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1669.0
  episode_reward_mean: 1547.9
  episode_reward_min: 525.0
  episodes_this_iter: 96
  episodes_total: 37152
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18797.093
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3872612714767456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009183272486552596
        model: {}
        policy_loss: -0.001416393555700779
        total_loss: 0.0014674803242087364
        vf_explained_var: 0.05400307476520538
        vf_loss: 35.65453338623047
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2704177498817444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017802889924496412
        model: {}
        policy_loss: -0.001979112159460783
        total_loss: 0.0010423522908240557
        vf_explained_var: 0.07635019719600677
        vf_loss: 34.97394561767578
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3125019073486328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010036518797278404
        model: {}
        policy_loss: -0.0019822665490210056
        total_loss: 0.003516271011903882
        vf_explained_var: 0.06220702826976776
        vf_loss: 60.485374450683594
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9159561395645142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009249750291928649
        model: {}
        policy_loss: -0.0026998165994882584
        total_loss: 0.0024687787517905235
        vf_explained_var: -0.05018681287765503
        vf_loss: 67.80677795410156
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9730594158172607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012596880551427603
        model: {}
        policy_loss: -0.0020862058736383915
        total_loss: -0.0004428623942658305
        vf_explained_var: 0.023179814219474792
        vf_loss: 33.55928039550781
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2576436698436737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006123093189671636
        model: {}
        policy_loss: -0.001812637783586979
        total_loss: 0.0007078149355947971
        vf_explained_var: 0.1377803534269333
        vf_loss: 29.739032745361328
    load_time_ms: 14050.732
    num_steps_sampled: 37152000
    num_steps_trained: 37152000
    sample_time_ms: 93851.728
    update_time_ms: 18.949
  iterations_since_restore: 17
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.86166666666666
    ram_util_percent: 14.139999999999999
  pid: 21723
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 356.0
    agent-3: 356.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 239.415
    agent-1: 239.415
    agent-2: 307.94
    agent-3: 307.94
    agent-4: 226.595
    agent-5: 226.595
  policy_reward_min:
    agent-0: 77.0
    agent-1: 77.0
    agent-2: 104.0
    agent-3: 104.0
    agent-4: 81.5
    agent-5: 81.5
  sampler_perf:
    mean_env_wait_ms: 24.9796575981074
    mean_inference_ms: 12.44792282102486
    mean_processing_ms: 54.34777470539864
  time_since_restore: 2215.502847671509
  time_this_iter_s: 126.49489974975586
  time_total_s: 49297.04839968681
  timestamp: 1637560328
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 37152000
  training_iteration: 387
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    387 |            49297 | 37152000 |   1547.9 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 29.83
    apples_agent-1_min: 15
    apples_agent-2_max: 431
    apples_agent-2_mean: 348.92
    apples_agent-2_min: 233
    apples_agent-3_max: 173
    apples_agent-3_mean: 113.66
    apples_agent-3_min: 48
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.45
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 381.58
    apples_agent-5_min: 245
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 420.35
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 4.99
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 114.17
    cleaning_beam_agent-3_min: 47
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 446.27
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 4.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-54-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1703.0
  episode_reward_mean: 1545.64
  episode_reward_min: 1016.0
  episodes_this_iter: 96
  episodes_total: 37248
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18786.232
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38992995023727417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012686641421169043
        model: {}
        policy_loss: -0.0014213465619832277
        total_loss: 0.0014712191186845303
        vf_explained_var: 0.058689191937446594
        vf_loss: 35.78841018676758
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26468801498413086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006948724621906877
        model: {}
        policy_loss: -0.0018338204827159643
        total_loss: 0.0012075414415448904
        vf_explained_var: 0.08572742342948914
        vf_loss: 35.07209014892578
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.318578839302063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011337766190990806
        model: {}
        policy_loss: -0.0020991135388612747
        total_loss: 0.00339106610044837
        vf_explained_var: 0.06644381582736969
        vf_loss: 60.50878143310547
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9096910357475281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014035729691386223
        model: {}
        policy_loss: -0.0027538957074284554
        total_loss: 0.0019834269769489765
        vf_explained_var: 0.024305373430252075
        vf_loss: 63.38374328613281
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9899994134902954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014197796117514372
        model: {}
        policy_loss: -0.0019846507348120213
        total_loss: -0.0005478032398968935
        vf_explained_var: 0.05789536237716675
        vf_loss: 31.79245376586914
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2637990415096283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006797893438488245
        model: {}
        policy_loss: -0.0016763097373768687
        total_loss: 0.0007548229768872261
        vf_explained_var: 0.14023976027965546
        vf_loss: 28.954185485839844
    load_time_ms: 13989.679
    num_steps_sampled: 37248000
    num_steps_trained: 37248000
    sample_time_ms: 93829.883
    update_time_ms: 18.708
  iterations_since_restore: 18
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.8061111111111
    ram_util_percent: 14.109999999999998
  pid: 21723
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 359.0
    agent-3: 359.0
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 237.665
    agent-1: 237.665
    agent-2: 310.54
    agent-3: 310.54
    agent-4: 224.615
    agent-5: 224.615
  policy_reward_min:
    agent-0: 147.0
    agent-1: 147.0
    agent-2: 198.0
    agent-3: 198.0
    agent-4: 151.0
    agent-5: 151.0
  sampler_perf:
    mean_env_wait_ms: 24.978608304650816
    mean_inference_ms: 12.44453140245851
    mean_processing_ms: 54.35192449898639
  time_since_restore: 2341.2848868370056
  time_this_iter_s: 125.78203916549683
  time_total_s: 49422.83043885231
  timestamp: 1637560454
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 37248000
  training_iteration: 388
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    388 |          49422.8 | 37248000 |  1545.64 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.12
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 32.42
    apples_agent-1_min: 18
    apples_agent-2_max: 411
    apples_agent-2_mean: 347.24
    apples_agent-2_min: 165
    apples_agent-3_max: 191
    apples_agent-3_mean: 109.41
    apples_agent-3_min: 39
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 384.61
    apples_agent-5_min: 241
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 424.8
    cleaning_beam_agent-0_min: 340
    cleaning_beam_agent-1_max: 35
    cleaning_beam_agent-1_mean: 2.61
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 6.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 222
    cleaning_beam_agent-3_mean: 116.95
    cleaning_beam_agent-3_min: 51
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 449.06
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 3.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-56-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1675.0
  episode_reward_mean: 1545.33
  episode_reward_min: 838.0
  episodes_this_iter: 96
  episodes_total: 37344
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18805.581
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3887859284877777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017422442324459553
        model: {}
        policy_loss: -0.0015581271145492792
        total_loss: 0.0011727293021976948
        vf_explained_var: 0.05259467661380768
        vf_loss: 34.15121078491211
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2613831162452698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012558379676192999
        model: {}
        policy_loss: -0.0016739824786782265
        total_loss: 0.0011392920278012753
        vf_explained_var: 0.10055412352085114
        vf_loss: 32.733116149902344
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3183854818344116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010641487315297127
        model: {}
        policy_loss: -0.002189260208979249
        total_loss: 0.0032843351364135742
        vf_explained_var: 0.05477963387966156
        vf_loss: 60.33955001831055
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9135754108428955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007365761557593942
        model: {}
        policy_loss: -0.0024602171033620834
        total_loss: 0.0023462953977286816
        vf_explained_var: -0.004886507987976074
        vf_loss: 64.1440658569336
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9765948057174683
        entropy_coeff: 0.0017600000137463212
        kl: 0.002729814499616623
        model: {}
        policy_loss: -0.0020728674717247486
        total_loss: -0.0004511482547968626
        vf_explained_var: 0.02471354603767395
        vf_loss: 33.40524673461914
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2536807656288147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008677709847688675
        model: {}
        policy_loss: -0.0019577147904783487
        total_loss: 0.0006083128973841667
        vf_explained_var: 0.12167666852474213
        vf_loss: 30.125030517578125
    load_time_ms: 14014.89
    num_steps_sampled: 37344000
    num_steps_trained: 37344000
    sample_time_ms: 93871.787
    update_time_ms: 18.911
  iterations_since_restore: 19
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.738333333333344
    ram_util_percent: 14.184444444444447
  pid: 21723
  policy_reward_max:
    agent-0: 283.0
    agent-1: 283.0
    agent-2: 350.0
    agent-3: 350.0
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 239.33
    agent-1: 239.33
    agent-2: 307.29
    agent-3: 307.29
    agent-4: 226.045
    agent-5: 226.045
  policy_reward_min:
    agent-0: 136.5
    agent-1: 136.5
    agent-2: 159.0
    agent-3: 159.0
    agent-4: 123.5
    agent-5: 123.5
  sampler_perf:
    mean_env_wait_ms: 24.976647253132604
    mean_inference_ms: 12.440444413026384
    mean_processing_ms: 54.35490060462641
  time_since_restore: 2467.543670654297
  time_this_iter_s: 126.25878381729126
  time_total_s: 49549.0892226696
  timestamp: 1637560580
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 37344000
  training_iteration: 389
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    389 |          49549.1 | 37344000 |  1545.33 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.08
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 32.45
    apples_agent-1_min: 10
    apples_agent-2_max: 440
    apples_agent-2_mean: 361.77
    apples_agent-2_min: 251
    apples_agent-3_max: 191
    apples_agent-3_mean: 117.1
    apples_agent-3_min: 53
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 381.93
    apples_agent-5_min: 294
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 435.48
    cleaning_beam_agent-0_min: 380
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 4.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 109.23
    cleaning_beam_agent-3_min: 63
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 438.58
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.79
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_00-58-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1722.0
  episode_reward_mean: 1571.46
  episode_reward_min: 1112.0
  episodes_this_iter: 96
  episodes_total: 37440
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18811.485
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3842570185661316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018632910214364529
        model: {}
        policy_loss: -0.0013329628854990005
        total_loss: 0.0014355015009641647
        vf_explained_var: 0.027752041816711426
        vf_loss: 34.447593688964844
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25523170828819275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008655597921460867
        model: {}
        policy_loss: -0.001885250210762024
        total_loss: 0.001123935915529728
        vf_explained_var: 0.035319000482559204
        vf_loss: 34.58393859863281
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30347347259521484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012787338346242905
        model: {}
        policy_loss: -0.0020339805632829666
        total_loss: 0.0034110688138753176
        vf_explained_var: 0.05381903052330017
        vf_loss: 59.7916259765625
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9199173450469971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010668158065527678
        model: {}
        policy_loss: -0.002344168722629547
        total_loss: 0.002407838823273778
        vf_explained_var: 0.004320427775382996
        vf_loss: 63.71061325073242
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.987271249294281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034910067915916443
        model: {}
        policy_loss: -0.0021814533974975348
        total_loss: -0.0009606208186596632
        vf_explained_var: 0.01429075002670288
        vf_loss: 29.584304809570312
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26023155450820923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007952572777867317
        model: {}
        policy_loss: -0.001889732200652361
        total_loss: 0.00032029300928115845
        vf_explained_var: 0.11145320534706116
        vf_loss: 26.680286407470703
    load_time_ms: 14014.633
    num_steps_sampled: 37440000
    num_steps_trained: 37440000
    sample_time_ms: 93893.225
    update_time_ms: 18.744
  iterations_since_restore: 20
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.49269662921349
    ram_util_percent: 14.23876404494382
  pid: 21723
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 359.5
    agent-3: 359.5
    agent-4: 254.0
    agent-5: 254.0
  policy_reward_mean:
    agent-0: 241.21
    agent-1: 241.21
    agent-2: 318.77
    agent-3: 318.77
    agent-4: 225.75
    agent-5: 225.75
  policy_reward_min:
    agent-0: 170.5
    agent-1: 170.5
    agent-2: 215.5
    agent-3: 215.5
    agent-4: 170.0
    agent-5: 170.0
  sampler_perf:
    mean_env_wait_ms: 24.97385162786825
    mean_inference_ms: 12.440559888235612
    mean_processing_ms: 54.356224676915936
  time_since_restore: 2593.2595880031586
  time_this_iter_s: 125.7159173488617
  time_total_s: 49674.80514001846
  timestamp: 1637560706
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 37440000
  training_iteration: 390
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    390 |          49674.8 | 37440000 |  1571.46 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.15
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 30.71
    apples_agent-1_min: 7
    apples_agent-2_max: 414
    apples_agent-2_mean: 346.15
    apples_agent-2_min: 35
    apples_agent-3_max: 186
    apples_agent-3_mean: 112.99
    apples_agent-3_min: 2
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.29
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 369.2
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 436.96
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 2.83
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 5.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 102.39
    cleaning_beam_agent-3_min: 54
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 426.2
    cleaning_beam_agent-4_min: 275
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 4.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-00-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1665.0
  episode_reward_mean: 1517.5
  episode_reward_min: 158.0
  episodes_this_iter: 96
  episodes_total: 37536
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18811.752
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.400721937417984
        entropy_coeff: 0.0017600000137463212
        kl: 0.002028744202107191
        model: {}
        policy_loss: -0.0014167320914566517
        total_loss: 0.0016955058090388775
        vf_explained_var: 0.1184084415435791
        vf_loss: 38.175086975097656
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26660069823265076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008063158602453768
        model: {}
        policy_loss: -0.0016938046319410205
        total_loss: 0.0015318908262997866
        vf_explained_var: 0.14001673460006714
        vf_loss: 36.94914627075195
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3196415603160858
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013219749089330435
        model: {}
        policy_loss: -0.0019443491473793983
        total_loss: 0.004436543211340904
        vf_explained_var: 0.13461370766162872
        vf_loss: 69.43461608886719
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9118301272392273
        entropy_coeff: 0.0017600000137463212
        kl: 0.001125701586715877
        model: {}
        policy_loss: -0.0024535702541470528
        total_loss: 0.0038868198171257973
        vf_explained_var: 0.010317400097846985
        vf_loss: 79.4521255493164
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9675090909004211
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017633240204304457
        model: {}
        policy_loss: -0.002020949497818947
        total_loss: 0.00029868027195334435
        vf_explained_var: 0.006228089332580566
        vf_loss: 40.22443389892578
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28289273381233215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007500043138861656
        model: {}
        policy_loss: -0.0023241406306624413
        total_loss: 0.0004975046031177044
        vf_explained_var: 0.18012776970863342
        vf_loss: 33.19537353515625
    load_time_ms: 13878.538
    num_steps_sampled: 37536000
    num_steps_trained: 37536000
    sample_time_ms: 93952.766
    update_time_ms: 18.752
  iterations_since_restore: 21
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.372527472527466
    ram_util_percent: 14.128571428571426
  pid: 21723
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 358.0
    agent-3: 358.0
    agent-4: 254.5
    agent-5: 254.5
  policy_reward_mean:
    agent-0: 233.07
    agent-1: 233.07
    agent-2: 307.56
    agent-3: 307.56
    agent-4: 218.12
    agent-5: 218.12
  policy_reward_min:
    agent-0: 27.0
    agent-1: 27.0
    agent-2: 31.5
    agent-3: 31.5
    agent-4: 20.5
    agent-5: 20.5
  sampler_perf:
    mean_env_wait_ms: 24.96929250014812
    mean_inference_ms: 12.438842245038705
    mean_processing_ms: 54.37064446674243
  time_since_restore: 2719.7615942955017
  time_this_iter_s: 126.50200629234314
  time_total_s: 49801.307146310806
  timestamp: 1637560833
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 37536000
  training_iteration: 391
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    391 |          49801.3 | 37536000 |   1517.5 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.18
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 29.58
    apples_agent-1_min: 16
    apples_agent-2_max: 420
    apples_agent-2_mean: 354.61
    apples_agent-2_min: 205
    apples_agent-3_max: 193
    apples_agent-3_mean: 118.16
    apples_agent-3_min: 49
    apples_agent-4_max: 38
    apples_agent-4_mean: 0.56
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 377.39
    apples_agent-5_min: 228
    cleaning_beam_agent-0_max: 481
    cleaning_beam_agent-0_mean: 432.99
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 3.1
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 4.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 101.17
    cleaning_beam_agent-3_min: 41
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 426.7
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-02-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1691.0
  episode_reward_mean: 1550.41
  episode_reward_min: 936.0
  episodes_this_iter: 96
  episodes_total: 37632
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18777.227
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3883584141731262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018964811461046338
        model: {}
        policy_loss: -0.0015777233056724072
        total_loss: 0.0011469051241874695
        vf_explained_var: 0.024096816778182983
        vf_loss: 34.081417083740234
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2604594826698303
        entropy_coeff: 0.0017600000137463212
        kl: 0.001022685901261866
        model: {}
        policy_loss: -0.001657004002481699
        total_loss: 0.001129063544794917
        vf_explained_var: 0.07485319674015045
        vf_loss: 32.444793701171875
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30656516551971436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013184008421376348
        model: {}
        policy_loss: -0.002019688952714205
        total_loss: 0.0034867702051997185
        vf_explained_var: 0.05266973376274109
        vf_loss: 60.46015167236328
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9079433083534241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009397139074280858
        model: {}
        policy_loss: -0.0024346907157450914
        total_loss: 0.002481122501194477
        vf_explained_var: -0.012990757822990417
        vf_loss: 65.137939453125
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9563257098197937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020939933601766825
        model: {}
        policy_loss: -0.002142096869647503
        total_loss: -0.0006928574293851852
        vf_explained_var: 0.025624096393585205
        vf_loss: 31.323734283447266
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27516910433769226
        entropy_coeff: 0.0017600000137463212
        kl: 0.001014775363728404
        model: {}
        policy_loss: -0.0018550145905464888
        total_loss: 0.0005106329917907715
        vf_explained_var: 0.11409544944763184
        vf_loss: 28.49949073791504
    load_time_ms: 13643.804
    num_steps_sampled: 37632000
    num_steps_trained: 37632000
    sample_time_ms: 93805.689
    update_time_ms: 18.674
  iterations_since_restore: 22
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.02090395480225
    ram_util_percent: 14.147457627118646
  pid: 21723
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 349.0
    agent-3: 349.0
    agent-4: 251.0
    agent-5: 251.0
  policy_reward_mean:
    agent-0: 237.955
    agent-1: 237.955
    agent-2: 314.1
    agent-3: 314.1
    agent-4: 223.15
    agent-5: 223.15
  policy_reward_min:
    agent-0: 150.5
    agent-1: 150.5
    agent-2: 183.5
    agent-3: 183.5
    agent-4: 134.0
    agent-5: 134.0
  sampler_perf:
    mean_env_wait_ms: 24.953340084019548
    mean_inference_ms: 12.432687255571206
    mean_processing_ms: 54.351398726688664
  time_since_restore: 2844.311919927597
  time_this_iter_s: 124.55032563209534
  time_total_s: 49925.8574719429
  timestamp: 1637560958
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 37632000
  training_iteration: 392
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    392 |          49925.9 | 37632000 |  1550.41 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 0.29
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 29.17
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 355.34
    apples_agent-2_min: 115
    apples_agent-3_max: 202
    apples_agent-3_mean: 122.81
    apples_agent-3_min: 36
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 388.19
    apples_agent-5_min: 130
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 421.12
    cleaning_beam_agent-0_min: 309
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 2.26
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 5.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 96.21
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 511
    cleaning_beam_agent-4_mean: 435.67
    cleaning_beam_agent-4_min: 357
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 4.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-04-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1720.0
  episode_reward_mean: 1576.17
  episode_reward_min: 586.0
  episodes_this_iter: 96
  episodes_total: 37728
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18776.493
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3813670873641968
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017303754575550556
        model: {}
        policy_loss: -0.0014446747954934835
        total_loss: 0.0013586236163973808
        vf_explained_var: 0.034076303243637085
        vf_loss: 34.74504852294922
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25464874505996704
        entropy_coeff: 0.0017600000137463212
        kl: 0.001079054782167077
        model: {}
        policy_loss: -0.0016599791124463081
        total_loss: 0.0012294412590563297
        vf_explained_var: 0.0754147469997406
        vf_loss: 33.37602996826172
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30443084239959717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005920638795942068
        model: {}
        policy_loss: -0.0018455572426319122
        total_loss: 0.003889278508722782
        vf_explained_var: 0.07951429486274719
        vf_loss: 62.706363677978516
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9073551893234253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016648177988827229
        model: {}
        policy_loss: -0.002793459687381983
        total_loss: 0.0026966987643390894
        vf_explained_var: -0.028261542320251465
        vf_loss: 70.87105560302734
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9778902530670166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018892391817644238
        model: {}
        policy_loss: -0.0018042693845927715
        total_loss: -0.00013431115075945854
        vf_explained_var: 0.013300701975822449
        vf_loss: 33.910465240478516
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.271053671836853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007812345284037292
        model: {}
        policy_loss: -0.0018870974890887737
        total_loss: 0.0007912381552159786
        vf_explained_var: 0.09324420988559723
        vf_loss: 31.553911209106445
    load_time_ms: 13437.716
    num_steps_sampled: 37728000
    num_steps_trained: 37728000
    sample_time_ms: 93622.729
    update_time_ms: 18.862
  iterations_since_restore: 23
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.09830508474576
    ram_util_percent: 14.107909604519774
  pid: 21723
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 359.0
    agent-3: 359.0
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 241.44
    agent-1: 241.44
    agent-2: 318.55
    agent-3: 318.55
    agent-4: 228.095
    agent-5: 228.095
  policy_reward_min:
    agent-0: 95.0
    agent-1: 95.0
    agent-2: 109.5
    agent-3: 109.5
    agent-4: 88.5
    agent-5: 88.5
  sampler_perf:
    mean_env_wait_ms: 24.936102456090254
    mean_inference_ms: 12.428887316694643
    mean_processing_ms: 54.33074796803754
  time_since_restore: 2969.213093519211
  time_this_iter_s: 124.90117359161377
  time_total_s: 50050.758645534515
  timestamp: 1637561083
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 37728000
  training_iteration: 393
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    393 |          50050.8 | 37728000 |  1576.17 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 30.42
    apples_agent-1_min: 18
    apples_agent-2_max: 417
    apples_agent-2_mean: 361.64
    apples_agent-2_min: 301
    apples_agent-3_max: 202
    apples_agent-3_mean: 118.93
    apples_agent-3_min: 45
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 386.51
    apples_agent-5_min: 297
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 410.7
    cleaning_beam_agent-0_min: 351
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 2.19
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 3.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 96.3
    cleaning_beam_agent-3_min: 47
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 423.46
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-06-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1674.0
  episode_reward_mean: 1582.27
  episode_reward_min: 1373.0
  episodes_this_iter: 96
  episodes_total: 37824
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18774.899
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3810451626777649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018561168108135462
        model: {}
        policy_loss: -0.001782972365617752
        total_loss: 0.0009263092651963234
        vf_explained_var: 0.006721228361129761
        vf_loss: 33.79916763305664
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2520914375782013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009285732521675527
        model: {}
        policy_loss: -0.001476936275139451
        total_loss: 0.001264848280698061
        vf_explained_var: 0.06772215664386749
        vf_loss: 31.854663848876953
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29826638102531433
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009755360661074519
        model: {}
        policy_loss: -0.0016930252313613892
        total_loss: 0.0036220792680978775
        vf_explained_var: 0.014152124524116516
        vf_loss: 58.4005241394043
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9014753103256226
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010741776786744595
        model: {}
        policy_loss: -0.0024142619222402573
        total_loss: 0.002030114410445094
        vf_explained_var: 0.002054542303085327
        vf_loss: 60.309722900390625
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.993523120880127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017827440751716495
        model: {}
        policy_loss: -0.0018043778836727142
        total_loss: -0.00036734528839588165
        vf_explained_var: -0.0034025609493255615
        vf_loss: 31.856340408325195
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25907355546951294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007181499386206269
        model: {}
        policy_loss: -0.0017300881445407867
        total_loss: 0.0006296494975686073
        vf_explained_var: 0.10792693495750427
        vf_loss: 28.15705108642578
    load_time_ms: 13409.796
    num_steps_sampled: 37824000
    num_steps_trained: 37824000
    sample_time_ms: 93533.365
    update_time_ms: 18.818
  iterations_since_restore: 24
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 52.47222222222222
    ram_util_percent: 14.312222222222227
  pid: 21723
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 363.0
    agent-3: 363.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 241.28
    agent-1: 241.28
    agent-2: 320.29
    agent-3: 320.29
    agent-4: 229.565
    agent-5: 229.565
  policy_reward_min:
    agent-0: 204.0
    agent-1: 204.0
    agent-2: 279.5
    agent-3: 279.5
    agent-4: 180.5
    agent-5: 180.5
  sampler_perf:
    mean_env_wait_ms: 24.918853016585206
    mean_inference_ms: 12.42214823844337
    mean_processing_ms: 54.3162759456037
  time_since_restore: 3095.2998971939087
  time_this_iter_s: 126.08680367469788
  time_total_s: 50176.84544920921
  timestamp: 1637561209
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 37824000
  training_iteration: 394
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    394 |          50176.8 | 37824000 |  1582.27 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 0.34
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.39
    apples_agent-1_min: 19
    apples_agent-2_max: 427
    apples_agent-2_mean: 351.58
    apples_agent-2_min: 167
    apples_agent-3_max: 193
    apples_agent-3_mean: 113.87
    apples_agent-3_min: 32
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 384.12
    apples_agent-5_min: 202
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 401.58
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 2.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 70
    cleaning_beam_agent-2_mean: 4.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 91.09
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 411.76
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 3.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-08-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1711.0
  episode_reward_mean: 1553.99
  episode_reward_min: 806.0
  episodes_this_iter: 96
  episodes_total: 37920
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18753.831
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39531511068344116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022611706517636776
        model: {}
        policy_loss: -0.0016631563194096088
        total_loss: 0.0012047290802001953
        vf_explained_var: 0.05169762670993805
        vf_loss: 35.63638687133789
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25634101033210754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006122252671048045
        model: {}
        policy_loss: -0.0015357444062829018
        total_loss: 0.0014608409255743027
        vf_explained_var: 0.08289432525634766
        vf_loss: 34.477455139160156
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31149646639823914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010099593782797456
        model: {}
        policy_loss: -0.001807074761018157
        total_loss: 0.003838253440335393
        vf_explained_var: 0.06808429956436157
        vf_loss: 61.93560791015625
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8994839191436768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009348064195364714
        model: {}
        policy_loss: -0.0024863146245479584
        total_loss: 0.002612418495118618
        vf_explained_var: -0.00032773613929748535
        vf_loss: 66.81827545166016
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.995048999786377
        entropy_coeff: 0.0017600000137463212
        kl: 0.00182890216819942
        model: {}
        policy_loss: -0.0019032348645851016
        total_loss: -0.00028995575848966837
        vf_explained_var: 0.019585028290748596
        vf_loss: 33.6456413269043
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26820042729377747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010388944065198302
        model: {}
        policy_loss: -0.0019215039210394025
        total_loss: 0.00044270779471844435
        vf_explained_var: 0.17322979867458344
        vf_loss: 28.3624267578125
    load_time_ms: 13387.007
    num_steps_sampled: 37920000
    num_steps_trained: 37920000
    sample_time_ms: 93565.336
    update_time_ms: 18.325
  iterations_since_restore: 25
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.90055865921788
    ram_util_percent: 14.192178770949722
  pid: 21723
  policy_reward_max:
    agent-0: 286.5
    agent-1: 286.5
    agent-2: 360.5
    agent-3: 360.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 238.665
    agent-1: 238.665
    agent-2: 312.185
    agent-3: 312.185
    agent-4: 226.145
    agent-5: 226.145
  policy_reward_min:
    agent-0: 102.5
    agent-1: 102.5
    agent-2: 178.5
    agent-3: 178.5
    agent-4: 122.0
    agent-5: 122.0
  sampler_perf:
    mean_env_wait_ms: 24.905587187282958
    mean_inference_ms: 12.419081641883661
    mean_processing_ms: 54.31576529140462
  time_since_restore: 3221.310974597931
  time_this_iter_s: 126.01107740402222
  time_total_s: 50302.856526613235
  timestamp: 1637561335
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 37920000
  training_iteration: 395
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    395 |          50302.9 | 37920000 |  1553.99 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 32.28
    apples_agent-1_min: 18
    apples_agent-2_max: 424
    apples_agent-2_mean: 354.99
    apples_agent-2_min: 287
    apples_agent-3_max: 236
    apples_agent-3_mean: 121.09
    apples_agent-3_min: 40
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 383.86
    apples_agent-5_min: 298
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 398.25
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.96
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 5.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 94.73
    cleaning_beam_agent-3_min: 43
    cleaning_beam_agent-4_max: 475
    cleaning_beam_agent-4_mean: 405.95
    cleaning_beam_agent-4_min: 319
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-11-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1687.0
  episode_reward_mean: 1572.07
  episode_reward_min: 1385.0
  episodes_this_iter: 96
  episodes_total: 38016
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18762.661
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3941280245780945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019612754695117474
        model: {}
        policy_loss: -0.001609686529263854
        total_loss: 0.0010206247679889202
        vf_explained_var: -0.0007623285055160522
        vf_loss: 33.23976135253906
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24751940369606018
        entropy_coeff: 0.0017600000137463212
        kl: 0.001220178441144526
        model: {}
        policy_loss: -0.0015078156720846891
        total_loss: 0.0012303604744374752
        vf_explained_var: 0.046840593218803406
        vf_loss: 31.738079071044922
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3063904047012329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012978239683434367
        model: {}
        policy_loss: -0.0021403455175459385
        total_loss: 0.0032677771523594856
        vf_explained_var: 0.010985463857650757
        vf_loss: 59.47370147705078
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8976750373840332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010542126838117838
        model: {}
        policy_loss: -0.00255172373726964
        total_loss: 0.002145166974514723
        vf_explained_var: -0.030801445245742798
        vf_loss: 62.767974853515625
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9971307516098022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019855122081935406
        model: {}
        policy_loss: -0.0017427285201847553
        total_loss: -0.0004796127323061228
        vf_explained_var: 0.00895005464553833
        vf_loss: 30.180646896362305
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2602924108505249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006386307068169117
        model: {}
        policy_loss: -0.0015975134447216988
        total_loss: 0.00072140758857131
        vf_explained_var: 0.08748230338096619
        vf_loss: 27.77035140991211
    load_time_ms: 13384.047
    num_steps_sampled: 38016000
    num_steps_trained: 38016000
    sample_time_ms: 93524.454
    update_time_ms: 18.101
  iterations_since_restore: 26
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.51675977653631
    ram_util_percent: 14.2659217877095
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 360.5
    agent-3: 360.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 242.595
    agent-1: 242.595
    agent-2: 316.69
    agent-3: 316.69
    agent-4: 226.75
    agent-5: 226.75
  policy_reward_min:
    agent-0: 201.5
    agent-1: 201.5
    agent-2: 265.0
    agent-3: 265.0
    agent-4: 185.0
    agent-5: 185.0
  sampler_perf:
    mean_env_wait_ms: 24.892948258698404
    mean_inference_ms: 12.419678913476808
    mean_processing_ms: 54.311272273819085
  time_since_restore: 3346.7087004184723
  time_this_iter_s: 125.39772582054138
  time_total_s: 50428.25425243378
  timestamp: 1637561461
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 38016000
  training_iteration: 396
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    396 |          50428.3 | 38016000 |  1572.07 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 0.59
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.29
    apples_agent-1_min: 8
    apples_agent-2_max: 427
    apples_agent-2_mean: 348.44
    apples_agent-2_min: 16
    apples_agent-3_max: 211
    apples_agent-3_mean: 111.46
    apples_agent-3_min: 8
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 461
    apples_agent-5_mean: 381.32
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 448
    cleaning_beam_agent-0_mean: 391.28
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.23
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 4.62
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 94.21
    cleaning_beam_agent-3_min: 48
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 411.64
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 72
    cleaning_beam_agent-5_mean: 4.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-13-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1701.0
  episode_reward_mean: 1535.0
  episode_reward_min: 102.0
  episodes_this_iter: 96
  episodes_total: 38112
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18774.194
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4055640399456024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013348670909181237
        model: {}
        policy_loss: -0.0018305480480194092
        total_loss: 0.0013364608166739345
        vf_explained_var: 0.1252838522195816
        vf_loss: 38.808021545410156
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25606074929237366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006246287957765162
        model: {}
        policy_loss: -0.0018404205329716206
        total_loss: 0.0015709548024460673
        vf_explained_var: 0.1287761926651001
        vf_loss: 38.62040328979492
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31701380014419556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010265068849548697
        model: {}
        policy_loss: -0.0020618708804249763
        total_loss: 0.003851043526083231
        vf_explained_var: 0.1378607153892517
        vf_loss: 64.70858001708984
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8865636587142944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018578717717900872
        model: {}
        policy_loss: -0.0025365056935697794
        total_loss: 0.0033209959510713816
        vf_explained_var: 0.008332714438438416
        vf_loss: 74.17853546142578
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0031201839447021
        entropy_coeff: 0.0017600000137463212
        kl: 0.002733689034357667
        model: {}
        policy_loss: -0.0021315086632966995
        total_loss: -0.00013307668268680573
        vf_explained_var: -0.007519543170928955
        vf_loss: 37.639244079589844
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26931220293045044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006810007616877556
        model: {}
        policy_loss: -0.002191910520195961
        total_loss: 0.0005433941259980202
        vf_explained_var: 0.13987760245800018
        vf_loss: 32.09294891357422
    load_time_ms: 13405.417
    num_steps_sampled: 38112000
    num_steps_trained: 38112000
    sample_time_ms: 93427.136
    update_time_ms: 17.975
  iterations_since_restore: 27
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.30223463687151
    ram_util_percent: 14.212290502793296
  pid: 21723
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 355.0
    agent-3: 355.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 235.135
    agent-1: 235.135
    agent-2: 308.57
    agent-3: 308.57
    agent-4: 223.795
    agent-5: 223.795
  policy_reward_min:
    agent-0: 19.0
    agent-1: 19.0
    agent-2: 20.0
    agent-3: 20.0
    agent-4: 12.0
    agent-5: 12.0
  sampler_perf:
    mean_env_wait_ms: 24.87842812244435
    mean_inference_ms: 12.418833532049137
    mean_processing_ms: 54.314505219110316
  time_since_restore: 3472.5898468494415
  time_this_iter_s: 125.88114643096924
  time_total_s: 50554.135398864746
  timestamp: 1637561587
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 38112000
  training_iteration: 397
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    397 |          50554.1 | 38112000 |     1535 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 45
    apples_agent-1_mean: 29.76
    apples_agent-1_min: 13
    apples_agent-2_max: 416
    apples_agent-2_mean: 353.48
    apples_agent-2_min: 288
    apples_agent-3_max: 178
    apples_agent-3_mean: 117.91
    apples_agent-3_min: 38
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 385.5
    apples_agent-5_min: 320
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 387.01
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 3.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 90.09
    cleaning_beam_agent-3_min: 49
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 416.59
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-15-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1708.0
  episode_reward_mean: 1560.48
  episode_reward_min: 1367.0
  episodes_this_iter: 96
  episodes_total: 38208
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18798.897
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3972277045249939
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017056629294529557
        model: {}
        policy_loss: -0.0016606601420789957
        total_loss: 0.0010520194191485643
        vf_explained_var: 0.022268950939178467
        vf_loss: 34.118019104003906
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25506848096847534
        entropy_coeff: 0.0017600000137463212
        kl: 0.001426896546036005
        model: {}
        policy_loss: -0.0018544802442193031
        total_loss: 0.0009575104340910912
        vf_explained_var: 0.06666050851345062
        vf_loss: 32.60908508300781
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30889514088630676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011828459100797772
        model: {}
        policy_loss: -0.002181063173338771
        total_loss: 0.0034303609281778336
        vf_explained_var: 0.03885047137737274
        vf_loss: 61.550811767578125
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.871119499206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.00175474863499403
        model: {}
        policy_loss: -0.0028045023791491985
        total_loss: 0.0021094768308103085
        vf_explained_var: 0.002441495656967163
        vf_loss: 64.47154235839844
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0139400959014893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019947802647948265
        model: {}
        policy_loss: -0.0019295848906040192
        total_loss: -0.0006188517436385155
        vf_explained_var: 0.012094616889953613
        vf_loss: 30.952669143676758
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25892260670661926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007640011608600616
        model: {}
        policy_loss: -0.0018988216761499643
        total_loss: 0.00041080848313868046
        vf_explained_var: 0.11714178323745728
        vf_loss: 27.65337371826172
    load_time_ms: 13562.256
    num_steps_sampled: 38208000
    num_steps_trained: 38208000
    sample_time_ms: 93416.912
    update_time_ms: 18.143
  iterations_since_restore: 28
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.994475138121544
    ram_util_percent: 14.88342541436464
  pid: 21723
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 363.0
    agent-3: 363.0
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 237.51
    agent-1: 237.51
    agent-2: 316.135
    agent-3: 316.135
    agent-4: 226.595
    agent-5: 226.595
  policy_reward_min:
    agent-0: 200.5
    agent-1: 200.5
    agent-2: 262.0
    agent-3: 262.0
    agent-4: 197.0
    agent-5: 197.0
  sampler_perf:
    mean_env_wait_ms: 24.868980863220806
    mean_inference_ms: 12.417660476535318
    mean_processing_ms: 54.32261275542919
  time_since_restore: 3600.0846586227417
  time_this_iter_s: 127.49481177330017
  time_total_s: 50681.630210638046
  timestamp: 1637561715
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 38208000
  training_iteration: 398
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    398 |          50681.6 | 38208000 |  1560.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.74
    apples_agent-1_min: 1
    apples_agent-2_max: 432
    apples_agent-2_mean: 348.16
    apples_agent-2_min: 31
    apples_agent-3_max: 191
    apples_agent-3_mean: 120.29
    apples_agent-3_min: 8
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.28
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 378.04
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 393.04
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 37
    cleaning_beam_agent-1_mean: 2.56
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 3.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 89.03
    cleaning_beam_agent-3_min: 42
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 409.12
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-17-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1729.0
  episode_reward_mean: 1544.41
  episode_reward_min: 219.0
  episodes_this_iter: 96
  episodes_total: 38304
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18785.605
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4060939848423004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014251647517085075
        model: {}
        policy_loss: -0.0013780072331428528
        total_loss: 0.0014537610113620758
        vf_explained_var: 0.09676221013069153
        vf_loss: 35.46494674682617
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2600059509277344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018185406224802136
        model: {}
        policy_loss: -0.0018280302174389362
        total_loss: 0.0012588826939463615
        vf_explained_var: 0.09771133959293365
        vf_loss: 35.44525146484375
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3120908737182617
        entropy_coeff: 0.0017600000137463212
        kl: 0.001141368062235415
        model: {}
        policy_loss: -0.001886383630335331
        total_loss: 0.003965976648032665
        vf_explained_var: 0.09541565179824829
        vf_loss: 64.01641082763672
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8878223896026611
        entropy_coeff: 0.0017600000137463212
        kl: 0.001031490508466959
        model: {}
        policy_loss: -0.002593016717582941
        total_loss: 0.002960712183266878
        vf_explained_var: 0.0016513466835021973
        vf_loss: 71.1629409790039
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0276544094085693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020536964293569326
        model: {}
        policy_loss: -0.002086303196847439
        total_loss: -0.00034805433824658394
        vf_explained_var: 0.024226263165473938
        vf_loss: 35.46920394897461
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2574309706687927
        entropy_coeff: 0.0017600000137463212
        kl: 0.000999213196337223
        model: {}
        policy_loss: -0.00200579222291708
        total_loss: 0.0006955669960007071
        vf_explained_var: 0.13181830942630768
        vf_loss: 31.544334411621094
    load_time_ms: 13574.264
    num_steps_sampled: 38304000
    num_steps_trained: 38304000
    sample_time_ms: 93367.16
    update_time_ms: 18.063
  iterations_since_restore: 29
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.82333333333334
    ram_util_percent: 15.589999999999996
  pid: 21723
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 355.5
    agent-3: 355.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 236.21
    agent-1: 236.21
    agent-2: 313.93
    agent-3: 313.93
    agent-4: 222.065
    agent-5: 222.065
  policy_reward_min:
    agent-0: 40.5
    agent-1: 40.5
    agent-2: 39.5
    agent-3: 39.5
    agent-4: 29.5
    agent-5: 29.5
  sampler_perf:
    mean_env_wait_ms: 24.854323287442195
    mean_inference_ms: 12.41711440278007
    mean_processing_ms: 54.32636719141312
  time_since_restore: 3725.824600458145
  time_this_iter_s: 125.73994183540344
  time_total_s: 50807.37015247345
  timestamp: 1637561840
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 38304000
  training_iteration: 399
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    399 |          50807.4 | 38304000 |  1544.41 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.42
    apples_agent-1_min: 16
    apples_agent-2_max: 426
    apples_agent-2_mean: 354.27
    apples_agent-2_min: 288
    apples_agent-3_max: 206
    apples_agent-3_mean: 127.53
    apples_agent-3_min: 69
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 385.97
    apples_agent-5_min: 326
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 394.98
    cleaning_beam_agent-0_min: 351
    cleaning_beam_agent-1_max: 37
    cleaning_beam_agent-1_mean: 2.95
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 3.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 84.91
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 476
    cleaning_beam_agent-4_mean: 390.98
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-19-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1720.0
  episode_reward_mean: 1565.71
  episode_reward_min: 1347.0
  episodes_this_iter: 96
  episodes_total: 38400
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18769.539
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39460963010787964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016365472692996264
        model: {}
        policy_loss: -0.00133775663562119
        total_loss: 0.0015217099571600556
        vf_explained_var: 0.02211017906665802
        vf_loss: 35.539794921875
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2557789981365204
        entropy_coeff: 0.0017600000137463212
        kl: 0.001181972911581397
        model: {}
        policy_loss: -0.0017551584169268608
        total_loss: 0.0011372677981853485
        vf_explained_var: 0.08268611133098602
        vf_loss: 33.425960540771484
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2985753118991852
        entropy_coeff: 0.0017600000137463212
        kl: 0.001089336583390832
        model: {}
        policy_loss: -0.0017481367103755474
        total_loss: 0.0036989361979067326
        vf_explained_var: 0.044177234172821045
        vf_loss: 59.72564697265625
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8667800426483154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014647113857790828
        model: {}
        policy_loss: -0.002628231653943658
        total_loss: 0.0022047720849514008
        vf_explained_var: 0.0024204254150390625
        vf_loss: 63.5853385925293
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0250868797302246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024087680503726006
        model: {}
        policy_loss: -0.0021016194950789213
        total_loss: -0.0009094583801925182
        vf_explained_var: 0.0031863003969192505
        vf_loss: 29.963146209716797
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2572011351585388
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008932689088396728
        model: {}
        policy_loss: -0.0017574431840330362
        total_loss: 0.0005057831294834614
        vf_explained_var: 0.10092486441135406
        vf_loss: 27.15901756286621
    load_time_ms: 13641.253
    num_steps_sampled: 38400000
    num_steps_trained: 38400000
    sample_time_ms: 93322.461
    update_time_ms: 17.803
  iterations_since_restore: 30
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.54804469273742
    ram_util_percent: 15.621229050279329
  pid: 21723
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 363.5
    agent-3: 363.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 235.985
    agent-1: 235.985
    agent-2: 319.455
    agent-3: 319.455
    agent-4: 227.415
    agent-5: 227.415
  policy_reward_min:
    agent-0: 138.0
    agent-1: 138.0
    agent-2: 266.5
    agent-3: 266.5
    agent-4: 187.0
    agent-5: 187.0
  sampler_perf:
    mean_env_wait_ms: 24.83894139947298
    mean_inference_ms: 12.415915987928086
    mean_processing_ms: 54.32570608725111
  time_since_restore: 3851.587906599045
  time_this_iter_s: 125.76330614089966
  time_total_s: 50933.13345861435
  timestamp: 1637561966
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 38400000
  training_iteration: 400
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    400 |          50933.1 | 38400000 |  1565.71 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.12
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 30.86
    apples_agent-1_min: 6
    apples_agent-2_max: 448
    apples_agent-2_mean: 348.44
    apples_agent-2_min: 18
    apples_agent-3_max: 184
    apples_agent-3_mean: 121.2
    apples_agent-3_min: 14
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 376.99
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 388.73
    cleaning_beam_agent-0_min: 301
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 2.13
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 3.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 81.18
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 515
    cleaning_beam_agent-4_mean: 410.18
    cleaning_beam_agent-4_min: 305
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 4.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-21-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1683.0
  episode_reward_mean: 1542.42
  episode_reward_min: 137.0
  episodes_this_iter: 96
  episodes_total: 38496
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18759.522
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40453284978866577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012859596172347665
        model: {}
        policy_loss: -0.001696284394711256
        total_loss: 0.0012036408297717571
        vf_explained_var: 0.06262253224849701
        vf_loss: 36.11903381347656
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2600758671760559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010403735795989633
        model: {}
        policy_loss: -0.001973218284547329
        total_loss: 0.0010773791000247002
        vf_explained_var: 0.08859631419181824
        vf_loss: 35.08332443237305
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3039611577987671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009157857857644558
        model: {}
        policy_loss: -0.0019051837734878063
        total_loss: 0.003949636127799749
        vf_explained_var: 0.11548730731010437
        vf_loss: 63.89794921875
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8614223599433899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013516193721443415
        model: {}
        policy_loss: -0.0024544592015445232
        total_loss: 0.003466316731646657
        vf_explained_var: -0.012945324182510376
        vf_loss: 74.36880493164062
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9966051578521729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020175445824861526
        model: {}
        policy_loss: -0.002093276707455516
        total_loss: -0.0003058293368667364
        vf_explained_var: 0.023887470364570618
        vf_loss: 35.414703369140625
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.263633131980896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008082690183073282
        model: {}
        policy_loss: -0.0023778537288308144
        total_loss: 0.00035032047890126705
        vf_explained_var: 0.11731290817260742
        vf_loss: 31.921676635742188
    load_time_ms: 13775.873
    num_steps_sampled: 38496000
    num_steps_trained: 38496000
    sample_time_ms: 93209.671
    update_time_ms: 17.899
  iterations_since_restore: 31
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.846703296703296
    ram_util_percent: 15.61153846153846
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 357.0
    agent-3: 357.0
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 234.375
    agent-1: 234.375
    agent-2: 314.68
    agent-3: 314.68
    agent-4: 222.155
    agent-5: 222.155
  policy_reward_min:
    agent-0: 23.0
    agent-1: 23.0
    agent-2: 24.5
    agent-3: 24.5
    agent-4: 21.0
    agent-5: 21.0
  sampler_perf:
    mean_env_wait_ms: 24.825999453124126
    mean_inference_ms: 12.414214838327911
    mean_processing_ms: 54.32934952414891
  time_since_restore: 3978.2003400325775
  time_this_iter_s: 126.61243343353271
  time_total_s: 51059.74589204788
  timestamp: 1637562094
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 38496000
  training_iteration: 401
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    401 |          51059.7 | 38496000 |  1542.42 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 32.1
    apples_agent-1_min: 14
    apples_agent-2_max: 424
    apples_agent-2_mean: 352.78
    apples_agent-2_min: 226
    apples_agent-3_max: 189
    apples_agent-3_mean: 126.04
    apples_agent-3_min: 53
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.47
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 380.04
    apples_agent-5_min: 130
    cleaning_beam_agent-0_max: 462
    cleaning_beam_agent-0_mean: 394.24
    cleaning_beam_agent-0_min: 309
    cleaning_beam_agent-1_max: 28
    cleaning_beam_agent-1_mean: 2.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 3.9
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 88.77
    cleaning_beam_agent-3_min: 44
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 429.95
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-23-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1713.0
  episode_reward_mean: 1563.24
  episode_reward_min: 921.0
  episodes_this_iter: 96
  episodes_total: 38592
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18769.838
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4000173807144165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013950952561572194
        model: {}
        policy_loss: -0.0011948924511671066
        total_loss: 0.0015215426683425903
        vf_explained_var: 0.034007444977760315
        vf_loss: 34.20470428466797
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25055503845214844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012016298715025187
        model: {}
        policy_loss: -0.0017066476866602898
        total_loss: 0.0011285175569355488
        vf_explained_var: 0.07493451237678528
        vf_loss: 32.761417388916016
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2987329363822937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009929370135068893
        model: {}
        policy_loss: -0.0018313112668693066
        total_loss: 0.0038971593603491783
        vf_explained_var: 0.07667165994644165
        vf_loss: 62.54241180419922
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8906382918357849
        entropy_coeff: 0.0017600000137463212
        kl: 0.001393393613398075
        model: {}
        policy_loss: -0.002434674184769392
        total_loss: 0.0028062965720891953
        vf_explained_var: 0.012899890542030334
        vf_loss: 68.08491516113281
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.000566840171814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026043616235256195
        model: {}
        policy_loss: -0.002276419196277857
        total_loss: -0.0005550326313823462
        vf_explained_var: 0.014295265078544617
        vf_loss: 34.82386779785156
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25729602575302124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009118724847212434
        model: {}
        policy_loss: -0.0019665928557515144
        total_loss: 0.0007390277460217476
        vf_explained_var: 0.10600356757640839
        vf_loss: 31.584611892700195
    load_time_ms: 13875.867
    num_steps_sampled: 38592000
    num_steps_trained: 38592000
    sample_time_ms: 93244.433
    update_time_ms: 18.219
  iterations_since_restore: 32
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.5217877094972
    ram_util_percent: 15.654748603351955
  pid: 21723
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 363.0
    agent-3: 363.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 240.54
    agent-1: 240.54
    agent-2: 316.89
    agent-3: 316.89
    agent-4: 224.19
    agent-5: 224.19
  policy_reward_min:
    agent-0: 153.0
    agent-1: 153.0
    agent-2: 181.0
    agent-3: 181.0
    agent-4: 74.5
    agent-5: 74.5
  sampler_perf:
    mean_env_wait_ms: 24.817652352645336
    mean_inference_ms: 12.413158186665077
    mean_processing_ms: 54.32726481141493
  time_since_restore: 4104.217801809311
  time_this_iter_s: 126.0174617767334
  time_total_s: 51185.763353824615
  timestamp: 1637562220
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 38592000
  training_iteration: 402
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    402 |          51185.8 | 38592000 |  1563.24 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 32.48
    apples_agent-1_min: 9
    apples_agent-2_max: 423
    apples_agent-2_mean: 357.11
    apples_agent-2_min: 260
    apples_agent-3_max: 215
    apples_agent-3_mean: 124.31
    apples_agent-3_min: 68
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.43
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 382.55
    apples_agent-5_min: 257
    cleaning_beam_agent-0_max: 430
    cleaning_beam_agent-0_mean: 387.3
    cleaning_beam_agent-0_min: 345
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.1
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 3.72
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 83.97
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 416.85
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-25-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1689.0
  episode_reward_mean: 1558.71
  episode_reward_min: 1262.0
  episodes_this_iter: 96
  episodes_total: 38688
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18775.163
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39421409368515015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013311036163941026
        model: {}
        policy_loss: -0.001275982242077589
        total_loss: 0.0016843685880303383
        vf_explained_var: 0.027358099818229675
        vf_loss: 36.541709899902344
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2544712424278259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011663591722026467
        model: {}
        policy_loss: -0.0018614488653838634
        total_loss: 0.0011025812709704041
        vf_explained_var: 0.09185227751731873
        vf_loss: 34.11901092529297
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29847344756126404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011026323772966862
        model: {}
        policy_loss: -0.001913019921630621
        total_loss: 0.003677669446915388
        vf_explained_var: 0.04713757336139679
        vf_loss: 61.1600341796875
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8709084987640381
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010650083422660828
        model: {}
        policy_loss: -0.0024440791457891464
        total_loss: 0.0023522693663835526
        vf_explained_var: 0.029351428151130676
        vf_loss: 63.29151153564453
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9706346988677979
        entropy_coeff: 0.0017600000137463212
        kl: 0.005669283680617809
        model: {}
        policy_loss: -0.002798093017190695
        total_loss: -0.0013237299863249063
        vf_explained_var: 0.05742287635803223
        vf_loss: 31.826801300048828
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2577589452266693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007790733361616731
        model: {}
        policy_loss: -0.001876799389719963
        total_loss: 0.0006431178189814091
        vf_explained_var: 0.11805935204029083
        vf_loss: 29.73573875427246
    load_time_ms: 13890.092
    num_steps_sampled: 38688000
    num_steps_trained: 38688000
    sample_time_ms: 93382.07
    update_time_ms: 17.659
  iterations_since_restore: 33
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.271666666666675
    ram_util_percent: 15.693333333333335
  pid: 21723
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 361.5
    agent-3: 361.5
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 237.01
    agent-1: 237.01
    agent-2: 317.55
    agent-3: 317.55
    agent-4: 224.795
    agent-5: 224.795
  policy_reward_min:
    agent-0: 161.5
    agent-1: 161.5
    agent-2: 249.0
    agent-3: 249.0
    agent-4: 154.0
    agent-5: 154.0
  sampler_perf:
    mean_env_wait_ms: 24.810263863939635
    mean_inference_ms: 12.414657465375027
    mean_processing_ms: 54.33321720732865
  time_since_restore: 4230.657820701599
  time_this_iter_s: 126.44001889228821
  time_total_s: 51312.203372716904
  timestamp: 1637562347
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 38688000
  training_iteration: 403
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    403 |          51312.2 | 38688000 |  1558.71 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 31.81
    apples_agent-1_min: 13
    apples_agent-2_max: 416
    apples_agent-2_mean: 354.85
    apples_agent-2_min: 247
    apples_agent-3_max: 198
    apples_agent-3_mean: 125.38
    apples_agent-3_min: 65
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 478
    apples_agent-5_mean: 387.77
    apples_agent-5_min: 276
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 392.57
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 2.35
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 3.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 85.48
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 461.22
    cleaning_beam_agent-4_min: 350
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 3.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-27-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1701.0
  episode_reward_mean: 1570.67
  episode_reward_min: 1104.0
  episodes_this_iter: 96
  episodes_total: 38784
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18758.788
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4090871214866638
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017870570300146937
        model: {}
        policy_loss: -0.0015765922144055367
        total_loss: 0.0011374582536518574
        vf_explained_var: 0.011430889368057251
        vf_loss: 34.340476989746094
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25064677000045776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009835567325353622
        model: {}
        policy_loss: -0.001462472602725029
        total_loss: 0.0013210042379796505
        vf_explained_var: 0.077712282538414
        vf_loss: 32.24617385864258
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2967725098133087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007572192698717117
        model: {}
        policy_loss: -0.0017429264262318611
        total_loss: 0.003903726115822792
        vf_explained_var: 0.05578935146331787
        vf_loss: 61.689720153808594
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.874398410320282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007742714951746166
        model: {}
        policy_loss: -0.002172906883060932
        total_loss: 0.002932005561888218
        vf_explained_var: 0.0028033703565597534
        vf_loss: 66.43852233886719
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9775938987731934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015948490472510457
        model: {}
        policy_loss: -0.0018575736321508884
        total_loss: -0.0004335688427090645
        vf_explained_var: 0.05201110243797302
        vf_loss: 31.4456787109375
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25119447708129883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007932225125841796
        model: {}
        policy_loss: -0.0017160605639219284
        total_loss: 0.0007300917059183121
        vf_explained_var: 0.12436181306838989
        vf_loss: 28.882564544677734
    load_time_ms: 13894.98
    num_steps_sampled: 38784000
    num_steps_trained: 38784000
    sample_time_ms: 93359.603
    update_time_ms: 17.799
  iterations_since_restore: 34
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.01787709497207
    ram_util_percent: 15.669273743016761
  pid: 21723
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 362.5
    agent-3: 362.5
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 239.2
    agent-1: 239.2
    agent-2: 317.94
    agent-3: 317.94
    agent-4: 228.195
    agent-5: 228.195
  policy_reward_min:
    agent-0: 166.5
    agent-1: 166.5
    agent-2: 216.0
    agent-3: 216.0
    agent-4: 169.5
    agent-5: 169.5
  sampler_perf:
    mean_env_wait_ms: 24.80366973950428
    mean_inference_ms: 12.412421316303218
    mean_processing_ms: 54.335474765548355
  time_since_restore: 4356.407229185104
  time_this_iter_s: 125.74940848350525
  time_total_s: 51437.95278120041
  timestamp: 1637562473
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 38784000
  training_iteration: 404
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    404 |            51438 | 38784000 |  1570.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.67
    apples_agent-1_min: 15
    apples_agent-2_max: 432
    apples_agent-2_mean: 351.34
    apples_agent-2_min: 249
    apples_agent-3_max: 187
    apples_agent-3_mean: 122.82
    apples_agent-3_min: 50
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 469
    apples_agent-5_mean: 382.37
    apples_agent-5_min: 289
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 386.78
    cleaning_beam_agent-0_min: 340
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.07
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 3.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 86.76
    cleaning_beam_agent-3_min: 41
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 440.38
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 3.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-29-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1716.0
  episode_reward_mean: 1555.55
  episode_reward_min: 1228.0
  episodes_this_iter: 96
  episodes_total: 38880
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18750.862
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.412929892539978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019286200404167175
        model: {}
        policy_loss: -0.0013624976854771376
        total_loss: 0.0014777129981666803
        vf_explained_var: 0.03432224690914154
        vf_loss: 35.66970443725586
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25182074308395386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010992883471772075
        model: {}
        policy_loss: -0.001743342261761427
        total_loss: 0.0011452902108430862
        vf_explained_var: 0.09913253784179688
        vf_loss: 33.318328857421875
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30558863282203674
        entropy_coeff: 0.0017600000137463212
        kl: 0.000774681568145752
        model: {}
        policy_loss: -0.0018430310301482677
        total_loss: 0.004054155666381121
        vf_explained_var: 0.07443004846572876
        vf_loss: 64.35022735595703
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8523159623146057
        entropy_coeff: 0.0017600000137463212
        kl: 0.001226504216901958
        model: {}
        policy_loss: -0.0024363547563552856
        total_loss: 0.0030188728123903275
        vf_explained_var: 0.004283085465431213
        vf_loss: 69.55308532714844
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9766631722450256
        entropy_coeff: 0.0017600000137463212
        kl: 0.002087912056595087
        model: {}
        policy_loss: -0.0020709503442049026
        total_loss: -0.0006853253580629826
        vf_explained_var: 0.06948676705360413
        vf_loss: 31.045549392700195
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25189030170440674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006011920049786568
        model: {}
        policy_loss: -0.0018889097264036536
        total_loss: 0.0005070106126368046
        vf_explained_var: 0.14633645117282867
        vf_loss: 28.392459869384766
    load_time_ms: 13890.73
    num_steps_sampled: 38880000
    num_steps_trained: 38880000
    sample_time_ms: 93291.676
    update_time_ms: 18.254
  iterations_since_restore: 35
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.61242937853108
    ram_util_percent: 15.619209039548021
  pid: 21723
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 372.0
    agent-3: 372.0
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 237.88
    agent-1: 237.88
    agent-2: 314.315
    agent-3: 314.315
    agent-4: 225.58
    agent-5: 225.58
  policy_reward_min:
    agent-0: 179.5
    agent-1: 179.5
    agent-2: 231.5
    agent-3: 231.5
    agent-4: 173.0
    agent-5: 173.0
  sampler_perf:
    mean_env_wait_ms: 24.797386424544275
    mean_inference_ms: 12.411754731139576
    mean_processing_ms: 54.33712553057242
  time_since_restore: 4481.616853237152
  time_this_iter_s: 125.20962405204773
  time_total_s: 51563.16240525246
  timestamp: 1637562598
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 38880000
  training_iteration: 405
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    405 |          51563.2 | 38880000 |  1555.55 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 31.41
    apples_agent-1_min: 15
    apples_agent-2_max: 427
    apples_agent-2_mean: 358.89
    apples_agent-2_min: 237
    apples_agent-3_max: 210
    apples_agent-3_mean: 124.81
    apples_agent-3_min: 41
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.29
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 387.15
    apples_agent-5_min: 256
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 392.57
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 2.18
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 2.68
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 85.93
    cleaning_beam_agent-3_min: 41
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 452.2
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-32-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1726.0
  episode_reward_mean: 1566.41
  episode_reward_min: 1069.0
  episodes_this_iter: 96
  episodes_total: 38976
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18755.954
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40996089577674866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014943270944058895
        model: {}
        policy_loss: -0.0014207328204065561
        total_loss: 0.0012620571069419384
        vf_explained_var: 0.03028815984725952
        vf_loss: 34.043212890625
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24986620247364044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006965873180888593
        model: {}
        policy_loss: -0.0016912543214857578
        total_loss: 0.0010916166938841343
        vf_explained_var: 0.08337174355983734
        vf_loss: 32.226348876953125
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2983688712120056
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011486453004181385
        model: {}
        policy_loss: -0.002088193316012621
        total_loss: 0.0036225738003849983
        vf_explained_var: 0.06412653625011444
        vf_loss: 62.35894012451172
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8572490215301514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014728385722264647
        model: {}
        policy_loss: -0.002218761248514056
        total_loss: 0.0029511814936995506
        vf_explained_var: 0.010522231459617615
        vf_loss: 66.7870101928711
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9650502800941467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018675134051591158
        model: {}
        policy_loss: -0.0019751274958252907
        total_loss: -0.00036883074790239334
        vf_explained_var: 0.037357985973358154
        vf_loss: 33.0478515625
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24702534079551697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008002052782103419
        model: {}
        policy_loss: -0.0018508037319406867
        total_loss: 0.0006828081095591187
        vf_explained_var: 0.13161028921604156
        vf_loss: 29.683753967285156
    load_time_ms: 13910.097
    num_steps_sampled: 38976000
    num_steps_trained: 38976000
    sample_time_ms: 93249.873
    update_time_ms: 18.26
  iterations_since_restore: 36
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.021787709497204
    ram_util_percent: 15.68268156424581
  pid: 21723
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 361.0
    agent-3: 361.0
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 237.91
    agent-1: 237.91
    agent-2: 317.495
    agent-3: 317.495
    agent-4: 227.8
    agent-5: 227.8
  policy_reward_min:
    agent-0: 158.5
    agent-1: 158.5
    agent-2: 214.0
    agent-3: 214.0
    agent-4: 160.0
    agent-5: 160.0
  sampler_perf:
    mean_env_wait_ms: 24.791899179090414
    mean_inference_ms: 12.40937473919995
    mean_processing_ms: 54.33166329013984
  time_since_restore: 4606.871495962143
  time_this_iter_s: 125.25464272499084
  time_total_s: 51688.41704797745
  timestamp: 1637562723
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 38976000
  training_iteration: 406
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    406 |          51688.4 | 38976000 |  1566.41 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.1
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.33
    apples_agent-1_min: 13
    apples_agent-2_max: 417
    apples_agent-2_mean: 350.54
    apples_agent-2_min: 210
    apples_agent-3_max: 194
    apples_agent-3_mean: 126.58
    apples_agent-3_min: 49
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.22
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 377.18
    apples_agent-5_min: 194
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 389.12
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 30
    cleaning_beam_agent-1_mean: 2.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 4.04
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 78.07
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 468.64
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 3.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-34-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1783.0
  episode_reward_mean: 1557.92
  episode_reward_min: 732.0
  episodes_this_iter: 96
  episodes_total: 39072
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18732.454
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41093909740448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010830436367541552
        model: {}
        policy_loss: -0.0014058356173336506
        total_loss: 0.001485920511186123
        vf_explained_var: 0.059149980545043945
        vf_loss: 36.15008544921875
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25336307287216187
        entropy_coeff: 0.0017600000137463212
        kl: 0.001071159727871418
        model: {}
        policy_loss: -0.0018011981155723333
        total_loss: 0.0011195102706551552
        vf_explained_var: 0.1236564964056015
        vf_loss: 33.666282653808594
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3051079213619232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010605875868350267
        model: {}
        policy_loss: -0.002213895320892334
        total_loss: 0.00397300673648715
        vf_explained_var: 0.0933220386505127
        vf_loss: 67.23893737792969
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8386792540550232
        entropy_coeff: 0.0017600000137463212
        kl: 0.001639856956899166
        model: {}
        policy_loss: -0.002752867992967367
        total_loss: 0.003537604585289955
        vf_explained_var: -0.03440627455711365
        vf_loss: 77.66551208496094
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9436578750610352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031883884221315384
        model: {}
        policy_loss: -0.0021753550972789526
        total_loss: -0.0004946230910718441
        vf_explained_var: 0.06442810595035553
        vf_loss: 33.41571044921875
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25789421796798706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008580766152590513
        model: {}
        policy_loss: -0.002007158938795328
        total_loss: 0.0005873273476026952
        vf_explained_var: 0.14699499309062958
        vf_loss: 30.483783721923828
    load_time_ms: 13917.12
    num_steps_sampled: 39072000
    num_steps_trained: 39072000
    sample_time_ms: 93290.46
    update_time_ms: 17.967
  iterations_since_restore: 37
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.632777777777775
    ram_util_percent: 15.678888888888888
  pid: 21723
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 361.5
    agent-3: 361.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 237.22
    agent-1: 237.22
    agent-2: 318.505
    agent-3: 318.505
    agent-4: 223.235
    agent-5: 223.235
  policy_reward_min:
    agent-0: 110.5
    agent-1: 110.5
    agent-2: 149.5
    agent-3: 149.5
    agent-4: 106.0
    agent-5: 106.0
  sampler_perf:
    mean_env_wait_ms: 24.790586719067193
    mean_inference_ms: 12.409222447710354
    mean_processing_ms: 54.33489553502422
  time_since_restore: 4732.960703372955
  time_this_iter_s: 126.08920741081238
  time_total_s: 51814.50625538826
  timestamp: 1637562850
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 39072000
  training_iteration: 407
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    407 |          51814.5 | 39072000 |  1557.92 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 31.8
    apples_agent-1_min: 17
    apples_agent-2_max: 407
    apples_agent-2_mean: 352.96
    apples_agent-2_min: 186
    apples_agent-3_max: 198
    apples_agent-3_mean: 131.09
    apples_agent-3_min: 59
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.35
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 381.56
    apples_agent-5_min: 237
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 390.28
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 2.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 2.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 73.43
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 486.49
    cleaning_beam_agent-4_min: 361
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 3.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-36-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1719.0
  episode_reward_mean: 1570.89
  episode_reward_min: 907.0
  episodes_this_iter: 96
  episodes_total: 39168
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18701.801
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40717071294784546
        entropy_coeff: 0.0017600000137463212
        kl: 0.001760790590196848
        model: {}
        policy_loss: -0.0014881608076393604
        total_loss: 0.0013841824838891625
        vf_explained_var: 0.012479662895202637
        vf_loss: 35.8896598815918
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24949419498443604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008913280908018351
        model: {}
        policy_loss: -0.001668634358793497
        total_loss: 0.0011969159822911024
        vf_explained_var: 0.09000112116336823
        vf_loss: 33.046607971191406
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3017345368862152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009875239338725805
        model: {}
        policy_loss: -0.0019919201731681824
        total_loss: 0.004154239781200886
        vf_explained_var: 0.057906463742256165
        vf_loss: 66.77210235595703
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8266564607620239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014704579953104258
        model: {}
        policy_loss: -0.002326312940567732
        total_loss: 0.003269222332164645
        vf_explained_var: 0.014453276991844177
        vf_loss: 70.50448608398438
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9381603598594666
        entropy_coeff: 0.0017600000137463212
        kl: 0.004339205101132393
        model: {}
        policy_loss: -0.002191334031522274
        total_loss: -0.0006273845210671425
        vf_explained_var: 0.04819893836975098
        vf_loss: 32.15114212036133
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25202417373657227
        entropy_coeff: 0.0017600000137463212
        kl: 0.000695378112141043
        model: {}
        policy_loss: -0.0017450693994760513
        total_loss: 0.0006827749311923981
        vf_explained_var: 0.1502012461423874
        vf_loss: 28.71405029296875
    load_time_ms: 13836.729
    num_steps_sampled: 39168000
    num_steps_trained: 39168000
    sample_time_ms: 93364.92
    update_time_ms: 17.948
  iterations_since_restore: 38
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.29226519337018
    ram_util_percent: 15.682872928176792
  pid: 21723
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 373.0
    agent-3: 373.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 238.78
    agent-1: 238.78
    agent-2: 321.725
    agent-3: 321.725
    agent-4: 224.94
    agent-5: 224.94
  policy_reward_min:
    agent-0: 119.5
    agent-1: 119.5
    agent-2: 184.0
    agent-3: 184.0
    agent-4: 150.0
    agent-5: 150.0
  sampler_perf:
    mean_env_wait_ms: 24.793877952253652
    mean_inference_ms: 12.409539227625189
    mean_processing_ms: 54.34431330247009
  time_since_restore: 4860.098413705826
  time_this_iter_s: 127.13771033287048
  time_total_s: 51941.64396572113
  timestamp: 1637562977
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 39168000
  training_iteration: 408
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    408 |          51941.6 | 39168000 |  1570.89 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 32.04
    apples_agent-1_min: 8
    apples_agent-2_max: 456
    apples_agent-2_mean: 365.63
    apples_agent-2_min: 76
    apples_agent-3_max: 224
    apples_agent-3_mean: 126.32
    apples_agent-3_min: 21
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 383.93
    apples_agent-5_min: 73
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 392.45
    cleaning_beam_agent-0_min: 270
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 2.08
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 3.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 76.87
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 513.9
    cleaning_beam_agent-4_min: 455
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 3.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-38-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1586.3
  episode_reward_min: 329.0
  episodes_this_iter: 96
  episodes_total: 39264
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18689.434
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4075421392917633
        entropy_coeff: 0.0017600000137463212
        kl: 0.001333206077106297
        model: {}
        policy_loss: -0.0015726247802376747
        total_loss: 0.0015323683619499207
        vf_explained_var: 0.05179643630981445
        vf_loss: 38.22271728515625
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24736928939819336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010943184606730938
        model: {}
        policy_loss: -0.0017171958461403847
        total_loss: 0.001398166874423623
        vf_explained_var: 0.11973081529140472
        vf_loss: 35.507328033447266
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3002015948295593
        entropy_coeff: 0.0017600000137463212
        kl: 0.000951332796830684
        model: {}
        policy_loss: -0.002073073759675026
        total_loss: 0.0040076556615531445
        vf_explained_var: 0.08027946949005127
        vf_loss: 66.09085083007812
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8394059538841248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007603657431900501
        model: {}
        policy_loss: -0.0023561688140034676
        total_loss: 0.0036324262619018555
        vf_explained_var: -0.02356286346912384
        vf_loss: 74.65951538085938
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9557490348815918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034825154580175877
        model: {}
        policy_loss: -0.0019996967166662216
        total_loss: -0.0002393517643213272
        vf_explained_var: 0.01667402684688568
        vf_loss: 34.42464828491211
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2547866702079773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009177184547297657
        model: {}
        policy_loss: -0.001826554536819458
        total_loss: 0.0007029827684164047
        vf_explained_var: 0.14755061268806458
        vf_loss: 29.779644012451172
    load_time_ms: 13794.273
    num_steps_sampled: 39264000
    num_steps_trained: 39264000
    sample_time_ms: 93440.725
    update_time_ms: 17.971
  iterations_since_restore: 39
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.468156424581004
    ram_util_percent: 15.677653631284917
  pid: 21723
  policy_reward_max:
    agent-0: 283.0
    agent-1: 283.0
    agent-2: 379.0
    agent-3: 379.0
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 241.195
    agent-1: 241.195
    agent-2: 324.885
    agent-3: 324.885
    agent-4: 227.07
    agent-5: 227.07
  policy_reward_min:
    agent-0: 54.0
    agent-1: 54.0
    agent-2: 64.0
    agent-3: 64.0
    agent-4: 46.5
    agent-5: 46.5
  sampler_perf:
    mean_env_wait_ms: 24.795714821590533
    mean_inference_ms: 12.407913158604002
    mean_processing_ms: 54.34161153797767
  time_since_restore: 4986.048061132431
  time_this_iter_s: 125.94964742660522
  time_total_s: 52067.593613147736
  timestamp: 1637563103
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 39264000
  training_iteration: 409
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    409 |          52067.6 | 39264000 |   1586.3 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 0.23
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 32.5
    apples_agent-1_min: 11
    apples_agent-2_max: 407
    apples_agent-2_mean: 352.83
    apples_agent-2_min: 120
    apples_agent-3_max: 204
    apples_agent-3_mean: 129.99
    apples_agent-3_min: 25
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 377.01
    apples_agent-5_min: 125
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 393.31
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 4.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 75.16
    cleaning_beam_agent-3_min: 38
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 487.96
    cleaning_beam_agent-4_min: 396
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 3.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-40-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1750.0
  episode_reward_mean: 1567.94
  episode_reward_min: 482.0
  episodes_this_iter: 96
  episodes_total: 39360
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18711.17
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40814125537872314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012612432474270463
        model: {}
        policy_loss: -0.0013100968208163977
        total_loss: 0.0017719387542456388
        vf_explained_var: 0.06272026896476746
        vf_loss: 38.00361633300781
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2519199252128601
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013104896061122417
        model: {}
        policy_loss: -0.0022009804379194975
        total_loss: 0.0009103766642510891
        vf_explained_var: 0.12714463472366333
        vf_loss: 35.5473747253418
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3060861825942993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009611061541363597
        model: {}
        policy_loss: -0.002113162772729993
        total_loss: 0.00432012602686882
        vf_explained_var: 0.10550768673419952
        vf_loss: 69.7199935913086
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8448412418365479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011096770176663995
        model: {}
        policy_loss: -0.0023889457806944847
        total_loss: 0.004064367152750492
        vf_explained_var: -0.009537160396575928
        vf_loss: 79.40234375
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9677331447601318
        entropy_coeff: 0.0017600000137463212
        kl: 0.002853088779374957
        model: {}
        policy_loss: -0.0021348977461457253
        total_loss: -0.00022299436386674643
        vf_explained_var: 0.019483521580696106
        vf_loss: 36.1511116027832
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2577848732471466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012767098378390074
        model: {}
        policy_loss: -0.0020996886305510998
        total_loss: 0.0005830980371683836
        vf_explained_var: 0.15043528378009796
        vf_loss: 31.364887237548828
    load_time_ms: 13714.702
    num_steps_sampled: 39360000
    num_steps_trained: 39360000
    sample_time_ms: 93513.965
    update_time_ms: 18.334
  iterations_since_restore: 40
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.879329608938534
    ram_util_percent: 15.587150837988824
  pid: 21723
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 364.0
    agent-3: 364.0
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 241.155
    agent-1: 241.155
    agent-2: 320.655
    agent-3: 320.655
    agent-4: 222.16
    agent-5: 222.16
  policy_reward_min:
    agent-0: 73.5
    agent-1: 73.5
    agent-2: 102.0
    agent-3: 102.0
    agent-4: 65.5
    agent-5: 65.5
  sampler_perf:
    mean_env_wait_ms: 24.793966676194824
    mean_inference_ms: 12.406965202204002
    mean_processing_ms: 54.34005582721348
  time_since_restore: 5111.971815824509
  time_this_iter_s: 125.92375469207764
  time_total_s: 52193.51736783981
  timestamp: 1637563229
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 39360000
  training_iteration: 410
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    410 |          52193.5 | 39360000 |  1567.94 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 0.14
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 30.82
    apples_agent-1_min: 11
    apples_agent-2_max: 412
    apples_agent-2_mean: 356.41
    apples_agent-2_min: 68
    apples_agent-3_max: 212
    apples_agent-3_mean: 124.99
    apples_agent-3_min: 26
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.26
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 385.46
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 399.98
    cleaning_beam_agent-0_min: 333
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.51
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 3.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 76.2
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 463.31
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 2.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-42-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1732.0
  episode_reward_mean: 1578.78
  episode_reward_min: 278.0
  episodes_this_iter: 96
  episodes_total: 39456
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18710.292
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3999898135662079
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018069467041641474
        model: {}
        policy_loss: -0.0018167500384151936
        total_loss: 0.0011036936193704605
        vf_explained_var: 0.03449524939060211
        vf_loss: 36.244266510009766
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25437676906585693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011086759623140097
        model: {}
        policy_loss: -0.0018455912359058857
        total_loss: 0.0010989533038809896
        vf_explained_var: 0.09622576832771301
        vf_loss: 33.92245101928711
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30610859394073486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007622009143233299
        model: {}
        policy_loss: -0.0018663080409169197
        total_loss: 0.0042086411267519
        vf_explained_var: 0.09719112515449524
        vf_loss: 66.13703918457031
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8290470838546753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009973824489861727
        model: {}
        policy_loss: -0.002183706732466817
        total_loss: 0.00353632471524179
        vf_explained_var: 0.026602789759635925
        vf_loss: 71.79153442382812
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9799321889877319
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024858966935425997
        model: {}
        policy_loss: -0.0019975281320512295
        total_loss: -0.0002661298494786024
        vf_explained_var: 0.04196572303771973
        vf_loss: 34.56078338623047
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.259111225605011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008683555060997605
        model: {}
        policy_loss: -0.0022486955858767033
        total_loss: 0.0004288521595299244
        vf_explained_var: 0.12789303064346313
        vf_loss: 31.33584213256836
    load_time_ms: 13555.274
    num_steps_sampled: 39456000
    num_steps_trained: 39456000
    sample_time_ms: 93494.711
    update_time_ms: 18.146
  iterations_since_restore: 41
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 53.94022346368715
    ram_util_percent: 15.715642458100561
  pid: 21723
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 364.5
    agent-3: 364.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 240.32
    agent-1: 240.32
    agent-2: 322.155
    agent-3: 322.155
    agent-4: 226.915
    agent-5: 226.915
  policy_reward_min:
    agent-0: 42.0
    agent-1: 42.0
    agent-2: 60.0
    agent-3: 60.0
    agent-4: 37.0
    agent-5: 37.0
  sampler_perf:
    mean_env_wait_ms: 24.791215180166592
    mean_inference_ms: 12.406176521905
    mean_processing_ms: 54.334414091909075
  time_since_restore: 5236.784707546234
  time_this_iter_s: 124.81289172172546
  time_total_s: 52318.33025956154
  timestamp: 1637563355
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 39456000
  training_iteration: 411
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    411 |          52318.3 | 39456000 |  1578.78 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 30.95
    apples_agent-1_min: 18
    apples_agent-2_max: 422
    apples_agent-2_mean: 354.41
    apples_agent-2_min: 175
    apples_agent-3_max: 202
    apples_agent-3_mean: 130.35
    apples_agent-3_min: 70
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.29
    apples_agent-4_min: 0
    apples_agent-5_max: 465
    apples_agent-5_mean: 385.22
    apples_agent-5_min: 161
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 396.81
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 2.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 3.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 72.89
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 445.35
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-44-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1732.0
  episode_reward_mean: 1578.87
  episode_reward_min: 887.0
  episodes_this_iter: 96
  episodes_total: 39552
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18728.739
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39436620473861694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011807505507022142
        model: {}
        policy_loss: -0.001143813133239746
        total_loss: 0.0019081737846136093
        vf_explained_var: -0.0002759099006652832
        vf_loss: 37.460723876953125
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2535213828086853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010428951354697347
        model: {}
        policy_loss: -0.001964844763278961
        total_loss: 0.0009660691721364856
        vf_explained_var: 0.09953242540359497
        vf_loss: 33.77110290527344
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3053198456764221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008322896319441497
        model: {}
        policy_loss: -0.0020850703585892916
        total_loss: 0.0038272934034466743
        vf_explained_var: 0.09005171060562134
        vf_loss: 64.4972915649414
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8266908526420593
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009723047842271626
        model: {}
        policy_loss: -0.00233919033780694
        total_loss: 0.0031532482244074345
        vf_explained_var: 0.026833653450012207
        vf_loss: 69.47415161132812
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9970295429229736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024107180070132017
        model: {}
        policy_loss: -0.002055767457932234
        total_loss: -0.0005941326962783933
        vf_explained_var: 0.07116849720478058
        vf_loss: 32.164024353027344
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26082277297973633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007474221638403833
        model: {}
        policy_loss: -0.001931777922436595
        total_loss: 0.000569004740100354
        vf_explained_var: 0.14338576793670654
        vf_loss: 29.59830665588379
    load_time_ms: 13493.665
    num_steps_sampled: 39552000
    num_steps_trained: 39552000
    sample_time_ms: 93538.832
    update_time_ms: 17.678
  iterations_since_restore: 42
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.282777777777774
    ram_util_percent: 15.730555555555558
  pid: 21723
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 364.5
    agent-3: 364.5
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 240.965
    agent-1: 240.965
    agent-2: 321.345
    agent-3: 321.345
    agent-4: 227.125
    agent-5: 227.125
  policy_reward_min:
    agent-0: 147.5
    agent-1: 147.5
    agent-2: 178.0
    agent-3: 178.0
    agent-4: 118.0
    agent-5: 118.0
  sampler_perf:
    mean_env_wait_ms: 24.78785621131851
    mean_inference_ms: 12.406151682192275
    mean_processing_ms: 54.341288328454446
  time_since_restore: 5362.811676979065
  time_this_iter_s: 126.02696943283081
  time_total_s: 52444.35722899437
  timestamp: 1637563481
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 39552000
  training_iteration: 412
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    412 |          52444.4 | 39552000 |  1578.87 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 31.85
    apples_agent-1_min: 19
    apples_agent-2_max: 427
    apples_agent-2_mean: 356.69
    apples_agent-2_min: 292
    apples_agent-3_max: 195
    apples_agent-3_mean: 126.83
    apples_agent-3_min: 48
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 386.07
    apples_agent-5_min: 289
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 402.51
    cleaning_beam_agent-0_min: 363
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 2.27
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 4.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 67.34
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 423.07
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-46-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1737.0
  episode_reward_mean: 1581.75
  episode_reward_min: 1237.0
  episodes_this_iter: 96
  episodes_total: 39648
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18728.486
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39249658584594727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010494758607819676
        model: {}
        policy_loss: -0.0011590337380766869
        total_loss: 0.0016594668850302696
        vf_explained_var: 0.015258803963661194
        vf_loss: 35.092899322509766
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24626904726028442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009017803240567446
        model: {}
        policy_loss: -0.0017343920189887285
        total_loss: 0.001020967261865735
        vf_explained_var: 0.10678009688854218
        vf_loss: 31.887943267822266
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30542609095573425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008166239131242037
        model: {}
        policy_loss: -0.0019209202146157622
        total_loss: 0.003887191182002425
        vf_explained_var: 0.06307259202003479
        vf_loss: 63.45660400390625
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.813666820526123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010918453335762024
        model: {}
        policy_loss: -0.0020779785700142384
        total_loss: 0.0032678432762622833
        vf_explained_var: 0.001818433403968811
        vf_loss: 67.77873992919922
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0011898279190063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019450030522421002
        model: {}
        policy_loss: -0.0020629242062568665
        total_loss: -0.0006420688005164266
        vf_explained_var: 0.05202746391296387
        vf_loss: 31.829496383666992
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25081655383110046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006987972883507609
        model: {}
        policy_loss: -0.0020381228532642126
        total_loss: 0.0004994177725166082
        vf_explained_var: 0.11254914104938507
        vf_loss: 29.789775848388672
    load_time_ms: 13471.683
    num_steps_sampled: 39648000
    num_steps_trained: 39648000
    sample_time_ms: 93544.785
    update_time_ms: 18.176
  iterations_since_restore: 43
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.36927374301675
    ram_util_percent: 15.697765363128495
  pid: 21723
  policy_reward_max:
    agent-0: 268.5
    agent-1: 268.5
    agent-2: 367.5
    agent-3: 367.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 241.775
    agent-1: 241.775
    agent-2: 320.555
    agent-3: 320.555
    agent-4: 228.545
    agent-5: 228.545
  policy_reward_min:
    agent-0: 190.0
    agent-1: 190.0
    agent-2: 257.5
    agent-3: 257.5
    agent-4: 166.0
    agent-5: 166.0
  sampler_perf:
    mean_env_wait_ms: 24.787400071510742
    mean_inference_ms: 12.407431547047832
    mean_processing_ms: 54.350341477640896
  time_since_restore: 5489.101066112518
  time_this_iter_s: 126.28938913345337
  time_total_s: 52570.64661812782
  timestamp: 1637563607
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 39648000
  training_iteration: 413
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    413 |          52570.6 | 39648000 |  1581.75 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 0.17
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 31.42
    apples_agent-1_min: 14
    apples_agent-2_max: 427
    apples_agent-2_mean: 355.82
    apples_agent-2_min: 178
    apples_agent-3_max: 209
    apples_agent-3_mean: 131.02
    apples_agent-3_min: 48
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 384.0
    apples_agent-5_min: 225
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 393.46
    cleaning_beam_agent-0_min: 344
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 1.86
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 3.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 137
    cleaning_beam_agent-3_mean: 64.94
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 438.27
    cleaning_beam_agent-4_min: 376
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-48-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1755.0
  episode_reward_mean: 1582.85
  episode_reward_min: 916.0
  episodes_this_iter: 96
  episodes_total: 39744
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18737.609
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39455515146255493
        entropy_coeff: 0.0017600000137463212
        kl: 0.001251622918061912
        model: {}
        policy_loss: -0.0013516265898942947
        total_loss: 0.0015356414951384068
        vf_explained_var: -0.0038026422262191772
        vf_loss: 35.81684875488281
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24244321882724762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008857594802975655
        model: {}
        policy_loss: -0.0016046552918851376
        total_loss: 0.0012201685458421707
        vf_explained_var: 0.0934872031211853
        vf_loss: 32.51524353027344
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30751538276672363
        entropy_coeff: 0.0017600000137463212
        kl: 0.000987240462563932
        model: {}
        policy_loss: -0.0018386198207736015
        total_loss: 0.004147043451666832
        vf_explained_var: 0.062180131673812866
        vf_loss: 65.26887512207031
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8105223774909973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008635451667942107
        model: {}
        policy_loss: -0.002139341551810503
        total_loss: 0.003451717086136341
        vf_explained_var: -0.00021383166313171387
        vf_loss: 70.17575073242188
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9938976764678955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023470562882721424
        model: {}
        policy_loss: -0.002067773137241602
        total_loss: -0.0007684528827667236
        vf_explained_var: 0.03093215823173523
        vf_loss: 30.485797882080078
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25840479135513306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007661856943741441
        model: {}
        policy_loss: -0.001640991074964404
        total_loss: 0.0005886880680918694
        vf_explained_var: 0.14600783586502075
        vf_loss: 26.844730377197266
    load_time_ms: 13390.603
    num_steps_sampled: 39744000
    num_steps_trained: 39744000
    sample_time_ms: 93619.968
    update_time_ms: 18.175
  iterations_since_restore: 44
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.21620111731843
    ram_util_percent: 15.693296089385475
  pid: 21723
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 382.5
    agent-3: 382.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 242.25
    agent-1: 242.25
    agent-2: 323.98
    agent-3: 323.98
    agent-4: 225.195
    agent-5: 225.195
  policy_reward_min:
    agent-0: 149.0
    agent-1: 149.0
    agent-2: 172.5
    agent-3: 172.5
    agent-4: 136.5
    agent-5: 136.5
  sampler_perf:
    mean_env_wait_ms: 24.782005009595537
    mean_inference_ms: 12.407319900161603
    mean_processing_ms: 54.35333957582195
  time_since_restore: 5614.87887430191
  time_this_iter_s: 125.77780818939209
  time_total_s: 52696.424426317215
  timestamp: 1637563733
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 39744000
  training_iteration: 414
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    414 |          52696.4 | 39744000 |  1582.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 0.2
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.12
    apples_agent-1_min: 9
    apples_agent-2_max: 416
    apples_agent-2_mean: 350.3
    apples_agent-2_min: 99
    apples_agent-3_max: 205
    apples_agent-3_mean: 129.59
    apples_agent-3_min: 6
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.48
    apples_agent-4_min: 0
    apples_agent-5_max: 461
    apples_agent-5_mean: 379.73
    apples_agent-5_min: 114
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 386.79
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 4.01
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 64.96
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 449.89
    cleaning_beam_agent-4_min: 373
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 3.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-50-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1777.0
  episode_reward_mean: 1558.45
  episode_reward_min: 434.0
  episodes_this_iter: 96
  episodes_total: 39840
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18712.846
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3866198658943176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008699169266037643
        model: {}
        policy_loss: -0.0013288107002153993
        total_loss: 0.0016611283645033836
        vf_explained_var: 0.04253554344177246
        vf_loss: 36.70392608642578
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2510187029838562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010897789616137743
        model: {}
        policy_loss: -0.0018885014578700066
        total_loss: 0.0010471837595105171
        vf_explained_var: 0.11777512729167938
        vf_loss: 33.7747802734375
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3082982301712036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014460544334724545
        model: {}
        policy_loss: -0.0021897805854678154
        total_loss: 0.004351462237536907
        vf_explained_var: 0.08236096799373627
        vf_loss: 70.83847045898438
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8206823468208313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009967796504497528
        model: {}
        policy_loss: -0.002327146241441369
        total_loss: 0.0038935586344450712
        vf_explained_var: 0.010682016611099243
        vf_loss: 76.65105438232422
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.984719455242157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017988355830311775
        model: {}
        policy_loss: -0.0021843193098902702
        total_loss: -0.000364813138730824
        vf_explained_var: 0.05740050971508026
        vf_loss: 35.526126861572266
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26671385765075684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005897812079638243
        model: {}
        policy_loss: -0.0018842252902686596
        total_loss: 0.0007825517095625401
        vf_explained_var: 0.16708798706531525
        vf_loss: 31.36196517944336
    load_time_ms: 13369.636
    num_steps_sampled: 39840000
    num_steps_trained: 39840000
    sample_time_ms: 93575.508
    update_time_ms: 18.056
  iterations_since_restore: 45
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.467796610169486
    ram_util_percent: 15.721468926553674
  pid: 21723
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 367.0
    agent-3: 367.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 237.85
    agent-1: 237.85
    agent-2: 318.75
    agent-3: 318.75
    agent-4: 222.625
    agent-5: 222.625
  policy_reward_min:
    agent-0: 77.0
    agent-1: 77.0
    agent-2: 77.0
    agent-3: 77.0
    agent-4: 63.0
    agent-5: 63.0
  sampler_perf:
    mean_env_wait_ms: 24.775357137378546
    mean_inference_ms: 12.406051704086478
    mean_processing_ms: 54.34841313190127
  time_since_restore: 5739.186520814896
  time_this_iter_s: 124.30764651298523
  time_total_s: 52820.7320728302
  timestamp: 1637563858
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 39840000
  training_iteration: 415
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    415 |          52820.7 | 39840000 |  1558.45 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 32.03
    apples_agent-1_min: 4
    apples_agent-2_max: 416
    apples_agent-2_mean: 352.69
    apples_agent-2_min: 19
    apples_agent-3_max: 194
    apples_agent-3_mean: 131.71
    apples_agent-3_min: 17
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 381.27
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 384.63
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.54
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 3.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 63.47
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 458.63
    cleaning_beam_agent-4_min: 378
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 3.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-53-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1777.0
  episode_reward_mean: 1566.11
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 39936
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18703.421
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39731940627098083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019169702427461743
        model: {}
        policy_loss: -0.0015647688414901495
        total_loss: 0.0014010644517838955
        vf_explained_var: 0.07954369485378265
        vf_loss: 36.651161193847656
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2498331367969513
        entropy_coeff: 0.0017600000137463212
        kl: 0.00088850362226367
        model: {}
        policy_loss: -0.0018291349988430738
        total_loss: 0.0011041156249120831
        vf_explained_var: 0.15245245397090912
        vf_loss: 33.72957229614258
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3111582100391388
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008858900982886553
        model: {}
        policy_loss: -0.0020175627432763577
        total_loss: 0.004057701211422682
        vf_explained_var: 0.12729506194591522
        vf_loss: 66.22904968261719
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8057522177696228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008974310476332903
        model: {}
        policy_loss: -0.0021606325171887875
        total_loss: 0.003952551633119583
        vf_explained_var: 0.01109495759010315
        vf_loss: 75.31307983398438
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.993025541305542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021139683667570353
        model: {}
        policy_loss: -0.0021811593323946
        total_loss: -0.0004038522019982338
        vf_explained_var: 0.029805660247802734
        vf_loss: 35.25031661987305
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2642517387866974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006870063371025026
        model: {}
        policy_loss: -0.0021689077839255333
        total_loss: 0.00038902193773537874
        vf_explained_var: 0.16890449821949005
        vf_loss: 30.230127334594727
    load_time_ms: 13324.806
    num_steps_sampled: 39936000
    num_steps_trained: 39936000
    sample_time_ms: 93597.388
    update_time_ms: 18.377
  iterations_since_restore: 46
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.79606741573034
    ram_util_percent: 15.759550561797758
  pid: 21723
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 379.0
    agent-3: 379.0
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 236.385
    agent-1: 236.385
    agent-2: 321.78
    agent-3: 321.78
    agent-4: 224.89
    agent-5: 224.89
  policy_reward_min:
    agent-0: 13.0
    agent-1: 13.0
    agent-2: 22.0
    agent-3: 22.0
    agent-4: 12.0
    agent-5: 12.0
  sampler_perf:
    mean_env_wait_ms: 24.767453677364966
    mean_inference_ms: 12.404017868980965
    mean_processing_ms: 54.34821890769892
  time_since_restore: 5864.098697185516
  time_this_iter_s: 124.91217637062073
  time_total_s: 52945.64424920082
  timestamp: 1637563983
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 39936000
  training_iteration: 416
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    416 |          52945.6 | 39936000 |  1566.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.37
    apples_agent-1_min: 14
    apples_agent-2_max: 430
    apples_agent-2_mean: 353.44
    apples_agent-2_min: 209
    apples_agent-3_max: 219
    apples_agent-3_mean: 133.78
    apples_agent-3_min: 58
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 379.51
    apples_agent-5_min: 258
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 375.13
    cleaning_beam_agent-0_min: 324
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.49
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 3.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 64.19
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 453.0
    cleaning_beam_agent-4_min: 303
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-55-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1710.0
  episode_reward_mean: 1568.41
  episode_reward_min: 1059.0
  episodes_this_iter: 96
  episodes_total: 40032
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18712.97
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3989756405353546
        entropy_coeff: 0.0017600000137463212
        kl: 0.001888795173726976
        model: {}
        policy_loss: -0.0016462202183902264
        total_loss: 0.001455292571336031
        vf_explained_var: 0.014429479837417603
        vf_loss: 38.037071228027344
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2552400231361389
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011375525500625372
        model: {}
        policy_loss: -0.0021136412397027016
        total_loss: 0.0008544400334358215
        vf_explained_var: 0.11478829383850098
        vf_loss: 34.173057556152344
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3063490390777588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011633157264441252
        model: {}
        policy_loss: -0.0019095069728791714
        total_loss: 0.00421763863414526
        vf_explained_var: 0.08492597937583923
        vf_loss: 66.66320037841797
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7993318438529968
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014478890225291252
        model: {}
        policy_loss: -0.0023266260977834463
        total_loss: 0.0035508419387042522
        vf_explained_var: 0.012509390711784363
        vf_loss: 72.84292602539062
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9883626699447632
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027267562691122293
        model: {}
        policy_loss: -0.0018606167286634445
        total_loss: -0.00037637841887772083
        vf_explained_var: 0.04549509286880493
        vf_loss: 32.2375373840332
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26212501525878906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008270854013971984
        model: {}
        policy_loss: -0.0018578758463263512
        total_loss: 0.0005749017000198364
        vf_explained_var: 0.14151345193386078
        vf_loss: 28.94115447998047
    load_time_ms: 13285.503
    num_steps_sampled: 40032000
    num_steps_trained: 40032000
    sample_time_ms: 93549.223
    update_time_ms: 18.993
  iterations_since_restore: 47
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.78146067415731
    ram_util_percent: 15.683707865168541
  pid: 21723
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 374.5
    agent-3: 374.5
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 236.78
    agent-1: 236.78
    agent-2: 323.895
    agent-3: 323.895
    agent-4: 223.53
    agent-5: 223.53
  policy_reward_min:
    agent-0: 131.5
    agent-1: 131.5
    agent-2: 205.0
    agent-3: 205.0
    agent-4: 169.0
    agent-5: 169.0
  sampler_perf:
    mean_env_wait_ms: 24.76161358949589
    mean_inference_ms: 12.402687349206731
    mean_processing_ms: 54.34934595713035
  time_since_restore: 5989.412835121155
  time_this_iter_s: 125.31413793563843
  time_total_s: 53070.95838713646
  timestamp: 1637564108
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 40032000
  training_iteration: 417
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    417 |            53071 | 40032000 |  1568.41 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 31.16
    apples_agent-1_min: 17
    apples_agent-2_max: 427
    apples_agent-2_mean: 356.67
    apples_agent-2_min: 157
    apples_agent-3_max: 243
    apples_agent-3_mean: 137.72
    apples_agent-3_min: 60
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 477
    apples_agent-5_mean: 386.18
    apples_agent-5_min: 178
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 369.36
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 3.65
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 57.9
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 456.18
    cleaning_beam_agent-4_min: 390
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 3.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-57-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1697.0
  episode_reward_mean: 1582.87
  episode_reward_min: 802.0
  episodes_this_iter: 96
  episodes_total: 40128
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18727.156
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3985093832015991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020840573124587536
        model: {}
        policy_loss: -0.0015073874965310097
        total_loss: 0.001342130359262228
        vf_explained_var: 0.03172159194946289
        vf_loss: 35.50895690917969
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2501969039440155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008008008589968085
        model: {}
        policy_loss: -0.0016959221102297306
        total_loss: 0.0012402043212205172
        vf_explained_var: 0.0791918933391571
        vf_loss: 33.764732360839844
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2973402142524719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011158962734043598
        model: {}
        policy_loss: -0.0017690642271190882
        total_loss: 0.004411490634083748
        vf_explained_var: 0.062488287687301636
        vf_loss: 67.03872680664062
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7977166175842285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009403873700648546
        model: {}
        policy_loss: -0.002218430396169424
        total_loss: 0.0035341395996510983
        vf_explained_var: 0.01232844591140747
        vf_loss: 71.56550598144531
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.976540744304657
        entropy_coeff: 0.0017600000137463212
        kl: 0.001654955674894154
        model: {}
        policy_loss: -0.0020881881937384605
        total_loss: -0.0005738772451877594
        vf_explained_var: 0.027816951274871826
        vf_loss: 32.33024215698242
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25937044620513916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008565124007873237
        model: {}
        policy_loss: -0.0019373050890862942
        total_loss: 0.0005578341661021113
        vf_explained_var: 0.11451411247253418
        vf_loss: 29.51632308959961
    load_time_ms: 13217.304
    num_steps_sampled: 40128000
    num_steps_trained: 40128000
    sample_time_ms: 93381.064
    update_time_ms: 19.113
  iterations_since_restore: 48
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.5685393258427
    ram_util_percent: 15.714044943820227
  pid: 21723
  policy_reward_max:
    agent-0: 289.0
    agent-1: 289.0
    agent-2: 371.5
    agent-3: 371.5
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 238.675
    agent-1: 238.675
    agent-2: 325.815
    agent-3: 325.815
    agent-4: 226.945
    agent-5: 226.945
  policy_reward_min:
    agent-0: 130.5
    agent-1: 130.5
    agent-2: 166.5
    agent-3: 166.5
    agent-4: 104.0
    agent-5: 104.0
  sampler_perf:
    mean_env_wait_ms: 24.75428771013593
    mean_inference_ms: 12.400958753711068
    mean_processing_ms: 54.34362790785688
  time_since_restore: 6114.346045732498
  time_this_iter_s: 124.93321061134338
  time_total_s: 53195.8915977478
  timestamp: 1637564233
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 40128000
  training_iteration: 418
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    418 |          53195.9 | 40128000 |  1582.87 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 31.68
    apples_agent-1_min: 5
    apples_agent-2_max: 409
    apples_agent-2_mean: 356.59
    apples_agent-2_min: 53
    apples_agent-3_max: 205
    apples_agent-3_mean: 127.76
    apples_agent-3_min: 31
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 382.29
    apples_agent-5_min: 68
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 384.23
    cleaning_beam_agent-0_min: 347
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 1.97
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 4.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 58.67
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 470.13
    cleaning_beam_agent-4_min: 384
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 4.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_01-59-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1739.0
  episode_reward_mean: 1579.9
  episode_reward_min: 275.0
  episodes_this_iter: 96
  episodes_total: 40224
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18718.323
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4042341113090515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013932911679148674
        model: {}
        policy_loss: -0.001299045979976654
        total_loss: 0.0017182612791657448
        vf_explained_var: 0.05902969837188721
        vf_loss: 37.2876091003418
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2515673041343689
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016494054580107331
        model: {}
        policy_loss: -0.0019637621007859707
        total_loss: 0.0010393434204161167
        vf_explained_var: 0.1292763203382492
        vf_loss: 34.45861053466797
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30433177947998047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012817777460440993
        model: {}
        policy_loss: -0.001999329775571823
        total_loss: 0.004140510223805904
        vf_explained_var: 0.10138632357120514
        vf_loss: 66.75465393066406
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8151508569717407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020968273747712374
        model: {}
        policy_loss: -0.0023689749650657177
        total_loss: 0.003675899701192975
        vf_explained_var: 0.0045313239097595215
        vf_loss: 74.7953872680664
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9761835932731628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011353897862136364
        model: {}
        policy_loss: -0.0016969069838523865
        total_loss: 0.00022987183183431625
        vf_explained_var: 0.024071723222732544
        vf_loss: 36.44863510131836
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2618492841720581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010763241443783045
        model: {}
        policy_loss: -0.0020721890032291412
        total_loss: 0.0006201555952429771
        vf_explained_var: 0.15713921189308167
        vf_loss: 31.531978607177734
    load_time_ms: 13217.187
    num_steps_sampled: 40224000
    num_steps_trained: 40224000
    sample_time_ms: 93165.79
    update_time_ms: 19.041
  iterations_since_restore: 49
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.87005649717514
    ram_util_percent: 15.703954802259888
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 371.0
    agent-3: 371.0
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 240.77
    agent-1: 240.77
    agent-2: 323.355
    agent-3: 323.355
    agent-4: 225.825
    agent-5: 225.825
  policy_reward_min:
    agent-0: 38.0
    agent-1: 38.0
    agent-2: 60.0
    agent-3: 60.0
    agent-4: 39.5
    agent-5: 39.5
  sampler_perf:
    mean_env_wait_ms: 24.74671347878158
    mean_inference_ms: 12.398382573254267
    mean_processing_ms: 54.33081771211346
  time_since_restore: 6238.055295467377
  time_this_iter_s: 123.70924973487854
  time_total_s: 53319.60084748268
  timestamp: 1637564357
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 40224000
  training_iteration: 419
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    419 |          53319.6 | 40224000 |   1579.9 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.64
    apples_agent-1_min: 3
    apples_agent-2_max: 423
    apples_agent-2_mean: 358.51
    apples_agent-2_min: 27
    apples_agent-3_max: 227
    apples_agent-3_mean: 134.97
    apples_agent-3_min: 8
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 384.82
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 387.71
    cleaning_beam_agent-0_min: 318
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.58
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 3.08
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 61.27
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 474.18
    cleaning_beam_agent-4_min: 402
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 3.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-01-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1720.0
  episode_reward_mean: 1587.72
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 40320
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18696.719
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4114450514316559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016246817540377378
        model: {}
        policy_loss: -0.00162489153444767
        total_loss: 0.0011961841955780983
        vf_explained_var: 0.06129266321659088
        vf_loss: 35.45220947265625
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25013771653175354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013703040312975645
        model: {}
        policy_loss: -0.001857386901974678
        total_loss: 0.0009764945134520531
        vf_explained_var: 0.13348734378814697
        vf_loss: 32.741249084472656
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3024176359176636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013012283015996218
        model: {}
        policy_loss: -0.001977016683667898
        total_loss: 0.0041151526384055614
        vf_explained_var: 0.10095134377479553
        vf_loss: 66.24424743652344
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8130297064781189
        entropy_coeff: 0.0017600000137463212
        kl: 0.001084940624423325
        model: {}
        policy_loss: -0.002163209021091461
        total_loss: 0.0038225767202675343
        vf_explained_var: 0.005496993660926819
        vf_loss: 74.16716766357422
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.979893147945404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021979426965117455
        model: {}
        policy_loss: -0.0021136514842510223
        total_loss: -0.0003235442563891411
        vf_explained_var: 0.011671334505081177
        vf_loss: 35.147193908691406
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2580661177635193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007667721947655082
        model: {}
        policy_loss: -0.0019864914938807487
        total_loss: 0.0005351523868739605
        vf_explained_var: 0.16742762923240662
        vf_loss: 29.75840187072754
    load_time_ms: 13210.95
    num_steps_sampled: 40320000
    num_steps_trained: 40320000
    sample_time_ms: 93096.832
    update_time_ms: 19.101
  iterations_since_restore: 50
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 53.55112359550562
    ram_util_percent: 15.873033707865167
  pid: 21723
  policy_reward_max:
    agent-0: 285.0
    agent-1: 285.0
    agent-2: 379.5
    agent-3: 379.5
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 240.465
    agent-1: 240.465
    agent-2: 325.68
    agent-3: 325.68
    agent-4: 227.715
    agent-5: 227.715
  policy_reward_min:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 20.5
    agent-3: 20.5
    agent-4: 8.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 24.74155997821015
    mean_inference_ms: 12.396343712799204
    mean_processing_ms: 54.32353171067599
  time_since_restore: 6363.008271217346
  time_this_iter_s: 124.95297574996948
  time_total_s: 53444.55382323265
  timestamp: 1637564482
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 40320000
  training_iteration: 420
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    420 |          53444.6 | 40320000 |  1587.72 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 0.42
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 29.82
    apples_agent-1_min: 7
    apples_agent-2_max: 413
    apples_agent-2_mean: 349.77
    apples_agent-2_min: 72
    apples_agent-3_max: 227
    apples_agent-3_mean: 135.27
    apples_agent-3_min: 26
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.53
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 375.81
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 382.38
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 35
    cleaning_beam_agent-1_mean: 1.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 4.01
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 57.01
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 466.94
    cleaning_beam_agent-4_min: 397
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 4.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-03-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1701.0
  episode_reward_mean: 1565.79
  episode_reward_min: 338.0
  episodes_this_iter: 96
  episodes_total: 40416
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18681.444
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41553667187690735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009552794508635998
        model: {}
        policy_loss: -0.001364988274872303
        total_loss: 0.0014139069244265556
        vf_explained_var: 0.06244751811027527
        vf_loss: 35.10236358642578
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25200244784355164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007485630922019482
        model: {}
        policy_loss: -0.0017990055494010448
        total_loss: 0.001049312762916088
        vf_explained_var: 0.12528428435325623
        vf_loss: 32.918434143066406
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3114503026008606
        entropy_coeff: 0.0017600000137463212
        kl: 0.001094276667572558
        model: {}
        policy_loss: -0.002163668628782034
        total_loss: 0.003946701064705849
        vf_explained_var: 0.10890297591686249
        vf_loss: 66.58522033691406
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7845410108566284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005791974836029112
        model: {}
        policy_loss: -0.002006261609494686
        total_loss: 0.004035884514451027
        vf_explained_var: 0.01641245186328888
        vf_loss: 74.2293701171875
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9891968965530396
        entropy_coeff: 0.0017600000137463212
        kl: 0.001533222384750843
        model: {}
        policy_loss: -0.002059727441519499
        total_loss: -0.0003798999823629856
        vf_explained_var: 0.04341880977153778
        vf_loss: 34.20814514160156
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2671321630477905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007755473488941789
        model: {}
        policy_loss: -0.0020231464877724648
        total_loss: 0.0005476633086800575
        vf_explained_var: 0.14986029267311096
        vf_loss: 30.409622192382812
    load_time_ms: 13215.398
    num_steps_sampled: 40416000
    num_steps_trained: 40416000
    sample_time_ms: 93060.668
    update_time_ms: 19.134
  iterations_since_restore: 51
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.85056179775281
    ram_util_percent: 15.809550561797753
  pid: 21723
  policy_reward_max:
    agent-0: 264.5
    agent-1: 264.5
    agent-2: 379.0
    agent-3: 379.0
    agent-4: 249.5
    agent-5: 249.5
  policy_reward_mean:
    agent-0: 236.07
    agent-1: 236.07
    agent-2: 324.135
    agent-3: 324.135
    agent-4: 222.69
    agent-5: 222.69
  policy_reward_min:
    agent-0: 56.0
    agent-1: 56.0
    agent-2: 72.5
    agent-3: 72.5
    agent-4: 40.5
    agent-5: 40.5
  sampler_perf:
    mean_env_wait_ms: 24.73459139152359
    mean_inference_ms: 12.394733920489806
    mean_processing_ms: 54.3190976789464
  time_since_restore: 6487.354891777039
  time_this_iter_s: 124.34662055969238
  time_total_s: 53568.90044379234
  timestamp: 1637564607
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 40416000
  training_iteration: 421
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    421 |          53568.9 | 40416000 |  1565.79 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.12
    apples_agent-0_min: 0
    apples_agent-1_max: 45
    apples_agent-1_mean: 30.15
    apples_agent-1_min: 10
    apples_agent-2_max: 420
    apples_agent-2_mean: 352.0
    apples_agent-2_min: 128
    apples_agent-3_max: 219
    apples_agent-3_mean: 131.74
    apples_agent-3_min: 44
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.2
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 381.33
    apples_agent-5_min: 178
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 383.81
    cleaning_beam_agent-0_min: 332
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.49
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 4.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 56.54
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 463.32
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 4.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-05-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1699.0
  episode_reward_mean: 1573.11
  episode_reward_min: 624.0
  episodes_this_iter: 96
  episodes_total: 40512
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18671.07
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4184195399284363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015733883483335376
        model: {}
        policy_loss: -0.001723302761092782
        total_loss: 0.0010902577778324485
        vf_explained_var: 0.04733337461948395
        vf_loss: 35.49977111816406
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2520769238471985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007030583801679313
        model: {}
        policy_loss: -0.001634548301808536
        total_loss: 0.0012380885891616344
        vf_explained_var: 0.11075112223625183
        vf_loss: 33.162925720214844
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.314063161611557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007177324150688946
        model: {}
        policy_loss: -0.001770785078406334
        total_loss: 0.004469162318855524
        vf_explained_var: 0.09168489277362823
        vf_loss: 67.92697143554688
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7850282192230225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014875632477924228
        model: {}
        policy_loss: -0.0020471324678510427
        total_loss: 0.0038795447908341885
        vf_explained_var: 0.03528276085853577
        vf_loss: 73.0832748413086
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.975885272026062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015436203684657812
        model: {}
        policy_loss: -0.002056858502328396
        total_loss: -0.0005495203658938408
        vf_explained_var: 0.053582802414894104
        vf_loss: 32.24896240234375
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26076164841651917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010259328410029411
        model: {}
        policy_loss: -0.001894125947728753
        total_loss: 0.0004412268754094839
        vf_explained_var: 0.1789436936378479
        vf_loss: 27.942955017089844
    load_time_ms: 13181.861
    num_steps_sampled: 40512000
    num_steps_trained: 40512000
    sample_time_ms: 93159.702
    update_time_ms: 19.303
  iterations_since_restore: 52
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.00833333333333
    ram_util_percent: 15.85777777777778
  pid: 21723
  policy_reward_max:
    agent-0: 265.0
    agent-1: 265.0
    agent-2: 382.5
    agent-3: 382.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 237.915
    agent-1: 237.915
    agent-2: 324.475
    agent-3: 324.475
    agent-4: 224.165
    agent-5: 224.165
  policy_reward_min:
    agent-0: 94.0
    agent-1: 94.0
    agent-2: 120.5
    agent-3: 120.5
    agent-4: 97.5
    agent-5: 97.5
  sampler_perf:
    mean_env_wait_ms: 24.73315031390411
    mean_inference_ms: 12.395991474891703
    mean_processing_ms: 54.329574358569545
  time_since_restore: 6613.913172006607
  time_this_iter_s: 126.55828022956848
  time_total_s: 53695.45872402191
  timestamp: 1637564734
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 40512000
  training_iteration: 422
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    422 |          53695.5 | 40512000 |  1573.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 0.47
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 30.7
    apples_agent-1_min: 3
    apples_agent-2_max: 415
    apples_agent-2_mean: 355.05
    apples_agent-2_min: 22
    apples_agent-3_max: 211
    apples_agent-3_mean: 136.76
    apples_agent-3_min: 14
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 380.7
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 390.76
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 32
    cleaning_beam_agent-1_mean: 1.63
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 33
    cleaning_beam_agent-2_mean: 4.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 59.73
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 466.17
    cleaning_beam_agent-4_min: 396
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 3.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-07-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1727.0
  episode_reward_mean: 1581.27
  episode_reward_min: 151.0
  episodes_this_iter: 96
  episodes_total: 40608
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18656.568
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4183838963508606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008321964414790273
        model: {}
        policy_loss: -0.0014319958863779902
        total_loss: 0.0014793616719543934
        vf_explained_var: 0.05906938016414642
        vf_loss: 36.477115631103516
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24769757688045502
        entropy_coeff: 0.0017600000137463212
        kl: 0.001046575140208006
        model: {}
        policy_loss: -0.0019653397612273693
        total_loss: 0.0009594472358003259
        vf_explained_var: 0.13293856382369995
        vf_loss: 33.60732650756836
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30576401948928833
        entropy_coeff: 0.0017600000137463212
        kl: 0.000986088183708489
        model: {}
        policy_loss: -0.002075214870274067
        total_loss: 0.004225726239383221
        vf_explained_var: 0.10578116774559021
        vf_loss: 68.39085388183594
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8064239025115967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007841795450076461
        model: {}
        policy_loss: -0.002254772000014782
        total_loss: 0.004055376164615154
        vf_explained_var: 0.00500449538230896
        vf_loss: 77.29454040527344
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9627318382263184
        entropy_coeff: 0.0017600000137463212
        kl: 0.002495206193998456
        model: {}
        policy_loss: -0.0021021575666964054
        total_loss: -0.00041738152503967285
        vf_explained_var: 0.029690325260162354
        vf_loss: 33.79181671142578
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25056737661361694
        entropy_coeff: 0.0017600000137463212
        kl: 0.000707463885191828
        model: {}
        policy_loss: -0.0019201547838747501
        total_loss: 0.0005725992377847433
        vf_explained_var: 0.15744894742965698
        vf_loss: 29.3375301361084
    load_time_ms: 13184.251
    num_steps_sampled: 40608000
    num_steps_trained: 40608000
    sample_time_ms: 93088.813
    update_time_ms: 19.072
  iterations_since_restore: 53
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.03854748603353
    ram_util_percent: 15.806703910614525
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 392.0
    agent-3: 392.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 237.395
    agent-1: 237.395
    agent-2: 327.305
    agent-3: 327.305
    agent-4: 225.935
    agent-5: 225.935
  policy_reward_min:
    agent-0: 24.0
    agent-1: 24.0
    agent-2: 31.0
    agent-3: 31.0
    agent-4: 20.5
    agent-5: 20.5
  sampler_perf:
    mean_env_wait_ms: 24.73057771550331
    mean_inference_ms: 12.396561271702756
    mean_processing_ms: 54.333271347234515
  time_since_restore: 6739.361847877502
  time_this_iter_s: 125.44867587089539
  time_total_s: 53820.90739989281
  timestamp: 1637564860
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 40608000
  training_iteration: 423
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    423 |          53820.9 | 40608000 |  1581.27 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 31.91
    apples_agent-1_min: 9
    apples_agent-2_max: 426
    apples_agent-2_mean: 358.33
    apples_agent-2_min: 51
    apples_agent-3_max: 234
    apples_agent-3_mean: 136.22
    apples_agent-3_min: 51
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 395.53
    apples_agent-5_min: 287
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 383.94
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 40
    cleaning_beam_agent-1_mean: 2.0
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 4.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 62.36
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 475.62
    cleaning_beam_agent-4_min: 428
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 2.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-09-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1741.0
  episode_reward_mean: 1609.32
  episode_reward_min: 806.0
  episodes_this_iter: 96
  episodes_total: 40704
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18669.373
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4062550961971283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015743845142424107
        model: {}
        policy_loss: -0.0013061224017292261
        total_loss: 0.0015954691916704178
        vf_explained_var: 0.006346121430397034
        vf_loss: 36.1660041809082
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2393902689218521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008631239761598408
        model: {}
        policy_loss: -0.0017728237435221672
        total_loss: 0.0011751092970371246
        vf_explained_var: 0.07379673421382904
        vf_loss: 33.69258117675781
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29833200573921204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008463291451334953
        model: {}
        policy_loss: -0.0020074020139873028
        total_loss: 0.0042336322367191315
        vf_explained_var: 0.04236102104187012
        vf_loss: 67.66099548339844
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8145195841789246
        entropy_coeff: 0.0017600000137463212
        kl: 0.001166410744190216
        model: {}
        policy_loss: -0.002155683469027281
        total_loss: 0.0035809138789772987
        vf_explained_var: -0.003998428583145142
        vf_loss: 71.70153045654297
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9633601903915405
        entropy_coeff: 0.0017600000137463212
        kl: 0.00238455506041646
        model: {}
        policy_loss: -0.0019337000558152795
        total_loss: -0.0003835936076939106
        vf_explained_var: 0.011428236961364746
        vf_loss: 32.456207275390625
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2401159703731537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006914763944223523
        model: {}
        policy_loss: -0.0017352253198623657
        total_loss: 0.0007340121082961559
        vf_explained_var: 0.11945776641368866
        vf_loss: 28.918407440185547
    load_time_ms: 13197.01
    num_steps_sampled: 40704000
    num_steps_trained: 40704000
    sample_time_ms: 93117.091
    update_time_ms: 19.096
  iterations_since_restore: 54
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.669999999999995
    ram_util_percent: 15.867222222222225
  pid: 21723
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 374.5
    agent-3: 374.5
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 242.435
    agent-1: 242.435
    agent-2: 328.83
    agent-3: 328.83
    agent-4: 233.395
    agent-5: 233.395
  policy_reward_min:
    agent-0: 162.5
    agent-1: 162.5
    agent-2: 66.0
    agent-3: 66.0
    agent-4: 174.5
    agent-5: 174.5
  sampler_perf:
    mean_env_wait_ms: 24.73213206090219
    mean_inference_ms: 12.397653401333118
    mean_processing_ms: 54.342890756158155
  time_since_restore: 6865.678724527359
  time_this_iter_s: 126.31687664985657
  time_total_s: 53947.22427654266
  timestamp: 1637564986
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 40704000
  training_iteration: 424
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    424 |          53947.2 | 40704000 |  1609.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.15
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.43
    apples_agent-1_min: 16
    apples_agent-2_max: 416
    apples_agent-2_mean: 355.16
    apples_agent-2_min: 185
    apples_agent-3_max: 200
    apples_agent-3_mean: 136.83
    apples_agent-3_min: 53
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.44
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 390.04
    apples_agent-5_min: 265
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 393.21
    cleaning_beam_agent-0_min: 326
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 2.25
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 3.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 64.09
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 478.36
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 3.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-11-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1728.0
  episode_reward_mean: 1588.04
  episode_reward_min: 941.0
  episodes_this_iter: 96
  episodes_total: 40800
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18708.085
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42146071791648865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021045671310275793
        model: {}
        policy_loss: -0.0017185406759381294
        total_loss: 0.0011474480852484703
        vf_explained_var: 0.04129679501056671
        vf_loss: 36.077598571777344
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24736879765987396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008578946581110358
        model: {}
        policy_loss: -0.0018305397825315595
        total_loss: 0.0010714693926274776
        vf_explained_var: 0.11372427642345428
        vf_loss: 33.37379455566406
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31394511461257935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011534900404512882
        model: {}
        policy_loss: -0.0019168988801538944
        total_loss: 0.004350374918431044
        vf_explained_var: 0.0758189707994461
        vf_loss: 68.19820404052734
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8183939456939697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013690771302208304
        model: {}
        policy_loss: -0.002346619963645935
        total_loss: 0.003671445418149233
        vf_explained_var: -0.0026935338973999023
        vf_loss: 74.58438110351562
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9681159257888794
        entropy_coeff: 0.0017600000137463212
        kl: 0.001249796012416482
        model: {}
        policy_loss: -0.001724702655337751
        total_loss: -0.00018693390302360058
        vf_explained_var: 0.04367890954017639
        vf_loss: 32.41652297973633
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25616830587387085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007565364940091968
        model: {}
        policy_loss: -0.002129055093973875
        total_loss: 0.00031680334359407425
        vf_explained_var: 0.14669853448867798
        vf_loss: 28.967172622680664
    load_time_ms: 13192.429
    num_steps_sampled: 40800000
    num_steps_trained: 40800000
    sample_time_ms: 93303.854
    update_time_ms: 19.301
  iterations_since_restore: 55
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.26111111111111
    ram_util_percent: 15.799444444444445
  pid: 21723
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 366.0
    agent-3: 366.0
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 238.12
    agent-1: 238.12
    agent-2: 326.795
    agent-3: 326.795
    agent-4: 229.105
    agent-5: 229.105
  policy_reward_min:
    agent-0: 135.5
    agent-1: 135.5
    agent-2: 175.0
    agent-3: 175.0
    agent-4: 160.0
    agent-5: 160.0
  sampler_perf:
    mean_env_wait_ms: 24.73556647112082
    mean_inference_ms: 12.398763733665799
    mean_processing_ms: 54.355821963291135
  time_since_restore: 6992.2280242443085
  time_this_iter_s: 126.54929971694946
  time_total_s: 54073.77357625961
  timestamp: 1637565113
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 40800000
  training_iteration: 425
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    425 |          54073.8 | 40800000 |  1588.04 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 0.2
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.96
    apples_agent-1_min: 18
    apples_agent-2_max: 423
    apples_agent-2_mean: 354.47
    apples_agent-2_min: 270
    apples_agent-3_max: 196
    apples_agent-3_mean: 134.24
    apples_agent-3_min: 53
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 457
    apples_agent-5_mean: 389.18
    apples_agent-5_min: 316
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 391.62
    cleaning_beam_agent-0_min: 339
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 1.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 3.08
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 60.86
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 597
    cleaning_beam_agent-4_mean: 477.76
    cleaning_beam_agent-4_min: 402
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 2.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-13-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1738.0
  episode_reward_mean: 1589.61
  episode_reward_min: 1126.0
  episodes_this_iter: 96
  episodes_total: 40896
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18692.8
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42941707372665405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019575522746890783
        model: {}
        policy_loss: -0.0013329610228538513
        total_loss: 0.0014099786058068275
        vf_explained_var: 0.01002703607082367
        vf_loss: 34.98713684082031
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24195930361747742
        entropy_coeff: 0.0017600000137463212
        kl: 0.001244895625859499
        model: {}
        policy_loss: -0.0018112321849912405
        total_loss: 0.0008782793302088976
        vf_explained_var: 0.11837369203567505
        vf_loss: 31.1535587310791
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30650874972343445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009665183024480939
        model: {}
        policy_loss: -0.0019818670116364956
        total_loss: 0.004220476374030113
        vf_explained_var: 0.037587836384773254
        vf_loss: 67.41799926757812
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8119745254516602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008013183833099902
        model: {}
        policy_loss: -0.002144026104360819
        total_loss: 0.00346711790189147
        vf_explained_var: -7.3015689849853516e-06
        vf_loss: 70.4021987915039
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.948643684387207
        entropy_coeff: 0.0017600000137463212
        kl: 0.002997935749590397
        model: {}
        policy_loss: -0.0019861310720443726
        total_loss: -0.0003817579708993435
        vf_explained_var: 0.020222678780555725
        vf_loss: 32.7398681640625
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25041624903678894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010464777005836368
        model: {}
        policy_loss: -0.0019383952021598816
        total_loss: 0.0005119950510561466
        vf_explained_var: 0.13395243883132935
        vf_loss: 28.91122055053711
    load_time_ms: 13194.08
    num_steps_sampled: 40896000
    num_steps_trained: 40896000
    sample_time_ms: 93304.304
    update_time_ms: 19.306
  iterations_since_restore: 56
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.38707865168539
    ram_util_percent: 15.80112359550562
  pid: 21723
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 362.5
    agent-3: 362.5
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 241.29
    agent-1: 241.29
    agent-2: 324.825
    agent-3: 324.825
    agent-4: 228.69
    agent-5: 228.69
  policy_reward_min:
    agent-0: 170.0
    agent-1: 170.0
    agent-2: 213.0
    agent-3: 213.0
    agent-4: 180.0
    agent-5: 180.0
  sampler_perf:
    mean_env_wait_ms: 24.734036782194963
    mean_inference_ms: 12.39940642394461
    mean_processing_ms: 54.35451449144807
  time_since_restore: 7116.999387025833
  time_this_iter_s: 124.77136278152466
  time_total_s: 54198.54493904114
  timestamp: 1637565238
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 40896000
  training_iteration: 426
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    426 |          54198.5 | 40896000 |  1589.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 30.04
    apples_agent-1_min: 14
    apples_agent-2_max: 421
    apples_agent-2_mean: 352.09
    apples_agent-2_min: 286
    apples_agent-3_max: 206
    apples_agent-3_mean: 138.54
    apples_agent-3_min: 65
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 385.6
    apples_agent-5_min: 324
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 387.96
    cleaning_beam_agent-0_min: 340
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.68
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 4.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 63.6
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 597
    cleaning_beam_agent-4_mean: 491.23
    cleaning_beam_agent-4_min: 414
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 3.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-16-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1713.0
  episode_reward_mean: 1590.95
  episode_reward_min: 1358.0
  episodes_this_iter: 96
  episodes_total: 40992
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18677.401
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4321017861366272
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014943921705707908
        model: {}
        policy_loss: -0.0016859497409313917
        total_loss: 0.0009764928836375475
        vf_explained_var: -0.010167598724365234
        vf_loss: 34.22941589355469
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23911236226558685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009309283923357725
        model: {}
        policy_loss: -0.0016635074280202389
        total_loss: 0.001042715273797512
        vf_explained_var: 0.0760916918516159
        vf_loss: 31.2706298828125
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3075728714466095
        entropy_coeff: 0.0017600000137463212
        kl: 0.000799581641331315
        model: {}
        policy_loss: -0.0018694614991545677
        total_loss: 0.004203749820590019
        vf_explained_var: 0.028405576944351196
        vf_loss: 66.1454086303711
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8185370564460754
        entropy_coeff: 0.0017600000137463212
        kl: 0.001268549356609583
        model: {}
        policy_loss: -0.00212473189458251
        total_loss: 0.0032468028366565704
        vf_explained_var: 0.00983409583568573
        vf_loss: 68.12158203125
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9417411684989929
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025787882041186094
        model: {}
        policy_loss: -0.001858733594417572
        total_loss: -0.0005304208025336266
        vf_explained_var: 0.015581756830215454
        vf_loss: 29.857778549194336
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25210726261138916
        entropy_coeff: 0.0017600000137463212
        kl: 0.000727600883692503
        model: {}
        policy_loss: -0.001626154175028205
        total_loss: 0.0006716901552863419
        vf_explained_var: 0.09765540063381195
        vf_loss: 27.415542602539062
    load_time_ms: 13186.981
    num_steps_sampled: 40992000
    num_steps_trained: 40992000
    sample_time_ms: 93256.725
    update_time_ms: 18.874
  iterations_since_restore: 57
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 60.25730337078651
    ram_util_percent: 15.771348314606742
  pid: 21723
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 371.0
    agent-3: 371.0
    agent-4: 262.5
    agent-5: 262.5
  policy_reward_mean:
    agent-0: 240.01
    agent-1: 240.01
    agent-2: 327.715
    agent-3: 327.715
    agent-4: 227.75
    agent-5: 227.75
  policy_reward_min:
    agent-0: 190.5
    agent-1: 190.5
    agent-2: 264.0
    agent-3: 264.0
    agent-4: 191.5
    agent-5: 191.5
  sampler_perf:
    mean_env_wait_ms: 24.732347640997222
    mean_inference_ms: 12.398351885453932
    mean_processing_ms: 54.34859930910754
  time_since_restore: 7241.613789558411
  time_this_iter_s: 124.61440253257751
  time_total_s: 54323.159341573715
  timestamp: 1637565363
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 40992000
  training_iteration: 427
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    427 |          54323.2 | 40992000 |  1590.95 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 30.67
    apples_agent-1_min: 19
    apples_agent-2_max: 417
    apples_agent-2_mean: 348.46
    apples_agent-2_min: 264
    apples_agent-3_max: 228
    apples_agent-3_mean: 132.41
    apples_agent-3_min: 61
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 381.01
    apples_agent-5_min: 292
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 391.14
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 33
    cleaning_beam_agent-2_mean: 4.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 58.25
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 492.07
    cleaning_beam_agent-4_min: 414
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 3.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-18-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1720.0
  episode_reward_mean: 1575.08
  episode_reward_min: 1272.0
  episodes_this_iter: 96
  episodes_total: 41088
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18677.056
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43236589431762695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015962175093591213
        model: {}
        policy_loss: -0.0015812460333108902
        total_loss: 0.0011849123984575272
        vf_explained_var: 0.020422950387001038
        vf_loss: 35.27125930786133
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.245473712682724
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009298749500885606
        model: {}
        policy_loss: -0.001853658934123814
        total_loss: 0.000991082633845508
        vf_explained_var: 0.09108521044254303
        vf_loss: 32.767757415771484
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30881059169769287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012386023299768567
        model: {}
        policy_loss: -0.001958683365955949
        total_loss: 0.003770295064896345
        vf_explained_var: 0.04587896168231964
        vf_loss: 62.72486114501953
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8085089921951294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013709485065191984
        model: {}
        policy_loss: -0.0019654780626296997
        total_loss: 0.0031088506802916527
        vf_explained_var: 0.016552463173866272
        vf_loss: 64.9730453491211
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9341722726821899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029345431830734015
        model: {}
        policy_loss: -0.002051485702395439
        total_loss: -0.0005553878145292401
        vf_explained_var: 0.036886513233184814
        vf_loss: 31.402429580688477
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2616252303123474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008510838379152119
        model: {}
        policy_loss: -0.0018060244619846344
        total_loss: 0.0005534235388040543
        vf_explained_var: 0.13521699607372284
        vf_loss: 28.199081420898438
    load_time_ms: 13181.124
    num_steps_sampled: 41088000
    num_steps_trained: 41088000
    sample_time_ms: 93243.603
    update_time_ms: 18.629
  iterations_since_restore: 58
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.76779661016949
    ram_util_percent: 15.864971751412432
  pid: 21723
  policy_reward_max:
    agent-0: 266.0
    agent-1: 266.0
    agent-2: 367.0
    agent-3: 367.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 237.74
    agent-1: 237.74
    agent-2: 324.775
    agent-3: 324.775
    agent-4: 225.025
    agent-5: 225.025
  policy_reward_min:
    agent-0: 184.5
    agent-1: 184.5
    agent-2: 272.5
    agent-3: 272.5
    agent-4: 167.5
    agent-5: 167.5
  sampler_perf:
    mean_env_wait_ms: 24.729696462008043
    mean_inference_ms: 12.39727400618171
    mean_processing_ms: 54.34541467686044
  time_since_restore: 7366.325237989426
  time_this_iter_s: 124.71144843101501
  time_total_s: 54447.87079000473
  timestamp: 1637565487
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 41088000
  training_iteration: 428
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    428 |          54447.9 | 41088000 |  1575.08 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.32
    apples_agent-1_min: 5
    apples_agent-2_max: 423
    apples_agent-2_mean: 351.12
    apples_agent-2_min: 97
    apples_agent-3_max: 227
    apples_agent-3_mean: 131.19
    apples_agent-3_min: 20
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 382.95
    apples_agent-5_min: 119
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 393.02
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 3.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 57.34
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 507.58
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 3.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-20-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1762.0
  episode_reward_mean: 1576.41
  episode_reward_min: 479.0
  episodes_this_iter: 96
  episodes_total: 41184
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18696.083
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43141746520996094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018215685850009322
        model: {}
        policy_loss: -0.0014404295943677425
        total_loss: 0.0015573306009173393
        vf_explained_var: 0.027783706784248352
        vf_loss: 37.570552825927734
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24806943535804749
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006700321682728827
        model: {}
        policy_loss: -0.0019179526716470718
        total_loss: 0.0011040577664971352
        vf_explained_var: 0.10686402022838593
        vf_loss: 34.586124420166016
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30996137857437134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010913463775068521
        model: {}
        policy_loss: -0.001943375333212316
        total_loss: 0.004510172177106142
        vf_explained_var: 0.07139931619167328
        vf_loss: 69.99076843261719
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7997013330459595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020339516922831535
        model: {}
        policy_loss: -0.002355583943426609
        total_loss: 0.0038603832945227623
        vf_explained_var: -0.01127934455871582
        vf_loss: 76.23443603515625
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9219832420349121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033531575463712215
        model: {}
        policy_loss: -0.002486257813870907
        total_loss: -0.0009065777994692326
        vf_explained_var: 0.08056449890136719
        vf_loss: 32.023712158203125
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2674075663089752
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006125723011791706
        model: {}
        policy_loss: -0.001773616299033165
        total_loss: 0.0007922144141048193
        vf_explained_var: 0.12903103232383728
        vf_loss: 30.364652633666992
    load_time_ms: 13186.412
    num_steps_sampled: 41184000
    num_steps_trained: 41184000
    sample_time_ms: 93490.764
    update_time_ms: 18.666
  iterations_since_restore: 59
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 53.23722222222223
    ram_util_percent: 16.00333333333333
  pid: 21723
  policy_reward_max:
    agent-0: 266.0
    agent-1: 266.0
    agent-2: 399.5
    agent-3: 399.5
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 237.83
    agent-1: 237.83
    agent-2: 324.43
    agent-3: 324.43
    agent-4: 225.945
    agent-5: 225.945
  policy_reward_min:
    agent-0: 70.0
    agent-1: 70.0
    agent-2: 82.5
    agent-3: 82.5
    agent-4: 87.0
    agent-5: 87.0
  sampler_perf:
    mean_env_wait_ms: 24.731044924056235
    mean_inference_ms: 12.397277032285455
    mean_processing_ms: 54.35188750320292
  time_since_restore: 7492.754102230072
  time_this_iter_s: 126.42886424064636
  time_total_s: 54574.29965424538
  timestamp: 1637565614
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 41184000
  training_iteration: 429
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    429 |          54574.3 | 41184000 |  1576.41 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 30.92
    apples_agent-1_min: 15
    apples_agent-2_max: 411
    apples_agent-2_mean: 354.28
    apples_agent-2_min: 283
    apples_agent-3_max: 212
    apples_agent-3_mean: 136.17
    apples_agent-3_min: 48
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 385.93
    apples_agent-5_min: 330
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 380.68
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.44
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 3.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 59.07
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 608
    cleaning_beam_agent-4_mean: 524.36
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-22-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1700.0
  episode_reward_mean: 1595.92
  episode_reward_min: 1392.0
  episodes_this_iter: 96
  episodes_total: 41280
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18708.434
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42514100670814514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010032695718109608
        model: {}
        policy_loss: -0.0012493969406932592
        total_loss: 0.0014160163700580597
        vf_explained_var: -0.019772320985794067
        vf_loss: 34.136627197265625
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24039992690086365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008003789698705077
        model: {}
        policy_loss: -0.0016376395942643285
        total_loss: 0.0010370835661888123
        vf_explained_var: 0.0733983963727951
        vf_loss: 30.978286743164062
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30209240317344666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007444345392286777
        model: {}
        policy_loss: -0.0016702285502105951
        total_loss: 0.004073005635291338
        vf_explained_var: 0.0396016389131546
        vf_loss: 62.74916076660156
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.815817654132843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016135988989844918
        model: {}
        policy_loss: -0.0021124277263879776
        total_loss: 0.002968756016343832
        vf_explained_var: 0.0033015161752700806
        vf_loss: 65.17022705078125
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9414706826210022
        entropy_coeff: 0.0017600000137463212
        kl: 0.002213438041508198
        model: {}
        policy_loss: -0.002058943035081029
        total_loss: -0.0006355307996273041
        vf_explained_var: 0.04214753210544586
        vf_loss: 30.804019927978516
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26056796312332153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012755854986608028
        model: {}
        policy_loss: -0.0017083741258829832
        total_loss: 0.0006605497910641134
        vf_explained_var: 0.1217125654220581
        vf_loss: 28.27523422241211
    load_time_ms: 13194.702
    num_steps_sampled: 41280000
    num_steps_trained: 41280000
    sample_time_ms: 93545.06
    update_time_ms: 18.533
  iterations_since_restore: 60
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.63966480446928
    ram_util_percent: 15.98603351955307
  pid: 21723
  policy_reward_max:
    agent-0: 263.5
    agent-1: 263.5
    agent-2: 370.5
    agent-3: 370.5
    agent-4: 254.0
    agent-5: 254.0
  policy_reward_mean:
    agent-0: 240.42
    agent-1: 240.42
    agent-2: 329.11
    agent-3: 329.11
    agent-4: 228.43
    agent-5: 228.43
  policy_reward_min:
    agent-0: 196.5
    agent-1: 196.5
    agent-2: 280.0
    agent-3: 280.0
    agent-4: 191.5
    agent-5: 191.5
  sampler_perf:
    mean_env_wait_ms: 24.73120948730777
    mean_inference_ms: 12.398160385667635
    mean_processing_ms: 54.3529854114133
  time_since_restore: 7618.456413269043
  time_this_iter_s: 125.70231103897095
  time_total_s: 54700.00196528435
  timestamp: 1637565740
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 41280000
  training_iteration: 430
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    430 |            54700 | 41280000 |  1595.92 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 29.75
    apples_agent-1_min: 4
    apples_agent-2_max: 425
    apples_agent-2_mean: 357.79
    apples_agent-2_min: 65
    apples_agent-3_max: 220
    apples_agent-3_mean: 136.56
    apples_agent-3_min: 25
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 385.68
    apples_agent-5_min: 63
    cleaning_beam_agent-0_max: 423
    cleaning_beam_agent-0_mean: 376.8
    cleaning_beam_agent-0_min: 310
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 2.31
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 4.88
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 106
    cleaning_beam_agent-3_mean: 58.89
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 499.33
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 3.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-24-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1779.0
  episode_reward_mean: 1588.61
  episode_reward_min: 258.0
  episodes_this_iter: 96
  episodes_total: 41376
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18703.263
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41954025626182556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016177988145500422
        model: {}
        policy_loss: -0.0016616899520158768
        total_loss: 0.0012975437566637993
        vf_explained_var: 0.05734437704086304
        vf_loss: 36.97623825073242
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24660491943359375
        entropy_coeff: 0.0017600000137463212
        kl: 0.001063953503035009
        model: {}
        policy_loss: -0.0018933294340968132
        total_loss: 0.001077953726053238
        vf_explained_var: 0.13089478015899658
        vf_loss: 34.053077697753906
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3034144639968872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008836716297082603
        model: {}
        policy_loss: -0.0019550654105842113
        total_loss: 0.004379626829177141
        vf_explained_var: 0.060395970940589905
        vf_loss: 68.6869888305664
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8148431777954102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010921197244897485
        model: {}
        policy_loss: -0.0023442707024514675
        total_loss: 0.003588737454265356
        vf_explained_var: -0.002190724015235901
        vf_loss: 73.67132568359375
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9636027216911316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017024771077558398
        model: {}
        policy_loss: -0.0021330174058675766
        total_loss: -0.0005609281361103058
        vf_explained_var: 0.05847372114658356
        vf_loss: 32.680267333984375
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26942604780197144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010506035760045052
        model: {}
        policy_loss: -0.0019490106496959925
        total_loss: 0.0005888496525585651
        vf_explained_var: 0.13225150108337402
        vf_loss: 30.120527267456055
    load_time_ms: 13175.002
    num_steps_sampled: 41376000
    num_steps_trained: 41376000
    sample_time_ms: 93642.953
    update_time_ms: 18.469
  iterations_since_restore: 61
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.498333333333335
    ram_util_percent: 15.948333333333332
  pid: 21723
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 381.5
    agent-3: 381.5
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 239.16
    agent-1: 239.16
    agent-2: 327.655
    agent-3: 327.655
    agent-4: 227.49
    agent-5: 227.49
  policy_reward_min:
    agent-0: 21.5
    agent-1: 21.5
    agent-2: 68.5
    agent-3: 68.5
    agent-4: 39.0
    agent-5: 39.0
  sampler_perf:
    mean_env_wait_ms: 24.728735675483453
    mean_inference_ms: 12.396951241634357
    mean_processing_ms: 54.35162843159051
  time_since_restore: 7743.53222322464
  time_this_iter_s: 125.07580995559692
  time_total_s: 54825.077775239944
  timestamp: 1637565866
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 41376000
  training_iteration: 431
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    431 |          54825.1 | 41376000 |  1588.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 30.82
    apples_agent-1_min: 14
    apples_agent-2_max: 405
    apples_agent-2_mean: 352.74
    apples_agent-2_min: 236
    apples_agent-3_max: 212
    apples_agent-3_mean: 139.32
    apples_agent-3_min: 68
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 383.36
    apples_agent-5_min: 271
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 370.09
    cleaning_beam_agent-0_min: 280
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 1.59
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 4.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 59.63
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 503.15
    cleaning_beam_agent-4_min: 426
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 3.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-26-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1720.0
  episode_reward_mean: 1585.49
  episode_reward_min: 1189.0
  episodes_this_iter: 96
  episodes_total: 41472
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18701.367
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4094173312187195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011033938499167562
        model: {}
        policy_loss: -0.001302589662373066
        total_loss: 0.001378465909510851
        vf_explained_var: 0.017517581582069397
        vf_loss: 34.01628875732422
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24426648020744324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011992999352514744
        model: {}
        policy_loss: -0.0018824368016794324
        total_loss: 0.0008140753488987684
        vf_explained_var: 0.09695810079574585
        vf_loss: 31.264190673828125
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30529671907424927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008800948271527886
        model: {}
        policy_loss: -0.0018266862025484443
        total_loss: 0.004108618479222059
        vf_explained_var: 0.039647042751312256
        vf_loss: 64.72625732421875
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8253037333488464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010424263309687376
        model: {}
        policy_loss: -0.002322711981832981
        total_loss: 0.0029691895470023155
        vf_explained_var: 0.004077479243278503
        vf_loss: 67.44438171386719
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9631083011627197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020448360592126846
        model: {}
        policy_loss: -0.0019117614720016718
        total_loss: -0.00048119015991687775
        vf_explained_var: 0.05335432291030884
        vf_loss: 31.256452560424805
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2735655605792999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010050489800050855
        model: {}
        policy_loss: -0.001867363229393959
        total_loss: 0.0005758777260780334
        vf_explained_var: 0.11221490800380707
        vf_loss: 29.247182846069336
    load_time_ms: 13173.536
    num_steps_sampled: 41472000
    num_steps_trained: 41472000
    sample_time_ms: 93544.219
    update_time_ms: 18.793
  iterations_since_restore: 62
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.755307262569836
    ram_util_percent: 15.907262569832403
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 379.5
    agent-3: 379.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 239.165
    agent-1: 239.165
    agent-2: 326.24
    agent-3: 326.24
    agent-4: 227.34
    agent-5: 227.34
  policy_reward_min:
    agent-0: 172.5
    agent-1: 172.5
    agent-2: 248.0
    agent-3: 248.0
    agent-4: 167.5
    agent-5: 167.5
  sampler_perf:
    mean_env_wait_ms: 24.727376588468825
    mean_inference_ms: 12.395934218058539
    mean_processing_ms: 54.35092104861117
  time_since_restore: 7869.073397159576
  time_this_iter_s: 125.54117393493652
  time_total_s: 54950.61894917488
  timestamp: 1637565992
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 41472000
  training_iteration: 432
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    432 |          54950.6 | 41472000 |  1585.49 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.16
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 32.2
    apples_agent-1_min: 9
    apples_agent-2_max: 404
    apples_agent-2_mean: 353.71
    apples_agent-2_min: 126
    apples_agent-3_max: 213
    apples_agent-3_mean: 138.73
    apples_agent-3_min: 26
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 383.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 370.05
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 31
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 4.07
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 63.23
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 500.57
    cleaning_beam_agent-4_min: 443
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 3.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-28-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1722.0
  episode_reward_mean: 1579.97
  episode_reward_min: 352.0
  episodes_this_iter: 96
  episodes_total: 41568
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18687.508
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41195645928382874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011941377306357026
        model: {}
        policy_loss: -0.0013528724666684866
        total_loss: 0.0015404471196234226
        vf_explained_var: 0.04105260968208313
        vf_loss: 36.18364334106445
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2468356192111969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013556622434407473
        model: {}
        policy_loss: -0.001813385752029717
        total_loss: 0.001151912845671177
        vf_explained_var: 0.09881587326526642
        vf_loss: 33.99729919433594
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3036350607872009
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009000126156024635
        model: {}
        policy_loss: -0.0017602697480469942
        total_loss: 0.004540424328297377
        vf_explained_var: 0.08020226657390594
        vf_loss: 68.3509292602539
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8417443633079529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015386181185021996
        model: {}
        policy_loss: -0.00248170318081975
        total_loss: 0.00343587389215827
        vf_explained_var: 0.006189629435539246
        vf_loss: 73.99047088623047
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9559794664382935
        entropy_coeff: 0.0017600000137463212
        kl: 0.00156287825666368
        model: {}
        policy_loss: -0.0017017358914017677
        total_loss: 0.00014865119010210037
        vf_explained_var: 0.05950680375099182
        vf_loss: 35.3290901184082
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27190664410591125
        entropy_coeff: 0.0017600000137463212
        kl: 0.001166604459285736
        model: {}
        policy_loss: -0.002010467927902937
        total_loss: 0.0007133255712687969
        vf_explained_var: 0.14579160511493683
        vf_loss: 32.023494720458984
    load_time_ms: 13149.461
    num_steps_sampled: 41568000
    num_steps_trained: 41568000
    sample_time_ms: 93539.673
    update_time_ms: 18.738
  iterations_since_restore: 63
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.73595505617976
    ram_util_percent: 15.907303370786517
  pid: 21723
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 367.0
    agent-3: 367.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 238.01
    agent-1: 238.01
    agent-2: 326.24
    agent-3: 326.24
    agent-4: 225.735
    agent-5: 225.735
  policy_reward_min:
    agent-0: 74.5
    agent-1: 74.5
    agent-2: 101.5
    agent-3: 101.5
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 24.725280134941073
    mean_inference_ms: 12.396122374740058
    mean_processing_ms: 54.35051328427774
  time_since_restore: 7994.117999792099
  time_this_iter_s: 125.04460263252258
  time_total_s: 55075.6635518074
  timestamp: 1637566117
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 41568000
  training_iteration: 433
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    433 |          55075.7 | 41568000 |  1579.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 0.21
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 30.05
    apples_agent-1_min: 13
    apples_agent-2_max: 406
    apples_agent-2_mean: 350.15
    apples_agent-2_min: 107
    apples_agent-3_max: 211
    apples_agent-3_mean: 135.72
    apples_agent-3_min: 13
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 380.26
    apples_agent-5_min: 108
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 369.74
    cleaning_beam_agent-0_min: 326
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.24
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 4.31
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 64.56
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 488.54
    cleaning_beam_agent-4_min: 378
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 3.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-30-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1698.0
  episode_reward_mean: 1574.21
  episode_reward_min: 437.0
  episodes_this_iter: 96
  episodes_total: 41664
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18648.952
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4178088307380676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011988301994279027
        model: {}
        policy_loss: -0.0014176219701766968
        total_loss: 0.0014869687147438526
        vf_explained_var: 0.03537014126777649
        vf_loss: 36.39932632446289
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24517223238945007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008083466673269868
        model: {}
        policy_loss: -0.0017889465671032667
        total_loss: 0.001246870495378971
        vf_explained_var: 0.08119849860668182
        vf_loss: 34.673194885253906
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3080824613571167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010712092043831944
        model: {}
        policy_loss: -0.0018869396299123764
        total_loss: 0.004455532878637314
        vf_explained_var: 0.054703518748283386
        vf_loss: 68.84693908691406
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8422998189926147
        entropy_coeff: 0.0017600000137463212
        kl: 0.000929567264392972
        model: {}
        policy_loss: -0.0021637803874909878
        total_loss: 0.0036571230739355087
        vf_explained_var: -0.0028268396854400635
        vf_loss: 73.03349304199219
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9702736735343933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021578765008598566
        model: {}
        policy_loss: -0.0019393125548958778
        total_loss: -0.00038557499647140503
        vf_explained_var: 0.025296255946159363
        vf_loss: 32.614173889160156
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27321285009384155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007966109551489353
        model: {}
        policy_loss: -0.001814574934542179
        total_loss: 0.0006513623520731926
        vf_explained_var: 0.11858217418193817
        vf_loss: 29.467899322509766
    load_time_ms: 13115.004
    num_steps_sampled: 41664000
    num_steps_trained: 41664000
    sample_time_ms: 93492.496
    update_time_ms: 18.512
  iterations_since_restore: 64
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.8691011235955
    ram_util_percent: 15.920224719101123
  pid: 21723
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 363.0
    agent-3: 363.0
    agent-4: 252.0
    agent-5: 252.0
  policy_reward_mean:
    agent-0: 238.74
    agent-1: 238.74
    agent-2: 324.425
    agent-3: 324.425
    agent-4: 223.94
    agent-5: 223.94
  policy_reward_min:
    agent-0: 73.5
    agent-1: 73.5
    agent-2: 85.5
    agent-3: 85.5
    agent-4: 59.5
    agent-5: 59.5
  sampler_perf:
    mean_env_wait_ms: 24.72481901094781
    mean_inference_ms: 12.395633950460462
    mean_processing_ms: 54.35482473636661
  time_since_restore: 8119.229282617569
  time_this_iter_s: 125.11128282546997
  time_total_s: 55200.77483463287
  timestamp: 1637566242
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 41664000
  training_iteration: 434
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    434 |          55200.8 | 41664000 |  1574.21 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 30.57
    apples_agent-1_min: 17
    apples_agent-2_max: 400
    apples_agent-2_mean: 351.48
    apples_agent-2_min: 196
    apples_agent-3_max: 221
    apples_agent-3_mean: 138.72
    apples_agent-3_min: 49
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 457
    apples_agent-5_mean: 386.93
    apples_agent-5_min: 213
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 375.1
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 1.72
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 3.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 65.41
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 479.59
    cleaning_beam_agent-4_min: 397
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 3.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-32-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1709.0
  episode_reward_mean: 1596.98
  episode_reward_min: 861.0
  episodes_this_iter: 96
  episodes_total: 41760
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18638.656
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4108063578605652
        entropy_coeff: 0.0017600000137463212
        kl: 0.001607040991075337
        model: {}
        policy_loss: -0.0012943344190716743
        total_loss: 0.001699021551758051
        vf_explained_var: -0.003865182399749756
        vf_loss: 37.16376495361328
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24145010113716125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007152588223107159
        model: {}
        policy_loss: -0.0015861750580370426
        total_loss: 0.0013461499474942684
        vf_explained_var: 0.09504616260528564
        vf_loss: 33.57279586791992
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30513766407966614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009064474143087864
        model: {}
        policy_loss: -0.0018717888742685318
        total_loss: 0.0041630747728049755
        vf_explained_var: 0.04329870641231537
        vf_loss: 65.71904754638672
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8360497355461121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017635209951549768
        model: {}
        policy_loss: -0.0022448748350143433
        total_loss: 0.0033121490851044655
        vf_explained_var: -0.020146816968917847
        vf_loss: 70.28472137451172
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9800792932510376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022083406802266836
        model: {}
        policy_loss: -0.002265026094391942
        total_loss: -0.0007930791471153498
        vf_explained_var: 0.04342693090438843
        vf_loss: 31.968835830688477
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2608914077281952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010072700679302216
        model: {}
        policy_loss: -0.00204079644754529
        total_loss: 0.0004936521872878075
        vf_explained_var: 0.1020478904247284
        vf_loss: 29.93616485595703
    load_time_ms: 13112.325
    num_steps_sampled: 41760000
    num_steps_trained: 41760000
    sample_time_ms: 93346.7
    update_time_ms: 18.795
  iterations_since_restore: 65
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.512359550561804
    ram_util_percent: 15.919662921348314
  pid: 21723
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 384.0
    agent-3: 384.0
    agent-4: 274.5
    agent-5: 274.5
  policy_reward_mean:
    agent-0: 242.97
    agent-1: 242.97
    agent-2: 326.44
    agent-3: 326.44
    agent-4: 229.08
    agent-5: 229.08
  policy_reward_min:
    agent-0: 140.0
    agent-1: 140.0
    agent-2: 165.0
    agent-3: 165.0
    agent-4: 125.5
    agent-5: 125.5
  sampler_perf:
    mean_env_wait_ms: 24.723186362260417
    mean_inference_ms: 12.394230466255049
    mean_processing_ms: 54.354339602980545
  time_since_restore: 8244.158186912537
  time_this_iter_s: 124.92890429496765
  time_total_s: 55325.70373892784
  timestamp: 1637566367
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 41760000
  training_iteration: 435
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    435 |          55325.7 | 41760000 |  1596.98 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.36
    apples_agent-1_min: 17
    apples_agent-2_max: 421
    apples_agent-2_mean: 359.56
    apples_agent-2_min: 271
    apples_agent-3_max: 211
    apples_agent-3_mean: 142.12
    apples_agent-3_min: 77
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 388.93
    apples_agent-5_min: 303
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 383.98
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 31
    cleaning_beam_agent-1_mean: 2.23
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 3.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 67.41
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 486.13
    cleaning_beam_agent-4_min: 376
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-34-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1746.0
  episode_reward_mean: 1608.1
  episode_reward_min: 1291.0
  episodes_this_iter: 96
  episodes_total: 41856
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18658.659
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41619160771369934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011772159487009048
        model: {}
        policy_loss: -0.0011794317979365587
        total_loss: 0.0015340803656727076
        vf_explained_var: -0.008897364139556885
        vf_loss: 34.46012878417969
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24579276144504547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008018692606128752
        model: {}
        policy_loss: -0.0014467427972704172
        total_loss: 0.001263342099264264
        vf_explained_var: 0.08312520384788513
        vf_loss: 31.426788330078125
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2939009368419647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008443843107670546
        model: {}
        policy_loss: -0.0017251437529921532
        total_loss: 0.004489271901547909
        vf_explained_var: 0.028412222862243652
        vf_loss: 67.31678771972656
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8287851214408875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014524143189191818
        model: {}
        policy_loss: -0.0022510942071676254
        total_loss: 0.0031859055161476135
        vf_explained_var: 0.004099786281585693
        vf_loss: 68.95662689208984
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9719487428665161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018866430036723614
        model: {}
        policy_loss: -0.001883898745290935
        total_loss: -0.00047412034473381937
        vf_explained_var: 0.03174026310443878
        vf_loss: 31.204097747802734
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2619599997997284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007574199698865414
        model: {}
        policy_loss: -0.0017427715938538313
        total_loss: 0.0006263828836381435
        vf_explained_var: 0.1237809807062149
        vf_loss: 28.302011489868164
    load_time_ms: 13143.478
    num_steps_sampled: 41856000
    num_steps_trained: 41856000
    sample_time_ms: 93400.807
    update_time_ms: 18.475
  iterations_since_restore: 66
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.017318435754184
    ram_util_percent: 15.933519553072623
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 374.5
    agent-3: 374.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 243.625
    agent-1: 243.625
    agent-2: 331.315
    agent-3: 331.315
    agent-4: 229.11
    agent-5: 229.11
  policy_reward_min:
    agent-0: 195.5
    agent-1: 195.5
    agent-2: 266.5
    agent-3: 266.5
    agent-4: 183.5
    agent-5: 183.5
  sampler_perf:
    mean_env_wait_ms: 24.72329051371832
    mean_inference_ms: 12.393560988110004
    mean_processing_ms: 54.35412926993179
  time_since_restore: 8369.985701322556
  time_this_iter_s: 125.82751441001892
  time_total_s: 55451.53125333786
  timestamp: 1637566493
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 41856000
  training_iteration: 436
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    436 |          55451.5 | 41856000 |   1608.1 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 29.49
    apples_agent-1_min: 18
    apples_agent-2_max: 424
    apples_agent-2_mean: 350.84
    apples_agent-2_min: 209
    apples_agent-3_max: 212
    apples_agent-3_mean: 137.35
    apples_agent-3_min: 60
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 464
    apples_agent-5_mean: 386.31
    apples_agent-5_min: 192
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 388.93
    cleaning_beam_agent-0_min: 301
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 3.73
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 67.64
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 486.17
    cleaning_beam_agent-4_min: 357
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 4.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-36-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1717.0
  episode_reward_mean: 1587.18
  episode_reward_min: 1077.0
  episodes_this_iter: 96
  episodes_total: 41952
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18660.384
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41568049788475037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014023559633642435
        model: {}
        policy_loss: -0.0014449134469032288
        total_loss: 0.0013176817446947098
        vf_explained_var: 0.013831153512001038
        vf_loss: 34.941951751708984
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24752366542816162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008902339031919837
        model: {}
        policy_loss: -0.001546373125165701
        total_loss: 0.0012999773025512695
        vf_explained_var: 0.0736888200044632
        vf_loss: 32.81993865966797
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30326855182647705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008756161550991237
        model: {}
        policy_loss: -0.0019074274459853768
        total_loss: 0.004021531902253628
        vf_explained_var: 0.04764191806316376
        vf_loss: 64.62712097167969
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8361623287200928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008617022540420294
        model: {}
        policy_loss: -0.0019573932513594627
        total_loss: 0.003524607513099909
        vf_explained_var: -0.026264280080795288
        vf_loss: 69.53648376464844
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9839358329772949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012642786605283618
        model: {}
        policy_loss: -0.0018190185073763132
        total_loss: -0.0002554960083216429
        vf_explained_var: 0.026735275983810425
        vf_loss: 32.952537536621094
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2709764242172241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008614301914349198
        model: {}
        policy_loss: -0.001983596943318844
        total_loss: 0.0004889825358986855
        vf_explained_var: 0.13056588172912598
        vf_loss: 29.494993209838867
    load_time_ms: 13166.251
    num_steps_sampled: 41952000
    num_steps_trained: 41952000
    sample_time_ms: 93425.241
    update_time_ms: 18.195
  iterations_since_restore: 67
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.62752808988765
    ram_util_percent: 15.901685393258427
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 364.0
    agent-3: 364.0
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 239.38
    agent-1: 239.38
    agent-2: 326.18
    agent-3: 326.18
    agent-4: 228.03
    agent-5: 228.03
  policy_reward_min:
    agent-0: 148.0
    agent-1: 148.0
    agent-2: 221.0
    agent-3: 221.0
    agent-4: 85.5
    agent-5: 85.5
  sampler_perf:
    mean_env_wait_ms: 24.72080637244338
    mean_inference_ms: 12.392551998542297
    mean_processing_ms: 54.350871019690345
  time_since_restore: 8495.080265283585
  time_this_iter_s: 125.09456396102905
  time_total_s: 55576.62581729889
  timestamp: 1637566618
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 41952000
  training_iteration: 437
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    437 |          55576.6 | 41952000 |  1587.18 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 29.99
    apples_agent-1_min: 15
    apples_agent-2_max: 434
    apples_agent-2_mean: 358.03
    apples_agent-2_min: 268
    apples_agent-3_max: 208
    apples_agent-3_mean: 138.39
    apples_agent-3_min: 65
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 387.18
    apples_agent-5_min: 298
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 383.87
    cleaning_beam_agent-0_min: 337
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 3.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 64.45
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 483.8
    cleaning_beam_agent-4_min: 402
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 3.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-39-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1741.0
  episode_reward_mean: 1596.55
  episode_reward_min: 1191.0
  episodes_this_iter: 96
  episodes_total: 42048
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18675.623
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4026249647140503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016564482357352972
        model: {}
        policy_loss: -0.0013048301916569471
        total_loss: 0.0014961131382733583
        vf_explained_var: 0.014839351177215576
        vf_loss: 35.09564208984375
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24492764472961426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012855040840804577
        model: {}
        policy_loss: -0.0018630875274538994
        total_loss: 0.0008916757069528103
        vf_explained_var: 0.10600171983242035
        vf_loss: 31.858339309692383
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29709380865097046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009375661029480398
        model: {}
        policy_loss: -0.001896245637908578
        total_loss: 0.004060425329953432
        vf_explained_var: 0.038934603333473206
        vf_loss: 64.79554748535156
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.805270791053772
        entropy_coeff: 0.0017600000137463212
        kl: 0.000852416327688843
        model: {}
        policy_loss: -0.0021835556253790855
        total_loss: 0.003131444565951824
        vf_explained_var: 0.0029186010360717773
        vf_loss: 67.32279968261719
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9629889726638794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017837253399193287
        model: {}
        policy_loss: -0.0021276865154504776
        total_loss: -0.0007278397679328918
        vf_explained_var: 0.03259138762950897
        vf_loss: 30.947032928466797
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2603418231010437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009552609408274293
        model: {}
        policy_loss: -0.0018061613664031029
        total_loss: 0.0005473373457789421
        vf_explained_var: 0.11912082135677338
        vf_loss: 28.116968154907227
    load_time_ms: 13157.733
    num_steps_sampled: 42048000
    num_steps_trained: 42048000
    sample_time_ms: 93489.814
    update_time_ms: 18.321
  iterations_since_restore: 68
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 53.77709497206704
    ram_util_percent: 16.110055865921787
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 396.0
    agent-3: 396.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 238.935
    agent-1: 238.935
    agent-2: 330.03
    agent-3: 330.03
    agent-4: 229.31
    agent-5: 229.31
  policy_reward_min:
    agent-0: 167.0
    agent-1: 167.0
    agent-2: 234.5
    agent-3: 234.5
    agent-4: 190.0
    agent-5: 190.0
  sampler_perf:
    mean_env_wait_ms: 24.719022554743134
    mean_inference_ms: 12.392780415308907
    mean_processing_ms: 54.35028961227292
  time_since_restore: 8620.50991654396
  time_this_iter_s: 125.42965126037598
  time_total_s: 55702.055468559265
  timestamp: 1637566744
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 42048000
  training_iteration: 438
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    438 |          55702.1 | 42048000 |  1596.55 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 30.8
    apples_agent-1_min: 18
    apples_agent-2_max: 431
    apples_agent-2_mean: 359.04
    apples_agent-2_min: 192
    apples_agent-3_max: 220
    apples_agent-3_mean: 138.83
    apples_agent-3_min: 58
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.2
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 381.4
    apples_agent-5_min: 236
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 380.11
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 3.65
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 64.43
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 499.84
    cleaning_beam_agent-4_min: 412
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-41-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1716.0
  episode_reward_mean: 1592.32
  episode_reward_min: 895.0
  episodes_this_iter: 96
  episodes_total: 42144
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18692.63
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40650275349617004
        entropy_coeff: 0.0017600000137463212
        kl: 0.001502813771367073
        model: {}
        policy_loss: -0.0014259787276387215
        total_loss: 0.0013676481321454048
        vf_explained_var: 0.02641451358795166
        vf_loss: 35.09071350097656
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2448963224887848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012820202391594648
        model: {}
        policy_loss: -0.0017659049481153488
        total_loss: 0.001090009231120348
        vf_explained_var: 0.08833280205726624
        vf_loss: 32.869300842285156
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29239267110824585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015007657930254936
        model: {}
        policy_loss: -0.0018543018959462643
        total_loss: 0.004411858040839434
        vf_explained_var: 0.0672384649515152
        vf_loss: 67.80772399902344
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.813795804977417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009931038366630673
        model: {}
        policy_loss: -0.0020718607120215893
        total_loss: 0.003953343257308006
        vf_explained_var: -0.015683293342590332
        vf_loss: 74.57484436035156
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9605865478515625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012734553311020136
        model: {}
        policy_loss: -0.001951692858710885
        total_loss: -0.00039230610127560794
        vf_explained_var: 0.039087772369384766
        vf_loss: 32.500221252441406
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2617630958557129
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009061651071533561
        model: {}
        policy_loss: -0.0019910903647542
        total_loss: 0.0005030457396060228
        vf_explained_var: 0.12558932602405548
        vf_loss: 29.548404693603516
    load_time_ms: 13147.95
    num_steps_sampled: 42144000
    num_steps_trained: 42144000
    sample_time_ms: 93464.215
    update_time_ms: 18.329
  iterations_since_restore: 69
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.70391061452514
    ram_util_percent: 15.97709497206704
  pid: 21723
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 372.5
    agent-3: 372.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 238.985
    agent-1: 238.985
    agent-2: 331.46
    agent-3: 331.46
    agent-4: 225.715
    agent-5: 225.715
  policy_reward_min:
    agent-0: 130.5
    agent-1: 130.5
    agent-2: 189.0
    agent-3: 189.0
    agent-4: 128.0
    agent-5: 128.0
  sampler_perf:
    mean_env_wait_ms: 24.72173651345742
    mean_inference_ms: 12.39378669423476
    mean_processing_ms: 54.359079466801504
  time_since_restore: 8746.751817941666
  time_this_iter_s: 126.24190139770508
  time_total_s: 55828.29736995697
  timestamp: 1637566870
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 42144000
  training_iteration: 439
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    439 |          55828.3 | 42144000 |  1592.32 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 0.08
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.47
    apples_agent-1_min: 13
    apples_agent-2_max: 439
    apples_agent-2_mean: 362.1
    apples_agent-2_min: 183
    apples_agent-3_max: 249
    apples_agent-3_mean: 141.02
    apples_agent-3_min: 16
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 389.65
    apples_agent-5_min: 173
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 385.72
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 3.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 65.79
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 486.72
    cleaning_beam_agent-4_min: 397
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 3.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-43-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1734.0
  episode_reward_mean: 1607.71
  episode_reward_min: 722.0
  episodes_this_iter: 96
  episodes_total: 42240
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18682.575
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4028688669204712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012999246828258038
        model: {}
        policy_loss: -0.0013239355757832527
        total_loss: 0.0015284977853298187
        vf_explained_var: 0.037463054060935974
        vf_loss: 35.614845275878906
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24541257321834564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009346756851300597
        model: {}
        policy_loss: -0.0018714636098593473
        total_loss: 0.000985950231552124
        vf_explained_var: 0.11020354926586151
        vf_loss: 32.89341735839844
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2942352890968323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010205457219853997
        model: {}
        policy_loss: -0.0017687599174678326
        total_loss: 0.004413076676428318
        vf_explained_var: 0.07568028569221497
        vf_loss: 66.99694061279297
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8210615515708923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014922814443707466
        model: {}
        policy_loss: -0.0022456690203398466
        total_loss: 0.0034822248853743076
        vf_explained_var: 0.015327438712120056
        vf_loss: 71.7296371459961
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9821915626525879
        entropy_coeff: 0.0017600000137463212
        kl: 0.002038379665464163
        model: {}
        policy_loss: -0.0018530511297285557
        total_loss: -0.0003325357101857662
        vf_explained_var: 0.04732555150985718
        vf_loss: 32.491729736328125
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2572360038757324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008249726379290223
        model: {}
        policy_loss: -0.0020511785987764597
        total_loss: 0.0004985389532521367
        vf_explained_var: 0.1141366958618164
        vf_loss: 30.0245361328125
    load_time_ms: 13131.805
    num_steps_sampled: 42240000
    num_steps_trained: 42240000
    sample_time_ms: 93492.667
    update_time_ms: 18.144
  iterations_since_restore: 70
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.809497206703895
    ram_util_percent: 15.983798882681564
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 380.5
    agent-3: 380.5
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 241.585
    agent-1: 241.585
    agent-2: 331.055
    agent-3: 331.055
    agent-4: 231.215
    agent-5: 231.215
  policy_reward_min:
    agent-0: 118.0
    agent-1: 118.0
    agent-2: 129.0
    agent-3: 129.0
    agent-4: 114.0
    agent-5: 114.0
  sampler_perf:
    mean_env_wait_ms: 24.72233236955083
    mean_inference_ms: 12.394079895149646
    mean_processing_ms: 54.364588226052525
  time_since_restore: 8872.481078624725
  time_this_iter_s: 125.72926068305969
  time_total_s: 55954.02663064003
  timestamp: 1637566996
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 42240000
  training_iteration: 440
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    440 |            55954 | 42240000 |  1607.71 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.96
    apples_agent-1_min: 19
    apples_agent-2_max: 409
    apples_agent-2_mean: 360.97
    apples_agent-2_min: 311
    apples_agent-3_max: 214
    apples_agent-3_mean: 143.45
    apples_agent-3_min: 87
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 392.54
    apples_agent-5_min: 325
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 389.66
    cleaning_beam_agent-0_min: 346
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.24
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 3.93
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 69.07
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 467.71
    cleaning_beam_agent-4_min: 384
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-45-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1737.0
  episode_reward_mean: 1621.19
  episode_reward_min: 1453.0
  episodes_this_iter: 96
  episodes_total: 42336
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18693.676
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40904438495635986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018344384152442217
        model: {}
        policy_loss: -0.0015381579287350178
        total_loss: 0.0010719194542616606
        vf_explained_var: -0.010631203651428223
        vf_loss: 33.299957275390625
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22929233312606812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008140362333506346
        model: {}
        policy_loss: -0.0016014168504625559
        total_loss: 0.001111876219511032
        vf_explained_var: 0.059555456042289734
        vf_loss: 31.16851043701172
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29599708318710327
        entropy_coeff: 0.0017600000137463212
        kl: 0.000872095231898129
        model: {}
        policy_loss: -0.001639174879528582
        total_loss: 0.004247359931468964
        vf_explained_var: 0.03180569410324097
        vf_loss: 64.07489776611328
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8198978900909424
        entropy_coeff: 0.0017600000137463212
        kl: 0.001032728818245232
        model: {}
        policy_loss: -0.002186079043895006
        total_loss: 0.0030648084357380867
        vf_explained_var: -0.003165692090988159
        vf_loss: 66.93903350830078
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9890087842941284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022275252267718315
        model: {}
        policy_loss: -0.002461448311805725
        total_loss: -0.0010753325186669827
        vf_explained_var: 0.026415765285491943
        vf_loss: 31.267711639404297
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24962317943572998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005238357116468251
        model: {}
        policy_loss: -0.0014479439705610275
        total_loss: 0.0009854387026280165
        vf_explained_var: 0.10214133560657501
        vf_loss: 28.727205276489258
    load_time_ms: 13170.296
    num_steps_sampled: 42336000
    num_steps_trained: 42336000
    sample_time_ms: 93595.611
    update_time_ms: 18.357
  iterations_since_restore: 71
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.91602209944751
    ram_util_percent: 16.023756906077345
  pid: 21723
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 371.0
    agent-3: 371.0
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 245.745
    agent-1: 245.745
    agent-2: 332.8
    agent-3: 332.8
    agent-4: 232.05
    agent-5: 232.05
  policy_reward_min:
    agent-0: 219.0
    agent-1: 219.0
    agent-2: 277.0
    agent-3: 277.0
    agent-4: 197.0
    agent-5: 197.0
  sampler_perf:
    mean_env_wait_ms: 24.723663471854344
    mean_inference_ms: 12.39515959275627
    mean_processing_ms: 54.369764360843625
  time_since_restore: 8999.081053733826
  time_this_iter_s: 126.59997510910034
  time_total_s: 56080.62660574913
  timestamp: 1637567124
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 42336000
  training_iteration: 441
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    441 |          56080.6 | 42336000 |  1621.19 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.94
    apples_agent-1_min: 11
    apples_agent-2_max: 417
    apples_agent-2_mean: 356.39
    apples_agent-2_min: 238
    apples_agent-3_max: 193
    apples_agent-3_mean: 137.19
    apples_agent-3_min: 46
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.26
    apples_agent-4_min: 0
    apples_agent-5_max: 471
    apples_agent-5_mean: 389.83
    apples_agent-5_min: 269
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 385.49
    cleaning_beam_agent-0_min: 345
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.78
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 4.2
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 66.34
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 465.67
    cleaning_beam_agent-4_min: 409
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-47-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1699.0
  episode_reward_mean: 1593.78
  episode_reward_min: 1118.0
  episodes_this_iter: 96
  episodes_total: 42432
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18709.196
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41818881034851074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014052652986720204
        model: {}
        policy_loss: -0.001424148678779602
        total_loss: 0.0014837831258773804
        vf_explained_var: 0.009134113788604736
        vf_loss: 36.4394645690918
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24538277089595795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011220356682315469
        model: {}
        policy_loss: -0.001714077778160572
        total_loss: 0.0011453405022621155
        vf_explained_var: 0.1077248752117157
        vf_loss: 32.91289138793945
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29518479108810425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013281897408887744
        model: {}
        policy_loss: -0.0019691605120897293
        total_loss: 0.00426342710852623
        vf_explained_var: 0.04817122220993042
        vf_loss: 67.5211181640625
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8061861991882324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022094720043241978
        model: {}
        policy_loss: -0.0025771253276616335
        total_loss: 0.003098044078797102
        vf_explained_var: 0.00031013786792755127
        vf_loss: 70.94056701660156
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9759219288825989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019770977087318897
        model: {}
        policy_loss: -0.0020584650337696075
        total_loss: -0.0004780067247338593
        vf_explained_var: 0.004575565457344055
        vf_loss: 32.98081970214844
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25485116243362427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009640691569074988
        model: {}
        policy_loss: -0.0019092303700745106
        total_loss: 0.0005057305097579956
        vf_explained_var: 0.12897764146327972
        vf_loss: 28.634965896606445
    load_time_ms: 13184.805
    num_steps_sampled: 42432000
    num_steps_trained: 42432000
    sample_time_ms: 93581.302
    update_time_ms: 17.949
  iterations_since_restore: 72
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.737988826815645
    ram_util_percent: 16.006145251396646
  pid: 21723
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 366.0
    agent-3: 366.0
    agent-4: 270.5
    agent-5: 270.5
  policy_reward_mean:
    agent-0: 240.375
    agent-1: 240.375
    agent-2: 327.63
    agent-3: 327.63
    agent-4: 228.885
    agent-5: 228.885
  policy_reward_min:
    agent-0: 169.5
    agent-1: 169.5
    agent-2: 213.0
    agent-3: 213.0
    agent-4: 157.0
    agent-5: 157.0
  sampler_perf:
    mean_env_wait_ms: 24.72426646790784
    mean_inference_ms: 12.395691255837678
    mean_processing_ms: 54.37139789662846
  time_since_restore: 9124.803792238235
  time_this_iter_s: 125.72273850440979
  time_total_s: 56206.34934425354
  timestamp: 1637567250
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 42432000
  training_iteration: 442
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    442 |          56206.3 | 42432000 |  1593.78 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 30.65
    apples_agent-1_min: 11
    apples_agent-2_max: 420
    apples_agent-2_mean: 355.81
    apples_agent-2_min: 115
    apples_agent-3_max: 240
    apples_agent-3_mean: 140.4
    apples_agent-3_min: 43
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.18
    apples_agent-4_min: 0
    apples_agent-5_max: 459
    apples_agent-5_mean: 386.43
    apples_agent-5_min: 114
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 382.83
    cleaning_beam_agent-0_min: 269
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.82
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 3.9
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 64.42
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 485.76
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 3.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-49-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1717.0
  episode_reward_mean: 1583.67
  episode_reward_min: 538.0
  episodes_this_iter: 96
  episodes_total: 42528
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18735.144
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41870278120040894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011256854049861431
        model: {}
        policy_loss: -0.0015421193093061447
        total_loss: 0.00119121465831995
        vf_explained_var: 0.04933610558509827
        vf_loss: 34.70253372192383
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24318377673625946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013393108965829015
        model: {}
        policy_loss: -0.001928882673382759
        total_loss: 0.0009159427136182785
        vf_explained_var: 0.10299761593341827
        vf_loss: 32.728271484375
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29891225695610046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006073291879147291
        model: {}
        policy_loss: -0.0017538638785481453
        total_loss: 0.0045971954241395
        vf_explained_var: 0.08151939511299133
        vf_loss: 68.77142333984375
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8092867732048035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011456721695140004
        model: {}
        policy_loss: -0.0021833316422998905
        total_loss: 0.003881854936480522
        vf_explained_var: -0.00014241039752960205
        vf_loss: 74.89533233642578
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9568450450897217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018193683354184031
        model: {}
        policy_loss: -0.0018503852188587189
        total_loss: 0.00022783223539590836
        vf_explained_var: 0.0008019357919692993
        vf_loss: 37.622642517089844
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2560587227344513
        entropy_coeff: 0.0017600000137463212
        kl: 0.00047944800462573767
        model: {}
        policy_loss: -0.0018767896108329296
        total_loss: 0.0009544596541672945
        vf_explained_var: 0.12451237440109253
        vf_loss: 32.81911849975586
    load_time_ms: 13209.191
    num_steps_sampled: 42528000
    num_steps_trained: 42528000
    sample_time_ms: 93638.092
    update_time_ms: 17.858
  iterations_since_restore: 73
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.246111111111105
    ram_util_percent: 15.991111111111111
  pid: 21723
  policy_reward_max:
    agent-0: 264.5
    agent-1: 264.5
    agent-2: 371.5
    agent-3: 371.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 238.53
    agent-1: 238.53
    agent-2: 325.155
    agent-3: 325.155
    agent-4: 228.15
    agent-5: 228.15
  policy_reward_min:
    agent-0: 77.0
    agent-1: 77.0
    agent-2: 108.0
    agent-3: 108.0
    agent-4: 67.5
    agent-5: 67.5
  sampler_perf:
    mean_env_wait_ms: 24.725461460281732
    mean_inference_ms: 12.395839538306978
    mean_processing_ms: 54.37546589489982
  time_since_restore: 9250.904072284698
  time_this_iter_s: 126.10028004646301
  time_total_s: 56332.4496243
  timestamp: 1637567376
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 42528000
  training_iteration: 443
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    443 |          56332.4 | 42528000 |  1583.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.47
    apples_agent-1_min: 17
    apples_agent-2_max: 405
    apples_agent-2_mean: 352.68
    apples_agent-2_min: 265
    apples_agent-3_max: 209
    apples_agent-3_mean: 144.03
    apples_agent-3_min: 70
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 383.72
    apples_agent-5_min: 277
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 385.95
    cleaning_beam_agent-0_min: 330
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.73
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 4.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 63.33
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 495.14
    cleaning_beam_agent-4_min: 414
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-51-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1754.0
  episode_reward_mean: 1590.89
  episode_reward_min: 1227.0
  episodes_this_iter: 96
  episodes_total: 42624
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18752.388
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4178949296474457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012442494044080377
        model: {}
        policy_loss: -0.0012573851272463799
        total_loss: 0.001419602194800973
        vf_explained_var: 0.010896369814872742
        vf_loss: 34.124855041503906
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24351391196250916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013737929984927177
        model: {}
        policy_loss: -0.001520456513389945
        total_loss: 0.0012362878769636154
        vf_explained_var: 0.07648061215877533
        vf_loss: 31.853296279907227
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2965852916240692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007197142113000154
        model: {}
        policy_loss: -0.0017408691346645355
        total_loss: 0.004357718862593174
        vf_explained_var: 0.05327284336090088
        vf_loss: 66.20576477050781
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8217072486877441
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009187344694510102
        model: {}
        policy_loss: -0.002457248978316784
        total_loss: 0.0030391127802431583
        vf_explained_var: 0.00964006781578064
        vf_loss: 69.42564392089844
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9595939517021179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013384186895564198
        model: {}
        policy_loss: -0.00202012131921947
        total_loss: -0.0005879702512174845
        vf_explained_var: 0.033271074295043945
        vf_loss: 31.210350036621094
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25923481583595276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006172710563987494
        model: {}
        policy_loss: -0.001743527129292488
        total_loss: 0.0007041580975055695
        vf_explained_var: 0.10046325623989105
        vf_loss: 29.039403915405273
    load_time_ms: 13245.693
    num_steps_sampled: 42624000
    num_steps_trained: 42624000
    sample_time_ms: 93704.179
    update_time_ms: 17.981
  iterations_since_restore: 74
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.166666666666664
    ram_util_percent: 16.019444444444446
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 377.5
    agent-3: 377.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 238.95
    agent-1: 238.95
    agent-2: 328.175
    agent-3: 328.175
    agent-4: 228.32
    agent-5: 228.32
  policy_reward_min:
    agent-0: 190.5
    agent-1: 190.5
    agent-2: 261.0
    agent-3: 261.0
    agent-4: 162.0
    agent-5: 162.0
  sampler_perf:
    mean_env_wait_ms: 24.728764354522262
    mean_inference_ms: 12.3970600301263
    mean_processing_ms: 54.38276001752709
  time_since_restore: 9377.214861631393
  time_this_iter_s: 126.31078934669495
  time_total_s: 56458.7604136467
  timestamp: 1637567502
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 42624000
  training_iteration: 444
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    444 |          56458.8 | 42624000 |  1590.89 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 29.88
    apples_agent-1_min: 17
    apples_agent-2_max: 415
    apples_agent-2_mean: 352.52
    apples_agent-2_min: 133
    apples_agent-3_max: 209
    apples_agent-3_mean: 142.6
    apples_agent-3_min: 52
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 381.05
    apples_agent-5_min: 171
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 375.4
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.53
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 3.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 63.62
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 612
    cleaning_beam_agent-4_mean: 499.08
    cleaning_beam_agent-4_min: 423
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.67
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-53-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1721.0
  episode_reward_mean: 1578.16
  episode_reward_min: 690.0
  episodes_this_iter: 96
  episodes_total: 42720
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18753.238
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.419705331325531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015495551051571965
        model: {}
        policy_loss: -0.0013739336282014847
        total_loss: 0.0013469578698277473
        vf_explained_var: 0.042289942502975464
        vf_loss: 34.59577178955078
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24913141131401062
        entropy_coeff: 0.0017600000137463212
        kl: 0.001330748782493174
        model: {}
        policy_loss: -0.0019144830293953419
        total_loss: 0.0008748145774006844
        vf_explained_var: 0.10616330802440643
        vf_loss: 32.27766418457031
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2967201769351959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008869054727256298
        model: {}
        policy_loss: -0.0017155474051833153
        total_loss: 0.004615342244505882
        vf_explained_var: 0.07850205898284912
        vf_loss: 68.53118133544922
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7915678024291992
        entropy_coeff: 0.0017600000137463212
        kl: 0.001460367813706398
        model: {}
        policy_loss: -0.0022566409315913916
        total_loss: 0.0038868808187544346
        vf_explained_var: -0.01294437050819397
        vf_loss: 75.36680603027344
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9632083773612976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018008277984336019
        model: {}
        policy_loss: -0.0022116410546004772
        total_loss: -0.0005751973949372768
        vf_explained_var: 0.02474452555179596
        vf_loss: 33.316917419433594
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26299330592155457
        entropy_coeff: 0.0017600000137463212
        kl: 0.001214830786921084
        model: {}
        policy_loss: -0.0019495817832648754
        total_loss: 0.0005999547429382801
        vf_explained_var: 0.11756269633769989
        vf_loss: 30.124059677124023
    load_time_ms: 13243.046
    num_steps_sampled: 42720000
    num_steps_trained: 42720000
    sample_time_ms: 93831.802
    update_time_ms: 17.34
  iterations_since_restore: 75
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.58722222222222
    ram_util_percent: 15.97388888888889
  pid: 21723
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 369.0
    agent-3: 369.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 236.55
    agent-1: 236.55
    agent-2: 326.8
    agent-3: 326.8
    agent-4: 225.73
    agent-5: 225.73
  policy_reward_min:
    agent-0: 112.5
    agent-1: 112.5
    agent-2: 130.0
    agent-3: 130.0
    agent-4: 102.5
    agent-5: 102.5
  sampler_perf:
    mean_env_wait_ms: 24.729907587519225
    mean_inference_ms: 12.397093258739128
    mean_processing_ms: 54.38743534521269
  time_since_restore: 9503.405428409576
  time_this_iter_s: 126.19056677818298
  time_total_s: 56584.95098042488
  timestamp: 1637567629
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 42720000
  training_iteration: 445
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    445 |            56585 | 42720000 |  1578.16 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 29.86
    apples_agent-1_min: 14
    apples_agent-2_max: 420
    apples_agent-2_mean: 353.16
    apples_agent-2_min: 73
    apples_agent-3_max: 224
    apples_agent-3_mean: 141.78
    apples_agent-3_min: 33
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 380.61
    apples_agent-5_min: 80
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 372.84
    cleaning_beam_agent-0_min: 325
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 2.22
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 5.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 59.65
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 493.8
    cleaning_beam_agent-4_min: 395
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-55-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1732.0
  episode_reward_mean: 1572.83
  episode_reward_min: 327.0
  episodes_this_iter: 96
  episodes_total: 42816
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18748.887
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4180947542190552
        entropy_coeff: 0.0017600000137463212
        kl: 0.001517064985819161
        model: {}
        policy_loss: -0.0017712842673063278
        total_loss: 0.0013448335230350494
        vf_explained_var: 0.03971533477306366
        vf_loss: 38.51964569091797
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2528130114078522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008637193823233247
        model: {}
        policy_loss: -0.0021166475489735603
        total_loss: 0.0009186603128910065
        vf_explained_var: 0.13153418898582458
        vf_loss: 34.80259704589844
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30028003454208374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008107608882710338
        model: {}
        policy_loss: -0.0019377358257770538
        total_loss: 0.0042749191634356976
        vf_explained_var: 0.08797313272953033
        vf_loss: 67.41149139404297
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7722775936126709
        entropy_coeff: 0.0017600000137463212
        kl: 0.001292060362175107
        model: {}
        policy_loss: -0.0022262251004576683
        total_loss: 0.003680212888866663
        vf_explained_var: 0.013845399022102356
        vf_loss: 72.656494140625
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9606359004974365
        entropy_coeff: 0.0017600000137463212
        kl: 0.002134104259312153
        model: {}
        policy_loss: -0.0020829408895224333
        total_loss: -0.0003497921861708164
        vf_explained_var: 0.023789018392562866
        vf_loss: 34.23871612548828
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27021902799606323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007964933756738901
        model: {}
        policy_loss: -0.001972297905012965
        total_loss: 0.0005649859085679054
        vf_explained_var: 0.1411294788122177
        vf_loss: 30.128700256347656
    load_time_ms: 13198.833
    num_steps_sampled: 42816000
    num_steps_trained: 42816000
    sample_time_ms: 93820.385
    update_time_ms: 17.547
  iterations_since_restore: 76
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.8056179775281
    ram_util_percent: 16.15
  pid: 21723
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 375.5
    agent-3: 375.5
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 233.235
    agent-1: 233.235
    agent-2: 328.59
    agent-3: 328.59
    agent-4: 224.59
    agent-5: 224.59
  policy_reward_min:
    agent-0: 37.5
    agent-1: 37.5
    agent-2: 74.5
    agent-3: 74.5
    agent-4: 51.5
    agent-5: 51.5
  sampler_perf:
    mean_env_wait_ms: 24.729411080822278
    mean_inference_ms: 12.39763096121912
    mean_processing_ms: 54.38501238538934
  time_since_restore: 9628.635347604752
  time_this_iter_s: 125.22991919517517
  time_total_s: 56710.180899620056
  timestamp: 1637567754
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 42816000
  training_iteration: 446
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    446 |          56710.2 | 42816000 |  1572.83 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 31.78
    apples_agent-1_min: 13
    apples_agent-2_max: 420
    apples_agent-2_mean: 359.29
    apples_agent-2_min: 193
    apples_agent-3_max: 225
    apples_agent-3_mean: 148.46
    apples_agent-3_min: 26
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 384.34
    apples_agent-5_min: 234
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 381.79
    cleaning_beam_agent-0_min: 344
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 3.99
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 66.17
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 507.66
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_02-58-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1742.0
  episode_reward_mean: 1601.74
  episode_reward_min: 951.0
  episodes_this_iter: 96
  episodes_total: 42912
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18759.704
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41161295771598816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016009606188163161
        model: {}
        policy_loss: -0.0015895962715148926
        total_loss: 0.0011625271290540695
        vf_explained_var: 0.002715766429901123
        vf_loss: 34.76559066772461
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2365618199110031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014599610585719347
        model: {}
        policy_loss: -0.0018501165322959423
        total_loss: 0.0010044099763035774
        vf_explained_var: 0.06434065103530884
        vf_loss: 32.70873260498047
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29334235191345215
        entropy_coeff: 0.0017600000137463212
        kl: 0.000982210272923112
        model: {}
        policy_loss: -0.0017695240676403046
        total_loss: 0.004587829578667879
        vf_explained_var: 0.05313044786453247
        vf_loss: 68.73634338378906
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7819251418113708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007578331860713661
        model: {}
        policy_loss: -0.0020466467831283808
        total_loss: 0.003951219841837883
        vf_explained_var: -0.016225874423980713
        vf_loss: 73.74053955078125
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9452283382415771
        entropy_coeff: 0.0017600000137463212
        kl: 0.002398216398432851
        model: {}
        policy_loss: -0.0019821173045784235
        total_loss: -0.00041934859473258257
        vf_explained_var: 0.045297831296920776
        vf_loss: 32.26370620727539
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2607611417770386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007756352424621582
        model: {}
        policy_loss: -0.0018294769106432796
        total_loss: 0.0006803245050832629
        vf_explained_var: 0.12090139091014862
        vf_loss: 29.68744468688965
    load_time_ms: 13187.669
    num_steps_sampled: 42912000
    num_steps_trained: 42912000
    sample_time_ms: 93903.408
    update_time_ms: 17.75
  iterations_since_restore: 77
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.98111111111111
    ram_util_percent: 16.04222222222222
  pid: 21723
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 373.0
    agent-3: 373.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 241.8
    agent-1: 241.8
    agent-2: 333.07
    agent-3: 333.07
    agent-4: 226.0
    agent-5: 226.0
  policy_reward_min:
    agent-0: 152.5
    agent-1: 152.5
    agent-2: 168.5
    agent-3: 168.5
    agent-4: 144.0
    agent-5: 144.0
  sampler_perf:
    mean_env_wait_ms: 24.731346487133248
    mean_inference_ms: 12.397757286988533
    mean_processing_ms: 54.386395975556134
  time_since_restore: 9754.56136918068
  time_this_iter_s: 125.92602157592773
  time_total_s: 56836.106921195984
  timestamp: 1637567880
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 42912000
  training_iteration: 447
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    447 |          56836.1 | 42912000 |  1601.74 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.83
    apples_agent-1_min: 11
    apples_agent-2_max: 410
    apples_agent-2_mean: 354.63
    apples_agent-2_min: 277
    apples_agent-3_max: 207
    apples_agent-3_mean: 145.72
    apples_agent-3_min: 72
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 386.85
    apples_agent-5_min: 325
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 389.02
    cleaning_beam_agent-0_min: 352
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.44
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 4.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 58.8
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 635
    cleaning_beam_agent-4_mean: 526.55
    cleaning_beam_agent-4_min: 446
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-00-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1722.0
  episode_reward_mean: 1606.61
  episode_reward_min: 1330.0
  episodes_this_iter: 96
  episodes_total: 43008
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18755.458
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4151139557361603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014246284263208508
        model: {}
        policy_loss: -0.0014628018252551556
        total_loss: 0.0012063223402947187
        vf_explained_var: -0.0013211220502853394
        vf_loss: 33.99724197387695
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24256542325019836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007723875460214913
        model: {}
        policy_loss: -0.0016091690631583333
        total_loss: 0.0011394810862839222
        vf_explained_var: 0.06220439076423645
        vf_loss: 31.755651473999023
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2963331341743469
        entropy_coeff: 0.0017600000137463212
        kl: 0.001249655382707715
        model: {}
        policy_loss: -0.001812769565731287
        total_loss: 0.0044527845457196236
        vf_explained_var: 0.0338994562625885
        vf_loss: 67.87104034423828
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7923351526260376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005046383594162762
        model: {}
        policy_loss: -0.001840751152485609
        total_loss: 0.003875656984746456
        vf_explained_var: -0.0019589662551879883
        vf_loss: 71.10917663574219
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9317399263381958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012465366162359715
        model: {}
        policy_loss: -0.0018143304623663425
        total_loss: -0.0004723537713289261
        vf_explained_var: 0.02786913514137268
        vf_loss: 29.818395614624023
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2559204697608948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009377797250635922
        model: {}
        policy_loss: -0.0017088961321860552
        total_loss: 0.0006193295121192932
        vf_explained_var: 0.09810397028923035
        vf_loss: 27.786476135253906
    load_time_ms: 13198.061
    num_steps_sampled: 43008000
    num_steps_trained: 43008000
    sample_time_ms: 93984.728
    update_time_ms: 17.717
  iterations_since_restore: 78
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.037222222222226
    ram_util_percent: 16.091666666666665
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 378.0
    agent-3: 378.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 240.535
    agent-1: 240.535
    agent-2: 333.97
    agent-3: 333.97
    agent-4: 228.8
    agent-5: 228.8
  policy_reward_min:
    agent-0: 201.5
    agent-1: 201.5
    agent-2: 264.0
    agent-3: 264.0
    agent-4: 189.0
    agent-5: 189.0
  sampler_perf:
    mean_env_wait_ms: 24.73476147981475
    mean_inference_ms: 12.39825845189719
    mean_processing_ms: 54.39104172252618
  time_since_restore: 9880.865336894989
  time_this_iter_s: 126.30396771430969
  time_total_s: 56962.41088891029
  timestamp: 1637568007
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 43008000
  training_iteration: 448
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    448 |          56962.4 | 43008000 |  1606.61 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 29.38
    apples_agent-1_min: 13
    apples_agent-2_max: 423
    apples_agent-2_mean: 359.96
    apples_agent-2_min: 255
    apples_agent-3_max: 209
    apples_agent-3_mean: 146.73
    apples_agent-3_min: 93
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 383.49
    apples_agent-5_min: 217
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 378.21
    cleaning_beam_agent-0_min: 274
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.08
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 4.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 59.58
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 522.53
    cleaning_beam_agent-4_min: 462
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-02-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1741.0
  episode_reward_mean: 1607.52
  episode_reward_min: 1235.0
  episodes_this_iter: 96
  episodes_total: 43104
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18747.784
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41244664788246155
        entropy_coeff: 0.0017600000137463212
        kl: 0.001801006612367928
        model: {}
        policy_loss: -0.0016398467123508453
        total_loss: 0.0010173621121793985
        vf_explained_var: 0.026252225041389465
        vf_loss: 33.83116912841797
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24721944332122803
        entropy_coeff: 0.0017600000137463212
        kl: 0.001202211482450366
        model: {}
        policy_loss: -0.001699298620223999
        total_loss: 0.0009851140202954412
        vf_explained_var: 0.10032930970191956
        vf_loss: 31.195205688476562
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29900088906288147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009370899060741067
        model: {}
        policy_loss: -0.0016561428783461452
        total_loss: 0.0045234463177621365
        vf_explained_var: 0.045155808329582214
        vf_loss: 67.05831909179688
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.770589292049408
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013385004131123424
        model: {}
        policy_loss: -0.0022504439111799
        total_loss: 0.0035547460429370403
        vf_explained_var: -0.015992939472198486
        vf_loss: 71.61427307128906
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9326330423355103
        entropy_coeff: 0.0017600000137463212
        kl: 0.001764161279425025
        model: {}
        policy_loss: -0.0020149131305515766
        total_loss: -0.00034303218126296997
        vf_explained_var: 0.02990134060382843
        vf_loss: 33.13313293457031
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26758575439453125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007709063938818872
        model: {}
        policy_loss: -0.00188733683899045
        total_loss: 0.0006809206679463387
        vf_explained_var: 0.11362786591053009
        vf_loss: 30.39208221435547
    load_time_ms: 13180.186
    num_steps_sampled: 43104000
    num_steps_trained: 43104000
    sample_time_ms: 93976.221
    update_time_ms: 17.762
  iterations_since_restore: 79
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.36871508379888
    ram_util_percent: 15.991061452513964
  pid: 21723
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 377.0
    agent-3: 377.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 241.345
    agent-1: 241.345
    agent-2: 335.385
    agent-3: 335.385
    agent-4: 227.03
    agent-5: 227.03
  policy_reward_min:
    agent-0: 197.0
    agent-1: 197.0
    agent-2: 261.5
    agent-3: 261.5
    agent-4: 130.0
    agent-5: 130.0
  sampler_perf:
    mean_env_wait_ms: 24.737174292310083
    mean_inference_ms: 12.399245887255324
    mean_processing_ms: 54.39433987728599
  time_since_restore: 10006.76596045494
  time_this_iter_s: 125.90062355995178
  time_total_s: 57088.311512470245
  timestamp: 1637568133
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 43104000
  training_iteration: 449
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    449 |          57088.3 | 43104000 |  1607.52 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.14
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.04
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 350.83
    apples_agent-2_min: 228
    apples_agent-3_max: 217
    apples_agent-3_mean: 142.02
    apples_agent-3_min: 42
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.32
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 383.12
    apples_agent-5_min: 248
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 384.22
    cleaning_beam_agent-0_min: 269
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 3.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 57.82
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 519.88
    cleaning_beam_agent-4_min: 414
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 3.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-04-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1706.0
  episode_reward_mean: 1580.03
  episode_reward_min: 990.0
  episodes_this_iter: 96
  episodes_total: 43200
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18739.61
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4146910011768341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015738363144919276
        model: {}
        policy_loss: -0.0015576842706650496
        total_loss: 0.001172331627458334
        vf_explained_var: 0.03530077636241913
        vf_loss: 34.598751068115234
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2479529082775116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013063077349215746
        model: {}
        policy_loss: -0.0018613534048199654
        total_loss: 0.0008639423176646233
        vf_explained_var: 0.11839953064918518
        vf_loss: 31.616931915283203
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2969117760658264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010116618359461427
        model: {}
        policy_loss: -0.002134034875780344
        total_loss: 0.004173852503299713
        vf_explained_var: 0.08416561782360077
        vf_loss: 68.30452728271484
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7919225096702576
        entropy_coeff: 0.0017600000137463212
        kl: 0.001307913800701499
        model: {}
        policy_loss: -0.002439528238028288
        total_loss: 0.00363792572170496
        vf_explained_var: -0.004011854529380798
        vf_loss: 74.71239471435547
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9632765054702759
        entropy_coeff: 0.0017600000137463212
        kl: 0.001996829640120268
        model: {}
        policy_loss: -0.0021382588893175125
        total_loss: -0.0007094806060194969
        vf_explained_var: 0.06136739253997803
        vf_loss: 31.241439819335938
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2678578794002533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008152996306307614
        model: {}
        policy_loss: -0.0017426088452339172
        total_loss: 0.0007223915308713913
        vf_explained_var: 0.11958765983581543
        vf_loss: 29.364295959472656
    load_time_ms: 13191.872
    num_steps_sampled: 43200000
    num_steps_trained: 43200000
    sample_time_ms: 94088.257
    update_time_ms: 17.817
  iterations_since_restore: 80
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.94475138121547
    ram_util_percent: 16.025414364640884
  pid: 21723
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 374.0
    agent-3: 374.0
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 238.045
    agent-1: 238.045
    agent-2: 326.56
    agent-3: 326.56
    agent-4: 225.41
    agent-5: 225.41
  policy_reward_min:
    agent-0: 159.0
    agent-1: 159.0
    agent-2: 190.5
    agent-3: 190.5
    agent-4: 145.5
    agent-5: 145.5
  sampler_perf:
    mean_env_wait_ms: 24.74179730050263
    mean_inference_ms: 12.400915620146229
    mean_processing_ms: 54.40342210495147
  time_since_restore: 10133.643675804138
  time_this_iter_s: 126.87771534919739
  time_total_s: 57215.18922781944
  timestamp: 1637568260
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 43200000
  training_iteration: 450
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    450 |          57215.2 | 43200000 |  1580.03 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 31.06
    apples_agent-1_min: 18
    apples_agent-2_max: 419
    apples_agent-2_mean: 354.07
    apples_agent-2_min: 273
    apples_agent-3_max: 255
    apples_agent-3_mean: 152.67
    apples_agent-3_min: 75
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 385.35
    apples_agent-5_min: 229
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 389.15
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 3.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 60.46
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 511.2
    cleaning_beam_agent-4_min: 435
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.06
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-06-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1751.0
  episode_reward_mean: 1600.29
  episode_reward_min: 1052.0
  episodes_this_iter: 96
  episodes_total: 43296
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18749.68
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41989022493362427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014532030327245593
        model: {}
        policy_loss: -0.0013176980428397655
        total_loss: 0.0013527227565646172
        vf_explained_var: 0.015485823154449463
        vf_loss: 34.094276428222656
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24448853731155396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013085302198305726
        model: {}
        policy_loss: -0.0018455671379342675
        total_loss: 0.0008938342216424644
        vf_explained_var: 0.083729088306427
        vf_loss: 31.69699478149414
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2970580458641052
        entropy_coeff: 0.0017600000137463212
        kl: 0.000728657643776387
        model: {}
        policy_loss: -0.0017830736469477415
        total_loss: 0.004617152735590935
        vf_explained_var: 0.03508366644382477
        vf_loss: 69.23047637939453
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7755146622657776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009141808841377497
        model: {}
        policy_loss: -0.002167076338082552
        total_loss: 0.0037951325066387653
        vf_explained_var: -0.017902106046676636
        vf_loss: 73.27117919921875
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9581443667411804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018057203851640224
        model: {}
        policy_loss: -0.002150180283933878
        total_loss: -0.0006751106120646
        vf_explained_var: 0.024828195571899414
        vf_loss: 31.61404800415039
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2606421113014221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011323271319270134
        model: {}
        policy_loss: -0.0019149687141180038
        total_loss: 0.0005705216899514198
        vf_explained_var: 0.0898747593164444
        vf_loss: 29.442203521728516
    load_time_ms: 13189.066
    num_steps_sampled: 43296000
    num_steps_trained: 43296000
    sample_time_ms: 94042.988
    update_time_ms: 17.578
  iterations_since_restore: 81
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.667403314917124
    ram_util_percent: 16.074033149171267
  pid: 21723
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 384.5
    agent-3: 384.5
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 240.345
    agent-1: 240.345
    agent-2: 333.42
    agent-3: 333.42
    agent-4: 226.38
    agent-5: 226.38
  policy_reward_min:
    agent-0: 162.5
    agent-1: 162.5
    agent-2: 227.5
    agent-3: 227.5
    agent-4: 136.0
    agent-5: 136.0
  sampler_perf:
    mean_env_wait_ms: 24.743081008397272
    mean_inference_ms: 12.401435828352625
    mean_processing_ms: 54.40892910991414
  time_since_restore: 10259.861077070236
  time_this_iter_s: 126.21740126609802
  time_total_s: 57341.40662908554
  timestamp: 1637568387
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 43296000
  training_iteration: 451
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    451 |          57341.4 | 43296000 |  1600.29 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.2
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.11
    apples_agent-1_min: 0
    apples_agent-2_max: 410
    apples_agent-2_mean: 353.81
    apples_agent-2_min: 14
    apples_agent-3_max: 207
    apples_agent-3_mean: 149.16
    apples_agent-3_min: 14
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 384.41
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 395.05
    cleaning_beam_agent-0_min: 248
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.05
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 3.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 60.63
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 627
    cleaning_beam_agent-4_mean: 527.48
    cleaning_beam_agent-4_min: 417
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-08-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1742.0
  episode_reward_mean: 1602.8
  episode_reward_min: 83.0
  episodes_this_iter: 96
  episodes_total: 43392
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18742.475
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4221283793449402
        entropy_coeff: 0.0017600000137463212
        kl: 0.001430596224963665
        model: {}
        policy_loss: -0.0017750170081853867
        total_loss: 0.0012597336899489164
        vf_explained_var: 0.05833424627780914
        vf_loss: 37.776954650878906
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24595817923545837
        entropy_coeff: 0.0017600000137463212
        kl: 0.00154377450235188
        model: {}
        policy_loss: -0.002254784805700183
        total_loss: 0.0007540498627349734
        vf_explained_var: 0.14203010499477386
        vf_loss: 34.41722869873047
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30085694789886475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013937967596575618
        model: {}
        policy_loss: -0.0020900340750813484
        total_loss: 0.004649112466722727
        vf_explained_var: 0.10513997077941895
        vf_loss: 72.6865463256836
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7766185998916626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010606704745441675
        model: {}
        policy_loss: -0.002158141229301691
        total_loss: 0.0047541470266878605
        vf_explained_var: -0.0156538188457489
        vf_loss: 82.7914047241211
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9416345357894897
        entropy_coeff: 0.0017600000137463212
        kl: 0.002033425960689783
        model: {}
        policy_loss: -0.002082476392388344
        total_loss: -5.2528863307088614e-05
        vf_explained_var: 0.01515844464302063
        vf_loss: 36.87225341796875
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2715579569339752
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007287240587174892
        model: {}
        policy_loss: -0.002145424485206604
        total_loss: 0.0006528990343213081
        vf_explained_var: 0.12477856874465942
        vf_loss: 32.762672424316406
    load_time_ms: 13202.442
    num_steps_sampled: 43392000
    num_steps_trained: 43392000
    sample_time_ms: 94137.002
    update_time_ms: 17.731
  iterations_since_restore: 82
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.44198895027624
    ram_util_percent: 16.100552486187844
  pid: 21723
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 386.0
    agent-3: 386.0
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 241.76
    agent-1: 241.76
    agent-2: 334.035
    agent-3: 334.035
    agent-4: 225.605
    agent-5: 225.605
  policy_reward_min:
    agent-0: 13.0
    agent-1: 13.0
    agent-2: 16.5
    agent-3: 16.5
    agent-4: 12.0
    agent-5: 12.0
  sampler_perf:
    mean_env_wait_ms: 24.746512828151985
    mean_inference_ms: 12.402091367158421
    mean_processing_ms: 54.41554445877065
  time_since_restore: 10386.571907281876
  time_this_iter_s: 126.7108302116394
  time_total_s: 57468.11745929718
  timestamp: 1637568514
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 43392000
  training_iteration: 452
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    452 |          57468.1 | 43392000 |   1602.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.11
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 32.97
    apples_agent-1_min: 14
    apples_agent-2_max: 422
    apples_agent-2_mean: 356.56
    apples_agent-2_min: 284
    apples_agent-3_max: 241
    apples_agent-3_mean: 157.2
    apples_agent-3_min: 97
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 385.04
    apples_agent-5_min: 288
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 395.6
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 3.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 60.54
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 646
    cleaning_beam_agent-4_mean: 517.42
    cleaning_beam_agent-4_min: 417
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-10-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1750.0
  episode_reward_mean: 1612.38
  episode_reward_min: 1412.0
  episodes_this_iter: 96
  episodes_total: 43488
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18738.088
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41246095299720764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016136389458552003
        model: {}
        policy_loss: -0.0015096180140972137
        total_loss: 0.0010400135070085526
        vf_explained_var: 0.016718149185180664
        vf_loss: 32.75565719604492
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24319157004356384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014022259274497628
        model: {}
        policy_loss: -0.001973961479961872
        total_loss: 0.0006271768361330032
        vf_explained_var: 0.08929796516895294
        vf_loss: 30.29154396057129
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2939438223838806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009811419295147061
        model: {}
        policy_loss: -0.0016304620075970888
        total_loss: 0.004457706119865179
        vf_explained_var: 0.04364670813083649
        vf_loss: 66.05509948730469
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7814764380455017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008038419764488935
        model: {}
        policy_loss: -0.0020788395777344704
        total_loss: 0.0036907363682985306
        vf_explained_var: -0.01853671669960022
        vf_loss: 71.44975280761719
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9688036441802979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023885415866971016
        model: {}
        policy_loss: -0.0020580790005624294
        total_loss: -0.0006592634017579257
        vf_explained_var: 0.048090144991874695
        vf_loss: 31.039058685302734
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2640807628631592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008251063991338015
        model: {}
        policy_loss: -0.0019530178979039192
        total_loss: 0.00040522124618291855
        vf_explained_var: 0.13409091532230377
        vf_loss: 28.2302303314209
    load_time_ms: 13190.852
    num_steps_sampled: 43488000
    num_steps_trained: 43488000
    sample_time_ms: 94132.99
    update_time_ms: 17.98
  iterations_since_restore: 83
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.00446927374301
    ram_util_percent: 16.058100558659216
  pid: 21723
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 380.0
    agent-3: 380.0
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 242.515
    agent-1: 242.515
    agent-2: 337.365
    agent-3: 337.365
    agent-4: 226.31
    agent-5: 226.31
  policy_reward_min:
    agent-0: 195.5
    agent-1: 195.5
    agent-2: 292.0
    agent-3: 292.0
    agent-4: 185.5
    agent-5: 185.5
  sampler_perf:
    mean_env_wait_ms: 24.748215976191247
    mean_inference_ms: 12.40268395516548
    mean_processing_ms: 54.418217872583476
  time_since_restore: 10512.4701628685
  time_this_iter_s: 125.89825558662415
  time_total_s: 57594.015714883804
  timestamp: 1637568640
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 43488000
  training_iteration: 453
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    453 |            57594 | 43488000 |  1612.38 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 31.92
    apples_agent-1_min: 12
    apples_agent-2_max: 422
    apples_agent-2_mean: 352.71
    apples_agent-2_min: 241
    apples_agent-3_max: 220
    apples_agent-3_mean: 158.55
    apples_agent-3_min: 78
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 384.02
    apples_agent-5_min: 215
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 389.52
    cleaning_beam_agent-0_min: 353
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 1.63
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 4.11
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 61.63
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 506.89
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-12-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1720.0
  episode_reward_mean: 1600.48
  episode_reward_min: 1081.0
  episodes_this_iter: 96
  episodes_total: 43584
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18735.275
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4144916236400604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011217603459954262
        model: {}
        policy_loss: -0.0011699055321514606
        total_loss: 0.00165970204398036
        vf_explained_var: 0.006186529994010925
        vf_loss: 35.591121673583984
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24413339793682098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012944764457643032
        model: {}
        policy_loss: -0.00205634580925107
        total_loss: 0.0007688915356993675
        vf_explained_var: 0.09027630090713501
        vf_loss: 32.54911804199219
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29784226417541504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011535834055393934
        model: {}
        policy_loss: -0.0019785556942224503
        total_loss: 0.004255639389157295
        vf_explained_var: 0.07346917688846588
        vf_loss: 67.58401489257812
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7796118259429932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007418025052174926
        model: {}
        policy_loss: -0.002218530047684908
        total_loss: 0.003899631090462208
        vf_explained_var: -0.015699833631515503
        vf_loss: 74.90277862548828
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9719212055206299
        entropy_coeff: 0.0017600000137463212
        kl: 0.001961253583431244
        model: {}
        policy_loss: -0.0020803986117243767
        total_loss: -0.0004971642047166824
        vf_explained_var: 0.045842498540878296
        vf_loss: 32.938175201416016
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2698603868484497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009661426884122193
        model: {}
        policy_loss: -0.0018088994547724724
        total_loss: 0.000825714785605669
        vf_explained_var: 0.10134296119213104
        vf_loss: 31.095687866210938
    load_time_ms: 13172.71
    num_steps_sampled: 43584000
    num_steps_trained: 43584000
    sample_time_ms: 94113.302
    update_time_ms: 18.021
  iterations_since_restore: 84
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.917222222222215
    ram_util_percent: 16.010555555555555
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 380.0
    agent-3: 380.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 238.525
    agent-1: 238.525
    agent-2: 335.2
    agent-3: 335.2
    agent-4: 226.515
    agent-5: 226.515
  policy_reward_min:
    agent-0: 160.0
    agent-1: 160.0
    agent-2: 217.0
    agent-3: 217.0
    agent-4: 127.0
    agent-5: 127.0
  sampler_perf:
    mean_env_wait_ms: 24.748950196575482
    mean_inference_ms: 12.402892417323667
    mean_processing_ms: 54.42000681289232
  time_since_restore: 10638.382111549377
  time_this_iter_s: 125.91194868087769
  time_total_s: 57719.92766356468
  timestamp: 1637568766
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 43584000
  training_iteration: 454
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    454 |          57719.9 | 43584000 |  1600.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 31.64
    apples_agent-1_min: 19
    apples_agent-2_max: 428
    apples_agent-2_mean: 358.47
    apples_agent-2_min: 294
    apples_agent-3_max: 224
    apples_agent-3_mean: 150.86
    apples_agent-3_min: 79
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 461
    apples_agent-5_mean: 386.01
    apples_agent-5_min: 258
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 393.98
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 4.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 63.44
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 618
    cleaning_beam_agent-4_mean: 502.24
    cleaning_beam_agent-4_min: 417
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 3.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-14-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1716.0
  episode_reward_mean: 1606.73
  episode_reward_min: 1305.0
  episodes_this_iter: 96
  episodes_total: 43680
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18740.76
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4192238450050354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013310592621564865
        model: {}
        policy_loss: -0.0016912358114495873
        total_loss: 0.0008139831479638815
        vf_explained_var: -0.007073700428009033
        vf_loss: 32.430519104003906
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24423785507678986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007685543969273567
        model: {}
        policy_loss: -0.0016223647398874164
        total_loss: 0.0009135278523899615
        vf_explained_var: 0.07870075106620789
        vf_loss: 29.657501220703125
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2948906123638153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008687728550285101
        model: {}
        policy_loss: -0.001807509921491146
        total_loss: 0.004032975994050503
        vf_explained_var: 0.045759767293930054
        vf_loss: 63.59489822387695
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7754344344139099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008188199135474861
        model: {}
        policy_loss: -0.002181366551667452
        total_loss: 0.003121790708974004
        vf_explained_var: 0.005237981677055359
        vf_loss: 66.6792221069336
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9718659520149231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016375917475670576
        model: {}
        policy_loss: -0.0020223879255354404
        total_loss: -0.000491853104904294
        vf_explained_var: 0.013594463467597961
        vf_loss: 32.410186767578125
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25529342889785767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014349488774314523
        model: {}
        policy_loss: -0.0018960465677082539
        total_loss: 0.0005321549251675606
        vf_explained_var: 0.12477816641330719
        vf_loss: 28.77520751953125
    load_time_ms: 13182.498
    num_steps_sampled: 43680000
    num_steps_trained: 43680000
    sample_time_ms: 93989.74
    update_time_ms: 18.144
  iterations_since_restore: 85
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.28202247191012
    ram_util_percent: 16.25449438202247
  pid: 21723
  policy_reward_max:
    agent-0: 283.5
    agent-1: 283.5
    agent-2: 379.5
    agent-3: 379.5
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 239.93
    agent-1: 239.93
    agent-2: 336.39
    agent-3: 336.39
    agent-4: 227.045
    agent-5: 227.045
  policy_reward_min:
    agent-0: 186.5
    agent-1: 186.5
    agent-2: 273.5
    agent-3: 273.5
    agent-4: 160.5
    agent-5: 160.5
  sampler_perf:
    mean_env_wait_ms: 24.74842534741483
    mean_inference_ms: 12.402398576686398
    mean_processing_ms: 54.418967253983176
  time_since_restore: 10763.482462406158
  time_this_iter_s: 125.100350856781
  time_total_s: 57845.02801442146
  timestamp: 1637568891
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 43680000
  training_iteration: 455
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    455 |            57845 | 43680000 |  1606.73 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 31.08
    apples_agent-1_min: 6
    apples_agent-2_max: 414
    apples_agent-2_mean: 351.68
    apples_agent-2_min: 86
    apples_agent-3_max: 234
    apples_agent-3_mean: 158.21
    apples_agent-3_min: 39
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 463
    apples_agent-5_mean: 382.54
    apples_agent-5_min: 89
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 393.32
    cleaning_beam_agent-0_min: 291
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.03
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 4.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 73.33
    cleaning_beam_agent-3_min: 41
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 497.9
    cleaning_beam_agent-4_min: 420
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 3.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-16-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1745.0
  episode_reward_mean: 1596.17
  episode_reward_min: 380.0
  episodes_this_iter: 96
  episodes_total: 43776
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18738.545
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4342859983444214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014895113417878747
        model: {}
        policy_loss: -0.0016474844887852669
        total_loss: 0.0012552648549899459
        vf_explained_var: 0.0721844732761383
        vf_loss: 36.670921325683594
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24155043065547943
        entropy_coeff: 0.0017600000137463212
        kl: 0.001073793857358396
        model: {}
        policy_loss: -0.001991224242374301
        total_loss: 0.0009290508460253477
        vf_explained_var: 0.15363046526908875
        vf_loss: 33.45404815673828
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29867082834243774
        entropy_coeff: 0.0017600000137463212
        kl: 0.000662210222799331
        model: {}
        policy_loss: -0.0020211120136082172
        total_loss: 0.004642019513994455
        vf_explained_var: 0.10452006757259369
        vf_loss: 71.88792419433594
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.791683554649353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010971076553687453
        model: {}
        policy_loss: -0.002206655917689204
        total_loss: 0.004525907803326845
        vf_explained_var: -0.009404197335243225
        vf_loss: 81.25923156738281
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9968276023864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020699147135019302
        model: {}
        policy_loss: -0.0020935547072440386
        total_loss: -0.00012839239207096398
        vf_explained_var: 0.008844703435897827
        vf_loss: 37.19580078125
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26431503891944885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007122528622858226
        model: {}
        policy_loss: -0.002000801730901003
        total_loss: 0.0007415948202833533
        vf_explained_var: 0.144859179854393
        vf_loss: 32.075923919677734
    load_time_ms: 13208.431
    num_steps_sampled: 43776000
    num_steps_trained: 43776000
    sample_time_ms: 94035.464
    update_time_ms: 17.845
  iterations_since_restore: 86
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.31117318435754
    ram_util_percent: 16.10167597765363
  pid: 21723
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 380.0
    agent-3: 380.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 238.5
    agent-1: 238.5
    agent-2: 334.38
    agent-3: 334.38
    agent-4: 225.205
    agent-5: 225.205
  policy_reward_min:
    agent-0: 53.0
    agent-1: 53.0
    agent-2: 79.5
    agent-3: 79.5
    agent-4: 54.5
    agent-5: 54.5
  sampler_perf:
    mean_env_wait_ms: 24.750343931282295
    mean_inference_ms: 12.402149843309822
    mean_processing_ms: 54.42145759055062
  time_since_restore: 10889.39579463005
  time_this_iter_s: 125.91333222389221
  time_total_s: 57970.941346645355
  timestamp: 1637569017
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 43776000
  training_iteration: 456
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    456 |          57970.9 | 43776000 |  1596.17 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 29.83
    apples_agent-1_min: 15
    apples_agent-2_max: 411
    apples_agent-2_mean: 351.26
    apples_agent-2_min: 180
    apples_agent-3_max: 234
    apples_agent-3_mean: 156.96
    apples_agent-3_min: 69
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 374.45
    apples_agent-5_min: 196
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 396.51
    cleaning_beam_agent-0_min: 353
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 2.58
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 4.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 67.09
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 482.31
    cleaning_beam_agent-4_min: 389
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 3.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-19-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1714.0
  episode_reward_mean: 1586.57
  episode_reward_min: 884.0
  episodes_this_iter: 96
  episodes_total: 43872
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18729.13
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43102359771728516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012683691456913948
        model: {}
        policy_loss: -0.0014050111640244722
        total_loss: 0.0014234967529773712
        vf_explained_var: 0.025512322783470154
        vf_loss: 35.87110900878906
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2467155158519745
        entropy_coeff: 0.0017600000137463212
        kl: 0.000838860752992332
        model: {}
        policy_loss: -0.001860175747424364
        total_loss: 0.0009931796230375767
        vf_explained_var: 0.10723498463630676
        vf_loss: 32.875762939453125
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29608044028282166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009800749830901623
        model: {}
        policy_loss: -0.0018529508961364627
        total_loss: 0.004583451431244612
        vf_explained_var: 0.06925009191036224
        vf_loss: 69.57498168945312
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7732713222503662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005513721844181418
        model: {}
        policy_loss: -0.00200885022059083
        total_loss: 0.004170637112110853
        vf_explained_var: -0.005260348320007324
        vf_loss: 75.4044418334961
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9977200031280518
        entropy_coeff: 0.0017600000137463212
        kl: 0.001696482882834971
        model: {}
        policy_loss: -0.0024047698825597763
        total_loss: -0.0008945500012487173
        vf_explained_var: 0.05602236092090607
        vf_loss: 32.66205596923828
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26381877064704895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008778646006248891
        model: {}
        policy_loss: -0.0019150963053107262
        total_loss: 0.0006627584807574749
        vf_explained_var: 0.12180690467357635
        vf_loss: 30.42174530029297
    load_time_ms: 13204.179
    num_steps_sampled: 43872000
    num_steps_trained: 43872000
    sample_time_ms: 93925.887
    update_time_ms: 17.846
  iterations_since_restore: 87
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.821910112359554
    ram_util_percent: 16.196629213483146
  pid: 21723
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 380.0
    agent-3: 380.0
    agent-4: 254.0
    agent-5: 254.0
  policy_reward_mean:
    agent-0: 237.45
    agent-1: 237.45
    agent-2: 333.3
    agent-3: 333.3
    agent-4: 222.535
    agent-5: 222.535
  policy_reward_min:
    agent-0: 129.5
    agent-1: 129.5
    agent-2: 202.5
    agent-3: 202.5
    agent-4: 109.5
    agent-5: 109.5
  sampler_perf:
    mean_env_wait_ms: 24.749327670258523
    mean_inference_ms: 12.4019838023926
    mean_processing_ms: 54.41881630039655
  time_since_restore: 11014.094894647598
  time_this_iter_s: 124.69910001754761
  time_total_s: 58095.6404466629
  timestamp: 1637569142
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 43872000
  training_iteration: 457
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    457 |          58095.6 | 43872000 |  1586.57 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.14
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.91
    apples_agent-1_min: 18
    apples_agent-2_max: 424
    apples_agent-2_mean: 354.04
    apples_agent-2_min: 227
    apples_agent-3_max: 227
    apples_agent-3_mean: 153.66
    apples_agent-3_min: 62
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.29
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 380.41
    apples_agent-5_min: 231
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 397.63
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.78
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 4.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 137
    cleaning_beam_agent-3_mean: 67.12
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 471.67
    cleaning_beam_agent-4_min: 410
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 3.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-21-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1734.0
  episode_reward_mean: 1594.22
  episode_reward_min: 1040.0
  episodes_this_iter: 96
  episodes_total: 43968
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18721.227
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4299699664115906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013630413450300694
        model: {}
        policy_loss: -0.0014291983097791672
        total_loss: 0.001421603374183178
        vf_explained_var: 0.03283041715621948
        vf_loss: 36.07549285888672
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24578487873077393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009122182964347303
        model: {}
        policy_loss: -0.001963155111297965
        total_loss: 0.0008923776913434267
        vf_explained_var: 0.11879181861877441
        vf_loss: 32.8811149597168
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29527154564857483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013555627083405852
        model: {}
        policy_loss: -0.0019370198715478182
        total_loss: 0.004386168904602528
        vf_explained_var: 0.0889129638671875
        vf_loss: 68.42864990234375
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7726348042488098
        entropy_coeff: 0.0017600000137463212
        kl: 0.000722703174687922
        model: {}
        policy_loss: -0.0019899161998182535
        total_loss: 0.004309466108679771
        vf_explained_var: -0.017235353589057922
        vf_loss: 76.5921859741211
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9833922386169434
        entropy_coeff: 0.0017600000137463212
        kl: 0.003355881664901972
        model: {}
        policy_loss: -0.002447419799864292
        total_loss: -0.0008367594564333558
        vf_explained_var: 0.025989577174186707
        vf_loss: 33.41434860229492
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26244261860847473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009174957522191107
        model: {}
        policy_loss: -0.0018286467529833317
        total_loss: 0.0006658255588263273
        vf_explained_var: 0.13659419119358063
        vf_loss: 29.563701629638672
    load_time_ms: 13188.369
    num_steps_sampled: 43968000
    num_steps_trained: 43968000
    sample_time_ms: 93868.953
    update_time_ms: 17.584
  iterations_since_restore: 88
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.74692737430168
    ram_util_percent: 16.19050279329609
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 380.5
    agent-3: 380.5
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 238.0
    agent-1: 238.0
    agent-2: 334.095
    agent-3: 334.095
    agent-4: 225.015
    agent-5: 225.015
  policy_reward_min:
    agent-0: 156.0
    agent-1: 156.0
    agent-2: 211.0
    agent-3: 211.0
    agent-4: 144.5
    agent-5: 144.5
  sampler_perf:
    mean_env_wait_ms: 24.74958236651659
    mean_inference_ms: 12.401332995436338
    mean_processing_ms: 54.41984122182048
  time_since_restore: 11139.587951898575
  time_this_iter_s: 125.49305725097656
  time_total_s: 58221.13350391388
  timestamp: 1637569268
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 43968000
  training_iteration: 458
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    458 |          58221.1 | 43968000 |  1594.22 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 0.23
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 31.48
    apples_agent-1_min: 17
    apples_agent-2_max: 425
    apples_agent-2_mean: 358.49
    apples_agent-2_min: 249
    apples_agent-3_max: 228
    apples_agent-3_mean: 153.16
    apples_agent-3_min: 64
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.33
    apples_agent-4_min: 0
    apples_agent-5_max: 470
    apples_agent-5_mean: 386.27
    apples_agent-5_min: 243
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 402.92
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 2.09
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 4.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 60.31
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 497.61
    cleaning_beam_agent-4_min: 412
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-23-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1768.0
  episode_reward_mean: 1606.81
  episode_reward_min: 1073.0
  episodes_this_iter: 96
  episodes_total: 44064
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18691.229
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41680335998535156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019039621111005545
        model: {}
        policy_loss: -0.0014853361062705517
        total_loss: 0.0012698136270046234
        vf_explained_var: 0.00662212073802948
        vf_loss: 34.887237548828125
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2473820596933365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008278379100374877
        model: {}
        policy_loss: -0.0018225647509098053
        total_loss: 0.0010105350520461798
        vf_explained_var: 0.07076975703239441
        vf_loss: 32.684967041015625
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29125988483428955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008620060398243368
        model: {}
        policy_loss: -0.001928852405399084
        total_loss: 0.004493221640586853
        vf_explained_var: 0.053858786821365356
        vf_loss: 69.34693145751953
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.76707923412323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008347767870873213
        model: {}
        policy_loss: -0.002124861581251025
        total_loss: 0.004264538176357746
        vf_explained_var: -0.05027508735656738
        vf_loss: 77.39462280273438
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.968639612197876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014262408949434757
        model: {}
        policy_loss: -0.0019912610296159983
        total_loss: -0.00040520355105400085
        vf_explained_var: 0.04881808161735535
        vf_loss: 32.90865707397461
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2693314552307129
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013180328533053398
        model: {}
        policy_loss: -0.002123774029314518
        total_loss: 0.00051139947026968
        vf_explained_var: 0.10361871123313904
        vf_loss: 31.091960906982422
    load_time_ms: 13205.221
    num_steps_sampled: 44064000
    num_steps_trained: 44064000
    sample_time_ms: 93898.983
    update_time_ms: 17.554
  iterations_since_restore: 89
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.79555555555556
    ram_util_percent: 16.17222222222222
  pid: 21723
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 389.0
    agent-3: 389.0
    agent-4: 273.5
    agent-5: 273.5
  policy_reward_mean:
    agent-0: 240.225
    agent-1: 240.225
    agent-2: 336.335
    agent-3: 336.335
    agent-4: 226.845
    agent-5: 226.845
  policy_reward_min:
    agent-0: 151.0
    agent-1: 151.0
    agent-2: 228.5
    agent-3: 228.5
    agent-4: 150.0
    agent-5: 150.0
  sampler_perf:
    mean_env_wait_ms: 24.75127381013855
    mean_inference_ms: 12.401120935873262
    mean_processing_ms: 54.422785107582655
  time_since_restore: 11265.665690422058
  time_this_iter_s: 126.07773852348328
  time_total_s: 58347.21124243736
  timestamp: 1637569394
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 44064000
  training_iteration: 459
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    459 |          58347.2 | 44064000 |  1606.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 31.48
    apples_agent-1_min: 10
    apples_agent-2_max: 428
    apples_agent-2_mean: 356.46
    apples_agent-2_min: 117
    apples_agent-3_max: 231
    apples_agent-3_mean: 151.94
    apples_agent-3_min: 34
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 468
    apples_agent-5_mean: 384.37
    apples_agent-5_min: 135
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 409.77
    cleaning_beam_agent-0_min: 297
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 1.9
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 5.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 63.94
    cleaning_beam_agent-3_min: 37
    cleaning_beam_agent-4_max: 608
    cleaning_beam_agent-4_mean: 508.57
    cleaning_beam_agent-4_min: 441
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-25-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1765.0
  episode_reward_mean: 1597.81
  episode_reward_min: 557.0
  episodes_this_iter: 96
  episodes_total: 44160
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18692.044
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4247310161590576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017280398169532418
        model: {}
        policy_loss: -0.001488993875682354
        total_loss: 0.001325603574514389
        vf_explained_var: 0.049544572830200195
        vf_loss: 35.62123107910156
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24854221940040588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013051772257313132
        model: {}
        policy_loss: -0.001820312812924385
        total_loss: 0.0010935268364846706
        vf_explained_var: 0.10584589838981628
        vf_loss: 33.51272201538086
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29776531457901
        entropy_coeff: 0.0017600000137463212
        kl: 0.000836552819237113
        model: {}
        policy_loss: -0.001963414251804352
        total_loss: 0.004253740422427654
        vf_explained_var: 0.08093473315238953
        vf_loss: 67.4122314453125
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7631431818008423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007937352638691664
        model: {}
        policy_loss: -0.002074461430311203
        total_loss: 0.004169751890003681
        vf_explained_var: -0.030776917934417725
        vf_loss: 75.87346649169922
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9622433185577393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010634127538651228
        model: {}
        policy_loss: -0.0019051139242947102
        total_loss: -0.00036744633689522743
        vf_explained_var: 0.045352011919021606
        vf_loss: 32.312164306640625
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2642163932323456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007051631109789014
        model: {}
        policy_loss: -0.0017571441130712628
        total_loss: 0.0006704970728605986
        vf_explained_var: 0.1455613374710083
        vf_loss: 28.9266300201416
    load_time_ms: 13188.899
    num_steps_sampled: 44160000
    num_steps_trained: 44160000
    sample_time_ms: 93709.062
    update_time_ms: 17.574
  iterations_since_restore: 90
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.46404494382022
    ram_util_percent: 16.179213483146068
  pid: 21723
  policy_reward_max:
    agent-0: 265.5
    agent-1: 265.5
    agent-2: 382.5
    agent-3: 382.5
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 237.745
    agent-1: 237.745
    agent-2: 334.56
    agent-3: 334.56
    agent-4: 226.6
    agent-5: 226.6
  policy_reward_min:
    agent-0: 93.5
    agent-1: 93.5
    agent-2: 102.5
    agent-3: 102.5
    agent-4: 82.5
    agent-5: 82.5
  sampler_perf:
    mean_env_wait_ms: 24.75121295113286
    mean_inference_ms: 12.400178990841821
    mean_processing_ms: 54.418616212666265
  time_since_restore: 11390.497621774673
  time_this_iter_s: 124.83193135261536
  time_total_s: 58472.04317378998
  timestamp: 1637569519
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 44160000
  training_iteration: 460
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    460 |            58472 | 44160000 |  1597.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 32.37
    apples_agent-1_min: 18
    apples_agent-2_max: 415
    apples_agent-2_mean: 354.49
    apples_agent-2_min: 220
    apples_agent-3_max: 257
    apples_agent-3_mean: 153.72
    apples_agent-3_min: 90
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 476
    apples_agent-5_mean: 383.36
    apples_agent-5_min: 250
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 408.25
    cleaning_beam_agent-0_min: 324
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 2.32
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 4.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 63.47
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 622
    cleaning_beam_agent-4_mean: 518.28
    cleaning_beam_agent-4_min: 441
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-27-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1726.0
  episode_reward_mean: 1600.18
  episode_reward_min: 1099.0
  episodes_this_iter: 96
  episodes_total: 44256
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18683.918
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42350679636001587
        entropy_coeff: 0.0017600000137463212
        kl: 0.001092568039894104
        model: {}
        policy_loss: -0.0013693086802959442
        total_loss: 0.0012277793139219284
        vf_explained_var: 0.02043822407722473
        vf_loss: 33.424625396728516
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2449362426996231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016196116339415312
        model: {}
        policy_loss: -0.0019752713851630688
        total_loss: 0.000679165415931493
        vf_explained_var: 0.09732471406459808
        vf_loss: 30.855266571044922
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2890847325325012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011601174483075738
        model: {}
        policy_loss: -0.0019162064418196678
        total_loss: 0.0044267019256949425
        vf_explained_var: 0.04224058985710144
        vf_loss: 68.51700592041016
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7605334520339966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008238066802732646
        model: {}
        policy_loss: -0.0020169466733932495
        total_loss: 0.0037726517766714096
        vf_explained_var: 0.007881015539169312
        vf_loss: 71.28138732910156
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9691138863563538
        entropy_coeff: 0.0017600000137463212
        kl: 0.002076386706903577
        model: {}
        policy_loss: -0.002160042989999056
        total_loss: -0.0008668866939842701
        vf_explained_var: 0.056600093841552734
        vf_loss: 29.987977981567383
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2530536353588104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009664823301136494
        model: {}
        policy_loss: -0.0017715021967887878
        total_loss: 0.0007976139895617962
        vf_explained_var: 0.05779378116130829
        vf_loss: 30.144866943359375
    load_time_ms: 13161.849
    num_steps_sampled: 44256000
    num_steps_trained: 44256000
    sample_time_ms: 93628.495
    update_time_ms: 17.776
  iterations_since_restore: 91
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.33128491620111
    ram_util_percent: 16.1927374301676
  pid: 21723
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 389.0
    agent-3: 389.0
    agent-4: 254.0
    agent-5: 254.0
  policy_reward_mean:
    agent-0: 238.085
    agent-1: 238.085
    agent-2: 335.91
    agent-3: 335.91
    agent-4: 226.095
    agent-5: 226.095
  policy_reward_min:
    agent-0: 168.0
    agent-1: 168.0
    agent-2: 228.5
    agent-3: 228.5
    agent-4: 153.0
    agent-5: 153.0
  sampler_perf:
    mean_env_wait_ms: 24.75241910870345
    mean_inference_ms: 12.398896179030412
    mean_processing_ms: 54.41498231613239
  time_since_restore: 11515.568575620651
  time_this_iter_s: 125.07095384597778
  time_total_s: 58597.114127635956
  timestamp: 1637569645
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 44256000
  training_iteration: 461
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    461 |          58597.1 | 44256000 |  1600.18 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 30.95
    apples_agent-1_min: 18
    apples_agent-2_max: 416
    apples_agent-2_mean: 350.93
    apples_agent-2_min: 226
    apples_agent-3_max: 351
    apples_agent-3_mean: 160.55
    apples_agent-3_min: 70
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.35
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 383.2
    apples_agent-5_min: 280
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 414.15
    cleaning_beam_agent-0_min: 375
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.97
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 4.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 65.33
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 623
    cleaning_beam_agent-4_mean: 514.17
    cleaning_beam_agent-4_min: 404
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-29-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1729.0
  episode_reward_mean: 1602.37
  episode_reward_min: 1169.0
  episodes_this_iter: 96
  episodes_total: 44352
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18660.58
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42619025707244873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013049178523942828
        model: {}
        policy_loss: -0.0017119564581662416
        total_loss: 0.001054882537573576
        vf_explained_var: -0.006027445197105408
        vf_loss: 35.16932678222656
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24011582136154175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006600108463317156
        model: {}
        policy_loss: -0.0014714398421347141
        total_loss: 0.0013149179285392165
        vf_explained_var: 0.083253875374794
        vf_loss: 32.08964920043945
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29174843430519104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009854716481640935
        model: {}
        policy_loss: -0.001866579637862742
        total_loss: 0.004386903718113899
        vf_explained_var: 0.056611061096191406
        vf_loss: 67.66959381103516
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7665785551071167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013003256171941757
        model: {}
        policy_loss: -0.0024303016252815723
        total_loss: 0.003575029317289591
        vf_explained_var: -0.021475493907928467
        vf_loss: 73.54505920410156
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9683971405029297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024741666857153177
        model: {}
        policy_loss: -0.0023633036762475967
        total_loss: -0.0009169300319626927
        vf_explained_var: 0.03600938618183136
        vf_loss: 31.507553100585938
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2515650689601898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005961225833743811
        model: {}
        policy_loss: -0.001964438008144498
        total_loss: 0.000561798457056284
        vf_explained_var: 0.0914335697889328
        vf_loss: 29.68988800048828
    load_time_ms: 13132.913
    num_steps_sampled: 44352000
    num_steps_trained: 44352000
    sample_time_ms: 93543.003
    update_time_ms: 17.883
  iterations_since_restore: 92
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.979888268156415
    ram_util_percent: 16.174860335195532
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 389.5
    agent-3: 389.5
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 239.485
    agent-1: 239.485
    agent-2: 335.645
    agent-3: 335.645
    agent-4: 226.055
    agent-5: 226.055
  policy_reward_min:
    agent-0: 172.5
    agent-1: 172.5
    agent-2: 215.5
    agent-3: 215.5
    agent-4: 176.5
    agent-5: 176.5
  sampler_perf:
    mean_env_wait_ms: 24.754698455608363
    mean_inference_ms: 12.399224228643343
    mean_processing_ms: 54.4131679371037
  time_since_restore: 11640.889937639236
  time_this_iter_s: 125.3213620185852
  time_total_s: 58722.43548965454
  timestamp: 1637569770
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 44352000
  training_iteration: 462
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    462 |          58722.4 | 44352000 |  1602.37 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 31.83
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 360.37
    apples_agent-2_min: 270
    apples_agent-3_max: 221
    apples_agent-3_mean: 151.6
    apples_agent-3_min: 52
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 383.7
    apples_agent-5_min: 276
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 421.47
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.56
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 33
    cleaning_beam_agent-2_mean: 3.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 59.62
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 496.63
    cleaning_beam_agent-4_min: 418
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-31-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1706.0
  episode_reward_mean: 1606.87
  episode_reward_min: 1285.0
  episodes_this_iter: 96
  episodes_total: 44448
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18646.0
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4219910800457001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009818156249821186
        model: {}
        policy_loss: -0.001256586518138647
        total_loss: 0.0014569349586963654
        vf_explained_var: 0.004232525825500488
        vf_loss: 34.56227111816406
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23908254504203796
        entropy_coeff: 0.0017600000137463212
        kl: 0.00044057879131287336
        model: {}
        policy_loss: -0.0015908097848296165
        total_loss: 0.0011753307189792395
        vf_explained_var: 0.0824381411075592
        vf_loss: 31.86925506591797
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.290961354970932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010214222129434347
        model: {}
        policy_loss: -0.0017630704678595066
        total_loss: 0.0042684744112193584
        vf_explained_var: 0.047683313488960266
        vf_loss: 65.4363784790039
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7529865503311157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020837888587266207
        model: {}
        policy_loss: -0.0025130780413746834
        total_loss: 0.0030429819598793983
        vf_explained_var: 0.00517687201499939
        vf_loss: 68.81317138671875
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9649433493614197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029837978072464466
        model: {}
        policy_loss: -0.002381362020969391
        total_loss: -0.0009141187183558941
        vf_explained_var: 0.03811034560203552
        vf_loss: 31.655426025390625
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2579038441181183
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008164510945789516
        model: {}
        policy_loss: -0.0017357566393911839
        total_loss: 0.0007247128523886204
        vf_explained_var: 0.1144622266292572
        vf_loss: 29.143795013427734
    load_time_ms: 13145.301
    num_steps_sampled: 44448000
    num_steps_trained: 44448000
    sample_time_ms: 93480.079
    update_time_ms: 18.078
  iterations_since_restore: 93
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.676404494382034
    ram_util_percent: 16.162359550561796
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 369.5
    agent-3: 369.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 239.765
    agent-1: 239.765
    agent-2: 337.205
    agent-3: 337.205
    agent-4: 226.465
    agent-5: 226.465
  policy_reward_min:
    agent-0: 193.0
    agent-1: 193.0
    agent-2: 271.5
    agent-3: 271.5
    agent-4: 170.5
    agent-5: 170.5
  sampler_perf:
    mean_env_wait_ms: 24.75575530164021
    mean_inference_ms: 12.398875939485883
    mean_processing_ms: 54.41175778788968
  time_since_restore: 11766.135109424591
  time_this_iter_s: 125.24517178535461
  time_total_s: 58847.680661439896
  timestamp: 1637569896
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 44448000
  training_iteration: 463
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    463 |          58847.7 | 44448000 |  1606.87 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 29.32
    apples_agent-1_min: 1
    apples_agent-2_max: 427
    apples_agent-2_mean: 350.84
    apples_agent-2_min: 34
    apples_agent-3_max: 249
    apples_agent-3_mean: 150.15
    apples_agent-3_min: 19
    apples_agent-4_max: 32
    apples_agent-4_mean: 0.49
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 373.48
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 422.17
    cleaning_beam_agent-0_min: 281
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.37
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 3.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 62.81
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 633
    cleaning_beam_agent-4_mean: 507.07
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 3.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-33-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1731.0
  episode_reward_mean: 1571.82
  episode_reward_min: 183.0
  episodes_this_iter: 96
  episodes_total: 44544
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18647.614
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43447691202163696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014466338325291872
        model: {}
        policy_loss: -0.001655740663409233
        total_loss: 0.0013647093437612057
        vf_explained_var: 0.06117050349712372
        vf_loss: 37.85128402709961
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24724790453910828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005343183293007314
        model: {}
        policy_loss: -0.0019079254707321525
        total_loss: 0.0011298832250759006
        vf_explained_var: 0.1382162868976593
        vf_loss: 34.729652404785156
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3042871057987213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011244373163208365
        model: {}
        policy_loss: -0.002173716900870204
        total_loss: 0.0045484695583581924
        vf_explained_var: 0.13972964882850647
        vf_loss: 72.57730865478516
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7671709060668945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007416902226395905
        model: {}
        policy_loss: -0.0021659634076058865
        total_loss: 0.0048840707167983055
        vf_explained_var: 0.005175113677978516
        vf_loss: 84.00251770019531
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9663942456245422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014002980897203088
        model: {}
        policy_loss: -0.001986835850402713
        total_loss: -8.140970021486282e-05
        vf_explained_var: 0.07426755130290985
        vf_loss: 36.06281280517578
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27191779017448425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007956984918564558
        model: {}
        policy_loss: -0.00200185040012002
        total_loss: 0.0007479866035282612
        vf_explained_var: 0.16979287564754486
        vf_loss: 32.28411865234375
    load_time_ms: 13153.113
    num_steps_sampled: 44544000
    num_steps_trained: 44544000
    sample_time_ms: 93445.295
    update_time_ms: 17.95
  iterations_since_restore: 94
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 53.93240223463688
    ram_util_percent: 16.38882681564246
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 382.0
    agent-3: 382.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 234.065
    agent-1: 234.065
    agent-2: 329.85
    agent-3: 329.85
    agent-4: 221.995
    agent-5: 221.995
  policy_reward_min:
    agent-0: 29.5
    agent-1: 29.5
    agent-2: 35.5
    agent-3: 35.5
    agent-4: 26.5
    agent-5: 26.5
  sampler_perf:
    mean_env_wait_ms: 24.758236942560085
    mean_inference_ms: 12.398715481553111
    mean_processing_ms: 54.40977568936449
  time_since_restore: 11891.813674926758
  time_this_iter_s: 125.67856550216675
  time_total_s: 58973.35922694206
  timestamp: 1637570022
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 44544000
  training_iteration: 464
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    464 |          58973.4 | 44544000 |  1571.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 30.8
    apples_agent-1_min: 14
    apples_agent-2_max: 414
    apples_agent-2_mean: 355.58
    apples_agent-2_min: 233
    apples_agent-3_max: 233
    apples_agent-3_mean: 148.04
    apples_agent-3_min: 53
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.32
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 378.68
    apples_agent-5_min: 239
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 419.35
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 1.9
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 4.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 62.03
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 623
    cleaning_beam_agent-4_mean: 513.2
    cleaning_beam_agent-4_min: 436
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 3.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-35-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1736.0
  episode_reward_mean: 1580.7
  episode_reward_min: 1066.0
  episodes_this_iter: 96
  episodes_total: 44640
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18634.517
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4239782691001892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009394462686032057
        model: {}
        policy_loss: -0.001268934109248221
        total_loss: 0.0016888957470655441
        vf_explained_var: 0.018807202577590942
        vf_loss: 37.04030227661133
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24199479818344116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005776146426796913
        model: {}
        policy_loss: -0.0015618819743394852
        total_loss: 0.0014492189511656761
        vf_explained_var: 0.09022054076194763
        vf_loss: 34.370147705078125
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2986558675765991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009722206741571426
        model: {}
        policy_loss: -0.0018405145965516567
        total_loss: 0.0047422610223293304
        vf_explained_var: 0.07135608792304993
        vf_loss: 71.0841064453125
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.758530855178833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008954480290412903
        model: {}
        policy_loss: -0.0020088900346308947
        total_loss: 0.004354395437985659
        vf_explained_var: -0.00594937801361084
        vf_loss: 76.98298645019531
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9816498756408691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0035323146730661392
        model: {}
        policy_loss: -0.0026247454807162285
        total_loss: -0.001166792120784521
        vf_explained_var: 0.055456340312957764
        vf_loss: 31.856557846069336
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2636226415634155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005115160020068288
        model: {}
        policy_loss: -0.0017733396962285042
        total_loss: 0.0007283054292201996
        vf_explained_var: 0.11999881267547607
        vf_loss: 29.65618324279785
    load_time_ms: 13151.846
    num_steps_sampled: 44640000
    num_steps_trained: 44640000
    sample_time_ms: 93449.008
    update_time_ms: 18.033
  iterations_since_restore: 95
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.73876404494382
    ram_util_percent: 16.214044943820223
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 383.0
    agent-3: 383.0
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 237.905
    agent-1: 237.905
    agent-2: 329.47
    agent-3: 329.47
    agent-4: 222.975
    agent-5: 222.975
  policy_reward_min:
    agent-0: 174.5
    agent-1: 174.5
    agent-2: 207.5
    agent-3: 207.5
    agent-4: 148.0
    agent-5: 148.0
  sampler_perf:
    mean_env_wait_ms: 24.760209810066
    mean_inference_ms: 12.397903988132525
    mean_processing_ms: 54.40735271923905
  time_since_restore: 12016.808272600174
  time_this_iter_s: 124.99459767341614
  time_total_s: 59098.35382461548
  timestamp: 1637570147
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 44640000
  training_iteration: 465
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    465 |          59098.4 | 44640000 |   1580.7 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.99
    apples_agent-1_min: 16
    apples_agent-2_max: 410
    apples_agent-2_mean: 355.06
    apples_agent-2_min: 268
    apples_agent-3_max: 226
    apples_agent-3_mean: 155.92
    apples_agent-3_min: 91
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.38
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 383.83
    apples_agent-5_min: 278
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 424.48
    cleaning_beam_agent-0_min: 382
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.65
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 3.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 62.83
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 612
    cleaning_beam_agent-4_mean: 503.59
    cleaning_beam_agent-4_min: 428
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-37-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1724.0
  episode_reward_mean: 1596.12
  episode_reward_min: 1307.0
  episodes_this_iter: 96
  episodes_total: 44736
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18625.415
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4234974980354309
        entropy_coeff: 0.0017600000137463212
        kl: 0.002004058798775077
        model: {}
        policy_loss: -0.0015651211142539978
        total_loss: 0.0010925806127488613
        vf_explained_var: -0.006554007530212402
        vf_loss: 34.030574798583984
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23992520570755005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010747998021543026
        model: {}
        policy_loss: -0.0018763493280857801
        total_loss: 0.0008109697373583913
        vf_explained_var: 0.0809546709060669
        vf_loss: 31.095928192138672
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2927025556564331
        entropy_coeff: 0.0017600000137463212
        kl: 0.001334797008894384
        model: {}
        policy_loss: -0.0017613011877983809
        total_loss: 0.004293801262974739
        vf_explained_var: 0.02795560657978058
        vf_loss: 65.70259857177734
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7594035267829895
        entropy_coeff: 0.0017600000137463212
        kl: 0.00147431087680161
        model: {}
        policy_loss: -0.0022935597226023674
        total_loss: 0.003385464660823345
        vf_explained_var: -0.0330444872379303
        vf_loss: 70.15576934814453
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9823817014694214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013660540571436286
        model: {}
        policy_loss: -0.0019549373537302017
        total_loss: -0.0007082177326083183
        vf_explained_var: 0.032424747943878174
        vf_loss: 29.757118225097656
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2567584216594696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008783622761256993
        model: {}
        policy_loss: -0.0015409442130476236
        total_loss: 0.000842807290609926
        vf_explained_var: 0.0852721780538559
        vf_loss: 28.35646629333496
    load_time_ms: 13149.51
    num_steps_sampled: 44736000
    num_steps_trained: 44736000
    sample_time_ms: 93411.231
    update_time_ms: 18.258
  iterations_since_restore: 96
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.902234636871505
    ram_util_percent: 16.289385474860335
  pid: 21723
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 364.5
    agent-3: 364.5
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 239.645
    agent-1: 239.645
    agent-2: 331.85
    agent-3: 331.85
    agent-4: 226.565
    agent-5: 226.565
  policy_reward_min:
    agent-0: 174.0
    agent-1: 174.0
    agent-2: 273.5
    agent-3: 273.5
    agent-4: 186.0
    agent-5: 186.0
  sampler_perf:
    mean_env_wait_ms: 24.76113928800024
    mean_inference_ms: 12.397231914569376
    mean_processing_ms: 54.406434370563105
  time_since_restore: 12142.238587856293
  time_this_iter_s: 125.43031525611877
  time_total_s: 59223.7841398716
  timestamp: 1637570272
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 44736000
  training_iteration: 466
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    466 |          59223.8 | 44736000 |  1596.12 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 31.12
    apples_agent-1_min: 20
    apples_agent-2_max: 403
    apples_agent-2_mean: 355.26
    apples_agent-2_min: 249
    apples_agent-3_max: 222
    apples_agent-3_mean: 157.03
    apples_agent-3_min: 79
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.36
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 380.55
    apples_agent-5_min: 242
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 414.59
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.86
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 3.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 60.09
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 489.47
    cleaning_beam_agent-4_min: 407
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-39-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1746.0
  episode_reward_mean: 1606.14
  episode_reward_min: 1209.0
  episodes_this_iter: 96
  episodes_total: 44832
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18615.608
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4221688210964203
        entropy_coeff: 0.0017600000137463212
        kl: 0.00144757772795856
        model: {}
        policy_loss: -0.0015031774528324604
        total_loss: 0.0011417358182370663
        vf_explained_var: 0.012833356857299805
        vf_loss: 33.879302978515625
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23707902431488037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010388856753706932
        model: {}
        policy_loss: -0.0017249007942155004
        total_loss: 0.001019214978441596
        vf_explained_var: 0.08037421107292175
        vf_loss: 31.613758087158203
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29281920194625854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014048601733520627
        model: {}
        policy_loss: -0.0018102675676345825
        total_loss: 0.004416168667376041
        vf_explained_var: 0.05719994008541107
        vf_loss: 67.41797637939453
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7412023544311523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006100746104493737
        model: {}
        policy_loss: -0.001969940960407257
        total_loss: 0.004076960030943155
        vf_explained_var: -0.02344939112663269
        vf_loss: 73.51419067382812
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9920459985733032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025662516709417105
        model: {}
        policy_loss: -0.0020639870781451464
        total_loss: -0.0004888016264885664
        vf_explained_var: 0.03391045331954956
        vf_loss: 33.211875915527344
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2653960883617401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010170789901167154
        model: {}
        policy_loss: -0.0019932501018047333
        total_loss: 0.0005765692330896854
        vf_explained_var: 0.11764907836914062
        vf_loss: 30.369152069091797
    load_time_ms: 13130.701
    num_steps_sampled: 44832000
    num_steps_trained: 44832000
    sample_time_ms: 93412.379
    update_time_ms: 18.487
  iterations_since_restore: 97
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.83389830508474
    ram_util_percent: 16.1864406779661
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 394.0
    agent-3: 394.0
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 239.93
    agent-1: 239.93
    agent-2: 338.325
    agent-3: 338.325
    agent-4: 224.815
    agent-5: 224.815
  policy_reward_min:
    agent-0: 178.0
    agent-1: 178.0
    agent-2: 225.5
    agent-3: 225.5
    agent-4: 138.0
    agent-5: 138.0
  sampler_perf:
    mean_env_wait_ms: 24.761461878699183
    mean_inference_ms: 12.396751122543638
    mean_processing_ms: 54.40235849482568
  time_since_restore: 12266.660563468933
  time_this_iter_s: 124.42197561264038
  time_total_s: 59348.20611548424
  timestamp: 1637570397
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 44832000
  training_iteration: 467
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    467 |          59348.2 | 44832000 |  1606.14 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.24
    apples_agent-1_min: 14
    apples_agent-2_max: 420
    apples_agent-2_mean: 358.57
    apples_agent-2_min: 283
    apples_agent-3_max: 234
    apples_agent-3_mean: 163.14
    apples_agent-3_min: 56
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 468
    apples_agent-5_mean: 386.86
    apples_agent-5_min: 322
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 413.14
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.55
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.68
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 59.19
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 473.72
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 2.96
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-42-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1726.0
  episode_reward_mean: 1621.51
  episode_reward_min: 1416.0
  episodes_this_iter: 96
  episodes_total: 44928
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18620.402
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4107499122619629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015848479233682156
        model: {}
        policy_loss: -0.001331839244812727
        total_loss: 0.0013126165140420198
        vf_explained_var: -0.002833649516105652
        vf_loss: 33.673744201660156
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23109091818332672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005465284921228886
        model: {}
        policy_loss: -0.0015106253558769822
        total_loss: 0.0012083806795999408
        vf_explained_var: 0.07261250913143158
        vf_loss: 31.25725555419922
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29534777998924255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012870784848928452
        model: {}
        policy_loss: -0.0019853049889206886
        total_loss: 0.004139420110732317
        vf_explained_var: 0.04172404110431671
        vf_loss: 66.44536590576172
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7624972462654114
        entropy_coeff: 0.0017600000137463212
        kl: 0.001403727801516652
        model: {}
        policy_loss: -0.0021685920655727386
        total_loss: 0.0036875205114483833
        vf_explained_var: -0.023365318775177002
        vf_loss: 71.98109436035156
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9898662567138672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022168010473251343
        model: {}
        policy_loss: -0.0022654933854937553
        total_loss: -0.0010599156375974417
        vf_explained_var: 0.026995792984962463
        vf_loss: 29.47743034362793
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25394928455352783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004810612299479544
        model: {}
        policy_loss: -0.0016968464478850365
        total_loss: 0.0005812458693981171
        vf_explained_var: 0.10334804654121399
        vf_loss: 27.250410079956055
    load_time_ms: 13127.504
    num_steps_sampled: 44928000
    num_steps_trained: 44928000
    sample_time_ms: 93364.096
    update_time_ms: 18.823
  iterations_since_restore: 98
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.17808988764045
    ram_util_percent: 16.266853932584272
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 389.0
    agent-3: 389.0
    agent-4: 273.0
    agent-5: 273.0
  policy_reward_mean:
    agent-0: 241.605
    agent-1: 241.605
    agent-2: 340.595
    agent-3: 340.595
    agent-4: 228.555
    agent-5: 228.555
  policy_reward_min:
    agent-0: 196.5
    agent-1: 196.5
    agent-2: 282.5
    agent-3: 282.5
    agent-4: 196.5
    agent-5: 196.5
  sampler_perf:
    mean_env_wait_ms: 24.76044383464371
    mean_inference_ms: 12.395940268607927
    mean_processing_ms: 54.40022207833876
  time_since_restore: 12391.695565223694
  time_this_iter_s: 125.03500175476074
  time_total_s: 59473.241117239
  timestamp: 1637570522
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 44928000
  training_iteration: 468
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    468 |          59473.2 | 44928000 |  1621.51 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.1
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.46
    apples_agent-1_min: 15
    apples_agent-2_max: 413
    apples_agent-2_mean: 354.41
    apples_agent-2_min: 212
    apples_agent-3_max: 220
    apples_agent-3_mean: 155.55
    apples_agent-3_min: 68
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 385.23
    apples_agent-5_min: 250
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 418.35
    cleaning_beam_agent-0_min: 378
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.83
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 5.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 60.14
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 623
    cleaning_beam_agent-4_mean: 470.58
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 3.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-44-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1724.0
  episode_reward_mean: 1606.96
  episode_reward_min: 975.0
  episodes_this_iter: 96
  episodes_total: 45024
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18633.983
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41640445590019226
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010082634398713708
        model: {}
        policy_loss: -0.0014394397148862481
        total_loss: 0.0013153518084436655
        vf_explained_var: 0.02101147174835205
        vf_loss: 34.87663269042969
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24273309111595154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007459198823198676
        model: {}
        policy_loss: -0.0017949696630239487
        total_loss: 0.001016042660921812
        vf_explained_var: 0.0940803736448288
        vf_loss: 32.38222885131836
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30373841524124146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012531689135357738
        model: {}
        policy_loss: -0.0018760885577648878
        total_loss: 0.004511830396950245
        vf_explained_var: 0.06115065515041351
        vf_loss: 69.22496032714844
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7630284428596497
        entropy_coeff: 0.0017600000137463212
        kl: 0.000967889092862606
        model: {}
        policy_loss: -0.002109467051923275
        total_loss: 0.0039895074442029
        vf_explained_var: 0.0004200190305709839
        vf_loss: 74.41902923583984
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9984956383705139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020149103365838528
        model: {}
        policy_loss: -0.00221242755651474
        total_loss: -0.0007282234728336334
        vf_explained_var: 0.008261829614639282
        vf_loss: 32.415550231933594
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26042938232421875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008177212439477444
        model: {}
        policy_loss: -0.0018206574022769928
        total_loss: 0.000600199680775404
        vf_explained_var: 0.11836668848991394
        vf_loss: 28.79214096069336
    load_time_ms: 13122.727
    num_steps_sampled: 45024000
    num_steps_trained: 45024000
    sample_time_ms: 93321.413
    update_time_ms: 18.818
  iterations_since_restore: 99
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.94748603351955
    ram_util_percent: 16.27709497206704
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 379.0
    agent-3: 379.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 241.095
    agent-1: 241.095
    agent-2: 335.6
    agent-3: 335.6
    agent-4: 226.785
    agent-5: 226.785
  policy_reward_min:
    agent-0: 152.5
    agent-1: 152.5
    agent-2: 190.5
    agent-3: 190.5
    agent-4: 144.5
    agent-5: 144.5
  sampler_perf:
    mean_env_wait_ms: 24.76126572547145
    mean_inference_ms: 12.395865383700155
    mean_processing_ms: 54.40254066907046
  time_since_restore: 12517.427110910416
  time_this_iter_s: 125.7315456867218
  time_total_s: 59598.97266292572
  timestamp: 1637570648
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 45024000
  training_iteration: 469
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    469 |            59599 | 45024000 |  1606.96 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 30.13
    apples_agent-1_min: 14
    apples_agent-2_max: 415
    apples_agent-2_mean: 356.69
    apples_agent-2_min: 153
    apples_agent-3_max: 268
    apples_agent-3_mean: 159.55
    apples_agent-3_min: 86
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 387.5
    apples_agent-5_min: 205
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 420.65
    cleaning_beam_agent-0_min: 383
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 2.73
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 4.45
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 57.22
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 454.57
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-46-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1744.0
  episode_reward_mean: 1615.01
  episode_reward_min: 819.0
  episodes_this_iter: 96
  episodes_total: 45120
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18627.488
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4138782322406769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013709058985114098
        model: {}
        policy_loss: -0.001507328124716878
        total_loss: 0.0013680211268365383
        vf_explained_var: 0.005441382527351379
        vf_loss: 36.03773880004883
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24007834494113922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013457882450893521
        model: {}
        policy_loss: -0.0018419558182358742
        total_loss: 0.0010097101330757141
        vf_explained_var: 0.09860308468341827
        vf_loss: 32.742034912109375
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29715514183044434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013942585792392492
        model: {}
        policy_loss: -0.0018349522724747658
        total_loss: 0.004484345205128193
        vf_explained_var: 0.05612392723560333
        vf_loss: 68.42288970947266
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7487425804138184
        entropy_coeff: 0.0017600000137463212
        kl: 0.000597082544118166
        model: {}
        policy_loss: -0.0017795072635635734
        total_loss: 0.004256715066730976
        vf_explained_var: -0.004529282450675964
        vf_loss: 73.54008483886719
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0050550699234009
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017268811352550983
        model: {}
        policy_loss: -0.0021273628808557987
        total_loss: -0.0006928164511919022
        vf_explained_var: 0.059835612773895264
        vf_loss: 32.034454345703125
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26805269718170166
        entropy_coeff: 0.0017600000137463212
        kl: 0.00047796632861718535
        model: {}
        policy_loss: -0.0015762485563755035
        total_loss: 0.0009502395987510681
        vf_explained_var: 0.11684568226337433
        vf_loss: 29.98259735107422
    load_time_ms: 13128.518
    num_steps_sampled: 45120000
    num_steps_trained: 45120000
    sample_time_ms: 93282.796
    update_time_ms: 18.919
  iterations_since_restore: 100
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.392090395480224
    ram_util_percent: 16.264971751412432
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 384.0
    agent-3: 384.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 240.08
    agent-1: 240.08
    agent-2: 339.335
    agent-3: 339.335
    agent-4: 228.09
    agent-5: 228.09
  policy_reward_min:
    agent-0: 117.5
    agent-1: 117.5
    agent-2: 159.5
    agent-3: 159.5
    agent-4: 132.5
    agent-5: 132.5
  sampler_perf:
    mean_env_wait_ms: 24.759934385286396
    mean_inference_ms: 12.395013814022628
    mean_processing_ms: 54.40015968758263
  time_since_restore: 12641.862879037857
  time_this_iter_s: 124.4357681274414
  time_total_s: 59723.40843105316
  timestamp: 1637570772
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 45120000
  training_iteration: 470
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    470 |          59723.4 | 45120000 |  1615.01 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.44
    apples_agent-1_min: 17
    apples_agent-2_max: 418
    apples_agent-2_mean: 358.66
    apples_agent-2_min: 287
    apples_agent-3_max: 268
    apples_agent-3_mean: 155.51
    apples_agent-3_min: 84
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.3
    apples_agent-4_min: 0
    apples_agent-5_max: 425
    apples_agent-5_mean: 384.34
    apples_agent-5_min: 283
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 427.13
    cleaning_beam_agent-0_min: 384
    cleaning_beam_agent-1_max: 32
    cleaning_beam_agent-1_mean: 2.38
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 5.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 54.26
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 460.23
    cleaning_beam_agent-4_min: 395
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 3.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-48-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1730.0
  episode_reward_mean: 1612.24
  episode_reward_min: 1246.0
  episodes_this_iter: 96
  episodes_total: 45216
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18633.105
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4139641523361206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015354473143815994
        model: {}
        policy_loss: -0.0013808598741889
        total_loss: 0.0013643540441989899
        vf_explained_var: 0.003923535346984863
        vf_loss: 34.737876892089844
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23921707272529602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006276427302509546
        model: {}
        policy_loss: -0.0017626588232815266
        total_loss: 0.0010405387729406357
        vf_explained_var: 0.07556606829166412
        vf_loss: 32.24219512939453
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2889792323112488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015407318715006113
        model: {}
        policy_loss: -0.0017810175195336342
        total_loss: 0.004271375015377998
        vf_explained_var: 0.06369934976100922
        vf_loss: 65.60997009277344
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7437862157821655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014102400746196508
        model: {}
        policy_loss: -0.002249490935355425
        total_loss: 0.0035425303503870964
        vf_explained_var: -0.002728477120399475
        vf_loss: 71.01084899902344
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0084359645843506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021589621901512146
        model: {}
        policy_loss: -0.0020813459996134043
        total_loss: -0.0008583327289670706
        vf_explained_var: 0.05464072525501251
        vf_loss: 29.978618621826172
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2713638246059418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009360091062262654
        model: {}
        policy_loss: -0.0018103837501257658
        total_loss: 0.000459519331343472
        vf_explained_var: 0.13128896057605743
        vf_loss: 27.475055694580078
    load_time_ms: 13122.403
    num_steps_sampled: 45216000
    num_steps_trained: 45216000
    sample_time_ms: 93266.878
    update_time_ms: 18.784
  iterations_since_restore: 101
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.44134078212291
    ram_util_percent: 16.188268156424584
  pid: 21723
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 381.0
    agent-3: 381.0
    agent-4: 253.0
    agent-5: 253.0
  policy_reward_mean:
    agent-0: 238.79
    agent-1: 238.79
    agent-2: 340.54
    agent-3: 340.54
    agent-4: 226.79
    agent-5: 226.79
  policy_reward_min:
    agent-0: 190.5
    agent-1: 190.5
    agent-2: 249.0
    agent-3: 249.0
    agent-4: 171.0
    agent-5: 171.0
  sampler_perf:
    mean_env_wait_ms: 24.75980651588077
    mean_inference_ms: 12.394653891585456
    mean_processing_ms: 54.39753311842452
  time_since_restore: 12766.762482643127
  time_this_iter_s: 124.89960360527039
  time_total_s: 59848.30803465843
  timestamp: 1637570898
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 45216000
  training_iteration: 471
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    471 |          59848.3 | 45216000 |  1612.24 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 32.18
    apples_agent-1_min: 17
    apples_agent-2_max: 427
    apples_agent-2_mean: 353.07
    apples_agent-2_min: 259
    apples_agent-3_max: 230
    apples_agent-3_mean: 158.46
    apples_agent-3_min: 24
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 381.86
    apples_agent-5_min: 256
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 427.37
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 57.61
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 449.96
    cleaning_beam_agent-4_min: 378
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-50-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1729.0
  episode_reward_mean: 1599.95
  episode_reward_min: 1122.0
  episodes_this_iter: 96
  episodes_total: 45312
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18626.996
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4049224555492401
        entropy_coeff: 0.0017600000137463212
        kl: 0.001647770986892283
        model: {}
        policy_loss: -0.0013388395309448242
        total_loss: 0.0013935333117842674
        vf_explained_var: 0.01798480749130249
        vf_loss: 34.450374603271484
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2386118769645691
        entropy_coeff: 0.0017600000137463212
        kl: 0.00045395741472020745
        model: {}
        policy_loss: -0.0015705153346061707
        total_loss: 0.001223537139594555
        vf_explained_var: 0.08486892282962799
        vf_loss: 32.14011001586914
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29256582260131836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010449785040691495
        model: {}
        policy_loss: -0.001850591041147709
        total_loss: 0.004588962532579899
        vf_explained_var: 0.05426080524921417
        vf_loss: 69.54470825195312
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7469874024391174
        entropy_coeff: 0.0017600000137463212
        kl: 0.001578736468218267
        model: {}
        policy_loss: -0.0022441125474870205
        total_loss: 0.0037922202609479427
        vf_explained_var: 0.006914317607879639
        vf_loss: 73.51028442382812
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0096004009246826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022286507301032543
        model: {}
        policy_loss: -0.002143586054444313
        total_loss: -0.0006968425586819649
        vf_explained_var: 0.03820917010307312
        vf_loss: 32.23637390136719
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27224671840667725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007196107762865722
        model: {}
        policy_loss: -0.0018305499106645584
        total_loss: 0.0006598485633730888
        vf_explained_var: 0.11322903633117676
        vf_loss: 29.695554733276367
    load_time_ms: 13120.863
    num_steps_sampled: 45312000
    num_steps_trained: 45312000
    sample_time_ms: 93200.934
    update_time_ms: 18.52
  iterations_since_restore: 102
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.60842696629213
    ram_util_percent: 16.334831460674152
  pid: 21723
  policy_reward_max:
    agent-0: 263.5
    agent-1: 263.5
    agent-2: 389.5
    agent-3: 389.5
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 237.44
    agent-1: 237.44
    agent-2: 337.93
    agent-3: 337.93
    agent-4: 224.605
    agent-5: 224.605
  policy_reward_min:
    agent-0: 179.5
    agent-1: 179.5
    agent-2: 205.0
    agent-3: 205.0
    agent-4: 154.5
    agent-5: 154.5
  sampler_perf:
    mean_env_wait_ms: 24.759051082152123
    mean_inference_ms: 12.39415637852132
    mean_processing_ms: 54.39455324594823
  time_since_restore: 12891.335102081299
  time_this_iter_s: 124.57261943817139
  time_total_s: 59972.8806540966
  timestamp: 1637571023
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 45312000
  training_iteration: 472
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    472 |          59972.9 | 45312000 |  1599.95 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.65
    apples_agent-1_min: 16
    apples_agent-2_max: 427
    apples_agent-2_mean: 355.06
    apples_agent-2_min: 261
    apples_agent-3_max: 205
    apples_agent-3_mean: 159.7
    apples_agent-3_min: 93
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 385.21
    apples_agent-5_min: 295
    cleaning_beam_agent-0_max: 487
    cleaning_beam_agent-0_mean: 440.29
    cleaning_beam_agent-0_min: 401
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.95
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.85
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 57.9
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 435.27
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 2.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-52-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1729.0
  episode_reward_mean: 1612.1
  episode_reward_min: 1244.0
  episodes_this_iter: 96
  episodes_total: 45408
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18651.481
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40995845198631287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014389950083568692
        model: {}
        policy_loss: -0.0012939036823809147
        total_loss: 0.0013267514295876026
        vf_explained_var: 0.003951728343963623
        vf_loss: 33.421817779541016
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23686355352401733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012664205860346556
        model: {}
        policy_loss: -0.0019246329320594668
        total_loss: 0.0007904883823357522
        vf_explained_var: 0.07108747959136963
        vf_loss: 31.320011138916016
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29070669412612915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006604824448004365
        model: {}
        policy_loss: -0.0015745338751003146
        total_loss: 0.0046067265793681145
        vf_explained_var: 0.035893455147743225
        vf_loss: 66.92903137207031
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7413829565048218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010761074954643846
        model: {}
        policy_loss: -0.002219557762145996
        total_loss: 0.003639952279627323
        vf_explained_var: -0.022319912910461426
        vf_loss: 71.64342498779297
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0129555463790894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013915472663939
        model: {}
        policy_loss: -0.0019938615150749683
        total_loss: -0.0007873524446040392
        vf_explained_var: 0.02910621464252472
        vf_loss: 29.89312744140625
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26935243606567383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011932597262784839
        model: {}
        policy_loss: -0.0016700765118002892
        total_loss: 0.0006600278429687023
        vf_explained_var: 0.09073977172374725
        vf_loss: 28.04166030883789
    load_time_ms: 13110.894
    num_steps_sampled: 45408000
    num_steps_trained: 45408000
    sample_time_ms: 93300.718
    update_time_ms: 18.374
  iterations_since_restore: 103
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.67821229050279
    ram_util_percent: 16.308379888268156
  pid: 21723
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 373.0
    agent-3: 373.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 240.175
    agent-1: 240.175
    agent-2: 338.265
    agent-3: 338.265
    agent-4: 227.61
    agent-5: 227.61
  policy_reward_min:
    agent-0: 170.0
    agent-1: 170.0
    agent-2: 264.5
    agent-3: 264.5
    agent-4: 187.5
    agent-5: 187.5
  sampler_perf:
    mean_env_wait_ms: 24.75943131970548
    mean_inference_ms: 12.394656933058547
    mean_processing_ms: 54.39796867899404
  time_since_restore: 13017.728874444962
  time_this_iter_s: 126.39377236366272
  time_total_s: 60099.274426460266
  timestamp: 1637571150
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 45408000
  training_iteration: 473
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    473 |          60099.3 | 45408000 |   1612.1 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.65
    apples_agent-1_min: 16
    apples_agent-2_max: 413
    apples_agent-2_mean: 355.06
    apples_agent-2_min: 260
    apples_agent-3_max: 218
    apples_agent-3_mean: 158.53
    apples_agent-3_min: 67
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 380.26
    apples_agent-5_min: 292
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 447.7
    cleaning_beam_agent-0_min: 403
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.64
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 4.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 54.94
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 448.74
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-54-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1755.0
  episode_reward_mean: 1612.35
  episode_reward_min: 1180.0
  episodes_this_iter: 96
  episodes_total: 45504
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18644.531
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41662395000457764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012890606885775924
        model: {}
        policy_loss: -0.0012628629337996244
        total_loss: 0.0014653786784037948
        vf_explained_var: 0.0169297456741333
        vf_loss: 34.61499786376953
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23945723474025726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007484413217753172
        model: {}
        policy_loss: -0.0019125333055853844
        total_loss: 0.0009527686052024364
        vf_explained_var: 0.07023824751377106
        vf_loss: 32.86746597290039
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2928480803966522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005021184333600104
        model: {}
        policy_loss: -0.0016987319104373455
        total_loss: 0.004746192134916782
        vf_explained_var: 0.05466262996196747
        vf_loss: 69.60335540771484
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7399184107780457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007048469851724803
        model: {}
        policy_loss: -0.0019207480363547802
        total_loss: 0.004331228323280811
        vf_explained_var: -0.013183951377868652
        vf_loss: 75.54235076904297
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9967097640037537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016642097616568208
        model: {}
        policy_loss: -0.0019497238099575043
        total_loss: -0.0005858894437551498
        vf_explained_var: 0.018223613500595093
        vf_loss: 31.180465698242188
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2724102735519409
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009448720957152545
        model: {}
        policy_loss: -0.0016688337782397866
        total_loss: 0.0007227808237075806
        vf_explained_var: 0.09558750689029694
        vf_loss: 28.710567474365234
    load_time_ms: 13116.67
    num_steps_sampled: 45504000
    num_steps_trained: 45504000
    sample_time_ms: 93337.8
    update_time_ms: 18.399
  iterations_since_restore: 104
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.97444444444443
    ram_util_percent: 16.362222222222222
  pid: 21723
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 375.0
    agent-3: 375.0
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 240.495
    agent-1: 240.495
    agent-2: 340.505
    agent-3: 340.505
    agent-4: 225.175
    agent-5: 225.175
  policy_reward_min:
    agent-0: 180.5
    agent-1: 180.5
    agent-2: 233.5
    agent-3: 233.5
    agent-4: 176.0
    agent-5: 176.0
  sampler_perf:
    mean_env_wait_ms: 24.759513361630802
    mean_inference_ms: 12.39507239082752
    mean_processing_ms: 54.40305713105041
  time_since_restore: 13143.740272045135
  time_this_iter_s: 126.01139760017395
  time_total_s: 60225.28582406044
  timestamp: 1637571276
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 45504000
  training_iteration: 474
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    474 |          60225.3 | 45504000 |  1612.35 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 31.05
    apples_agent-1_min: 19
    apples_agent-2_max: 416
    apples_agent-2_mean: 355.15
    apples_agent-2_min: 267
    apples_agent-3_max: 220
    apples_agent-3_mean: 159.28
    apples_agent-3_min: 73
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 377.28
    apples_agent-5_min: 266
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 443.84
    cleaning_beam_agent-0_min: 393
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 4.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 52.2
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 447.48
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-56-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1762.0
  episode_reward_mean: 1597.66
  episode_reward_min: 1204.0
  episodes_this_iter: 96
  episodes_total: 45600
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18665.613
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4059205949306488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005255236173979938
        model: {}
        policy_loss: -0.0011132266372442245
        total_loss: 0.0017039040103554726
        vf_explained_var: 0.015224277973175049
        vf_loss: 35.31550979614258
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24406784772872925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010312213562428951
        model: {}
        policy_loss: -0.0018484683241695166
        total_loss: 0.001030707499012351
        vf_explained_var: 0.07854664325714111
        vf_loss: 33.08734893798828
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29379650950431824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008408344583585858
        model: {}
        policy_loss: -0.0017407615669071674
        total_loss: 0.004711046349257231
        vf_explained_var: 0.060000255703926086
        vf_loss: 69.68891906738281
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7402698397636414
        entropy_coeff: 0.0017600000137463212
        kl: 0.001618657959625125
        model: {}
        policy_loss: -0.0023984918370842934
        total_loss: 0.0038745931815356016
        vf_explained_var: -0.015598297119140625
        vf_loss: 75.75962829589844
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0086400508880615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029142615385353565
        model: {}
        policy_loss: -0.0022937976755201817
        total_loss: -0.0008661183528602123
        vf_explained_var: 0.045082345604896545
        vf_loss: 32.02886962890625
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27817922830581665
        entropy_coeff: 0.0017600000137463212
        kl: 0.000664275314193219
        model: {}
        policy_loss: -0.001798504963517189
        total_loss: 0.0006419417914003134
        vf_explained_var: 0.12417204678058624
        vf_loss: 29.300418853759766
    load_time_ms: 13132.966
    num_steps_sampled: 45600000
    num_steps_trained: 45600000
    sample_time_ms: 93480.225
    update_time_ms: 18.829
  iterations_since_restore: 105
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.712777777777774
    ram_util_percent: 16.33888888888889
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 382.5
    agent-3: 382.5
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 238.38
    agent-1: 238.38
    agent-2: 337.685
    agent-3: 337.685
    agent-4: 222.765
    agent-5: 222.765
  policy_reward_min:
    agent-0: 161.0
    agent-1: 161.0
    agent-2: 260.0
    agent-3: 260.0
    agent-4: 163.5
    agent-5: 163.5
  sampler_perf:
    mean_env_wait_ms: 24.7607623287673
    mean_inference_ms: 12.395749736215564
    mean_processing_ms: 54.4054778193138
  time_since_restore: 13270.539109230042
  time_this_iter_s: 126.798837184906
  time_total_s: 60352.084661245346
  timestamp: 1637571403
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 45600000
  training_iteration: 475
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    475 |          60352.1 | 45600000 |  1597.66 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.97
    apples_agent-1_min: 17
    apples_agent-2_max: 416
    apples_agent-2_mean: 352.24
    apples_agent-2_min: 231
    apples_agent-3_max: 235
    apples_agent-3_mean: 160.57
    apples_agent-3_min: 46
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.35
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 375.32
    apples_agent-5_min: 241
    cleaning_beam_agent-0_max: 524
    cleaning_beam_agent-0_mean: 453.88
    cleaning_beam_agent-0_min: 422
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 5.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 50.27
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 466.84
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_03-58-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1723.0
  episode_reward_mean: 1600.7
  episode_reward_min: 1003.0
  episodes_this_iter: 96
  episodes_total: 45696
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18699.122
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4031676948070526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014801484066992998
        model: {}
        policy_loss: -0.0013052847934886813
        total_loss: 0.0014921686379238963
        vf_explained_var: 0.005045384168624878
        vf_loss: 35.07027816772461
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24163028597831726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009071823442354798
        model: {}
        policy_loss: -0.0017901374958455563
        total_loss: 0.0010077604092657566
        vf_explained_var: 0.08839289844036102
        vf_loss: 32.23167037963867
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2952914535999298
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010457232128828764
        model: {}
        policy_loss: -0.0017159306444227695
        total_loss: 0.004546818323433399
        vf_explained_var: 0.05932004749774933
        vf_loss: 67.82460021972656
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7328858375549316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017474356573075056
        model: {}
        policy_loss: -0.002146661514416337
        total_loss: 0.003761457744985819
        vf_explained_var: 0.008750975131988525
        vf_loss: 71.98001098632812
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0094695091247559
        entropy_coeff: 0.0017600000137463212
        kl: 0.002344339620321989
        model: {}
        policy_loss: -0.0021280255168676376
        total_loss: -0.0009414823725819588
        vf_explained_var: 0.04649992287158966
        vf_loss: 29.632125854492188
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.270754337310791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005775935132987797
        model: {}
        policy_loss: -0.0017653298564255238
        total_loss: 0.000526809599250555
        vf_explained_var: 0.10951201617717743
        vf_loss: 27.68668556213379
    load_time_ms: 13122.218
    num_steps_sampled: 45696000
    num_steps_trained: 45696000
    sample_time_ms: 93541.958
    update_time_ms: 18.568
  iterations_since_restore: 106
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.95303867403315
    ram_util_percent: 16.307182320441992
  pid: 21723
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 382.5
    agent-3: 382.5
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 238.66
    agent-1: 238.66
    agent-2: 338.45
    agent-3: 338.45
    agent-4: 223.24
    agent-5: 223.24
  policy_reward_min:
    agent-0: 137.5
    agent-1: 137.5
    agent-2: 213.5
    agent-3: 213.5
    agent-4: 150.5
    agent-5: 150.5
  sampler_perf:
    mean_env_wait_ms: 24.762418963541748
    mean_inference_ms: 12.39629821619531
    mean_processing_ms: 54.406447075157274
  time_since_restore: 13396.8056473732
  time_this_iter_s: 126.26653814315796
  time_total_s: 60478.351199388504
  timestamp: 1637571529
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 45696000
  training_iteration: 476
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    476 |          60478.4 | 45696000 |   1600.7 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 29.58
    apples_agent-1_min: 16
    apples_agent-2_max: 417
    apples_agent-2_mean: 353.42
    apples_agent-2_min: 231
    apples_agent-3_max: 215
    apples_agent-3_mean: 154.73
    apples_agent-3_min: 79
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 380.1
    apples_agent-5_min: 241
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 449.03
    cleaning_beam_agent-0_min: 369
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.55
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 4.84
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 50.26
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 460.81
    cleaning_beam_agent-4_min: 394
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-00-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1744.0
  episode_reward_mean: 1599.65
  episode_reward_min: 1003.0
  episodes_this_iter: 96
  episodes_total: 45792
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18697.917
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40645402669906616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014235212001949549
        model: {}
        policy_loss: -0.0013102591037750244
        total_loss: 0.0014002295210957527
        vf_explained_var: 0.020903706550598145
        vf_loss: 34.25846481323242
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24063652753829956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007728934288024902
        model: {}
        policy_loss: -0.0017056716606020927
        total_loss: 0.0010770279914140701
        vf_explained_var: 0.08743073046207428
        vf_loss: 32.06218719482422
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29470834136009216
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010387066286057234
        model: {}
        policy_loss: -0.0018908199854195118
        total_loss: 0.004233863204717636
        vf_explained_var: 0.06263813376426697
        vf_loss: 66.4336929321289
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7511395812034607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010249388869851828
        model: {}
        policy_loss: -0.0020435608457773924
        total_loss: 0.00376484589651227
        vf_explained_var: -0.00043711066246032715
        vf_loss: 71.30412292480469
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0386295318603516
        entropy_coeff: 0.0017600000137463212
        kl: 0.002265137154608965
        model: {}
        policy_loss: -0.002233372535556555
        total_loss: -0.001052633160725236
        vf_explained_var: 0.041703611612319946
        vf_loss: 30.087284088134766
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26684677600860596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009265206754207611
        model: {}
        policy_loss: -0.0017907677683979273
        total_loss: 0.000436221482232213
        vf_explained_var: 0.14341992139816284
        vf_loss: 26.96639633178711
    load_time_ms: 13117.347
    num_steps_sampled: 45792000
    num_steps_trained: 45792000
    sample_time_ms: 93571.973
    update_time_ms: 18.636
  iterations_since_restore: 107
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.814124293785326
    ram_util_percent: 16.353672316384184
  pid: 21723
  policy_reward_max:
    agent-0: 265.5
    agent-1: 265.5
    agent-2: 391.0
    agent-3: 391.0
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 238.895
    agent-1: 238.895
    agent-2: 336.53
    agent-3: 336.53
    agent-4: 224.4
    agent-5: 224.4
  policy_reward_min:
    agent-0: 137.5
    agent-1: 137.5
    agent-2: 213.5
    agent-3: 213.5
    agent-4: 150.5
    agent-5: 150.5
  sampler_perf:
    mean_env_wait_ms: 24.76189378034131
    mean_inference_ms: 12.395836593470635
    mean_processing_ms: 54.403470063058165
  time_since_restore: 13521.497252941132
  time_this_iter_s: 124.69160556793213
  time_total_s: 60603.042804956436
  timestamp: 1637571654
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 45792000
  training_iteration: 477
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    477 |            60603 | 45792000 |  1599.65 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 31.94
    apples_agent-1_min: 17
    apples_agent-2_max: 420
    apples_agent-2_mean: 352.13
    apples_agent-2_min: 274
    apples_agent-3_max: 216
    apples_agent-3_mean: 156.04
    apples_agent-3_min: 68
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.33
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 379.47
    apples_agent-5_min: 296
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 458.39
    cleaning_beam_agent-0_min: 412
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 2.42
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.92
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 52.83
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 460.85
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-03-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1745.0
  episode_reward_mean: 1592.67
  episode_reward_min: 1269.0
  episodes_this_iter: 96
  episodes_total: 45888
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18712.658
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4103529155254364
        entropy_coeff: 0.0017600000137463212
        kl: 0.00129679252859205
        model: {}
        policy_loss: -0.0013703503645956516
        total_loss: 0.0013271966017782688
        vf_explained_var: 0.003655552864074707
        vf_loss: 34.19770812988281
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24583549797534943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017752291169017553
        model: {}
        policy_loss: -0.0019046145025640726
        total_loss: 0.0009120870381593704
        vf_explained_var: 0.05356873571872711
        vf_loss: 32.49374008178711
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2940804958343506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006079743034206331
        model: {}
        policy_loss: -0.0017051305621862411
        total_loss: 0.004420260898768902
        vf_explained_var: 0.05561433732509613
        vf_loss: 66.42969512939453
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7588650584220886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010948170674964786
        model: {}
        policy_loss: -0.002326089423149824
        total_loss: 0.0033630826510488987
        vf_explained_var: 0.0033544600009918213
        vf_loss: 70.24778747558594
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0380043983459473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023853504098951817
        model: {}
        policy_loss: -0.0021615636069327593
        total_loss: -0.0009651114232838154
        vf_explained_var: 0.012460708618164062
        vf_loss: 30.23339080810547
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27313584089279175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015835303347557783
        model: {}
        policy_loss: -0.0019169971346855164
        total_loss: 0.0004054959863424301
        vf_explained_var: 0.08513060212135315
        vf_loss: 28.032135009765625
    load_time_ms: 13124.487
    num_steps_sampled: 45888000
    num_steps_trained: 45888000
    sample_time_ms: 93652.81
    update_time_ms: 18.525
  iterations_since_restore: 108
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.35921787709497
    ram_util_percent: 16.37486033519553
  pid: 21723
  policy_reward_max:
    agent-0: 264.5
    agent-1: 264.5
    agent-2: 378.0
    agent-3: 378.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 236.865
    agent-1: 236.865
    agent-2: 335.085
    agent-3: 335.085
    agent-4: 224.385
    agent-5: 224.385
  policy_reward_min:
    agent-0: 183.5
    agent-1: 183.5
    agent-2: 269.0
    agent-3: 269.0
    agent-4: 177.0
    agent-5: 177.0
  sampler_perf:
    mean_env_wait_ms: 24.763253454831876
    mean_inference_ms: 12.395453347654373
    mean_processing_ms: 54.40442588151112
  time_since_restore: 13647.55400300026
  time_this_iter_s: 126.05675005912781
  time_total_s: 60729.099555015564
  timestamp: 1637571780
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 45888000
  training_iteration: 478
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    478 |          60729.1 | 45888000 |  1592.67 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 30.38
    apples_agent-1_min: 14
    apples_agent-2_max: 434
    apples_agent-2_mean: 357.04
    apples_agent-2_min: 277
    apples_agent-3_max: 227
    apples_agent-3_mean: 157.63
    apples_agent-3_min: 89
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.32
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 381.89
    apples_agent-5_min: 295
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 457.79
    cleaning_beam_agent-0_min: 382
    cleaning_beam_agent-1_max: 27
    cleaning_beam_agent-1_mean: 2.29
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 4.32
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 58.03
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 446.64
    cleaning_beam_agent-4_min: 381
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 2.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-05-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1746.0
  episode_reward_mean: 1604.88
  episode_reward_min: 1294.0
  episodes_this_iter: 96
  episodes_total: 45984
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18728.983
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40229424834251404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011141712311655283
        model: {}
        policy_loss: -0.0013105596881359816
        total_loss: 0.001416479703038931
        vf_explained_var: -0.005995765328407288
        vf_loss: 34.35075378417969
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2429819107055664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010573859326541424
        model: {}
        policy_loss: -0.0016278410330414772
        total_loss: 0.001143279718235135
        vf_explained_var: 0.06361041963100433
        vf_loss: 31.987712860107422
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29043135046958923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010557920904830098
        model: {}
        policy_loss: -0.0017507580341771245
        total_loss: 0.004328619688749313
        vf_explained_var: 0.040246352553367615
        vf_loss: 65.90536499023438
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7497940063476562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007765142363496125
        model: {}
        policy_loss: -0.0018664626404643059
        total_loss: 0.0037002298049628735
        vf_explained_var: 0.0009886324405670166
        vf_loss: 68.86326599121094
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0380561351776123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017514057690277696
        model: {}
        policy_loss: -0.0022202704567462206
        total_loss: -0.0009787495946511626
        vf_explained_var: 0.017776787281036377
        vf_loss: 30.684978485107422
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27218130230903625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005376525223255157
        model: {}
        policy_loss: -0.0016487170942127705
        total_loss: 0.0006903379689902067
        vf_explained_var: 0.10451865196228027
        vf_loss: 28.180957794189453
    load_time_ms: 13138.475
    num_steps_sampled: 45984000
    num_steps_trained: 45984000
    sample_time_ms: 93576.541
    update_time_ms: 18.31
  iterations_since_restore: 109
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.06235955056179
    ram_util_percent: 16.336516853932586
  pid: 21723
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 387.0
    agent-3: 387.0
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 237.41
    agent-1: 237.41
    agent-2: 339.11
    agent-3: 339.11
    agent-4: 225.92
    agent-5: 225.92
  policy_reward_min:
    agent-0: 180.0
    agent-1: 180.0
    agent-2: 261.5
    agent-3: 261.5
    agent-4: 171.0
    agent-5: 171.0
  sampler_perf:
    mean_env_wait_ms: 24.763890107395046
    mean_inference_ms: 12.394972407899978
    mean_processing_ms: 54.40228918012557
  time_since_restore: 13772.822038888931
  time_this_iter_s: 125.26803588867188
  time_total_s: 60854.367590904236
  timestamp: 1637571906
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 45984000
  training_iteration: 479
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    479 |          60854.4 | 45984000 |  1604.88 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 31.38
    apples_agent-1_min: 15
    apples_agent-2_max: 442
    apples_agent-2_mean: 353.93
    apples_agent-2_min: 285
    apples_agent-3_max: 229
    apples_agent-3_mean: 159.94
    apples_agent-3_min: 94
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 380.22
    apples_agent-5_min: 203
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 453.25
    cleaning_beam_agent-0_min: 415
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 2.05
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 5.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 58.11
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 457.89
    cleaning_beam_agent-4_min: 352
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-07-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1746.0
  episode_reward_mean: 1609.8
  episode_reward_min: 1327.0
  episodes_this_iter: 96
  episodes_total: 46080
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18758.474
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40113285183906555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015857930993661284
        model: {}
        policy_loss: -0.0013066250830888748
        total_loss: 0.0014557258691638708
        vf_explained_var: 0.001474723219871521
        vf_loss: 34.683433532714844
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2400606870651245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014147693291306496
        model: {}
        policy_loss: -0.0018282392993569374
        total_loss: 0.0008915364742279053
        vf_explained_var: 0.09678110480308533
        vf_loss: 31.422855377197266
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2958452105522156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008454319322481751
        model: {}
        policy_loss: -0.0017440174706280231
        total_loss: 0.004577941726893187
        vf_explained_var: 0.052815333008766174
        vf_loss: 68.42645263671875
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7566381692886353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014968530740588903
        model: {}
        policy_loss: -0.002320799045264721
        total_loss: 0.003542507067322731
        vf_explained_var: 0.012976586818695068
        vf_loss: 71.94985961914062
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0240739583969116
        entropy_coeff: 0.0017600000137463212
        kl: 0.001504506915807724
        model: {}
        policy_loss: -0.002083846367895603
        total_loss: -0.0007552891038358212
        vf_explained_var: 0.03890533745288849
        vf_loss: 31.309284210205078
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2775757908821106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006505364435724914
        model: {}
        policy_loss: -0.0019142980454489589
        total_loss: 0.0005528460023924708
        vf_explained_var: 0.09867283701896667
        vf_loss: 29.556821823120117
    load_time_ms: 13139.786
    num_steps_sampled: 46080000
    num_steps_trained: 46080000
    sample_time_ms: 93786.464
    update_time_ms: 18.267
  iterations_since_restore: 110
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.406629834254154
    ram_util_percent: 16.287845303867407
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 398.0
    agent-3: 398.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 237.53
    agent-1: 237.53
    agent-2: 341.715
    agent-3: 341.715
    agent-4: 225.655
    agent-5: 225.655
  policy_reward_min:
    agent-0: 193.0
    agent-1: 193.0
    agent-2: 274.0
    agent-3: 274.0
    agent-4: 122.5
    agent-5: 122.5
  sampler_perf:
    mean_env_wait_ms: 24.767522046613355
    mean_inference_ms: 12.39554780034752
    mean_processing_ms: 54.40767231162359
  time_since_restore: 13899.658410787582
  time_this_iter_s: 126.83637189865112
  time_total_s: 60981.20396280289
  timestamp: 1637572033
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 46080000
  training_iteration: 480
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    480 |          60981.2 | 46080000 |   1609.8 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.08
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.91
    apples_agent-1_min: 17
    apples_agent-2_max: 406
    apples_agent-2_mean: 350.7
    apples_agent-2_min: 264
    apples_agent-3_max: 234
    apples_agent-3_mean: 160.16
    apples_agent-3_min: 89
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 377.47
    apples_agent-5_min: 296
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 446.58
    cleaning_beam_agent-0_min: 408
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 2.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 4.33
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 60.74
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 452.68
    cleaning_beam_agent-4_min: 368
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 4.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-09-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1746.0
  episode_reward_mean: 1594.13
  episode_reward_min: 1223.0
  episodes_this_iter: 96
  episodes_total: 46176
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18768.181
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4013676643371582
        entropy_coeff: 0.0017600000137463212
        kl: 0.001611124025657773
        model: {}
        policy_loss: -0.0017504878342151642
        total_loss: 0.0009412067010998726
        vf_explained_var: 0.009731680154800415
        vf_loss: 33.98100280761719
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24306556582450867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014026828575879335
        model: {}
        policy_loss: -0.0018503610044717789
        total_loss: 0.0009279090445488691
        vf_explained_var: 0.06513956189155579
        vf_loss: 32.06062316894531
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29426050186157227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007376270950771868
        model: {}
        policy_loss: -0.0018224832601845264
        total_loss: 0.004812375642359257
        vf_explained_var: 0.06747481226921082
        vf_loss: 71.52757263183594
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.750338077545166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014630371006205678
        model: {}
        policy_loss: -0.0023553925566375256
        total_loss: 0.003904113546013832
        vf_explained_var: 0.014779821038246155
        vf_loss: 75.80104064941406
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0390979051589966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015223075170069933
        model: {}
        policy_loss: -0.0019859187304973602
        total_loss: -0.0007373817497864366
        vf_explained_var: 0.052710890769958496
        vf_loss: 30.773469924926758
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27674996852874756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009083118056878448
        model: {}
        policy_loss: -0.001600533491000533
        total_loss: 0.0006969841197133064
        vf_explained_var: 0.14131423830986023
        vf_loss: 27.845966339111328
    load_time_ms: 13171.706
    num_steps_sampled: 46176000
    num_steps_trained: 46176000
    sample_time_ms: 93861.148
    update_time_ms: 18.247
  iterations_since_restore: 111
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 53.76353591160221
    ram_util_percent: 16.47182320441989
  pid: 21723
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 378.5
    agent-3: 378.5
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 236.505
    agent-1: 236.505
    agent-2: 337.515
    agent-3: 337.515
    agent-4: 223.045
    agent-5: 223.045
  policy_reward_min:
    agent-0: 177.5
    agent-1: 177.5
    agent-2: 255.5
    agent-3: 255.5
    agent-4: 178.5
    agent-5: 178.5
  sampler_perf:
    mean_env_wait_ms: 24.768837773072242
    mean_inference_ms: 12.395757983651762
    mean_processing_ms: 54.407701420071334
  time_since_restore: 14025.726623773575
  time_this_iter_s: 126.06821298599243
  time_total_s: 61107.27217578888
  timestamp: 1637572160
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 46176000
  training_iteration: 481
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    481 |          61107.3 | 46176000 |  1594.13 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.17
    apples_agent-1_min: 18
    apples_agent-2_max: 408
    apples_agent-2_mean: 355.74
    apples_agent-2_min: 262
    apples_agent-3_max: 231
    apples_agent-3_mean: 152.8
    apples_agent-3_min: 68
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 381.94
    apples_agent-5_min: 237
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 452.76
    cleaning_beam_agent-0_min: 400
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.66
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 5.02
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 56.75
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 443.76
    cleaning_beam_agent-4_min: 350
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-11-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1722.0
  episode_reward_mean: 1607.62
  episode_reward_min: 1218.0
  episodes_this_iter: 96
  episodes_total: 46272
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18788.0
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40033775568008423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013152076862752438
        model: {}
        policy_loss: -0.001419347245246172
        total_loss: 0.0015192138962447643
        vf_explained_var: -0.007756814360618591
        vf_loss: 36.43156433105469
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.236185684800148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008312364225275815
        model: {}
        policy_loss: -0.001758149592205882
        total_loss: 0.0011274656280875206
        vf_explained_var: 0.08849373459815979
        vf_loss: 33.01302719116211
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29282328486442566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013489527627825737
        model: {}
        policy_loss: -0.001979494234547019
        total_loss: 0.004200813826173544
        vf_explained_var: 0.06078213453292847
        vf_loss: 66.95682525634766
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7612091302871704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004905544919893146
        model: {}
        policy_loss: -0.0018877703696489334
        total_loss: 0.003763932269066572
        vf_explained_var: 0.020729675889015198
        vf_loss: 69.91427612304688
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0503830909729004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022242420818656683
        model: {}
        policy_loss: -0.002388701308518648
        total_loss: -0.0010678633116185665
        vf_explained_var: 0.03351344168186188
        vf_loss: 31.695096969604492
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2709328234195709
        entropy_coeff: 0.0017600000137463212
        kl: 0.001182061736471951
        model: {}
        policy_loss: -0.002157921437174082
        total_loss: 0.0003007469931617379
        vf_explained_var: 0.10521179437637329
        vf_loss: 29.3551025390625
    load_time_ms: 13176.014
    num_steps_sampled: 46272000
    num_steps_trained: 46272000
    sample_time_ms: 93981.792
    update_time_ms: 18.416
  iterations_since_restore: 112
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.39833333333334
    ram_util_percent: 16.406666666666666
  pid: 21723
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 381.5
    agent-3: 381.5
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 242.41
    agent-1: 242.41
    agent-2: 336.245
    agent-3: 336.245
    agent-4: 225.155
    agent-5: 225.155
  policy_reward_min:
    agent-0: 195.5
    agent-1: 195.5
    agent-2: 239.5
    agent-3: 239.5
    agent-4: 146.0
    agent-5: 146.0
  sampler_perf:
    mean_env_wait_ms: 24.770530730709165
    mean_inference_ms: 12.396230694156381
    mean_processing_ms: 54.41060151229329
  time_since_restore: 14151.752016305923
  time_this_iter_s: 126.02539253234863
  time_total_s: 61233.29756832123
  timestamp: 1637572286
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 46272000
  training_iteration: 482
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    482 |          61233.3 | 46272000 |  1607.62 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 31.05
    apples_agent-1_min: 16
    apples_agent-2_max: 406
    apples_agent-2_mean: 357.35
    apples_agent-2_min: 289
    apples_agent-3_max: 227
    apples_agent-3_mean: 160.0
    apples_agent-3_min: 68
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 379.79
    apples_agent-5_min: 316
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 451.27
    cleaning_beam_agent-0_min: 381
    cleaning_beam_agent-1_max: 35
    cleaning_beam_agent-1_mean: 2.29
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 4.31
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 55.73
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 428.6
    cleaning_beam_agent-4_min: 333
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 3.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-13-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1721.0
  episode_reward_mean: 1606.73
  episode_reward_min: 1383.0
  episodes_this_iter: 96
  episodes_total: 46368
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18757.177
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3996369242668152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014744405634701252
        model: {}
        policy_loss: -0.001385606825351715
        total_loss: 0.0014454852789640427
        vf_explained_var: 0.0012706667184829712
        vf_loss: 35.34455490112305
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2376304566860199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006953728152438998
        model: {}
        policy_loss: -0.0016514789313077927
        total_loss: 0.0011598393321037292
        vf_explained_var: 0.08885620534420013
        vf_loss: 32.295494079589844
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2911819815635681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012080695014446974
        model: {}
        policy_loss: -0.0018590637482702732
        total_loss: 0.004637331701815128
        vf_explained_var: 0.059962138533592224
        vf_loss: 70.08876037597656
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7352023124694824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010414340067654848
        model: {}
        policy_loss: -0.0021909093484282494
        total_loss: 0.0039346711710095406
        vf_explained_var: 0.009497284889221191
        vf_loss: 74.19532775878906
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0569764375686646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024193450808525085
        model: {}
        policy_loss: -0.0024329060688614845
        total_loss: -0.0013441350311040878
        vf_explained_var: 0.015954241156578064
        vf_loss: 29.490497589111328
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26775380969047546
        entropy_coeff: 0.0017600000137463212
        kl: 0.00111056724563241
        model: {}
        policy_loss: -0.0017930520698428154
        total_loss: 0.0005225019995123148
        vf_explained_var: 0.06791038811206818
        vf_loss: 27.86800765991211
    load_time_ms: 13170.193
    num_steps_sampled: 46368000
    num_steps_trained: 46368000
    sample_time_ms: 93835.237
    update_time_ms: 18.27
  iterations_since_restore: 113
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.720224719101125
    ram_util_percent: 16.43483146067416
  pid: 21723
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 383.5
    agent-3: 383.5
    agent-4: 253.0
    agent-5: 253.0
  policy_reward_mean:
    agent-0: 238.685
    agent-1: 238.685
    agent-2: 340.705
    agent-3: 340.705
    agent-4: 223.975
    agent-5: 223.975
  policy_reward_min:
    agent-0: 178.0
    agent-1: 178.0
    agent-2: 278.5
    agent-3: 278.5
    agent-4: 191.0
    agent-5: 191.0
  sampler_perf:
    mean_env_wait_ms: 24.76941533384459
    mean_inference_ms: 12.395951864876888
    mean_processing_ms: 54.406680509894095
  time_since_restore: 14276.313216924667
  time_this_iter_s: 124.5612006187439
  time_total_s: 61357.85876893997
  timestamp: 1637572410
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 46368000
  training_iteration: 483
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    483 |          61357.9 | 46368000 |  1606.73 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.37
    apples_agent-1_min: 15
    apples_agent-2_max: 417
    apples_agent-2_mean: 350.81
    apples_agent-2_min: 218
    apples_agent-3_max: 229
    apples_agent-3_mean: 155.91
    apples_agent-3_min: 80
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.36
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 379.26
    apples_agent-5_min: 259
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 458.11
    cleaning_beam_agent-0_min: 413
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 2.31
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 5.11
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 54.98
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 417.82
    cleaning_beam_agent-4_min: 333
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-15-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1716.0
  episode_reward_mean: 1593.81
  episode_reward_min: 1016.0
  episodes_this_iter: 96
  episodes_total: 46464
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18747.625
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39751726388931274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023466169368475676
        model: {}
        policy_loss: -0.001385219395160675
        total_loss: 0.0013631535694003105
        vf_explained_var: 0.0033448785543441772
        vf_loss: 34.480018615722656
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23922854661941528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011880021775141358
        model: {}
        policy_loss: -0.0016392578836530447
        total_loss: 0.0011253375560045242
        vf_explained_var: 0.07940389215946198
        vf_loss: 31.856395721435547
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.300474613904953
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011384531389921904
        model: {}
        policy_loss: -0.0018619901966303587
        total_loss: 0.004256818443536758
        vf_explained_var: 0.04450571537017822
        vf_loss: 66.47644805908203
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7445144057273865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009960324969142675
        model: {}
        policy_loss: -0.001926120836287737
        total_loss: 0.0038632433861494064
        vf_explained_var: -0.023257851600646973
        vf_loss: 70.99710083007812
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0608153343200684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024624187499284744
        model: {}
        policy_loss: -0.0021203109063208103
        total_loss: -0.0008828733116388321
        vf_explained_var: 0.02656368911266327
        vf_loss: 31.044742584228516
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2757941484451294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010311882942914963
        model: {}
        policy_loss: -0.0018555130809545517
        total_loss: 0.0005024569109082222
        vf_explained_var: 0.10824437439441681
        vf_loss: 28.43370819091797
    load_time_ms: 13152.52
    num_steps_sampled: 46464000
    num_steps_trained: 46464000
    sample_time_ms: 93707.208
    update_time_ms: 18.402
  iterations_since_restore: 114
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 60.97401129943503
    ram_util_percent: 16.406779661016948
  pid: 21723
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 373.0
    agent-3: 373.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 239.095
    agent-1: 239.095
    agent-2: 334.83
    agent-3: 334.83
    agent-4: 222.98
    agent-5: 222.98
  policy_reward_min:
    agent-0: 150.5
    agent-1: 150.5
    agent-2: 208.5
    agent-3: 208.5
    agent-4: 149.0
    agent-5: 149.0
  sampler_perf:
    mean_env_wait_ms: 24.76780072315918
    mean_inference_ms: 12.395163124823844
    mean_processing_ms: 54.4030418761484
  time_since_restore: 14400.769014120102
  time_this_iter_s: 124.45579719543457
  time_total_s: 61482.31456613541
  timestamp: 1637572535
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 46464000
  training_iteration: 484
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    484 |          61482.3 | 46464000 |  1593.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.8
    apples_agent-1_min: 18
    apples_agent-2_max: 403
    apples_agent-2_mean: 353.1
    apples_agent-2_min: 293
    apples_agent-3_max: 228
    apples_agent-3_mean: 158.95
    apples_agent-3_min: 87
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 449
    apples_agent-5_mean: 383.89
    apples_agent-5_min: 301
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 446.21
    cleaning_beam_agent-0_min: 402
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 3.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 55.88
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 423.67
    cleaning_beam_agent-4_min: 337
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 2.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-17-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1734.0
  episode_reward_mean: 1614.36
  episode_reward_min: 1368.0
  episodes_this_iter: 96
  episodes_total: 46560
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18722.872
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38977915048599243
        entropy_coeff: 0.0017600000137463212
        kl: 0.001478185411542654
        model: {}
        policy_loss: -0.0012916314881294966
        total_loss: 0.0012731216847896576
        vf_explained_var: 0.00910612940788269
        vf_loss: 32.507652282714844
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23399367928504944
        entropy_coeff: 0.0017600000137463212
        kl: 0.001308071194216609
        model: {}
        policy_loss: -0.0018604584038257599
        total_loss: 0.0008008647710084915
        vf_explained_var: 0.06344865262508392
        vf_loss: 30.731504440307617
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3006652593612671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012822734424844384
        model: {}
        policy_loss: -0.001986219547688961
        total_loss: 0.004605085588991642
        vf_explained_var: 0.04403814673423767
        vf_loss: 71.20477294921875
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.730644941329956
        entropy_coeff: 0.0017600000137463212
        kl: 0.001292703440412879
        model: {}
        policy_loss: -0.0020567351020872593
        total_loss: 0.004132895730435848
        vf_explained_var: 0.0011844784021377563
        vf_loss: 74.75564575195312
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0497417449951172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016918567707762122
        model: {}
        policy_loss: -0.002078876830637455
        total_loss: -0.0009584268555045128
        vf_explained_var: 0.008241862058639526
        vf_loss: 29.67993927001953
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2655907869338989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004446598468348384
        model: {}
        policy_loss: -0.0015819671098142862
        total_loss: 0.0007084584794938564
        vf_explained_var: 0.08063419163227081
        vf_loss: 27.578630447387695
    load_time_ms: 13139.653
    num_steps_sampled: 46560000
    num_steps_trained: 46560000
    sample_time_ms: 93554.725
    update_time_ms: 17.578
  iterations_since_restore: 115
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.17977528089887
    ram_util_percent: 16.389325842696632
  pid: 21723
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 374.0
    agent-3: 374.0
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 240.64
    agent-1: 240.64
    agent-2: 339.71
    agent-3: 339.71
    agent-4: 226.83
    agent-5: 226.83
  policy_reward_min:
    agent-0: 200.0
    agent-1: 200.0
    agent-2: 292.5
    agent-3: 292.5
    agent-4: 173.5
    agent-5: 173.5
  sampler_perf:
    mean_env_wait_ms: 24.76692396355429
    mean_inference_ms: 12.395392517044252
    mean_processing_ms: 54.401420073912405
  time_since_restore: 14525.65745306015
  time_this_iter_s: 124.88843894004822
  time_total_s: 61607.203005075455
  timestamp: 1637572660
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 46560000
  training_iteration: 485
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    485 |          61607.2 | 46560000 |  1614.36 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 29.9
    apples_agent-1_min: 15
    apples_agent-2_max: 402
    apples_agent-2_mean: 351.42
    apples_agent-2_min: 224
    apples_agent-3_max: 221
    apples_agent-3_mean: 160.26
    apples_agent-3_min: 83
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.3
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 378.62
    apples_agent-5_min: 250
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 450.4
    cleaning_beam_agent-0_min: 385
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.96
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 4.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 51.46
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 413.95
    cleaning_beam_agent-4_min: 308
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 3.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-19-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1742.0
  episode_reward_mean: 1605.82
  episode_reward_min: 1046.0
  episodes_this_iter: 96
  episodes_total: 46656
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18683.982
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38451728224754333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014917724765837193
        model: {}
        policy_loss: -0.0014103362336754799
        total_loss: 0.0013724681921303272
        vf_explained_var: 0.006255492568016052
        vf_loss: 34.59554672241211
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24028322100639343
        entropy_coeff: 0.0017600000137463212
        kl: 0.000543571833986789
        model: {}
        policy_loss: -0.0015622866339981556
        total_loss: 0.0011440124362707138
        vf_explained_var: 0.10304264724254608
        vf_loss: 31.291990280151367
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2961561679840088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010376406135037541
        model: {}
        policy_loss: -0.0018182247877120972
        total_loss: 0.004681553691625595
        vf_explained_var: 0.062003374099731445
        vf_loss: 70.21017456054688
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7336321473121643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013250993797555566
        model: {}
        policy_loss: -0.002084545325487852
        total_loss: 0.004063733853399754
        vf_explained_var: 0.016141220927238464
        vf_loss: 74.39474487304688
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.060106873512268
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031783292070031166
        model: {}
        policy_loss: -0.0022071688435971737
        total_loss: -0.0010821581818163395
        vf_explained_var: 0.06215806305408478
        vf_loss: 29.90799331665039
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2735029458999634
        entropy_coeff: 0.0017600000137463212
        kl: 0.001054234104231
        model: {}
        policy_loss: -0.0018289219588041306
        total_loss: 0.0005217816215008497
        vf_explained_var: 0.11191324889659882
        vf_loss: 28.320722579956055
    load_time_ms: 13156.965
    num_steps_sampled: 46656000
    num_steps_trained: 46656000
    sample_time_ms: 93456.221
    update_time_ms: 17.719
  iterations_since_restore: 116
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.57640449438201
    ram_util_percent: 16.356179775280896
  pid: 21723
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 384.0
    agent-3: 384.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 238.545
    agent-1: 238.545
    agent-2: 340.36
    agent-3: 340.36
    agent-4: 224.005
    agent-5: 224.005
  policy_reward_min:
    agent-0: 150.5
    agent-1: 150.5
    agent-2: 202.0
    agent-3: 202.0
    agent-4: 162.0
    agent-5: 162.0
  sampler_perf:
    mean_env_wait_ms: 24.765618904512667
    mean_inference_ms: 12.394917087197987
    mean_processing_ms: 54.40005411114115
  time_since_restore: 14650.730320692062
  time_this_iter_s: 125.07286763191223
  time_total_s: 61732.27587270737
  timestamp: 1637572785
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 46656000
  training_iteration: 486
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    486 |          61732.3 | 46656000 |  1605.82 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.0
    apples_agent-1_min: 14
    apples_agent-2_max: 406
    apples_agent-2_mean: 346.06
    apples_agent-2_min: 143
    apples_agent-3_max: 237
    apples_agent-3_mean: 155.4
    apples_agent-3_min: 67
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 449
    apples_agent-5_mean: 379.3
    apples_agent-5_min: 147
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 444.72
    cleaning_beam_agent-0_min: 294
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.15
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 4.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 52.68
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 511
    cleaning_beam_agent-4_mean: 398.37
    cleaning_beam_agent-4_min: 299
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 2.9
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-21-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1747.0
  episode_reward_mean: 1583.41
  episode_reward_min: 665.0
  episodes_this_iter: 96
  episodes_total: 46752
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18705.604
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38828617334365845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014261410105973482
        model: {}
        policy_loss: -0.001370745711028576
        total_loss: 0.0015919171273708344
        vf_explained_var: 0.0361177921295166
        vf_loss: 36.46047592163086
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24440951645374298
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009532574331387877
        model: {}
        policy_loss: -0.0017750152619555593
        total_loss: 0.0011478778906166553
        vf_explained_var: 0.11354507505893707
        vf_loss: 33.53052520751953
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30286726355552673
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011685211211442947
        model: {}
        policy_loss: -0.0020573255605995655
        total_loss: 0.004388274159282446
        vf_explained_var: 0.071896493434906
        vf_loss: 69.78646850585938
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7286307215690613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008116966346278787
        model: {}
        policy_loss: -0.002136971801519394
        total_loss: 0.004175545647740364
        vf_explained_var: -0.010964930057525635
        vf_loss: 75.94906616210938
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0535073280334473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017788136610761285
        model: {}
        policy_loss: -0.0017812598962336779
        total_loss: -0.000368678942322731
        vf_explained_var: 0.04061192274093628
        vf_loss: 32.667572021484375
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2747155427932739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005035949870944023
        model: {}
        policy_loss: -0.0017378671327605844
        total_loss: 0.00075139245018363
        vf_explained_var: 0.1261112242937088
        vf_loss: 29.727617263793945
    load_time_ms: 13184.096
    num_steps_sampled: 46752000
    num_steps_trained: 46752000
    sample_time_ms: 93469.533
    update_time_ms: 17.074
  iterations_since_restore: 117
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.741011235955064
    ram_util_percent: 16.423033707865166
  pid: 21723
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 377.5
    agent-3: 377.5
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 234.88
    agent-1: 234.88
    agent-2: 332.27
    agent-3: 332.27
    agent-4: 224.555
    agent-5: 224.555
  policy_reward_min:
    agent-0: 104.0
    agent-1: 104.0
    agent-2: 143.5
    agent-3: 143.5
    agent-4: 85.0
    agent-5: 85.0
  sampler_perf:
    mean_env_wait_ms: 24.76307936618329
    mean_inference_ms: 12.394291561014427
    mean_processing_ms: 54.39855060756278
  time_since_restore: 14776.007273674011
  time_this_iter_s: 125.27695298194885
  time_total_s: 61857.552825689316
  timestamp: 1637572911
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 46752000
  training_iteration: 487
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    487 |          61857.6 | 46752000 |  1583.41 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 29.94
    apples_agent-1_min: 12
    apples_agent-2_max: 405
    apples_agent-2_mean: 349.19
    apples_agent-2_min: 200
    apples_agent-3_max: 224
    apples_agent-3_mean: 159.3
    apples_agent-3_min: 68
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 381.3
    apples_agent-5_min: 227
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 445.51
    cleaning_beam_agent-0_min: 380
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 1.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 4.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 53.15
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 504
    cleaning_beam_agent-4_mean: 405.61
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 3.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-23-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1755.0
  episode_reward_mean: 1595.45
  episode_reward_min: 922.0
  episodes_this_iter: 96
  episodes_total: 46848
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18701.797
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38131341338157654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011912849731743336
        model: {}
        policy_loss: -0.0013082325458526611
        total_loss: 0.0014820192009210587
        vf_explained_var: 0.033289358019828796
        vf_loss: 34.613616943359375
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24508245289325714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008415218908339739
        model: {}
        policy_loss: -0.0018121156608685851
        total_loss: 0.000930753827560693
        vf_explained_var: 0.11276741325855255
        vf_loss: 31.742145538330078
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3055766224861145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009878948330879211
        model: {}
        policy_loss: -0.0017779497429728508
        total_loss: 0.004826495889574289
        vf_explained_var: 0.072050079703331
        vf_loss: 71.42262268066406
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7192972898483276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008550450438633561
        model: {}
        policy_loss: -0.002229332458227873
        total_loss: 0.004119532182812691
        vf_explained_var: 0.012461528182029724
        vf_loss: 76.14828491210938
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.069589376449585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016659596003592014
        model: {}
        policy_loss: -0.0019368843641132116
        total_loss: -0.0006656143814325333
        vf_explained_var: 0.04479403793811798
        vf_loss: 31.537487030029297
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2759993076324463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008344082161784172
        model: {}
        policy_loss: -0.0016859499737620354
        total_loss: 0.0007209614850580692
        vf_explained_var: 0.12416180968284607
        vf_loss: 28.926715850830078
    load_time_ms: 13197.009
    num_steps_sampled: 46848000
    num_steps_trained: 46848000
    sample_time_ms: 93471.894
    update_time_ms: 17.274
  iterations_since_restore: 118
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.251955307262584
    ram_util_percent: 16.368156424581002
  pid: 21723
  policy_reward_max:
    agent-0: 262.0
    agent-1: 262.0
    agent-2: 381.0
    agent-3: 381.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 236.565
    agent-1: 236.565
    agent-2: 336.13
    agent-3: 336.13
    agent-4: 225.03
    agent-5: 225.03
  policy_reward_min:
    agent-0: 133.5
    agent-1: 133.5
    agent-2: 193.5
    agent-3: 193.5
    agent-4: 134.0
    agent-5: 134.0
  sampler_perf:
    mean_env_wait_ms: 24.762986375045486
    mean_inference_ms: 12.394435682237313
    mean_processing_ms: 54.40154744303291
  time_since_restore: 14902.182535886765
  time_this_iter_s: 126.1752622127533
  time_total_s: 61983.72808790207
  timestamp: 1637573037
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 46848000
  training_iteration: 488
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    488 |          61983.7 | 46848000 |  1595.45 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.83
    apples_agent-1_min: 16
    apples_agent-2_max: 413
    apples_agent-2_mean: 347.95
    apples_agent-2_min: 270
    apples_agent-3_max: 246
    apples_agent-3_mean: 161.26
    apples_agent-3_min: 75
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.4
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 374.61
    apples_agent-5_min: 274
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 445.67
    cleaning_beam_agent-0_min: 383
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.51
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 4.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 48.21
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 516
    cleaning_beam_agent-4_mean: 406.16
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 3.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-26-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1778.0
  episode_reward_mean: 1584.96
  episode_reward_min: 1238.0
  episodes_this_iter: 96
  episodes_total: 46944
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18688.651
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3867179751396179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012559614842757583
        model: {}
        policy_loss: -0.0015489449724555016
        total_loss: 0.0012044813483953476
        vf_explained_var: 0.043380752205848694
        vf_loss: 34.340476989746094
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24493402242660522
        entropy_coeff: 0.0017600000137463212
        kl: 0.001052736071869731
        model: {}
        policy_loss: -0.0018423192668706179
        total_loss: 0.0009865150786936283
        vf_explained_var: 0.09227053821086884
        vf_loss: 32.59918212890625
      agent-2:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3066578805446625
        entropy_coeff: 0.0017600000137463212
        kl: 0.000863800582010299
        model: {}
        policy_loss: -0.00225881882943213
        total_loss: 0.003982879221439362
        vf_explained_var: 0.0916982889175415
        vf_loss: 67.81416320800781
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7272888422012329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007639292161911726
        model: {}
        policy_loss: -0.00218827067874372
        total_loss: 0.004006548784673214
        vf_explained_var: 0.002561897039413452
        vf_loss: 74.74845886230469
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.055585265159607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012810081243515015
        model: {}
        policy_loss: -0.001891945255920291
        total_loss: -0.000707462546415627
        vf_explained_var: 0.05151902139186859
        vf_loss: 30.42313003540039
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2758829891681671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011274910066276789
        model: {}
        policy_loss: -0.0017116470262408257
        total_loss: 0.0006119618192315102
        vf_explained_var: 0.11936385929584503
        vf_loss: 28.091650009155273
    load_time_ms: 13197.766
    num_steps_sampled: 46944000
    num_steps_trained: 46944000
    sample_time_ms: 93524.128
    update_time_ms: 17.671
  iterations_since_restore: 119
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.81396648044693
    ram_util_percent: 16.384357541899444
  pid: 21723
  policy_reward_max:
    agent-0: 264.5
    agent-1: 264.5
    agent-2: 379.0
    agent-3: 379.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 234.865
    agent-1: 234.865
    agent-2: 336.005
    agent-3: 336.005
    agent-4: 221.61
    agent-5: 221.61
  policy_reward_min:
    agent-0: 173.5
    agent-1: 173.5
    agent-2: 239.5
    agent-3: 239.5
    agent-4: 176.5
    agent-5: 176.5
  sampler_perf:
    mean_env_wait_ms: 24.762653807084018
    mean_inference_ms: 12.39462956737459
    mean_processing_ms: 54.402774051825425
  time_since_restore: 15027.857170343399
  time_this_iter_s: 125.67463445663452
  time_total_s: 62109.4027223587
  timestamp: 1637573163
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 46944000
  training_iteration: 489
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    489 |          62109.4 | 46944000 |  1584.96 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.91
    apples_agent-1_min: 16
    apples_agent-2_max: 421
    apples_agent-2_mean: 351.11
    apples_agent-2_min: 215
    apples_agent-3_max: 232
    apples_agent-3_mean: 164.95
    apples_agent-3_min: 92
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 457
    apples_agent-5_mean: 382.65
    apples_agent-5_min: 251
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 443.99
    cleaning_beam_agent-0_min: 409
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.95
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 4.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 44.86
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 413.86
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 3.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-28-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1730.0
  episode_reward_mean: 1615.71
  episode_reward_min: 1058.0
  episodes_this_iter: 96
  episodes_total: 47040
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18681.151
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3758113980293274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012579072499647737
        model: {}
        policy_loss: -0.0013989582657814026
        total_loss: 0.0013768281787633896
        vf_explained_var: 0.0017539262771606445
        vf_loss: 34.37213134765625
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23592917621135712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006387053290382028
        model: {}
        policy_loss: -0.0018250045832246542
        total_loss: 0.0009513036347925663
        vf_explained_var: 0.07464005053043365
        vf_loss: 31.91546058654785
      agent-2:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2966918647289276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008635895792394876
        model: {}
        policy_loss: -0.0019360794685781002
        total_loss: 0.004394404124468565
        vf_explained_var: 0.05194634199142456
        vf_loss: 68.52661895751953
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7229365110397339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008635141421109438
        model: {}
        policy_loss: -0.0019606524147093296
        total_loss: 0.004028039053082466
        vf_explained_var: 0.004240125417709351
        vf_loss: 72.61058044433594
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0562468767166138
        entropy_coeff: 0.0017600000137463212
        kl: 0.001385031035169959
        model: {}
        policy_loss: -0.001950634177774191
        total_loss: -0.0007405448704957962
        vf_explained_var: 0.02649974822998047
        vf_loss: 30.690860748291016
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26561301946640015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006817739340476692
        model: {}
        policy_loss: -0.0016495268791913986
        total_loss: 0.0007020577322691679
        vf_explained_var: 0.105255126953125
        vf_loss: 28.190643310546875
    load_time_ms: 13217.364
    num_steps_sampled: 47040000
    num_steps_trained: 47040000
    sample_time_ms: 93364.748
    update_time_ms: 17.535
  iterations_since_restore: 120
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.26741573033708
    ram_util_percent: 16.421910112359548
  pid: 21723
  policy_reward_max:
    agent-0: 264.5
    agent-1: 264.5
    agent-2: 390.5
    agent-3: 390.5
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 238.74
    agent-1: 238.74
    agent-2: 342.195
    agent-3: 342.195
    agent-4: 226.92
    agent-5: 226.92
  policy_reward_min:
    agent-0: 156.5
    agent-1: 156.5
    agent-2: 218.0
    agent-3: 218.0
    agent-4: 154.5
    agent-5: 154.5
  sampler_perf:
    mean_env_wait_ms: 24.761703525090876
    mean_inference_ms: 12.394443632205997
    mean_processing_ms: 54.40285909181089
  time_since_restore: 15153.224334478378
  time_this_iter_s: 125.36716413497925
  time_total_s: 62234.76988649368
  timestamp: 1637573288
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 47040000
  training_iteration: 490
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    490 |          62234.8 | 47040000 |  1615.71 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.8
    apples_agent-1_min: 16
    apples_agent-2_max: 421
    apples_agent-2_mean: 345.75
    apples_agent-2_min: 262
    apples_agent-3_max: 224
    apples_agent-3_mean: 167.06
    apples_agent-3_min: 103
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 381.32
    apples_agent-5_min: 325
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 442.32
    cleaning_beam_agent-0_min: 367
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 1.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 4.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 46.6
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 416.7
    cleaning_beam_agent-4_min: 307
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-30-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1719.0
  episode_reward_mean: 1605.99
  episode_reward_min: 1342.0
  episodes_this_iter: 96
  episodes_total: 47136
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18659.195
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37999290227890015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011217715218663216
        model: {}
        policy_loss: -0.0011688442900776863
        total_loss: 0.0014774221926927567
        vf_explained_var: 0.005958646535873413
        vf_loss: 33.15055847167969
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24258187413215637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005990382051095366
        model: {}
        policy_loss: -0.00163867580704391
        total_loss: 0.0010778350988402963
        vf_explained_var: 0.05727119743824005
        vf_loss: 31.434551239013672
      agent-2:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3017846941947937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008997612167149782
        model: {}
        policy_loss: -0.001784428022801876
        total_loss: 0.004602033644914627
        vf_explained_var: 0.050273388624191284
        vf_loss: 69.17605590820312
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7236659526824951
        entropy_coeff: 0.0017600000137463212
        kl: 0.001267524203285575
        model: {}
        policy_loss: -0.002311691176146269
        total_loss: 0.0038087160792201757
        vf_explained_var: -0.009893685579299927
        vf_loss: 73.94062805175781
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0522129535675049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028138295747339725
        model: {}
        policy_loss: -0.002527894452214241
        total_loss: -0.0014389501884579659
        vf_explained_var: 0.027811288833618164
        vf_loss: 29.408397674560547
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2681099772453308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008705542422831059
        model: {}
        policy_loss: -0.0015894845128059387
        total_loss: 0.0007507116533815861
        vf_explained_var: 0.0729532539844513
        vf_loss: 28.12069320678711
    load_time_ms: 13227.282
    num_steps_sampled: 47136000
    num_steps_trained: 47136000
    sample_time_ms: 93295.336
    update_time_ms: 17.702
  iterations_since_restore: 121
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.36444444444444
    ram_util_percent: 16.474999999999998
  pid: 21723
  policy_reward_max:
    agent-0: 268.5
    agent-1: 268.5
    agent-2: 387.0
    agent-3: 387.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 236.63
    agent-1: 236.63
    agent-2: 340.985
    agent-3: 340.985
    agent-4: 225.38
    agent-5: 225.38
  policy_reward_min:
    agent-0: 203.0
    agent-1: 203.0
    agent-2: 279.0
    agent-3: 279.0
    agent-4: 189.0
    agent-5: 189.0
  sampler_perf:
    mean_env_wait_ms: 24.760488173935766
    mean_inference_ms: 12.394079621891892
    mean_processing_ms: 54.402040838225254
  time_since_restore: 15278.47521352768
  time_this_iter_s: 125.25087904930115
  time_total_s: 62360.020765542984
  timestamp: 1637573415
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 47136000
  training_iteration: 491
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    491 |            62360 | 47136000 |  1605.99 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 33.41
    apples_agent-1_min: 9
    apples_agent-2_max: 409
    apples_agent-2_mean: 347.63
    apples_agent-2_min: 233
    apples_agent-3_max: 234
    apples_agent-3_mean: 168.05
    apples_agent-3_min: 75
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.31
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 375.74
    apples_agent-5_min: 254
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 439.55
    cleaning_beam_agent-0_min: 396
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.53
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 4.27
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 44.89
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 404.62
    cleaning_beam_agent-4_min: 314
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-32-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1744.0
  episode_reward_mean: 1604.37
  episode_reward_min: 1095.0
  episodes_this_iter: 96
  episodes_total: 47232
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18656.805
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37976592779159546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014144708402454853
        model: {}
        policy_loss: -0.0012142891064286232
        total_loss: 0.0016213739290833473
        vf_explained_var: 0.011778607964515686
        vf_loss: 35.0405158996582
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23998065292835236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011210964294150472
        model: {}
        policy_loss: -0.001782354200258851
        total_loss: 0.001145403366535902
        vf_explained_var: 0.056454360485076904
        vf_loss: 33.50126266479492
      agent-2:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30123916268348694
        entropy_coeff: 0.0017600000137463212
        kl: 0.001307148253545165
        model: {}
        policy_loss: -0.002005703514441848
        total_loss: 0.004484791774302721
        vf_explained_var: 0.07195240259170532
        vf_loss: 70.20677185058594
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.714084267616272
        entropy_coeff: 0.0017600000137463212
        kl: 0.000779517344199121
        model: {}
        policy_loss: -0.0019329236820340157
        total_loss: 0.004335406702011824
        vf_explained_var: 0.007164910435676575
        vf_loss: 75.25119018554688
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0586484670639038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014756564050912857
        model: {}
        policy_loss: -0.0020190714858472347
        total_loss: -0.0008651535026729107
        vf_explained_var: 0.034870296716690063
        vf_loss: 30.17142105102539
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27261221408843994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008219939772970974
        model: {}
        policy_loss: -0.001855514943599701
        total_loss: 0.00045481882989406586
        vf_explained_var: 0.1072167158126831
        vf_loss: 27.901338577270508
    load_time_ms: 13229.893
    num_steps_sampled: 47232000
    num_steps_trained: 47232000
    sample_time_ms: 93163.695
    update_time_ms: 17.623
  iterations_since_restore: 122
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 59.00561797752809
    ram_util_percent: 16.484831460674158
  pid: 21723
  policy_reward_max:
    agent-0: 266.0
    agent-1: 266.0
    agent-2: 385.5
    agent-3: 385.5
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 237.795
    agent-1: 237.795
    agent-2: 341.175
    agent-3: 341.175
    agent-4: 223.215
    agent-5: 223.215
  policy_reward_min:
    agent-0: 154.0
    agent-1: 154.0
    agent-2: 232.5
    agent-3: 232.5
    agent-4: 161.0
    agent-5: 161.0
  sampler_perf:
    mean_env_wait_ms: 24.757554853677465
    mean_inference_ms: 12.393326568372254
    mean_processing_ms: 54.39847301054946
  time_since_restore: 15403.199785470963
  time_this_iter_s: 124.72457194328308
  time_total_s: 62484.74533748627
  timestamp: 1637573539
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 47232000
  training_iteration: 492
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    492 |          62484.7 | 47232000 |  1604.37 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 29.66
    apples_agent-1_min: 18
    apples_agent-2_max: 401
    apples_agent-2_mean: 352.02
    apples_agent-2_min: 255
    apples_agent-3_max: 250
    apples_agent-3_mean: 165.56
    apples_agent-3_min: 81
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 384.46
    apples_agent-5_min: 324
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 432.41
    cleaning_beam_agent-0_min: 386
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 4.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 47.3
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 402.13
    cleaning_beam_agent-4_min: 311
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 2.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-34-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1762.0
  episode_reward_mean: 1623.81
  episode_reward_min: 1372.0
  episodes_this_iter: 96
  episodes_total: 47328
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18653.02
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3824513554573059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017179944552481174
        model: {}
        policy_loss: -0.0016254540532827377
        total_loss: 0.0011202730238437653
        vf_explained_var: -0.000942617654800415
        vf_loss: 34.18843078613281
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23849450051784515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007065371028147638
        model: {}
        policy_loss: -0.0016005425713956356
        total_loss: 0.0011705963406711817
        vf_explained_var: 0.06727474927902222
        vf_loss: 31.90889549255371
      agent-2:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29864266514778137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014592551160603762
        model: {}
        policy_loss: -0.0020294045098125935
        total_loss: 0.004222944378852844
        vf_explained_var: 0.04634016752243042
        vf_loss: 67.77960205078125
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7133615016937256
        entropy_coeff: 0.0017600000137463212
        kl: 0.001462868181988597
        model: {}
        policy_loss: -0.00223410758189857
        total_loss: 0.0037970230914652348
        vf_explained_var: -0.01823088526725769
        vf_loss: 72.86647033691406
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.069854736328125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024547858629375696
        model: {}
        policy_loss: -0.002294965088367462
        total_loss: -0.0011946074664592743
        vf_explained_var: 0.01799093186855316
        vf_loss: 29.83304214477539
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2667652368545532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007338311988860369
        model: {}
        policy_loss: -0.0016796530690044165
        total_loss: 0.0006248857825994492
        vf_explained_var: 0.09638053178787231
        vf_loss: 27.740463256835938
    load_time_ms: 13247.485
    num_steps_sampled: 47328000
    num_steps_trained: 47328000
    sample_time_ms: 93151.281
    update_time_ms: 18.47
  iterations_since_restore: 123
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 60.24350282485875
    ram_util_percent: 16.484745762711864
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 386.5
    agent-3: 386.5
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 240.8
    agent-1: 240.8
    agent-2: 343.735
    agent-3: 343.735
    agent-4: 227.37
    agent-5: 227.37
  policy_reward_min:
    agent-0: 207.0
    agent-1: 207.0
    agent-2: 275.0
    agent-3: 275.0
    agent-4: 194.5
    agent-5: 194.5
  sampler_perf:
    mean_env_wait_ms: 24.75499501401688
    mean_inference_ms: 12.392465721729904
    mean_processing_ms: 54.393679852936756
  time_since_restore: 15527.803636550903
  time_this_iter_s: 124.6038510799408
  time_total_s: 62609.34918856621
  timestamp: 1637573664
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 47328000
  training_iteration: 493
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    493 |          62609.3 | 47328000 |  1623.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.13
    apples_agent-1_min: 18
    apples_agent-2_max: 409
    apples_agent-2_mean: 349.08
    apples_agent-2_min: 216
    apples_agent-3_max: 222
    apples_agent-3_mean: 160.91
    apples_agent-3_min: 78
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 380.71
    apples_agent-5_min: 224
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 431.21
    cleaning_beam_agent-0_min: 388
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.52
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 4.32
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 45.04
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 389.77
    cleaning_beam_agent-4_min: 305
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 2.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-36-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1743.0
  episode_reward_mean: 1603.81
  episode_reward_min: 1152.0
  episodes_this_iter: 96
  episodes_total: 47424
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18664.566
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3746691048145294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016103658126667142
        model: {}
        policy_loss: -0.0011138098780065775
        total_loss: 0.0016171695897355676
        vf_explained_var: 0.004741042852401733
        vf_loss: 33.90397262573242
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24415722489356995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010485186940059066
        model: {}
        policy_loss: -0.0018842127174139023
        total_loss: 0.0008559329435229301
        vf_explained_var: 0.06851068139076233
        vf_loss: 31.69862174987793
      agent-2:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30557990074157715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011322367936372757
        model: {}
        policy_loss: -0.0018716473132371902
        total_loss: 0.004304614383727312
        vf_explained_var: 0.05411915481090546
        vf_loss: 67.14083099365234
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7081285715103149
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008933773497119546
        model: {}
        policy_loss: -0.0022023916244506836
        total_loss: 0.003685000352561474
        vf_explained_var: -0.0029882043600082397
        vf_loss: 71.33697509765625
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0665316581726074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013555780751630664
        model: {}
        policy_loss: -0.0020830179564654827
        total_loss: -0.0009205499663949013
        vf_explained_var: 0.05041387677192688
        vf_loss: 30.395631790161133
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27537253499031067
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012077274732291698
        model: {}
        policy_loss: -0.001928799320012331
        total_loss: 0.0004632496275007725
        vf_explained_var: 0.10710589587688446
        vf_loss: 28.767045974731445
    load_time_ms: 13308.949
    num_steps_sampled: 47424000
    num_steps_trained: 47424000
    sample_time_ms: 93267.387
    update_time_ms: 18.3
  iterations_since_restore: 124
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.2188888888889
    ram_util_percent: 16.481666666666666
  pid: 21723
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 378.5
    agent-3: 378.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 237.765
    agent-1: 237.765
    agent-2: 338.315
    agent-3: 338.315
    agent-4: 225.825
    agent-5: 225.825
  policy_reward_min:
    agent-0: 172.0
    agent-1: 172.0
    agent-2: 231.0
    agent-3: 231.0
    agent-4: 142.0
    agent-5: 142.0
  sampler_perf:
    mean_env_wait_ms: 24.752561922754886
    mean_inference_ms: 12.39258724309538
    mean_processing_ms: 54.39607555844795
  time_since_restore: 15654.161654472351
  time_this_iter_s: 126.35801792144775
  time_total_s: 62735.707206487656
  timestamp: 1637573791
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 47424000
  training_iteration: 494
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    494 |          62735.7 | 47424000 |  1603.81 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 30.67
    apples_agent-1_min: 11
    apples_agent-2_max: 398
    apples_agent-2_mean: 345.13
    apples_agent-2_min: 161
    apples_agent-3_max: 237
    apples_agent-3_mean: 162.07
    apples_agent-3_min: 36
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 378.47
    apples_agent-5_min: 145
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 438.22
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.72
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 6.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 44.01
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 395.56
    cleaning_beam_agent-4_min: 331
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-38-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1752.0
  episode_reward_mean: 1593.42
  episode_reward_min: 596.0
  episodes_this_iter: 96
  episodes_total: 47520
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18683.624
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.376706600189209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015312397154048085
        model: {}
        policy_loss: -0.001389640849083662
        total_loss: 0.0014379448257386684
        vf_explained_var: 0.07159455120563507
        vf_loss: 34.905906677246094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24796205759048462
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010230158222839236
        model: {}
        policy_loss: -0.0019505810923874378
        total_loss: 0.0008780192583799362
        vf_explained_var: 0.13215133547782898
        vf_loss: 32.65012741088867
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30946528911590576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012431783834472299
        model: {}
        policy_loss: -0.002022287342697382
        total_loss: 0.004516656510531902
        vf_explained_var: 0.08605793118476868
        vf_loss: 70.83602905273438
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7204889059066772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009345372673124075
        model: {}
        policy_loss: -0.001957286847755313
        total_loss: 0.004559818189591169
        vf_explained_var: -0.0006808191537857056
        vf_loss: 77.85165405273438
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0477217435836792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020602447912096977
        model: {}
        policy_loss: -0.002001767046749592
        total_loss: -0.00046844780445098877
        vf_explained_var: 0.02153956890106201
        vf_loss: 33.773094177246094
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2730107009410858
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007743362803012133
        model: {}
        policy_loss: -0.0020355540327727795
        total_loss: 0.0005384685937315226
        vf_explained_var: 0.11864407360553741
        vf_loss: 30.545223236083984
    load_time_ms: 13334.601
    num_steps_sampled: 47520000
    num_steps_trained: 47520000
    sample_time_ms: 93315.777
    update_time_ms: 18.45
  iterations_since_restore: 125
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.12234636871509
    ram_util_percent: 16.41675977653631
  pid: 21723
  policy_reward_max:
    agent-0: 265.5
    agent-1: 265.5
    agent-2: 383.0
    agent-3: 383.0
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 235.22
    agent-1: 235.22
    agent-2: 336.9
    agent-3: 336.9
    agent-4: 224.59
    agent-5: 224.59
  policy_reward_min:
    agent-0: 71.5
    agent-1: 71.5
    agent-2: 135.5
    agent-3: 135.5
    agent-4: 91.0
    agent-5: 91.0
  sampler_perf:
    mean_env_wait_ms: 24.750550322927733
    mean_inference_ms: 12.392446644932104
    mean_processing_ms: 54.39635363699908
  time_since_restore: 15779.981763839722
  time_this_iter_s: 125.8201093673706
  time_total_s: 62861.527315855026
  timestamp: 1637573917
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 47520000
  training_iteration: 495
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    495 |          62861.5 | 47520000 |  1593.42 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 30.97
    apples_agent-1_min: 10
    apples_agent-2_max: 421
    apples_agent-2_mean: 349.61
    apples_agent-2_min: 132
    apples_agent-3_max: 242
    apples_agent-3_mean: 170.63
    apples_agent-3_min: 52
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 377.21
    apples_agent-5_min: 155
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 430.34
    cleaning_beam_agent-0_min: 317
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.91
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 33
    cleaning_beam_agent-2_mean: 5.85
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 42.88
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 409.77
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 3.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-40-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1754.0
  episode_reward_mean: 1603.84
  episode_reward_min: 604.0
  episodes_this_iter: 96
  episodes_total: 47616
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18711.877
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38545727729797363
        entropy_coeff: 0.0017600000137463212
        kl: 0.001334749860689044
        model: {}
        policy_loss: -0.0012553916312754154
        total_loss: 0.0015534968115389347
        vf_explained_var: 0.03161688148975372
        vf_loss: 34.872947692871094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2448848932981491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011662701144814491
        model: {}
        policy_loss: -0.0021101459860801697
        total_loss: 0.0007542166858911514
        vf_explained_var: 0.08642794191837311
        vf_loss: 32.95364761352539
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3129572868347168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010361415334045887
        model: {}
        policy_loss: -0.0020562978461384773
        total_loss: 0.004566485062241554
        vf_explained_var: 0.08719037473201752
        vf_loss: 71.73589324951172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7214245796203613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009927016217261553
        model: {}
        policy_loss: -0.0022442364133894444
        total_loss: 0.0045433263294398785
        vf_explained_var: -0.01909637451171875
        vf_loss: 80.57270812988281
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0548611879348755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028934211004525423
        model: {}
        policy_loss: -0.002223053714260459
        total_loss: -0.0009880150901153684
        vf_explained_var: 0.03910760581493378
        vf_loss: 30.915912628173828
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2748144865036011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008872137987054884
        model: {}
        policy_loss: -0.0019915802404284477
        total_loss: 0.0004296202678233385
        vf_explained_var: 0.09626433253288269
        vf_loss: 29.048709869384766
    load_time_ms: 13356.233
    num_steps_sampled: 47616000
    num_steps_trained: 47616000
    sample_time_ms: 93336.04
    update_time_ms: 18.5
  iterations_since_restore: 126
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.42333333333334
    ram_util_percent: 16.481666666666666
  pid: 21723
  policy_reward_max:
    agent-0: 266.0
    agent-1: 266.0
    agent-2: 386.0
    agent-3: 386.0
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 235.99
    agent-1: 235.99
    agent-2: 342.76
    agent-3: 342.76
    agent-4: 223.17
    agent-5: 223.17
  policy_reward_min:
    agent-0: 93.5
    agent-1: 93.5
    agent-2: 124.5
    agent-3: 124.5
    agent-4: 84.0
    agent-5: 84.0
  sampler_perf:
    mean_env_wait_ms: 24.748993910611897
    mean_inference_ms: 12.392237424902056
    mean_processing_ms: 54.39782949057546
  time_since_restore: 15905.747874498367
  time_this_iter_s: 125.76611065864563
  time_total_s: 62987.29342651367
  timestamp: 1637574043
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 47616000
  training_iteration: 496
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    496 |          62987.3 | 47616000 |  1603.84 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 29.7
    apples_agent-1_min: 11
    apples_agent-2_max: 411
    apples_agent-2_mean: 347.73
    apples_agent-2_min: 274
    apples_agent-3_max: 242
    apples_agent-3_mean: 171.07
    apples_agent-3_min: 97
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.22
    apples_agent-4_min: 0
    apples_agent-5_max: 426
    apples_agent-5_mean: 385.15
    apples_agent-5_min: 253
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 440.34
    cleaning_beam_agent-0_min: 383
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.79
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 5.55
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 42.81
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 413.61
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-42-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1751.0
  episode_reward_mean: 1616.94
  episode_reward_min: 1117.0
  episodes_this_iter: 96
  episodes_total: 47712
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18723.96
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3768569827079773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015112935798242688
        model: {}
        policy_loss: -0.0011824206449091434
        total_loss: 0.001579800620675087
        vf_explained_var: -0.0028914213180541992
        vf_loss: 34.254905700683594
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24288234114646912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009821378625929356
        model: {}
        policy_loss: -0.001958359032869339
        total_loss: 0.000824914313852787
        vf_explained_var: 0.059795111417770386
        vf_loss: 32.1074333190918
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30845898389816284
        entropy_coeff: 0.0017600000137463212
        kl: 0.001348701654933393
        model: {}
        policy_loss: -0.00184324337169528
        total_loss: 0.004395084455609322
        vf_explained_var: 0.06352192163467407
        vf_loss: 67.81214904785156
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7032082676887512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015429591294378042
        model: {}
        policy_loss: -0.0020985789597034454
        total_loss: 0.0038324911147356033
        vf_explained_var: 0.011375829577445984
        vf_loss: 71.6871337890625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.04941987991333
        entropy_coeff: 0.0017600000137463212
        kl: 0.004048801958560944
        model: {}
        policy_loss: -0.0025748321786522865
        total_loss: -0.0014423010870814323
        vf_explained_var: 0.039236873388290405
        vf_loss: 29.79510498046875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2634337544441223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009186390088871121
        model: {}
        policy_loss: -0.0018761157989501953
        total_loss: 0.0005407547578215599
        vf_explained_var: 0.0754835456609726
        vf_loss: 28.80514907836914
    load_time_ms: 13361.281
    num_steps_sampled: 47712000
    num_steps_trained: 47712000
    sample_time_ms: 93340.029
    update_time_ms: 18.914
  iterations_since_restore: 127
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.61629213483147
    ram_util_percent: 16.47921348314607
  pid: 21723
  policy_reward_max:
    agent-0: 264.5
    agent-1: 264.5
    agent-2: 387.0
    agent-3: 387.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 238.525
    agent-1: 238.525
    agent-2: 342.565
    agent-3: 342.565
    agent-4: 227.38
    agent-5: 227.38
  policy_reward_min:
    agent-0: 163.5
    agent-1: 163.5
    agent-2: 234.5
    agent-3: 234.5
    agent-4: 160.5
    agent-5: 160.5
  sampler_perf:
    mean_env_wait_ms: 24.7472928234705
    mean_inference_ms: 12.391939465230696
    mean_processing_ms: 54.397789105875376
  time_since_restore: 16031.23853302002
  time_this_iter_s: 125.49065852165222
  time_total_s: 63112.784085035324
  timestamp: 1637574168
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 47712000
  training_iteration: 497
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    497 |          63112.8 | 47712000 |  1616.94 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 30.56
    apples_agent-1_min: 15
    apples_agent-2_max: 399
    apples_agent-2_mean: 351.86
    apples_agent-2_min: 270
    apples_agent-3_max: 262
    apples_agent-3_mean: 176.6
    apples_agent-3_min: 99
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 379.03
    apples_agent-5_min: 258
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 446.86
    cleaning_beam_agent-0_min: 395
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 4.04
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 39.1
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 431.95
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-44-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1780.0
  episode_reward_mean: 1628.97
  episode_reward_min: 1283.0
  episodes_this_iter: 96
  episodes_total: 47808
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18687.07
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37962669134140015
        entropy_coeff: 0.0017600000137463212
        kl: 0.000972381210885942
        model: {}
        policy_loss: -0.0012883624294772744
        total_loss: 0.0013854976277798414
        vf_explained_var: -0.011771291494369507
        vf_loss: 33.42005920410156
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24443449079990387
        entropy_coeff: 0.0017600000137463212
        kl: 0.001411553006619215
        model: {}
        policy_loss: -0.001752036390826106
        total_loss: 0.0009120495524257421
        vf_explained_var: 0.06271937489509583
        vf_loss: 30.94288444519043
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30533021688461304
        entropy_coeff: 0.0017600000137463212
        kl: 0.001336498069576919
        model: {}
        policy_loss: -0.0018895542016252875
        total_loss: 0.004649143200367689
        vf_explained_var: 0.0421464741230011
        vf_loss: 70.76077270507812
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7096203565597534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011427097488194704
        model: {}
        policy_loss: -0.0020237555727362633
        total_loss: 0.004263916052877903
        vf_explained_var: -0.01176103949546814
        vf_loss: 75.36601257324219
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0338090658187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.002789524383842945
        model: {}
        policy_loss: -0.0021938253194093704
        total_loss: -0.0010190763277933002
        vf_explained_var: 0.0436929315328598
        vf_loss: 29.942520141601562
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2654659152030945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009958036243915558
        model: {}
        policy_loss: -0.0017363103106617928
        total_loss: 0.0006092851981520653
        vf_explained_var: 0.10064797103404999
        vf_loss: 28.128137588500977
    load_time_ms: 13368.866
    num_steps_sampled: 47808000
    num_steps_trained: 47808000
    sample_time_ms: 93313.152
    update_time_ms: 18.645
  iterations_since_restore: 128
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.19106145251397
    ram_util_percent: 16.46648044692737
  pid: 21723
  policy_reward_max:
    agent-0: 285.0
    agent-1: 285.0
    agent-2: 405.0
    agent-3: 405.0
    agent-4: 251.5
    agent-5: 251.5
  policy_reward_mean:
    agent-0: 239.795
    agent-1: 239.795
    agent-2: 348.92
    agent-3: 348.92
    agent-4: 225.77
    agent-5: 225.77
  policy_reward_min:
    agent-0: 186.0
    agent-1: 186.0
    agent-2: 285.0
    agent-3: 285.0
    agent-4: 144.0
    agent-5: 144.0
  sampler_perf:
    mean_env_wait_ms: 24.747084556655153
    mean_inference_ms: 12.391954379709977
    mean_processing_ms: 54.39914010370203
  time_since_restore: 16156.853147029877
  time_this_iter_s: 125.61461400985718
  time_total_s: 63238.39869904518
  timestamp: 1637574294
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 47808000
  training_iteration: 498
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    498 |          63238.4 | 47808000 |  1628.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.11
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.57
    apples_agent-1_min: 16
    apples_agent-2_max: 408
    apples_agent-2_mean: 349.74
    apples_agent-2_min: 249
    apples_agent-3_max: 236
    apples_agent-3_mean: 179.31
    apples_agent-3_min: 104
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.18
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 384.81
    apples_agent-5_min: 258
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 444.59
    cleaning_beam_agent-0_min: 400
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.46
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 4.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 41.59
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 440.08
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-46-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1770.0
  episode_reward_mean: 1619.17
  episode_reward_min: 1095.0
  episodes_this_iter: 96
  episodes_total: 47904
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18702.86
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3689790368080139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018640010384842753
        model: {}
        policy_loss: -0.0016112206503748894
        total_loss: 0.0010838788002729416
        vf_explained_var: 0.008852720260620117
        vf_loss: 33.44505310058594
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24588419497013092
        entropy_coeff: 0.0017600000137463212
        kl: 0.000982324592769146
        model: {}
        policy_loss: -0.0015188499819487333
        total_loss: 0.0012192090507596731
        vf_explained_var: 0.06102028489112854
        vf_loss: 31.708181381225586
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3035943806171417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008560627466067672
        model: {}
        policy_loss: -0.0018560276366770267
        total_loss: 0.004644422326236963
        vf_explained_var: 0.05601893365383148
        vf_loss: 70.34773254394531
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7155393958091736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008322097128257155
        model: {}
        policy_loss: -0.002254232531413436
        total_loss: 0.0040445816703140736
        vf_explained_var: -0.00809219479560852
        vf_loss: 75.58161926269531
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0368914604187012
        entropy_coeff: 0.0017600000137463212
        kl: 0.002399131190031767
        model: {}
        policy_loss: -0.0021373957861214876
        total_loss: -0.0009131338447332382
        vf_explained_var: -0.017989367246627808
        vf_loss: 30.491933822631836
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2577696442604065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009706186829134822
        model: {}
        policy_loss: -0.0017086509615182877
        total_loss: 0.0005942025454714894
        vf_explained_var: 0.08192597329616547
        vf_loss: 27.565271377563477
    load_time_ms: 13369.009
    num_steps_sampled: 47904000
    num_steps_trained: 47904000
    sample_time_ms: 93229.209
    update_time_ms: 18.435
  iterations_since_restore: 129
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.16101694915254
    ram_util_percent: 16.63502824858757
  pid: 21723
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 397.5
    agent-3: 397.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 235.755
    agent-1: 235.755
    agent-2: 346.575
    agent-3: 346.575
    agent-4: 227.255
    agent-5: 227.255
  policy_reward_min:
    agent-0: 146.5
    agent-1: 146.5
    agent-2: 255.5
    agent-3: 255.5
    agent-4: 144.0
    agent-5: 144.0
  sampler_perf:
    mean_env_wait_ms: 24.745092749433432
    mean_inference_ms: 12.391280527378552
    mean_processing_ms: 54.39635631778374
  time_since_restore: 16281.847712039948
  time_this_iter_s: 124.9945650100708
  time_total_s: 63363.39326405525
  timestamp: 1637574419
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 47904000
  training_iteration: 499
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    499 |          63363.4 | 47904000 |  1619.17 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 29.97
    apples_agent-1_min: 17
    apples_agent-2_max: 401
    apples_agent-2_mean: 350.61
    apples_agent-2_min: 213
    apples_agent-3_max: 250
    apples_agent-3_mean: 176.82
    apples_agent-3_min: 83
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 383.99
    apples_agent-5_min: 260
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 449.16
    cleaning_beam_agent-0_min: 403
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 2.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 5.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 41.31
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 433.23
    cleaning_beam_agent-4_min: 286
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-49-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1806.0
  episode_reward_mean: 1630.76
  episode_reward_min: 1012.0
  episodes_this_iter: 96
  episodes_total: 48000
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18695.252
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37423455715179443
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013701950665563345
        model: {}
        policy_loss: -0.0012178996112197638
        total_loss: 0.0015496985288336873
        vf_explained_var: -0.01353752613067627
        vf_loss: 34.26250457763672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2398073822259903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007938186172395945
        model: {}
        policy_loss: -0.0018219456542283297
        total_loss: 0.001043976517394185
        vf_explained_var: 0.03331972658634186
        vf_loss: 32.87984848022461
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30478498339653015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009149243123829365
        model: {}
        policy_loss: -0.0016491812421008945
        total_loss: 0.004585126414895058
        vf_explained_var: 0.03906182944774628
        vf_loss: 67.70726776123047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7191691398620605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009333545458503067
        model: {}
        policy_loss: -0.0019988552667200565
        total_loss: 0.003922187723219395
        vf_explained_var: -0.012094646692276001
        vf_loss: 71.86782836914062
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0445003509521484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009767221054062247
        model: {}
        policy_loss: -0.001562435645610094
        total_loss: -0.0003664446994662285
        vf_explained_var: -0.01732856035232544
        vf_loss: 30.343128204345703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24928098917007446
        entropy_coeff: 0.0017600000137463212
        kl: 0.001019724179059267
        model: {}
        policy_loss: -0.0015581748448312283
        total_loss: 0.0007837542798370123
        vf_explained_var: 0.07055914402008057
        vf_loss: 27.806659698486328
    load_time_ms: 13358.146
    num_steps_sampled: 48000000
    num_steps_trained: 48000000
    sample_time_ms: 93334.514
    update_time_ms: 18.693
  iterations_since_restore: 130
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.86666666666667
    ram_util_percent: 16.49222222222222
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 389.5
    agent-3: 389.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 240.43
    agent-1: 240.43
    agent-2: 346.07
    agent-3: 346.07
    agent-4: 228.88
    agent-5: 228.88
  policy_reward_min:
    agent-0: 146.0
    agent-1: 146.0
    agent-2: 193.0
    agent-3: 193.0
    agent-4: 167.0
    agent-5: 167.0
  sampler_perf:
    mean_env_wait_ms: 24.745378430920063
    mean_inference_ms: 12.391557219975914
    mean_processing_ms: 54.39781520057857
  time_since_restore: 16408.08104467392
  time_this_iter_s: 126.23333263397217
  time_total_s: 63489.626596689224
  timestamp: 1637574546
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 48000000
  training_iteration: 500
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    500 |          63489.6 | 48000000 |  1630.76 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 30.16
    apples_agent-1_min: 12
    apples_agent-2_max: 411
    apples_agent-2_mean: 346.66
    apples_agent-2_min: 0
    apples_agent-3_max: 258
    apples_agent-3_mean: 170.98
    apples_agent-3_min: 92
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.51
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 379.63
    apples_agent-5_min: 257
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 440.09
    cleaning_beam_agent-0_min: 409
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.55
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 5.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 45.67
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 435.26
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 2.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-51-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1719.0
  episode_reward_mean: 1605.85
  episode_reward_min: 770.0
  episodes_this_iter: 96
  episodes_total: 48096
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18696.8
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37801334261894226
        entropy_coeff: 0.0017600000137463212
        kl: 0.001335753477178514
        model: {}
        policy_loss: -0.0013247448951005936
        total_loss: 0.001507389359176159
        vf_explained_var: 0.019664525985717773
        vf_loss: 34.974361419677734
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24960821866989136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005678561283275485
        model: {}
        policy_loss: -0.0016760025173425674
        total_loss: 0.0010831472463905811
        vf_explained_var: 0.10419456660747528
        vf_loss: 31.98459815979004
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31211963295936584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011785144452005625
        model: {}
        policy_loss: -0.0018590018153190613
        total_loss: 0.004992002621293068
        vf_explained_var: 0.08285972476005554
        vf_loss: 74.00335693359375
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7155962586402893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006764638237655163
        model: {}
        policy_loss: -0.0019433841807767749
        total_loss: 0.004855160601437092
        vf_explained_var: 0.0013013631105422974
        vf_loss: 80.57992553710938
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0334702730178833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018397512612864375
        model: {}
        policy_loss: -0.002188487909734249
        total_loss: -0.0008057416416704655
        vf_explained_var: 0.011247649788856506
        vf_loss: 32.01654052734375
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2580154240131378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004927904810756445
        model: {}
        policy_loss: -0.001583355711773038
        total_loss: 0.0008568036719225347
        vf_explained_var: 0.10472819209098816
        vf_loss: 28.942670822143555
    load_time_ms: 13352.047
    num_steps_sampled: 48096000
    num_steps_trained: 48096000
    sample_time_ms: 93350.897
    update_time_ms: 18.498
  iterations_since_restore: 131
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.56111111111111
    ram_util_percent: 16.508333333333333
  pid: 21723
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 384.5
    agent-3: 384.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 237.105
    agent-1: 237.105
    agent-2: 340.195
    agent-3: 340.195
    agent-4: 225.625
    agent-5: 225.625
  policy_reward_min:
    agent-0: 171.5
    agent-1: 171.5
    agent-2: 60.5
    agent-3: 60.5
    agent-4: 153.0
    agent-5: 153.0
  sampler_perf:
    mean_env_wait_ms: 24.74470155804475
    mean_inference_ms: 12.391327660247399
    mean_processing_ms: 54.39673870628232
  time_since_restore: 16533.450832605362
  time_this_iter_s: 125.36978793144226
  time_total_s: 63614.99638462067
  timestamp: 1637574672
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 48096000
  training_iteration: 501
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    501 |            63615 | 48096000 |  1605.85 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 42
    apples_agent-1_mean: 29.89
    apples_agent-1_min: 15
    apples_agent-2_max: 441
    apples_agent-2_mean: 349.35
    apples_agent-2_min: 230
    apples_agent-3_max: 251
    apples_agent-3_mean: 176.37
    apples_agent-3_min: 113
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 379.65
    apples_agent-5_min: 273
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 434.83
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 1.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 6.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 45.23
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 444.1
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 2.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-53-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1774.0
  episode_reward_mean: 1615.58
  episode_reward_min: 1132.0
  episodes_this_iter: 96
  episodes_total: 48192
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18713.918
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37492313981056213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012935952981933951
        model: {}
        policy_loss: -0.0015316298231482506
        total_loss: 0.0011885548010468483
        vf_explained_var: 0.014184698462486267
        vf_loss: 33.80048751831055
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24541327357292175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008659955346956849
        model: {}
        policy_loss: -0.0016827662475407124
        total_loss: 0.0010220464318990707
        vf_explained_var: 0.08678729832172394
        vf_loss: 31.36739730834961
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3028760552406311
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010323512833565474
        model: {}
        policy_loss: -0.0018049204954877496
        total_loss: 0.004817016422748566
        vf_explained_var: 0.06776420772075653
        vf_loss: 71.54998779296875
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7125426530838013
        entropy_coeff: 0.0017600000137463212
        kl: 0.00171379663515836
        model: {}
        policy_loss: -0.0022003017365932465
        total_loss: 0.0041337828151881695
        vf_explained_var: 0.00976620614528656
        vf_loss: 75.881591796875
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0202250480651855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014777380274608731
        model: {}
        policy_loss: -0.0017155101522803307
        total_loss: -0.0003040402662009001
        vf_explained_var: -0.00045552849769592285
        vf_loss: 32.07064437866211
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25473323464393616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013159728841856122
        model: {}
        policy_loss: -0.0017895501805469394
        total_loss: 0.0006662879604846239
        vf_explained_var: 0.09313265979290009
        vf_loss: 29.04168128967285
    load_time_ms: 13368.358
    num_steps_sampled: 48192000
    num_steps_trained: 48192000
    sample_time_ms: 93417.881
    update_time_ms: 18.632
  iterations_since_restore: 132
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.19329608938547
    ram_util_percent: 16.521787709497204
  pid: 21723
  policy_reward_max:
    agent-0: 268.5
    agent-1: 268.5
    agent-2: 401.0
    agent-3: 401.0
    agent-4: 252.0
    agent-5: 252.0
  policy_reward_mean:
    agent-0: 238.56
    agent-1: 238.56
    agent-2: 344.655
    agent-3: 344.655
    agent-4: 224.575
    agent-5: 224.575
  policy_reward_min:
    agent-0: 173.0
    agent-1: 173.0
    agent-2: 238.5
    agent-3: 238.5
    agent-4: 154.5
    agent-5: 154.5
  sampler_perf:
    mean_env_wait_ms: 24.744770428375556
    mean_inference_ms: 12.391460697877289
    mean_processing_ms: 54.397060600872734
  time_since_restore: 16659.166291952133
  time_this_iter_s: 125.71545934677124
  time_total_s: 63740.71184396744
  timestamp: 1637574798
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 48192000
  training_iteration: 502
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    502 |          63740.7 | 48192000 |  1615.58 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 30.04
    apples_agent-1_min: 17
    apples_agent-2_max: 414
    apples_agent-2_mean: 356.74
    apples_agent-2_min: 266
    apples_agent-3_max: 272
    apples_agent-3_mean: 179.38
    apples_agent-3_min: 39
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 461
    apples_agent-5_mean: 389.22
    apples_agent-5_min: 301
    cleaning_beam_agent-0_max: 505
    cleaning_beam_agent-0_mean: 441.44
    cleaning_beam_agent-0_min: 386
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.67
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 5.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 46.36
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 444.67
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-55-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1755.0
  episode_reward_mean: 1639.97
  episode_reward_min: 1366.0
  episodes_this_iter: 96
  episodes_total: 48288
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18714.071
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37024614214897156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012182432692497969
        model: {}
        policy_loss: -0.0012430534698069096
        total_loss: 0.001488958834670484
        vf_explained_var: -0.011690869927406311
        vf_loss: 33.836456298828125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24487264454364777
        entropy_coeff: 0.0017600000137463212
        kl: 0.00047838359023444355
        model: {}
        policy_loss: -0.0014420070219784975
        total_loss: 0.0012044429313391447
        vf_explained_var: 0.08277301490306854
        vf_loss: 30.77424430847168
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29976755380630493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008921692497096956
        model: {}
        policy_loss: -0.0016866507939994335
        total_loss: 0.0048601035960018635
        vf_explained_var: 0.0504637211561203
        vf_loss: 70.7434310913086
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7105897665023804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010361275635659695
        model: {}
        policy_loss: -0.0019082236103713512
        total_loss: 0.004392790608108044
        vf_explained_var: -0.015324980020523071
        vf_loss: 75.51653289794922
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.025120496749878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012321373214945197
        model: {}
        policy_loss: -0.001635200111195445
        total_loss: -0.00039934879168868065
        vf_explained_var: -0.014775693416595459
        vf_loss: 30.400623321533203
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24622443318367004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010324155446141958
        model: {}
        policy_loss: -0.0016533248126506805
        total_loss: 0.0006726075662299991
        vf_explained_var: 0.0863155722618103
        vf_loss: 27.592880249023438
    load_time_ms: 13353.153
    num_steps_sampled: 48288000
    num_steps_trained: 48288000
    sample_time_ms: 93492.695
    update_time_ms: 17.847
  iterations_since_restore: 133
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.91508379888268
    ram_util_percent: 16.55921787709497
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 385.0
    agent-3: 385.0
    agent-4: 271.0
    agent-5: 271.0
  policy_reward_mean:
    agent-0: 240.675
    agent-1: 240.675
    agent-2: 349.595
    agent-3: 349.595
    agent-4: 229.715
    agent-5: 229.715
  policy_reward_min:
    agent-0: 194.0
    agent-1: 194.0
    agent-2: 272.0
    agent-3: 272.0
    agent-4: 189.5
    agent-5: 189.5
  sampler_perf:
    mean_env_wait_ms: 24.744554391029634
    mean_inference_ms: 12.391095304878254
    mean_processing_ms: 54.39612939731658
  time_since_restore: 16784.333751678467
  time_this_iter_s: 125.16745972633362
  time_total_s: 63865.87930369377
  timestamp: 1637574923
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 48288000
  training_iteration: 503
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    503 |          63865.9 | 48288000 |  1639.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 31.43
    apples_agent-1_min: 16
    apples_agent-2_max: 409
    apples_agent-2_mean: 351.0
    apples_agent-2_min: 184
    apples_agent-3_max: 234
    apples_agent-3_mean: 173.87
    apples_agent-3_min: 83
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.46
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 383.89
    apples_agent-5_min: 232
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 444.99
    cleaning_beam_agent-0_min: 401
    cleaning_beam_agent-1_max: 33
    cleaning_beam_agent-1_mean: 2.74
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 5.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 45.9
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 447.46
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 2.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-57-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1755.0
  episode_reward_mean: 1626.96
  episode_reward_min: 1025.0
  episodes_this_iter: 96
  episodes_total: 48384
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18726.073
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38046568632125854
        entropy_coeff: 0.0017600000137463212
        kl: 0.002148859901353717
        model: {}
        policy_loss: -0.001662582391873002
        total_loss: 0.0012663796078413725
        vf_explained_var: 0.003100752830505371
        vf_loss: 35.985836029052734
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24929283559322357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014343555085361004
        model: {}
        policy_loss: -0.001641902606934309
        total_loss: 0.0012863194569945335
        vf_explained_var: 0.06876304745674133
        vf_loss: 33.66975402832031
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3041462004184723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012611710699275136
        model: {}
        policy_loss: -0.00209041777998209
        total_loss: 0.00463104760274291
        vf_explained_var: 0.06767304241657257
        vf_loss: 72.567626953125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7037281394004822
        entropy_coeff: 0.0017600000137463212
        kl: 0.002029906492680311
        model: {}
        policy_loss: -0.0021685799583792686
        total_loss: 0.004570534452795982
        vf_explained_var: -0.027714848518371582
        vf_loss: 79.77677917480469
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.023814082145691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016616315115243196
        model: {}
        policy_loss: -0.0021405601873993874
        total_loss: -0.0008270635735243559
        vf_explained_var: 0.05276826024055481
        vf_loss: 31.15408706665039
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24815185368061066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006298248772509396
        model: {}
        policy_loss: -0.0017172396183013916
        total_loss: 0.0008227573707699776
        vf_explained_var: 0.0973934680223465
        vf_loss: 29.767452239990234
    load_time_ms: 13310.454
    num_steps_sampled: 48384000
    num_steps_trained: 48384000
    sample_time_ms: 93418.766
    update_time_ms: 18.043
  iterations_since_restore: 134
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.96011235955055
    ram_util_percent: 16.568539325842696
  pid: 21723
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 388.0
    agent-3: 388.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 238.955
    agent-1: 238.955
    agent-2: 346.62
    agent-3: 346.62
    agent-4: 227.905
    agent-5: 227.905
  policy_reward_min:
    agent-0: 155.0
    agent-1: 155.0
    agent-2: 210.0
    agent-3: 210.0
    agent-4: 147.5
    agent-5: 147.5
  sampler_perf:
    mean_env_wait_ms: 24.744091924063174
    mean_inference_ms: 12.390909010990399
    mean_processing_ms: 54.394292613706995
  time_since_restore: 16909.635808706284
  time_this_iter_s: 125.30205702781677
  time_total_s: 63991.18136072159
  timestamp: 1637575049
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 48384000
  training_iteration: 504
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    504 |          63991.2 | 48384000 |  1626.96 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 30.98
    apples_agent-1_min: 1
    apples_agent-2_max: 407
    apples_agent-2_mean: 350.56
    apples_agent-2_min: 11
    apples_agent-3_max: 243
    apples_agent-3_mean: 180.06
    apples_agent-3_min: 8
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 385.56
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 431.94
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 1.79
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 6.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 43.84
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 586
    cleaning_beam_agent-4_mean: 460.24
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 3.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_04-59-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1766.0
  episode_reward_mean: 1628.35
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 48480
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18727.852
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37881574034690857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016587168211117387
        model: {}
        policy_loss: -0.0014854962937533855
        total_loss: 0.0014097969979047775
        vf_explained_var: 0.05624127388000488
        vf_loss: 35.62006759643555
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2516530156135559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006957794539630413
        model: {}
        policy_loss: -0.001747859176248312
        total_loss: 0.001191700343042612
        vf_explained_var: 0.105894535779953
        vf_loss: 33.824676513671875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3090810775756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010936371982097626
        model: {}
        policy_loss: -0.0018943389877676964
        total_loss: 0.004982386715710163
        vf_explained_var: 0.08878231048583984
        vf_loss: 74.20707702636719
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7254637479782104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018488665809854865
        model: {}
        policy_loss: -0.002047184854745865
        total_loss: 0.004914633464068174
        vf_explained_var: -0.01539735496044159
        vf_loss: 82.3863525390625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0189311504364014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018088002689182758
        model: {}
        policy_loss: -0.002166526857763529
        total_loss: -0.00037844572216272354
        vf_explained_var: 0.014699578285217285
        vf_loss: 35.81397247314453
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.251483291387558
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010070414282381535
        model: {}
        policy_loss: -0.0020197583362460136
        total_loss: 0.0007312772795557976
        vf_explained_var: 0.12101735174655914
        vf_loss: 31.93644142150879
    load_time_ms: 13315.729
    num_steps_sampled: 48480000
    num_steps_trained: 48480000
    sample_time_ms: 93368.867
    update_time_ms: 18.525
  iterations_since_restore: 135
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.92359550561799
    ram_util_percent: 16.52640449438202
  pid: 21723
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 386.0
    agent-3: 386.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 238.88
    agent-1: 238.88
    agent-2: 347.875
    agent-3: 347.875
    agent-4: 227.42
    agent-5: 227.42
  policy_reward_min:
    agent-0: 9.5
    agent-1: 9.5
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 11.5
    agent-5: 11.5
  sampler_perf:
    mean_env_wait_ms: 24.743384762763853
    mean_inference_ms: 12.390737113119055
    mean_processing_ms: 54.39178565225454
  time_since_restore: 17035.046529769897
  time_this_iter_s: 125.41072106361389
  time_total_s: 64116.5920817852
  timestamp: 1637575174
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 48480000
  training_iteration: 505
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    505 |          64116.6 | 48480000 |  1628.35 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 0.39
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.17
    apples_agent-1_min: 7
    apples_agent-2_max: 424
    apples_agent-2_mean: 346.56
    apples_agent-2_min: 19
    apples_agent-3_max: 257
    apples_agent-3_mean: 172.9
    apples_agent-3_min: 3
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 374.68
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 443.62
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 28
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 5.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 44.17
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 451.87
    cleaning_beam_agent-4_min: 357
    cleaning_beam_agent-5_max: 30
    cleaning_beam_agent-5_mean: 3.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-01-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1806.0
  episode_reward_mean: 1596.68
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 48576
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18699.958
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38873833417892456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012487719068303704
        model: {}
        policy_loss: -0.0015331548638641834
        total_loss: 0.0016069496050477028
        vf_explained_var: 0.09583425521850586
        vf_loss: 38.24287033081055
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25200504064559937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008878249209374189
        model: {}
        policy_loss: -0.0022637047804892063
        total_loss: 0.0008249422535300255
        vf_explained_var: 0.16526035964488983
        vf_loss: 35.321754455566406
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3152287006378174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009136490989476442
        model: {}
        policy_loss: -0.002199017908424139
        total_loss: 0.005348898936063051
        vf_explained_var: 0.11258228123188019
        vf_loss: 81.02719116210938
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7111472487449646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009092190302908421
        model: {}
        policy_loss: -0.0021756570786237717
        total_loss: 0.005461575463414192
        vf_explained_var: 0.02500702440738678
        vf_loss: 88.88849639892578
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0086232423782349
        entropy_coeff: 0.0017600000137463212
        kl: 0.001668797922320664
        model: {}
        policy_loss: -0.0021213283762335777
        total_loss: -0.00022921990603208542
        vf_explained_var: 0.05191244184970856
        vf_loss: 36.67287063598633
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2609427571296692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008971863426268101
        model: {}
        policy_loss: -0.002509408164769411
        total_loss: 0.00031568389385938644
        vf_explained_var: 0.15153086185455322
        vf_loss: 32.843528747558594
    load_time_ms: 13294.427
    num_steps_sampled: 48576000
    num_steps_trained: 48576000
    sample_time_ms: 93265.879
    update_time_ms: 18.637
  iterations_since_restore: 136
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.40112994350282
    ram_util_percent: 16.56327683615819
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 414.5
    agent-3: 414.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 234.91
    agent-1: 234.91
    agent-2: 340.275
    agent-3: 340.275
    agent-4: 223.155
    agent-5: 223.155
  policy_reward_min:
    agent-0: 14.0
    agent-1: 14.0
    agent-2: 16.5
    agent-3: 16.5
    agent-4: 7.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 24.742483937399108
    mean_inference_ms: 12.390018581953422
    mean_processing_ms: 54.38676334798385
  time_since_restore: 17159.293580293655
  time_this_iter_s: 124.24705052375793
  time_total_s: 64240.83913230896
  timestamp: 1637575299
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 48576000
  training_iteration: 506
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    506 |          64240.8 | 48576000 |  1596.68 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 30.01
    apples_agent-1_min: 12
    apples_agent-2_max: 423
    apples_agent-2_mean: 353.93
    apples_agent-2_min: 159
    apples_agent-3_max: 251
    apples_agent-3_mean: 179.32
    apples_agent-3_min: 71
    apples_agent-4_max: 34
    apples_agent-4_mean: 0.44
    apples_agent-4_min: 0
    apples_agent-5_max: 470
    apples_agent-5_mean: 385.27
    apples_agent-5_min: 152
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 452.09
    cleaning_beam_agent-0_min: 412
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 1.96
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 5.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 43.75
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 451.68
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-03-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1777.0
  episode_reward_mean: 1635.11
  episode_reward_min: 764.0
  episodes_this_iter: 96
  episodes_total: 48672
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18674.196
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38742291927337646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017660477897152305
        model: {}
        policy_loss: -0.0013391138054430485
        total_loss: 0.001361386151984334
        vf_explained_var: 0.001338854432106018
        vf_loss: 33.8236083984375
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24752728641033173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013303423766046762
        model: {}
        policy_loss: -0.0018438061233609915
        total_loss: 0.0009777528466656804
        vf_explained_var: 0.03877359628677368
        vf_loss: 32.57205581665039
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30206918716430664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008880984387360513
        model: {}
        policy_loss: -0.0019615963101387024
        total_loss: 0.004639517515897751
        vf_explained_var: 0.05442771315574646
        vf_loss: 71.32758331298828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6904588341712952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011750201229006052
        model: {}
        policy_loss: -0.00205602147616446
        total_loss: 0.004374830983579159
        vf_explained_var: -0.020782947540283203
        vf_loss: 76.46062469482422
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0189921855926514
        entropy_coeff: 0.0017600000137463212
        kl: 0.001239091856405139
        model: {}
        policy_loss: -0.0019129682332277298
        total_loss: -0.0006055044941604137
        vf_explained_var: 0.04591841995716095
        vf_loss: 31.0089111328125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24988755583763123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010084337554872036
        model: {}
        policy_loss: -0.0018358607776463032
        total_loss: 0.0006926273927092552
        vf_explained_var: 0.08241960406303406
        vf_loss: 29.68294906616211
    load_time_ms: 13303.087
    num_steps_sampled: 48672000
    num_steps_trained: 48672000
    sample_time_ms: 93276.15
    update_time_ms: 18.895
  iterations_since_restore: 137
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.495505617977535
    ram_util_percent: 16.550000000000004
  pid: 21723
  policy_reward_max:
    agent-0: 265.5
    agent-1: 265.5
    agent-2: 404.5
    agent-3: 404.5
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 239.18
    agent-1: 239.18
    agent-2: 350.32
    agent-3: 350.32
    agent-4: 228.055
    agent-5: 228.055
  policy_reward_min:
    agent-0: 115.0
    agent-1: 115.0
    agent-2: 155.5
    agent-3: 155.5
    agent-4: 111.5
    agent-5: 111.5
  sampler_perf:
    mean_env_wait_ms: 24.742851236284586
    mean_inference_ms: 12.39022946121512
    mean_processing_ms: 54.38608076458408
  time_since_restore: 17284.727706193924
  time_this_iter_s: 125.43412590026855
  time_total_s: 64366.27325820923
  timestamp: 1637575424
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 48672000
  training_iteration: 507
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    507 |          64366.3 | 48672000 |  1635.11 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 30.84
    apples_agent-1_min: 14
    apples_agent-2_max: 414
    apples_agent-2_mean: 354.63
    apples_agent-2_min: 228
    apples_agent-3_max: 255
    apples_agent-3_mean: 170.98
    apples_agent-3_min: 72
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 387.92
    apples_agent-5_min: 258
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 439.19
    cleaning_beam_agent-0_min: 365
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.88
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 5.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 44.56
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 454.48
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 3.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-05-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1757.0
  episode_reward_mean: 1632.95
  episode_reward_min: 1102.0
  episodes_this_iter: 96
  episodes_total: 48768
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18680.391
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3828340768814087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009998396271839738
        model: {}
        policy_loss: -0.0013005051296204329
        total_loss: 0.0013953845482319593
        vf_explained_var: 0.010486498475074768
        vf_loss: 33.69676208496094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24659502506256104
        entropy_coeff: 0.0017600000137463212
        kl: 0.001283405814319849
        model: {}
        policy_loss: -0.0017878911457955837
        total_loss: 0.0009452556259930134
        vf_explained_var: 0.07104179263114929
        vf_loss: 31.67155647277832
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30734577775001526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008985963650047779
        model: {}
        policy_loss: -0.0019179536029696465
        total_loss: 0.004703068174421787
        vf_explained_var: 0.0629177838563919
        vf_loss: 71.61949157714844
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6886179447174072
        entropy_coeff: 0.0017600000137463212
        kl: 0.001220031757839024
        model: {}
        policy_loss: -0.0020020417869091034
        total_loss: 0.004534353036433458
        vf_explained_var: -0.014194279909133911
        vf_loss: 77.48362731933594
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0385932922363281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016971316654235125
        model: {}
        policy_loss: -0.002130641136318445
        total_loss: -0.0008824901888146996
        vf_explained_var: 0.027276992797851562
        vf_loss: 30.7607479095459
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2448202669620514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004827160155400634
        model: {}
        policy_loss: -0.001754969242028892
        total_loss: 0.0006487453356385231
        vf_explained_var: 0.09631170332431793
        vf_loss: 28.345943450927734
    load_time_ms: 13289.393
    num_steps_sampled: 48768000
    num_steps_trained: 48768000
    sample_time_ms: 93180.661
    update_time_ms: 19.026
  iterations_since_restore: 138
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.890449438202246
    ram_util_percent: 16.728651685393256
  pid: 21723
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 394.0
    agent-3: 394.0
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 239.45
    agent-1: 239.45
    agent-2: 348.01
    agent-3: 348.01
    agent-4: 229.015
    agent-5: 229.015
  policy_reward_min:
    agent-0: 174.0
    agent-1: 174.0
    agent-2: 225.0
    agent-3: 225.0
    agent-4: 152.0
    agent-5: 152.0
  sampler_perf:
    mean_env_wait_ms: 24.741297708551773
    mean_inference_ms: 12.390508286935763
    mean_processing_ms: 54.381659566024894
  time_since_restore: 17409.30694961548
  time_this_iter_s: 124.57924342155457
  time_total_s: 64490.85250163078
  timestamp: 1637575549
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 48768000
  training_iteration: 508
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    508 |          64490.9 | 48768000 |  1632.95 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 29.25
    apples_agent-1_min: 13
    apples_agent-2_max: 410
    apples_agent-2_mean: 349.7
    apples_agent-2_min: 269
    apples_agent-3_max: 255
    apples_agent-3_mean: 174.62
    apples_agent-3_min: 103
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 382.27
    apples_agent-5_min: 289
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 436.16
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 5.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 41.38
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 442.73
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-07-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1747.0
  episode_reward_mean: 1620.43
  episode_reward_min: 1153.0
  episodes_this_iter: 96
  episodes_total: 48864
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18683.073
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39152318239212036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017171262297779322
        model: {}
        policy_loss: -0.001680989284068346
        total_loss: 0.0012034252285957336
        vf_explained_var: 0.027544215321540833
        vf_loss: 35.7349967956543
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.246572345495224
        entropy_coeff: 0.0017600000137463212
        kl: 0.00048353837337344885
        model: {}
        policy_loss: -0.0017958289245143533
        total_loss: 0.0010968445567414165
        vf_explained_var: 0.09591065347194672
        vf_loss: 33.26640319824219
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3098975419998169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010944728273898363
        model: {}
        policy_loss: -0.001656802254728973
        total_loss: 0.004783635027706623
        vf_explained_var: 0.07071731984615326
        vf_loss: 69.8585433959961
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6795583963394165
        entropy_coeff: 0.0017600000137463212
        kl: 0.00041472859447821975
        model: {}
        policy_loss: -0.0016929229022935033
        total_loss: 0.004755168221890926
        vf_explained_var: -0.013075083494186401
        vf_loss: 76.44115447998047
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0460115671157837
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025776992551982403
        model: {}
        policy_loss: -0.0022134417667984962
        total_loss: -0.0009825760498642921
        vf_explained_var: 0.03814662992954254
        vf_loss: 30.71849250793457
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25293517112731934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009228014387190342
        model: {}
        policy_loss: -0.0018271633889526129
        total_loss: 0.0005972434300929308
        vf_explained_var: 0.09953539073467255
        vf_loss: 28.69573974609375
    load_time_ms: 13298.996
    num_steps_sampled: 48864000
    num_steps_trained: 48864000
    sample_time_ms: 93236.056
    update_time_ms: 19.081
  iterations_since_restore: 139
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.421229050279315
    ram_util_percent: 16.662569832402234
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 393.5
    agent-3: 393.5
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 237.035
    agent-1: 237.035
    agent-2: 345.44
    agent-3: 345.44
    agent-4: 227.74
    agent-5: 227.74
  policy_reward_min:
    agent-0: 155.5
    agent-1: 155.5
    agent-2: 256.5
    agent-3: 256.5
    agent-4: 157.5
    agent-5: 157.5
  sampler_perf:
    mean_env_wait_ms: 24.740181178275275
    mean_inference_ms: 12.390407980756027
    mean_processing_ms: 54.37953387895918
  time_since_restore: 17534.9703104496
  time_this_iter_s: 125.6633608341217
  time_total_s: 64616.515862464905
  timestamp: 1637575675
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 48864000
  training_iteration: 509
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    509 |          64616.5 | 48864000 |  1620.43 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 0.18
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 30.21
    apples_agent-1_min: 1
    apples_agent-2_max: 402
    apples_agent-2_mean: 343.05
    apples_agent-2_min: 4
    apples_agent-3_max: 283
    apples_agent-3_mean: 173.23
    apples_agent-3_min: 10
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 375.11
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 430.2
    cleaning_beam_agent-0_min: 275
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.75
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 58
    cleaning_beam_agent-2_mean: 5.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 37.72
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 421.14
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 3.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-10-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1789.0
  episode_reward_mean: 1597.62
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 48960
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18684.344
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39875859022140503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013191147008910775
        model: {}
        policy_loss: -0.0015211354475468397
        total_loss: 0.0014530504122376442
        vf_explained_var: 0.09509050846099854
        vf_loss: 36.76003646850586
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2418748140335083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010786834172904491
        model: {}
        policy_loss: -0.0022333813831210136
        total_loss: 0.0008825738914310932
        vf_explained_var: 0.1274741142988205
        vf_loss: 35.41650390625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3148189187049866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010818152222782373
        model: {}
        policy_loss: -0.0019984482787549496
        total_loss: 0.004967763088643551
        vf_explained_var: 0.11311708390712738
        vf_loss: 75.20289611816406
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6944894194602966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014414363540709019
        model: {}
        policy_loss: -0.00199111457914114
        total_loss: 0.0052696717903018
        vf_explained_var: 0.0002603083848953247
        vf_loss: 84.83089447021484
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0583113431930542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025696270167827606
        model: {}
        policy_loss: -0.0023708678781986237
        total_loss: -0.0006917179562151432
        vf_explained_var: 0.020811736583709717
        vf_loss: 35.417781829833984
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26470518112182617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006765011930838227
        model: {}
        policy_loss: -0.002063528634607792
        total_loss: 0.0006432058289647102
        vf_explained_var: 0.11950550973415375
        vf_loss: 31.726133346557617
    load_time_ms: 13314.586
    num_steps_sampled: 48960000
    num_steps_trained: 48960000
    sample_time_ms: 93179.597
    update_time_ms: 19.013
  iterations_since_restore: 140
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.13314606741574
    ram_util_percent: 16.60056179775281
  pid: 21723
  policy_reward_max:
    agent-0: 282.0
    agent-1: 282.0
    agent-2: 402.0
    agent-3: 402.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 235.15
    agent-1: 235.15
    agent-2: 340.93
    agent-3: 340.93
    agent-4: 222.73
    agent-5: 222.73
  policy_reward_min:
    agent-0: 5.0
    agent-1: 5.0
    agent-2: 16.5
    agent-3: 16.5
    agent-4: 13.0
    agent-5: 13.0
  sampler_perf:
    mean_env_wait_ms: 24.739316469096348
    mean_inference_ms: 12.390823216775978
    mean_processing_ms: 54.38070798208721
  time_since_restore: 17660.837139844894
  time_this_iter_s: 125.86682939529419
  time_total_s: 64742.3826918602
  timestamp: 1637575801
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 48960000
  training_iteration: 510
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    510 |          64742.4 | 48960000 |  1597.62 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 0.31
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 30.47
    apples_agent-1_min: 15
    apples_agent-2_max: 410
    apples_agent-2_mean: 345.91
    apples_agent-2_min: 240
    apples_agent-3_max: 245
    apples_agent-3_mean: 175.09
    apples_agent-3_min: 110
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 383.9
    apples_agent-5_min: 279
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 437.08
    cleaning_beam_agent-0_min: 384
    cleaning_beam_agent-1_max: 29
    cleaning_beam_agent-1_mean: 2.16
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 33
    cleaning_beam_agent-2_mean: 5.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 38.41
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 403.96
    cleaning_beam_agent-4_min: 295
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 2.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-12-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1812.0
  episode_reward_mean: 1622.97
  episode_reward_min: 1225.0
  episodes_this_iter: 96
  episodes_total: 49056
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18689.162
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40326666831970215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013629215536639094
        model: {}
        policy_loss: -0.0011921171098947525
        total_loss: 0.001221733633428812
        vf_explained_var: 0.005535870790481567
        vf_loss: 31.236019134521484
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24342018365859985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008082757121883333
        model: {}
        policy_loss: -0.0017109280452132225
        total_loss: 0.000808478333055973
        vf_explained_var: 0.06352920830249786
        vf_loss: 29.47825050354004
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31044626235961914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010235755471512675
        model: {}
        policy_loss: -0.0016618026420474052
        total_loss: 0.00476805679500103
        vf_explained_var: 0.05403381586074829
        vf_loss: 69.76243591308594
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6755800843238831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016238942043855786
        model: {}
        policy_loss: -0.0022278500255197287
        total_loss: 0.004017608240246773
        vf_explained_var: -0.005394235253334045
        vf_loss: 74.34479522705078
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0587140321731567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028375142719596624
        model: {}
        policy_loss: -0.0020123112481087446
        total_loss: -0.0007516158511862159
        vf_explained_var: 0.01131555438041687
        vf_loss: 31.240318298339844
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2503453195095062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006693065515719354
        model: {}
        policy_loss: -0.0016171406023204327
        total_loss: 0.0008189752697944641
        vf_explained_var: 0.08817952871322632
        vf_loss: 28.767210006713867
    load_time_ms: 13304.897
    num_steps_sampled: 49056000
    num_steps_trained: 49056000
    sample_time_ms: 93087.093
    update_time_ms: 19.124
  iterations_since_restore: 141
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.06201117318435
    ram_util_percent: 16.694413407821227
  pid: 21723
  policy_reward_max:
    agent-0: 265.5
    agent-1: 265.5
    agent-2: 422.0
    agent-3: 422.0
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 237.985
    agent-1: 237.985
    agent-2: 346.285
    agent-3: 346.285
    agent-4: 227.215
    agent-5: 227.215
  policy_reward_min:
    agent-0: 183.5
    agent-1: 183.5
    agent-2: 267.0
    agent-3: 267.0
    agent-4: 162.0
    agent-5: 162.0
  sampler_perf:
    mean_env_wait_ms: 24.73725380407843
    mean_inference_ms: 12.390572016126947
    mean_processing_ms: 54.3792904885847
  time_since_restore: 17785.23520708084
  time_this_iter_s: 124.39806723594666
  time_total_s: 64866.780759096146
  timestamp: 1637575926
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 49056000
  training_iteration: 511
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    511 |          64866.8 | 49056000 |  1622.97 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 31.61
    apples_agent-1_min: 14
    apples_agent-2_max: 408
    apples_agent-2_mean: 348.0
    apples_agent-2_min: 259
    apples_agent-3_max: 251
    apples_agent-3_mean: 174.88
    apples_agent-3_min: 86
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 383.37
    apples_agent-5_min: 272
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 432.41
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.92
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 4.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 39.28
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 406.08
    cleaning_beam_agent-4_min: 298
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-14-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1760.0
  episode_reward_mean: 1624.15
  episode_reward_min: 1147.0
  episodes_this_iter: 96
  episodes_total: 49152
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18662.025
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4032212197780609
        entropy_coeff: 0.0017600000137463212
        kl: 0.001644932315684855
        model: {}
        policy_loss: -0.0013081156648695469
        total_loss: 0.0015652282163500786
        vf_explained_var: 0.024880945682525635
        vf_loss: 35.83013916015625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24090741574764252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007049073465168476
        model: {}
        policy_loss: -0.0016638506203889847
        total_loss: 0.0012549972161650658
        vf_explained_var: 0.09139384329319
        vf_loss: 33.428436279296875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3101712763309479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010053724981844425
        model: {}
        policy_loss: -0.0018995516002178192
        total_loss: 0.004535012878477573
        vf_explained_var: 0.07939141988754272
        vf_loss: 69.8046646118164
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6790670156478882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005487585440278053
        model: {}
        policy_loss: -0.001936309039592743
        total_loss: 0.004411851987242699
        vf_explained_var: 0.008923590183258057
        vf_loss: 75.43319702148438
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.047982931137085
        entropy_coeff: 0.0017600000137463212
        kl: 0.002100336831063032
        model: {}
        policy_loss: -0.00219270889647305
        total_loss: -0.001086891395971179
        vf_explained_var: 0.03411881625652313
        vf_loss: 29.502696990966797
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2469068020582199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008497273665852845
        model: {}
        policy_loss: -0.00193178525660187
        total_loss: 0.0004213188076391816
        vf_explained_var: 0.08751007914543152
        vf_loss: 27.876583099365234
    load_time_ms: 13298.23
    num_steps_sampled: 49152000
    num_steps_trained: 49152000
    sample_time_ms: 92984.498
    update_time_ms: 19.152
  iterations_since_restore: 142
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.78192090395481
    ram_util_percent: 16.66723163841808
  pid: 21723
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 389.0
    agent-3: 389.0
    agent-4: 255.0
    agent-5: 255.0
  policy_reward_mean:
    agent-0: 239.055
    agent-1: 239.055
    agent-2: 346.01
    agent-3: 346.01
    agent-4: 227.01
    agent-5: 227.01
  policy_reward_min:
    agent-0: 138.5
    agent-1: 138.5
    agent-2: 265.5
    agent-3: 265.5
    agent-4: 169.5
    agent-5: 169.5
  sampler_perf:
    mean_env_wait_ms: 24.73441275997897
    mean_inference_ms: 12.39010896366482
    mean_processing_ms: 54.37706798679395
  time_since_restore: 17909.58384156227
  time_this_iter_s: 124.34863448143005
  time_total_s: 64991.129393577576
  timestamp: 1637576051
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 49152000
  training_iteration: 512
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    512 |          64991.1 | 49152000 |  1624.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.49
    apples_agent-1_min: 13
    apples_agent-2_max: 402
    apples_agent-2_mean: 350.64
    apples_agent-2_min: 177
    apples_agent-3_max: 264
    apples_agent-3_mean: 174.28
    apples_agent-3_min: 82
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 383.85
    apples_agent-5_min: 67
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 424.0
    cleaning_beam_agent-0_min: 381
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 2.3
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 4.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 41.27
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 425.15
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-16-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1765.0
  episode_reward_mean: 1630.15
  episode_reward_min: 616.0
  episodes_this_iter: 96
  episodes_total: 49248
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18677.556
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39551597833633423
        entropy_coeff: 0.0017600000137463212
        kl: 0.001397741143591702
        model: {}
        policy_loss: -0.0015805861912667751
        total_loss: 0.0012654345482587814
        vf_explained_var: 0.009299606084823608
        vf_loss: 35.42123031616211
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2449743002653122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011425144039094448
        model: {}
        policy_loss: -0.0021259274799376726
        total_loss: 0.0007002216298133135
        vf_explained_var: 0.08829228579998016
        vf_loss: 32.57307052612305
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30954229831695557
        entropy_coeff: 0.0017600000137463212
        kl: 0.001812780275940895
        model: {}
        policy_loss: -0.00220027519389987
        total_loss: 0.004553282167762518
        vf_explained_var: 0.06896927952766418
        vf_loss: 72.98352813720703
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6911276578903198
        entropy_coeff: 0.0017600000137463212
        kl: 0.001235340372659266
        model: {}
        policy_loss: -0.0020685982890427113
        total_loss: 0.004712570458650589
        vf_explained_var: -0.01408606767654419
        vf_loss: 79.97552490234375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.041589617729187
        entropy_coeff: 0.0017600000137463212
        kl: 0.001151657197624445
        model: {}
        policy_loss: -0.0019141393713653088
        total_loss: -0.00041442038491368294
        vf_explained_var: 0.04262764751911163
        vf_loss: 33.3292121887207
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24814563989639282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007316839182749391
        model: {}
        policy_loss: -0.001725249458104372
        total_loss: 0.000902894011233002
        vf_explained_var: 0.11915254592895508
        vf_loss: 30.648807525634766
    load_time_ms: 13325.058
    num_steps_sampled: 49248000
    num_steps_trained: 49248000
    sample_time_ms: 92924.416
    update_time_ms: 19.093
  iterations_since_restore: 143
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 57.78089887640449
    ram_util_percent: 16.606741573033705
  pid: 21723
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 400.5
    agent-3: 400.5
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 238.44
    agent-1: 238.44
    agent-2: 348.105
    agent-3: 348.105
    agent-4: 228.53
    agent-5: 228.53
  policy_reward_min:
    agent-0: 119.0
    agent-1: 119.0
    agent-2: 148.5
    agent-3: 148.5
    agent-4: 40.5
    agent-5: 40.5
  sampler_perf:
    mean_env_wait_ms: 24.731975682344327
    mean_inference_ms: 12.389870532053804
    mean_processing_ms: 54.37638238205909
  time_since_restore: 18034.57368850708
  time_this_iter_s: 124.98984694480896
  time_total_s: 65116.119240522385
  timestamp: 1637576176
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 49248000
  training_iteration: 513
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    513 |          65116.1 | 49248000 |  1630.15 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.33
    apples_agent-1_min: 13
    apples_agent-2_max: 398
    apples_agent-2_mean: 347.38
    apples_agent-2_min: 206
    apples_agent-3_max: 244
    apples_agent-3_mean: 181.78
    apples_agent-3_min: 99
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 381.12
    apples_agent-5_min: 117
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 423.56
    cleaning_beam_agent-0_min: 351
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 2.21
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 5.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 41.07
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 436.1
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.67
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-18-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1757.0
  episode_reward_mean: 1629.19
  episode_reward_min: 955.0
  episodes_this_iter: 96
  episodes_total: 49344
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18683.33
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39348462224006653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025852113030850887
        model: {}
        policy_loss: -0.0015918577555567026
        total_loss: 0.0011888784356415272
        vf_explained_var: 0.010945037007331848
        vf_loss: 34.732669830322266
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2456246018409729
        entropy_coeff: 0.0017600000137463212
        kl: 0.001477222191169858
        model: {}
        policy_loss: -0.001775657758116722
        total_loss: 0.0010871682316064835
        vf_explained_var: 0.062332525849342346
        vf_loss: 32.95123291015625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3104284107685089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009550479589961469
        model: {}
        policy_loss: -0.002061977982521057
        total_loss: 0.004430967383086681
        vf_explained_var: 0.07830348610877991
        vf_loss: 70.39300537109375
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6754812002182007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012679313076660037
        model: {}
        policy_loss: -0.0020347638055682182
        total_loss: 0.004282660316675901
        vf_explained_var: 0.02302730083465576
        vf_loss: 75.062744140625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0258058309555054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024536720011383295
        model: {}
        policy_loss: -0.0023038750514388084
        total_loss: -0.000925273634493351
        vf_explained_var: 0.03219744563102722
        vf_loss: 31.8402042388916
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25544923543930054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009562070481479168
        model: {}
        policy_loss: -0.0017384550301358104
        total_loss: 0.000762090552598238
        vf_explained_var: 0.0983361005783081
        vf_loss: 29.501346588134766
    load_time_ms: 13337.439
    num_steps_sampled: 49344000
    num_steps_trained: 49344000
    sample_time_ms: 92889.751
    update_time_ms: 18.603
  iterations_since_restore: 144
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 58.67808988764045
    ram_util_percent: 16.579775280898875
  pid: 21723
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 398.0
    agent-3: 398.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 238.56
    agent-1: 238.56
    agent-2: 350.68
    agent-3: 350.68
    agent-4: 225.355
    agent-5: 225.355
  policy_reward_min:
    agent-0: 154.5
    agent-1: 154.5
    agent-2: 226.0
    agent-3: 226.0
    agent-4: 77.5
    agent-5: 77.5
  sampler_perf:
    mean_env_wait_ms: 24.729452739974366
    mean_inference_ms: 12.389613155987135
    mean_processing_ms: 54.374167361909734
  time_since_restore: 18159.706450223923
  time_this_iter_s: 125.13276171684265
  time_total_s: 65241.25200223923
  timestamp: 1637576301
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 49344000
  training_iteration: 514
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    514 |          65241.3 | 49344000 |  1629.19 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 31.09
    apples_agent-1_min: 17
    apples_agent-2_max: 400
    apples_agent-2_mean: 350.49
    apples_agent-2_min: 282
    apples_agent-3_max: 245
    apples_agent-3_mean: 176.08
    apples_agent-3_min: 93
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 458
    apples_agent-5_mean: 390.7
    apples_agent-5_min: 305
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 412.42
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 5.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 38.86
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 458.81
    cleaning_beam_agent-4_min: 368
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-20-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1801.0
  episode_reward_mean: 1644.83
  episode_reward_min: 1282.0
  episodes_this_iter: 96
  episodes_total: 49440
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18645.875
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38447779417037964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009443501476198435
        model: {}
        policy_loss: -0.0011303292121738195
        total_loss: 0.001657775486819446
        vf_explained_var: 0.028153657913208008
        vf_loss: 34.64786148071289
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24787068367004395
        entropy_coeff: 0.0017600000137463212
        kl: 0.001190229901112616
        model: {}
        policy_loss: -0.0017331992276012897
        total_loss: 0.0011602819431573153
        vf_explained_var: 0.06597843766212463
        vf_loss: 33.29730987548828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3078591227531433
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008570067002438009
        model: {}
        policy_loss: -0.0015821473207324743
        total_loss: 0.004901392851024866
        vf_explained_var: 0.06041572988033295
        vf_loss: 70.25371551513672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6783930063247681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008562256116420031
        model: {}
        policy_loss: -0.001981748268008232
        total_loss: 0.004331881180405617
        vf_explained_var: 0.002596929669380188
        vf_loss: 75.07601928710938
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0042966604232788
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013917419128119946
        model: {}
        policy_loss: -0.0018198241014033556
        total_loss: -0.00044262432493269444
        vf_explained_var: 0.04794420301914215
        vf_loss: 31.447616577148438
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2518470883369446
        entropy_coeff: 0.0017600000137463212
        kl: 0.001108849886804819
        model: {}
        policy_loss: -0.0019621248356997967
        total_loss: 0.0005734474398195744
        vf_explained_var: 0.08882495760917664
        vf_loss: 29.788238525390625
    load_time_ms: 13308.427
    num_steps_sampled: 49440000
    num_steps_trained: 49440000
    sample_time_ms: 92868.385
    update_time_ms: 18.477
  iterations_since_restore: 145
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 56.2505617977528
    ram_util_percent: 15.923595505617975
  pid: 21723
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 404.0
    agent-3: 404.0
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 240.265
    agent-1: 240.265
    agent-2: 350.805
    agent-3: 350.805
    agent-4: 231.345
    agent-5: 231.345
  policy_reward_min:
    agent-0: 176.0
    agent-1: 176.0
    agent-2: 266.0
    agent-3: 266.0
    agent-4: 190.5
    agent-5: 190.5
  sampler_perf:
    mean_env_wait_ms: 24.728631543236983
    mean_inference_ms: 12.389520889905837
    mean_processing_ms: 54.37125790571659
  time_since_restore: 18284.22657108307
  time_this_iter_s: 124.52012085914612
  time_total_s: 65365.77212309837
  timestamp: 1637576426
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 49440000
  training_iteration: 515
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    515 |          65365.8 | 49440000 |  1644.83 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 30.28
    apples_agent-1_min: 14
    apples_agent-2_max: 425
    apples_agent-2_mean: 347.91
    apples_agent-2_min: 246
    apples_agent-3_max: 263
    apples_agent-3_mean: 172.95
    apples_agent-3_min: 101
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 386.92
    apples_agent-5_min: 257
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 416.63
    cleaning_beam_agent-0_min: 382
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.65
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 5.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 38.82
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 457.58
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-22-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1769.0
  episode_reward_mean: 1642.18
  episode_reward_min: 1180.0
  episodes_this_iter: 96
  episodes_total: 49536
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18628.375
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38360536098480225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017076104413717985
        model: {}
        policy_loss: -0.0012439307756721973
        total_loss: 0.00166369101498276
        vf_explained_var: 0.011136934161186218
        vf_loss: 35.82767105102539
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24462249875068665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007009434048086405
        model: {}
        policy_loss: -0.0016070015262812376
        total_loss: 0.0012835809029638767
        vf_explained_var: 0.08333717286586761
        vf_loss: 33.21118927001953
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3060127794742584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009865568717941642
        model: {}
        policy_loss: -0.001863008365035057
        total_loss: 0.004578080493956804
        vf_explained_var: 0.06719663739204407
        vf_loss: 69.79670715332031
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6778585910797119
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005099700065329671
        model: {}
        policy_loss: -0.0017701713368296623
        total_loss: 0.004561006557196379
        vf_explained_var: -0.0035735368728637695
        vf_loss: 75.24211120605469
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0085092782974243
        entropy_coeff: 0.0017600000137463212
        kl: 0.001927071250975132
        model: {}
        policy_loss: -0.0020430029835551977
        total_loss: -0.0007394477725028992
        vf_explained_var: 0.06759020686149597
        vf_loss: 30.785280227661133
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25486722588539124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010389768285676837
        model: {}
        policy_loss: -0.001886291429400444
        total_loss: 0.0006803895812481642
        vf_explained_var: 0.08771958947181702
        vf_loss: 30.152484893798828
    load_time_ms: 13289.397
    num_steps_sampled: 49536000
    num_steps_trained: 49536000
    sample_time_ms: 92747.283
    update_time_ms: 18.279
  iterations_since_restore: 146
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 51.182285714285726
    ram_util_percent: 15.43371428571428
  pid: 21723
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 413.5
    agent-3: 413.5
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 243.785
    agent-1: 243.785
    agent-2: 348.65
    agent-3: 348.65
    agent-4: 228.655
    agent-5: 228.655
  policy_reward_min:
    agent-0: 163.0
    agent-1: 163.0
    agent-2: 267.0
    agent-3: 267.0
    agent-4: 159.5
    agent-5: 159.5
  sampler_perf:
    mean_env_wait_ms: 24.72540862487712
    mean_inference_ms: 12.38861340152728
    mean_processing_ms: 54.36278142428016
  time_since_restore: 18406.892395734787
  time_this_iter_s: 122.66582465171814
  time_total_s: 65488.43794775009
  timestamp: 1637576549
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 49536000
  training_iteration: 516
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    516 |          65488.4 | 49536000 |  1642.18 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 32.18
    apples_agent-1_min: 6
    apples_agent-2_max: 425
    apples_agent-2_mean: 348.76
    apples_agent-2_min: 65
    apples_agent-3_max: 238
    apples_agent-3_mean: 176.42
    apples_agent-3_min: 53
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 470
    apples_agent-5_mean: 387.09
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 423.82
    cleaning_beam_agent-0_min: 273
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 1.71
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 4.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 37.9
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 453.32
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-24-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1742.0
  episode_reward_mean: 1629.05
  episode_reward_min: 325.0
  episodes_this_iter: 96
  episodes_total: 49632
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18654.32
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3885430097579956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014112690696492791
        model: {}
        policy_loss: -0.0017082691192626953
        total_loss: 0.001354346051812172
        vf_explained_var: 0.053573548793792725
        vf_loss: 37.46454620361328
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2466583549976349
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015811354387551546
        model: {}
        policy_loss: -0.0021862718276679516
        total_loss: 0.0009273337200284004
        vf_explained_var: 0.10297040641307831
        vf_loss: 35.47721862792969
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.308902382850647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012857748661190271
        model: {}
        policy_loss: -0.0018683485686779022
        total_loss: 0.004973102360963821
        vf_explained_var: 0.10918797552585602
        vf_loss: 73.85120391845703
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6745709180831909
        entropy_coeff: 0.0017600000137463212
        kl: 0.001165656023658812
        model: {}
        policy_loss: -0.0019788879435509443
        total_loss: 0.005084780976176262
        vf_explained_var: 0.009532168507575989
        vf_loss: 82.50916290283203
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9999151229858398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019904118962585926
        model: {}
        policy_loss: -0.0020231176167726517
        total_loss: -0.00013846243382431567
        vf_explained_var: 0.024610459804534912
        vf_loss: 36.445045471191406
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2607155442237854
        entropy_coeff: 0.0017600000137463212
        kl: 0.000884712440893054
        model: {}
        policy_loss: -0.0020600557327270508
        total_loss: 0.0007276004180312157
        vf_explained_var: 0.1296379417181015
        vf_loss: 32.46514129638672
    load_time_ms: 13292.732
    num_steps_sampled: 49632000
    num_steps_trained: 49632000
    sample_time_ms: 92625.197
    update_time_ms: 17.95
  iterations_since_restore: 147
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.309039548022604
    ram_util_percent: 15.311864406779662
  pid: 21723
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 390.0
    agent-3: 390.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 239.465
    agent-1: 239.465
    agent-2: 346.945
    agent-3: 346.945
    agent-4: 228.115
    agent-5: 228.115
  policy_reward_min:
    agent-0: 39.0
    agent-1: 39.0
    agent-2: 77.5
    agent-3: 77.5
    agent-4: 46.0
    agent-5: 46.0
  sampler_perf:
    mean_env_wait_ms: 24.722426486980613
    mean_inference_ms: 12.3874843343017
    mean_processing_ms: 54.356784243756465
  time_since_restore: 18531.386353731155
  time_this_iter_s: 124.49395799636841
  time_total_s: 65612.93190574646
  timestamp: 1637576673
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 49632000
  training_iteration: 517
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    517 |          65612.9 | 49632000 |  1629.05 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.86
    apples_agent-1_min: 6
    apples_agent-2_max: 405
    apples_agent-2_mean: 345.47
    apples_agent-2_min: 65
    apples_agent-3_max: 257
    apples_agent-3_mean: 183.13
    apples_agent-3_min: 53
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.25
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 378.87
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 420.47
    cleaning_beam_agent-0_min: 273
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 4.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 33.91
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 441.47
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 2.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-26-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1813.0
  episode_reward_mean: 1624.17
  episode_reward_min: 325.0
  episodes_this_iter: 96
  episodes_total: 49728
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18658.86
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3868461847305298
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016769961221143603
        model: {}
        policy_loss: -0.001441247295588255
        total_loss: 0.0012294335756450891
        vf_explained_var: 0.009966656565666199
        vf_loss: 33.51530075073242
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2507070302963257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011754374718293548
        model: {}
        policy_loss: -0.0018490706570446491
        total_loss: 0.0008060407126322389
        vf_explained_var: 0.08731880784034729
        vf_loss: 30.96360969543457
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3111059367656708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013090528082102537
        model: {}
        policy_loss: -0.0020382623188197613
        total_loss: 0.004814072512090206
        vf_explained_var: 0.06457014381885529
        vf_loss: 73.99883270263672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6661617159843445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014456211356446147
        model: {}
        policy_loss: -0.0019940491765737534
        total_loss: 0.004990493878722191
        vf_explained_var: -0.02123183012008667
        vf_loss: 81.56989288330078
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.998575747013092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017624888569116592
        model: {}
        policy_loss: -0.0017772925784811378
        total_loss: -0.0005923000862821937
        vf_explained_var: 0.05333590507507324
        vf_loss: 29.424850463867188
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26082414388656616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005391498561948538
        model: {}
        policy_loss: -0.0016655542422086
        total_loss: 0.0007144166738726199
        vf_explained_var: 0.08737361431121826
        vf_loss: 28.390233993530273
    load_time_ms: 13302.04
    num_steps_sampled: 49728000
    num_steps_trained: 49728000
    sample_time_ms: 92554.093
    update_time_ms: 17.785
  iterations_since_restore: 148
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.60397727272727
    ram_util_percent: 15.344886363636363
  pid: 21723
  policy_reward_max:
    agent-0: 267.0
    agent-1: 267.0
    agent-2: 394.5
    agent-3: 394.5
    agent-4: 251.0
    agent-5: 251.0
  policy_reward_mean:
    agent-0: 238.585
    agent-1: 238.585
    agent-2: 348.97
    agent-3: 348.97
    agent-4: 224.53
    agent-5: 224.53
  policy_reward_min:
    agent-0: 39.0
    agent-1: 39.0
    agent-2: 77.5
    agent-3: 77.5
    agent-4: 46.0
    agent-5: 46.0
  sampler_perf:
    mean_env_wait_ms: 24.719168619593596
    mean_inference_ms: 12.386666140591373
    mean_processing_ms: 54.35112640561315
  time_since_restore: 18655.391014814377
  time_this_iter_s: 124.00466108322144
  time_total_s: 65736.93656682968
  timestamp: 1637576797
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 49728000
  training_iteration: 518
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    518 |          65736.9 | 49728000 |  1624.17 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 32.82
    apples_agent-1_min: 16
    apples_agent-2_max: 405
    apples_agent-2_mean: 354.37
    apples_agent-2_min: 245
    apples_agent-3_max: 262
    apples_agent-3_mean: 190.31
    apples_agent-3_min: 107
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 384.18
    apples_agent-5_min: 281
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 417.36
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.61
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 4.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 34.92
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 441.96
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-28-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1794.0
  episode_reward_mean: 1651.35
  episode_reward_min: 1241.0
  episodes_this_iter: 96
  episodes_total: 49824
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18639.437
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3817279040813446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012026835465803742
        model: {}
        policy_loss: -0.001132710138335824
        total_loss: 0.0015911096706986427
        vf_explained_var: 0.024272292852401733
        vf_loss: 33.956600189208984
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2504050135612488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016372852260246873
        model: {}
        policy_loss: -0.001878206618130207
        total_loss: 0.0007989592850208282
        vf_explained_var: 0.1046050488948822
        vf_loss: 31.17878532409668
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.306498259305954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011033117771148682
        model: {}
        policy_loss: -0.0018434086814522743
        total_loss: 0.005003945901989937
        vf_explained_var: 0.06170763075351715
        vf_loss: 73.8678970336914
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6620125770568848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010232424829155207
        model: {}
        policy_loss: -0.0018662139773368835
        total_loss: 0.0049913180992007256
        vf_explained_var: -0.008895829319953918
        vf_loss: 80.22671508789062
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.000024437904358
        entropy_coeff: 0.0017600000137463212
        kl: 0.001578459283336997
        model: {}
        policy_loss: -0.002023341367021203
        total_loss: -0.0006892727687954903
        vf_explained_var: 0.04594835638999939
        vf_loss: 30.941129684448242
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2560504972934723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010616743238642812
        model: {}
        policy_loss: -0.0018843011930584908
        total_loss: 0.0005958573892712593
        vf_explained_var: 0.09684143960475922
        vf_loss: 29.30809211730957
    load_time_ms: 13278.988
    num_steps_sampled: 49824000
    num_steps_trained: 49824000
    sample_time_ms: 92476.14
    update_time_ms: 17.792
  iterations_since_restore: 149
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.7954802259887
    ram_util_percent: 15.309604519774009
  pid: 21723
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 405.0
    agent-3: 405.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 241.35
    agent-1: 241.35
    agent-2: 356.005
    agent-3: 356.005
    agent-4: 228.32
    agent-5: 228.32
  policy_reward_min:
    agent-0: 163.0
    agent-1: 163.0
    agent-2: 281.5
    agent-3: 281.5
    agent-4: 176.0
    agent-5: 176.0
  sampler_perf:
    mean_env_wait_ms: 24.71685875423587
    mean_inference_ms: 12.386489710407226
    mean_processing_ms: 54.34887966963321
  time_since_restore: 18779.86007118225
  time_this_iter_s: 124.46905636787415
  time_total_s: 65861.40562319756
  timestamp: 1637576922
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 49824000
  training_iteration: 519
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    519 |          65861.4 | 49824000 |  1651.35 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.23
    apples_agent-1_min: 17
    apples_agent-2_max: 409
    apples_agent-2_mean: 348.46
    apples_agent-2_min: 190
    apples_agent-3_max: 240
    apples_agent-3_mean: 179.68
    apples_agent-3_min: 78
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 387.52
    apples_agent-5_min: 184
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 427.44
    cleaning_beam_agent-0_min: 388
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.46
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 4.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 35.63
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 500
    cleaning_beam_agent-4_mean: 421.07
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-30-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1783.0
  episode_reward_mean: 1638.39
  episode_reward_min: 889.0
  episodes_this_iter: 96
  episodes_total: 49920
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18649.093
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3850483298301697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012496308190748096
        model: {}
        policy_loss: -0.0012991658877581358
        total_loss: 0.0015611790586262941
        vf_explained_var: 0.01798146963119507
        vf_loss: 35.380279541015625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25185489654541016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007862207712605596
        model: {}
        policy_loss: -0.0017170459032058716
        total_loss: 0.0010929452255368233
        vf_explained_var: 0.09620238840579987
        vf_loss: 32.53257369995117
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3108857274055481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012846130412071943
        model: {}
        policy_loss: -0.001782757113687694
        total_loss: 0.004784023854881525
        vf_explained_var: 0.06020061671733856
        vf_loss: 71.13937377929688
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6668317914009094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015514179831370711
        model: {}
        policy_loss: -0.0018258867785334587
        total_loss: 0.004645143635571003
        vf_explained_var: -0.006128385663032532
        vf_loss: 76.446533203125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9969245791435242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019950063433498144
        model: {}
        policy_loss: -0.002083605621010065
        total_loss: -0.0006965472712181509
        vf_explained_var: 0.04053652286529541
        vf_loss: 31.416446685791016
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25918495655059814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004121472011320293
        model: {}
        policy_loss: -0.0016019330359995365
        total_loss: 0.0008452967740595341
        vf_explained_var: 0.11827792227268219
        vf_loss: 29.033952713012695
    load_time_ms: 13278.314
    num_steps_sampled: 49920000
    num_steps_trained: 49920000
    sample_time_ms: 92315.892
    update_time_ms: 18.138
  iterations_since_restore: 150
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 55.78418079096045
    ram_util_percent: 15.390960451977403
  pid: 21723
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 390.0
    agent-3: 390.0
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 239.13
    agent-1: 239.13
    agent-2: 351.6
    agent-3: 351.6
    agent-4: 228.465
    agent-5: 228.465
  policy_reward_min:
    agent-0: 139.0
    agent-1: 139.0
    agent-2: 192.5
    agent-3: 192.5
    agent-4: 113.0
    agent-5: 113.0
  sampler_perf:
    mean_env_wait_ms: 24.71238345491731
    mean_inference_ms: 12.385511960351941
    mean_processing_ms: 54.343666568419195
  time_since_restore: 18904.192113637924
  time_this_iter_s: 124.33204245567322
  time_total_s: 65985.73766565323
  timestamp: 1637577047
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 49920000
  training_iteration: 520
  trial_id: '00000'
  
[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/38.13 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.139:21723 |    520 |          65985.7 | 49920000 |  1638.39 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=21723)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f910e4d8550> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.21
    apples_agent-1_min: 17
    apples_agent-2_max: 397
    apples_agent-2_mean: 348.27
    apples_agent-2_min: 190
    apples_agent-3_max: 252
    apples_agent-3_mean: 183.43
    apples_agent-3_min: 78
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.3
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 386.68
    apples_agent-5_min: 184
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 432.05
    cleaning_beam_agent-0_min: 390
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.75
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 4.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 35.44
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 440.31
    cleaning_beam_agent-4_min: 350
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_05-32-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1802.0
  episode_reward_mean: 1648.64
  episode_reward_min: 889.0
  episodes_this_iter: 96
  episodes_total: 50016
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu139.cluster.local
  info:
    grad_time_ms: 18645.072
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3782256841659546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012074108235538006
        model: {}
        policy_loss: -0.0013646837323904037
        total_loss: 0.0013326643966138363
        vf_explained_var: -0.0011471658945083618
        vf_loss: 33.63025665283203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24999378621578217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007690670900046825
        model: {}
        policy_loss: -0.0014950111508369446
        total_loss: 0.0011616582050919533
        vf_explained_var: 0.076565220952034
        vf_loss: 30.966583251953125
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3031197786331177
        entropy_coeff: 0.0017600000137463212
        kl: 0.001682516885921359
        model: {}
        policy_loss: -0.0019205481512472034
        total_loss: 0.0044234562665224075
        vf_explained_var: 0.04900144040584564
        vf_loss: 68.77496337890625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.667500913143158
        entropy_coeff: 0.0017600000137463212
        kl: 0.000744979246519506
        model: {}
        policy_loss: -0.001945434371009469
        total_loss: 0.004254769999533892
        vf_explained_var: -0.007809668779373169
        vf_loss: 73.75007629394531
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9795905947685242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015430280473083258
        model: {}
        policy_loss: -0.0017388607375323772
        total_loss: -0.00041325436905026436
        vf_explained_var: 0.04399944841861725
        vf_loss: 30.496826171875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2562214434146881
        entropy_coeff: 0.0017600000137463212
        kl: 0.000971779169049114
        model: {}
        policy_loss: -0.0016452675918117166
        total_loss: 0.0007935265311971307
        vf_explained_var: 0.09946219623088837
        vf_loss: 28.89740753173828
    load_time_ms: 13278.582
    num_steps_sampled: 50016000
    num_steps_trained: 50016000
    sample_time_ms: 92364.775
    update_time_ms: 17.986
  iterations_since_restore: 151
  node_ip: 172.17.8.139
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 54.86368715083798
    ram_util_percent: 15.266480446927373
  pid: 21723
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 397.0
    agent-3: 397.0
    agent-4: 254.5
    agent-5: 254.5
  policy_reward_mean:
    agent-0: 242.525
    agent-1: 242.525
    agent-2: 353.25
    agent-3: 353.25
    agent-4: 228.545
    agent-5: 228.545
  policy_reward_min:
    agent-0: 139.0
    agent-1: 139.0
    agent-2: 192.5
    agent-3: 192.5
    agent-4: 113.0
    agent-5: 113.0
  sampler_perf:
    mean_env_wait_ms: 24.710319696986318
    mean_inference_ms: 12.385131694217305
    mean_processing_ms: 54.34044354904372
  time_since_restore: 19029.04409480095
  time_this_iter_s: 124.8519811630249
  time_total_s: 66110.58964681625
  timestamp: 1637577173
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 50016000
  training_iteration: 521
  trial_id: '00000'
  >>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-21_10-26-43brn7k2yg/checkpoint_650
== Status ==
Memory usage on this node: 17.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    650 |          82132.4 | 62400000 |  1717.05 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m 2021-11-22 10:10:17,797	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=28385)[0m 2021-11-22 10:10:17,814	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=28385)[0m 2021-11-22 10:12:10,225	INFO trainable.py:180 -- _setup took 112.427 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=28385)[0m 2021-11-22 10:12:10,225	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=28385)[0m 2021-11-22 10:12:10,226	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=28385)[0m 2021-11-22 10:12:13,569	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=28385)[0m 2021-11-22 10:12:13,569	INFO trainable.py:423 -- Restored on 172.17.8.3 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-21_10-26-43brn7k2yg/tmpe83omhlirestore_from_object/checkpoint-650
[2m[36m(pid=28385)[0m 2021-11-22 10:12:13,570	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 650, '_timesteps_total': 62400000, '_time_total': 82132.44237017632, '_episodes_total': 62400}
== Status ==
Memory usage on this node: 23.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    650 |          82132.4 | 62400000 |  1717.05 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 31.083333333333332
    apples_agent-1_min: 17
    apples_agent-2_max: 418
    apples_agent-2_mean: 355.1458333333333
    apples_agent-2_min: 277
    apples_agent-3_max: 279
    apples_agent-3_mean: 221.6875
    apples_agent-3_min: 148
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.13541666666666666
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 386.71875
    apples_agent-5_min: 331
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 409.7291666666667
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 2.1666666666666665
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 2.28125
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 24.875
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 471.8541666666667
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.3333333333333335
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.010416666666666666
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.020833333333333332
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.020833333333333332
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.041666666666666664
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-14-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1837.0
  episode_reward_mean: 1717.1875
  episode_reward_min: 1395.0
  episodes_this_iter: 96
  episodes_total: 62496
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 22217.33
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36011645197868347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007065928657539189
        model: {}
        policy_loss: -0.001283927820622921
        total_loss: 0.0013894066214561462
        vf_explained_var: 0.0066715627908706665
        vf_loss: 31.658245086669922
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23377269506454468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006118601886555552
        model: {}
        policy_loss: -0.0021848883479833603
        total_loss: 0.00035401509376242757
        vf_explained_var: 0.11305443942546844
        vf_loss: 28.279733657836914
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2836722135543823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007307634223252535
        model: {}
        policy_loss: -0.0021138761658221483
        total_loss: 0.005710462108254433
        vf_explained_var: 0.03565247356891632
        vf_loss: 81.77452087402344
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.531484842300415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007591627072542906
        model: {}
        policy_loss: -0.0019332394003868103
        total_loss: 0.005382699426263571
        vf_explained_var: 0.045311152935028076
        vf_loss: 80.99523162841797
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8827055096626282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011478716041892767
        model: {}
        policy_loss: -0.002166319638490677
        total_loss: -0.0005461582913994789
        vf_explained_var: 0.012834534049034119
        vf_loss: 29.441495895385742
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25514689087867737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007323928293772042
        model: {}
        policy_loss: -0.00203164154663682
        total_loss: 0.00027133990079164505
        vf_explained_var: 0.1311674267053604
        vf_loss: 26.055635452270508
    load_time_ms: 17865.531
    num_steps_sampled: 62496000
    num_steps_trained: 62496000
    sample_time_ms: 103091.325
    update_time_ms: 3384.814
  iterations_since_restore: 1
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.125
    ram_util_percent: 13.653508771929825
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 422.0
    agent-3: 422.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 241.09895833333334
    agent-1: 241.09895833333334
    agent-2: 384.8072916666667
    agent-3: 384.8072916666667
    agent-4: 232.6875
    agent-5: 232.6875
  policy_reward_min:
    agent-0: 198.5
    agent-1: 198.5
    agent-2: 291.5
    agent-3: 291.5
    agent-4: 199.5
    agent-5: 199.5
  sampler_perf:
    mean_env_wait_ms: 26.76120913509047
    mean_inference_ms: 14.244302407606739
    mean_processing_ms: 59.86834874440543
  time_since_restore: 149.82078742980957
  time_this_iter_s: 149.82078742980957
  time_total_s: 82282.26315760612
  timestamp: 1637594089
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 62496000
  training_iteration: 651
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    651 |          82282.3 | 62496000 |  1717.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 32.68
    apples_agent-1_min: 18
    apples_agent-2_max: 402
    apples_agent-2_mean: 353.45
    apples_agent-2_min: 261
    apples_agent-3_max: 280
    apples_agent-3_mean: 221.56
    apples_agent-3_min: 160
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 429
    apples_agent-5_mean: 387.66
    apples_agent-5_min: 285
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 406.76
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 1.96
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 23.85
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 471.93
    cleaning_beam_agent-4_min: 423
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 2.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-16-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1845.0
  episode_reward_mean: 1716.23
  episode_reward_min: 1324.0
  episodes_this_iter: 96
  episodes_total: 62592
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 16933.8
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35247817635536194
        entropy_coeff: 0.0017600000137463212
        kl: 0.000988429062999785
        model: {}
        policy_loss: -0.0014857123605906963
        total_loss: 0.00123507808893919
        vf_explained_var: 0.02946116030216217
        vf_loss: 32.42310333251953
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22973524034023285
        entropy_coeff: 0.0017600000137463212
        kl: 0.00073492486262694
        model: {}
        policy_loss: -0.001723647117614746
        total_loss: 0.0008935821242630482
        vf_explained_var: 0.11889013648033142
        vf_loss: 29.480695724487305
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2870316505432129
        entropy_coeff: 0.0017600000137463212
        kl: 0.000676387338899076
        model: {}
        policy_loss: -0.0016170591115951538
        total_loss: 0.005936501547694206
        vf_explained_var: 0.045965492725372314
        vf_loss: 79.91098022460938
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5250185132026672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005561847938224673
        model: {}
        policy_loss: -0.0014648893848061562
        total_loss: 0.005655820481479168
        vf_explained_var: 0.050662726163864136
        vf_loss: 79.8912353515625
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8800609111785889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010839200112968683
        model: {}
        policy_loss: -0.0019257571548223495
        total_loss: -0.00032010406721383333
        vf_explained_var: 0.010192617774009705
        vf_loss: 30.46167755126953
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25852295756340027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008538545807823539
        model: {}
        policy_loss: -0.0021404572762548923
        total_loss: 5.587516352534294e-05
        vf_explained_var: 0.16577066481113434
        vf_loss: 25.65952491760254
    load_time_ms: 16227.396
    num_steps_sampled: 62592000
    num_steps_trained: 62592000
    sample_time_ms: 102169.97
    update_time_ms: 1698.975
  iterations_since_restore: 2
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.467403314917128
    ram_util_percent: 15.11049723756906
  pid: 28385
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 418.0
    agent-3: 418.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 243.06
    agent-1: 243.06
    agent-2: 383.945
    agent-3: 383.945
    agent-4: 231.11
    agent-5: 231.11
  policy_reward_min:
    agent-0: 181.5
    agent-1: 181.5
    agent-2: 305.0
    agent-3: 305.0
    agent-4: 175.5
    agent-5: 175.5
  sampler_perf:
    mean_env_wait_ms: 26.78402727348558
    mean_inference_ms: 13.62177669827574
    mean_processing_ms: 59.80967860286924
  time_since_restore: 277.45855832099915
  time_this_iter_s: 127.63777089118958
  time_total_s: 82409.90092849731
  timestamp: 1637594217
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 62592000
  training_iteration: 652
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    652 |          82409.9 | 62592000 |  1716.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 31.15
    apples_agent-1_min: 12
    apples_agent-2_max: 400
    apples_agent-2_mean: 350.67
    apples_agent-2_min: 291
    apples_agent-3_max: 278
    apples_agent-3_mean: 219.55
    apples_agent-3_min: 116
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 384.11
    apples_agent-5_min: 327
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 408.86
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.61
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 2.62
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 23.03
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 472.33
    cleaning_beam_agent-4_min: 412
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-19-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1815.0
  episode_reward_mean: 1707.65
  episode_reward_min: 1434.0
  episodes_this_iter: 96
  episodes_total: 62688
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 15177.073
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3521730899810791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011093947105109692
        model: {}
        policy_loss: -0.0012788837775588036
        total_loss: 0.0014622779563069344
        vf_explained_var: 0.016059011220932007
        vf_loss: 33.05516052246094
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22912251949310303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009167961543425918
        model: {}
        policy_loss: -0.0018496792763471603
        total_loss: 0.0007091753650456667
        vf_explained_var: 0.13163542747497559
        vf_loss: 29.162715911865234
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2876659035682678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008878975640982389
        model: {}
        policy_loss: -0.0018902746960520744
        total_loss: 0.0052720834501087666
        vf_explained_var: 0.04681044816970825
        vf_loss: 76.24259185791016
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5326254963874817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007809295784682035
        model: {}
        policy_loss: -0.0016574361361563206
        total_loss: 0.005323215387761593
        vf_explained_var: 0.016569852828979492
        vf_loss: 78.79027557373047
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.882808268070221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017086447915062308
        model: {}
        policy_loss: -0.001927931560203433
        total_loss: -0.00046869576908648014
        vf_explained_var: 0.01763884723186493
        vf_loss: 29.275453567504883
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26161205768585205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008349874406121671
        model: {}
        policy_loss: -0.0019009949173778296
        total_loss: 0.00022093200823292136
        vf_explained_var: 0.1469373106956482
        vf_loss: 25.406164169311523
    load_time_ms: 15683.095
    num_steps_sampled: 62688000
    num_steps_trained: 62688000
    sample_time_ms: 102051.164
    update_time_ms: 1137.664
  iterations_since_restore: 3
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.774863387978144
    ram_util_percent: 15.144808743169405
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 419.5
    agent-3: 419.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 242.04
    agent-1: 242.04
    agent-2: 381.215
    agent-3: 381.215
    agent-4: 230.57
    agent-5: 230.57
  policy_reward_min:
    agent-0: 179.5
    agent-1: 179.5
    agent-2: 313.0
    agent-3: 313.0
    agent-4: 195.0
    agent-5: 195.0
  sampler_perf:
    mean_env_wait_ms: 26.795649403519686
    mean_inference_ms: 13.39352182771275
    mean_processing_ms: 59.959592978887976
  time_since_restore: 405.6530268192291
  time_this_iter_s: 128.19446849822998
  time_total_s: 82538.09539699554
  timestamp: 1637594345
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 62688000
  training_iteration: 653
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    653 |          82538.1 | 62688000 |  1707.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 32.72
    apples_agent-1_min: 1
    apples_agent-2_max: 410
    apples_agent-2_mean: 355.03
    apples_agent-2_min: 7
    apples_agent-3_max: 267
    apples_agent-3_mean: 219.21
    apples_agent-3_min: 7
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 385.37
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 411.13
    cleaning_beam_agent-0_min: 368
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.88
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 2.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 25.25
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 459.36
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 2.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-21-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1857.0
  episode_reward_mean: 1709.04
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 62784
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 14307.171
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35544735193252563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018401944544166327
        model: {}
        policy_loss: -0.001964124385267496
        total_loss: 0.0008373726159334183
        vf_explained_var: 0.08207391202449799
        vf_loss: 33.81079864501953
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2309284806251526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006134785362519324
        model: {}
        policy_loss: -0.0017767827957868576
        total_loss: 0.0008474616333842278
        vf_explained_var: 0.18199361860752106
        vf_loss: 30.153430938720703
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2866538465023041
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009595755836926401
        model: {}
        policy_loss: -0.001946409698575735
        total_loss: 0.006050577852874994
        vf_explained_var: 0.09785622358322144
        vf_loss: 84.77511596679688
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5311675667762756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012438578996807337
        model: {}
        policy_loss: -0.001739567844197154
        total_loss: 0.006567992735654116
        vf_explained_var: 0.02277165651321411
        vf_loss: 92.11319732666016
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8845465183258057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018786859000101686
        model: {}
        policy_loss: -0.00203654239885509
        total_loss: -0.0001585736172273755
        vf_explained_var: 0.0030599236488342285
        vf_loss: 33.878028869628906
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2658415734767914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010557171190157533
        model: {}
        policy_loss: -0.0022863526828587055
        total_loss: 0.00010067014954984188
        vf_explained_var: 0.16748900711536407
        vf_loss: 28.28514289855957
    load_time_ms: 15428.614
    num_steps_sampled: 62784000
    num_steps_trained: 62784000
    sample_time_ms: 102153.855
    update_time_ms: 857.006
  iterations_since_restore: 4
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.28804347826087
    ram_util_percent: 15.176630434782611
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 418.0
    agent-3: 418.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 240.755
    agent-1: 240.755
    agent-2: 383.755
    agent-3: 383.755
    agent-4: 230.01
    agent-5: 230.01
  policy_reward_min:
    agent-0: 6.5
    agent-1: 6.5
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 11.0
    agent-5: 11.0
  sampler_perf:
    mean_env_wait_ms: 26.756600067169437
    mean_inference_ms: 13.293954015401944
    mean_processing_ms: 59.97127064286119
  time_since_restore: 534.6004552841187
  time_this_iter_s: 128.94742846488953
  time_total_s: 82667.04282546043
  timestamp: 1637594475
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 62784000
  training_iteration: 654
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    654 |            82667 | 62784000 |  1709.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 31.59
    apples_agent-1_min: 16
    apples_agent-2_max: 402
    apples_agent-2_mean: 351.9
    apples_agent-2_min: 298
    apples_agent-3_max: 291
    apples_agent-3_mean: 219.38
    apples_agent-3_min: 151
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 390.69
    apples_agent-5_min: 340
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 412.8
    cleaning_beam_agent-0_min: 375
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 2.09
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 25.39
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 462.96
    cleaning_beam_agent-4_min: 403
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 2.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-23-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1850.0
  episode_reward_mean: 1727.43
  episode_reward_min: 1625.0
  episodes_this_iter: 96
  episodes_total: 62880
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13766.461
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.350152850151062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016951155848801136
        model: {}
        policy_loss: -0.0013631796464323997
        total_loss: 0.001224852865561843
        vf_explained_var: 0.0008288174867630005
        vf_loss: 31.831146240234375
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22806459665298462
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011426317505538464
        model: {}
        policy_loss: -0.0016520459903404117
        total_loss: 0.0008782371878623962
        vf_explained_var: 0.08368614315986633
        vf_loss: 29.17395782470703
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2816048264503479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008079992840066552
        model: {}
        policy_loss: -0.001683809794485569
        total_loss: 0.005763438530266285
        vf_explained_var: 0.03160867094993591
        vf_loss: 79.32770538330078
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5209937691688538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017612825613468885
        model: {}
        policy_loss: -0.0017300290055572987
        total_loss: 0.005357855930924416
        vf_explained_var: 0.028221428394317627
        vf_loss: 79.82815551757812
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8936835527420044
        entropy_coeff: 0.0017600000137463212
        kl: 0.002945292741060257
        model: {}
        policy_loss: -0.0018703893292695284
        total_loss: -0.0005817168857902288
        vf_explained_var: -0.005066439509391785
        vf_loss: 28.24740982055664
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25434044003486633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004915191675536335
        model: {}
        policy_loss: -0.0015996657311916351
        total_loss: 0.0003738701343536377
        vf_explained_var: 0.14286090433597565
        vf_loss: 24.15030860900879
    load_time_ms: 15241.663
    num_steps_sampled: 62880000
    num_steps_trained: 62880000
    sample_time_ms: 102252.628
    update_time_ms: 688.713
  iterations_since_restore: 5
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.88478260869565
    ram_util_percent: 15.13804347826087
  pid: 28385
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 420.0
    agent-3: 420.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 244.98
    agent-1: 244.98
    agent-2: 385.51
    agent-3: 385.51
    agent-4: 233.225
    agent-5: 233.225
  policy_reward_min:
    agent-0: 205.0
    agent-1: 205.0
    agent-2: 343.5
    agent-3: 343.5
    agent-4: 184.0
    agent-5: 184.0
  sampler_perf:
    mean_env_wait_ms: 26.75094123705091
    mean_inference_ms: 13.241669835020657
    mean_processing_ms: 59.93600015566186
  time_since_restore: 663.4261677265167
  time_this_iter_s: 128.82571244239807
  time_total_s: 82795.86853790283
  timestamp: 1637594604
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 62880000
  training_iteration: 655
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    655 |          82795.9 | 62880000 |  1727.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.4
    apples_agent-1_min: 11
    apples_agent-2_max: 406
    apples_agent-2_mean: 347.54
    apples_agent-2_min: 48
    apples_agent-3_max: 269
    apples_agent-3_mean: 215.91
    apples_agent-3_min: 35
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 384.53
    apples_agent-5_min: 60
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 414.83
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 2.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 3.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 28.0
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 444.84
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 2.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-25-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1842.0
  episode_reward_mean: 1695.69
  episode_reward_min: 243.0
  episodes_this_iter: 96
  episodes_total: 62976
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13403.93
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35581114888191223
        entropy_coeff: 0.0017600000137463212
        kl: 0.001817821292206645
        model: {}
        policy_loss: -0.0016399342566728592
        total_loss: 0.0011855573393404484
        vf_explained_var: 0.06483764946460724
        vf_loss: 34.40357208251953
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23340880870819092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005893467459827662
        model: {}
        policy_loss: -0.0018295994959771633
        total_loss: 0.0008393317693844438
        vf_explained_var: 0.1655021756887436
        vf_loss: 30.76050567626953
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28702348470687866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012077760184183717
        model: {}
        policy_loss: -0.002009978052228689
        total_loss: 0.005866123829036951
        vf_explained_var: 0.08532349765300751
        vf_loss: 83.73716735839844
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5384065508842468
        entropy_coeff: 0.0017600000137463212
        kl: 0.001514524221420288
        model: {}
        policy_loss: -0.0019060196354985237
        total_loss: 0.0060140639543533325
        vf_explained_var: 0.033179640769958496
        vf_loss: 88.58212280273438
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8766974806785583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032903363462537527
        model: {}
        policy_loss: -0.002306213602423668
        total_loss: -0.0005321814678609371
        vf_explained_var: 0.011469021439552307
        vf_loss: 32.96455764770508
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2675139307975769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008081748965196311
        model: {}
        policy_loss: -0.0017570755444467068
        total_loss: 0.0006672714371234179
        vf_explained_var: 0.13489355146884918
        vf_loss: 28.901235580444336
    load_time_ms: 15143.104
    num_steps_sampled: 62976000
    num_steps_trained: 62976000
    sample_time_ms: 102158.475
    update_time_ms: 576.392
  iterations_since_restore: 6
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.861538461538462
    ram_util_percent: 15.150549450549454
  pid: 28385
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 239.335
    agent-1: 239.335
    agent-2: 379.29
    agent-3: 379.29
    agent-4: 229.22
    agent-5: 229.22
  policy_reward_min:
    agent-0: 30.5
    agent-1: 30.5
    agent-2: 52.5
    agent-3: 52.5
    agent-4: 38.5
    agent-5: 38.5
  sampler_perf:
    mean_env_wait_ms: 26.743341770983196
    mean_inference_ms: 13.21108071121666
    mean_processing_ms: 59.98273804515386
  time_since_restore: 791.4341292381287
  time_this_iter_s: 128.00796151161194
  time_total_s: 82923.87649941444
  timestamp: 1637594732
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 62976000
  training_iteration: 656
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    656 |          82923.9 | 62976000 |  1695.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 0.5
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 31.04
    apples_agent-1_min: 13
    apples_agent-2_max: 415
    apples_agent-2_mean: 353.67
    apples_agent-2_min: 236
    apples_agent-3_max: 286
    apples_agent-3_mean: 217.44
    apples_agent-3_min: 144
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.22
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 385.81
    apples_agent-5_min: 229
    cleaning_beam_agent-0_max: 462
    cleaning_beam_agent-0_mean: 420.06
    cleaning_beam_agent-0_min: 289
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.71
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 2.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 26.93
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 458.04
    cleaning_beam_agent-4_min: 389
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 2.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-27-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1839.0
  episode_reward_mean: 1708.56
  episode_reward_min: 1042.0
  episodes_this_iter: 96
  episodes_total: 63072
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13141.043
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36274123191833496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014916423242539167
        model: {}
        policy_loss: -0.0013243462890386581
        total_loss: 0.001445569097995758
        vf_explained_var: 0.050295621156692505
        vf_loss: 34.036773681640625
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23144929111003876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008941259002313018
        model: {}
        policy_loss: -0.00195941049605608
        total_loss: 0.0007044626399874687
        vf_explained_var: 0.14602620899677277
        vf_loss: 30.684314727783203
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28505659103393555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006464346661232412
        model: {}
        policy_loss: -0.0017824312672019005
        total_loss: 0.0058222548104822636
        vf_explained_var: 0.07751107215881348
        vf_loss: 81.04367065429688
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5393132567405701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011193645186722279
        model: {}
        policy_loss: -0.0018928349018096924
        total_loss: 0.0056150732561945915
        vf_explained_var: 0.04010920226573944
        vf_loss: 84.53607177734375
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8624079823493958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032891142182052135
        model: {}
        policy_loss: -0.001998485531657934
        total_loss: -0.0004118261858820915
        vf_explained_var: 0.031317055225372314
        vf_loss: 30.942195892333984
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26166030764579773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006712524336762726
        model: {}
        policy_loss: -0.0016992539167404175
        total_loss: 0.0006219306960701942
        vf_explained_var: 0.1299285739660263
        vf_loss: 27.79608917236328
    load_time_ms: 15182.353
    num_steps_sampled: 63072000
    num_steps_trained: 63072000
    sample_time_ms: 101972.873
    update_time_ms: 496.156
  iterations_since_restore: 7
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.8032786885246
    ram_util_percent: 15.177595628415302
  pid: 28385
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 423.5
    agent-3: 423.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 243.205
    agent-1: 243.205
    agent-2: 381.645
    agent-3: 381.645
    agent-4: 229.43
    agent-5: 229.43
  policy_reward_min:
    agent-0: 125.5
    agent-1: 125.5
    agent-2: 257.5
    agent-3: 257.5
    agent-4: 138.0
    agent-5: 138.0
  sampler_perf:
    mean_env_wait_ms: 26.731052655677715
    mean_inference_ms: 13.170691328316993
    mean_processing_ms: 59.92556597548286
  time_since_restore: 919.3692388534546
  time_this_iter_s: 127.93510961532593
  time_total_s: 83051.81160902977
  timestamp: 1637594860
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 63072000
  training_iteration: 657
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    657 |          83051.8 | 63072000 |  1708.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 30.63
    apples_agent-1_min: 5
    apples_agent-2_max: 405
    apples_agent-2_mean: 350.0
    apples_agent-2_min: 62
    apples_agent-3_max: 268
    apples_agent-3_mean: 213.43
    apples_agent-3_min: 41
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 462
    apples_agent-5_mean: 385.97
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 420.19
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.55
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 2.85
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 27.78
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 575
    cleaning_beam_agent-4_mean: 479.54
    cleaning_beam_agent-4_min: 396
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-29-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1817.0
  episode_reward_mean: 1702.28
  episode_reward_min: 268.0
  episodes_this_iter: 96
  episodes_total: 63168
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 12951.037
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36450058221817017
        entropy_coeff: 0.0017600000137463212
        kl: 0.001358155976049602
        model: {}
        policy_loss: -0.001641795039176941
        total_loss: 0.0012323390692472458
        vf_explained_var: 0.08773384988307953
        vf_loss: 35.135345458984375
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23097288608551025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007717657717876136
        model: {}
        policy_loss: -0.0020155885722488165
        total_loss: 0.0008221735479310155
        vf_explained_var: 0.1586364060640335
        vf_loss: 32.43068313598633
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28567934036254883
        entropy_coeff: 0.0017600000137463212
        kl: 0.000996734481304884
        model: {}
        policy_loss: -0.0019380106823518872
        total_loss: 0.0061591025441884995
        vf_explained_var: 0.1018921285867691
        vf_loss: 85.98355102539062
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5358526706695557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009476961567997932
        model: {}
        policy_loss: -0.0017111091874539852
        total_loss: 0.00666467472910881
        vf_explained_var: 0.02632369101047516
        vf_loss: 93.17405700683594
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8736997246742249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012720490340143442
        model: {}
        policy_loss: -0.001808863366022706
        total_loss: 0.00016336410772055387
        vf_explained_var: 0.004543483257293701
        vf_loss: 35.079532623291016
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26427048444747925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007712810765951872
        model: {}
        policy_loss: -0.002027880633249879
        total_loss: 0.00047060800716280937
        vf_explained_var: 0.1582624763250351
        vf_loss: 29.624042510986328
    load_time_ms: 15117.73
    num_steps_sampled: 63168000
    num_steps_trained: 63168000
    sample_time_ms: 101900.426
    update_time_ms: 435.82
  iterations_since_restore: 8
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.957142857142863
    ram_util_percent: 15.111538461538466
  pid: 28385
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 420.0
    agent-3: 420.0
    agent-4: 267.0
    agent-5: 267.0
  policy_reward_mean:
    agent-0: 241.765
    agent-1: 241.765
    agent-2: 378.405
    agent-3: 378.405
    agent-4: 230.97
    agent-5: 230.97
  policy_reward_min:
    agent-0: 36.5
    agent-1: 36.5
    agent-2: 63.0
    agent-3: 63.0
    agent-4: 34.5
    agent-5: 34.5
  sampler_perf:
    mean_env_wait_ms: 26.717276121633695
    mean_inference_ms: 13.142900513209304
    mean_processing_ms: 59.86057423801584
  time_since_restore: 1047.1866228580475
  time_this_iter_s: 127.8173840045929
  time_total_s: 83179.62899303436
  timestamp: 1637594988
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 63168000
  training_iteration: 658
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    658 |          83179.6 | 63168000 |  1702.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.17
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 29.41
    apples_agent-1_min: 11
    apples_agent-2_max: 410
    apples_agent-2_mean: 344.97
    apples_agent-2_min: 159
    apples_agent-3_max: 288
    apples_agent-3_mean: 216.3
    apples_agent-3_min: 59
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 377.25
    apples_agent-5_min: 137
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 416.67
    cleaning_beam_agent-0_min: 317
    cleaning_beam_agent-1_max: 37
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 3.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 25.26
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 461.25
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 3.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-31-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1843.0
  episode_reward_mean: 1672.21
  episode_reward_min: 749.0
  episodes_this_iter: 96
  episodes_total: 63264
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 12800.265
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3724299669265747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015383812133222818
        model: {}
        policy_loss: -0.0017573628574609756
        total_loss: 0.0009353598579764366
        vf_explained_var: 0.1054004579782486
        vf_loss: 33.46997833251953
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2352982461452484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012355453800410032
        model: {}
        policy_loss: -0.0020007872954010963
        total_loss: 0.0006932104006409645
        vf_explained_var: 0.16820144653320312
        vf_loss: 31.071638107299805
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28829967975616455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008413658360950649
        model: {}
        policy_loss: -0.001849478343501687
        total_loss: 0.006372809875756502
        vf_explained_var: 0.10911069810390472
        vf_loss: 87.2904052734375
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5345019698143005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004104437248315662
        model: {}
        policy_loss: -0.0016750073991715908
        total_loss: 0.006777586415410042
        vf_explained_var: 0.04139120876789093
        vf_loss: 93.92998504638672
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8769034743309021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022497205063700676
        model: {}
        policy_loss: -0.002204644726589322
        total_loss: -0.0003261202946305275
        vf_explained_var: 0.030707046389579773
        vf_loss: 34.20119094848633
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27040261030197144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007848478853702545
        model: {}
        policy_loss: -0.0020716367289423943
        total_loss: 0.00039579346776008606
        vf_explained_var: 0.17195594310760498
        vf_loss: 29.427257537841797
    load_time_ms: 15017.726
    num_steps_sampled: 63264000
    num_steps_trained: 63264000
    sample_time_ms: 101980.642
    update_time_ms: 389.041
  iterations_since_restore: 9
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.619125683060105
    ram_util_percent: 15.138251366120222
  pid: 28385
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 431.0
    agent-3: 431.0
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 236.4
    agent-1: 236.4
    agent-2: 374.555
    agent-3: 374.555
    agent-4: 225.15
    agent-5: 225.15
  policy_reward_min:
    agent-0: 100.5
    agent-1: 100.5
    agent-2: 165.0
    agent-3: 165.0
    agent-4: 89.5
    agent-5: 89.5
  sampler_perf:
    mean_env_wait_ms: 26.729038441140116
    mean_inference_ms: 13.12633752281247
    mean_processing_ms: 59.86161153103911
  time_since_restore: 1175.7010514736176
  time_this_iter_s: 128.51442861557007
  time_total_s: 83308.14342164993
  timestamp: 1637595116
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 63264000
  training_iteration: 659
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    659 |          83308.1 | 63264000 |  1672.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 30.12
    apples_agent-1_min: 4
    apples_agent-2_max: 420
    apples_agent-2_mean: 354.19
    apples_agent-2_min: 52
    apples_agent-3_max: 262
    apples_agent-3_mean: 212.44
    apples_agent-3_min: 32
    apples_agent-4_max: 21
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 458
    apples_agent-5_mean: 385.91
    apples_agent-5_min: 72
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 420.53
    cleaning_beam_agent-0_min: 310
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.24
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 28.41
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 466.15
    cleaning_beam_agent-4_min: 391
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-34-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1828.0
  episode_reward_mean: 1702.76
  episode_reward_min: 304.0
  episodes_this_iter: 96
  episodes_total: 63360
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 12681.559
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3664441704750061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022047569509595633
        model: {}
        policy_loss: -0.0014679599553346634
        total_loss: 0.001590469852089882
        vf_explained_var: 0.053929880261421204
        vf_loss: 37.02511978149414
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23289519548416138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008238977752625942
        model: {}
        policy_loss: -0.0019486533710733056
        total_loss: 0.0009144194191321731
        vf_explained_var: 0.1651381254196167
        vf_loss: 32.7264518737793
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2820281982421875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008980353595688939
        model: {}
        policy_loss: -0.002112031914293766
        total_loss: 0.0058497292920947075
        vf_explained_var: 0.10553674399852753
        vf_loss: 84.57785034179688
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5407031774520874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006717308424413204
        model: {}
        policy_loss: -0.0015884172171354294
        total_loss: 0.006465539336204529
        vf_explained_var: 0.047600552439689636
        vf_loss: 90.05333709716797
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8812414407730103
        entropy_coeff: 0.0017600000137463212
        kl: 0.002051962073892355
        model: {}
        policy_loss: -0.002134389476850629
        total_loss: -0.00018608442042022943
        vf_explained_var: 0.045726463198661804
        vf_loss: 34.98489761352539
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2739972472190857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015115756541490555
        model: {}
        policy_loss: -0.002247082069516182
        total_loss: 0.00026605650782585144
        vf_explained_var: 0.18015028536319733
        vf_loss: 29.947900772094727
    load_time_ms: 14965.425
    num_steps_sampled: 63360000
    num_steps_trained: 63360000
    sample_time_ms: 102019.816
    update_time_ms: 351.346
  iterations_since_restore: 10
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.838251366120218
    ram_util_percent: 15.10874316939891
  pid: 28385
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 240.615
    agent-1: 240.615
    agent-2: 379.43
    agent-3: 379.43
    agent-4: 231.335
    agent-5: 231.335
  policy_reward_min:
    agent-0: 35.5
    agent-1: 35.5
    agent-2: 71.0
    agent-3: 71.0
    agent-4: 45.5
    agent-5: 45.5
  sampler_perf:
    mean_env_wait_ms: 26.74233795931088
    mean_inference_ms: 13.114555524064604
    mean_processing_ms: 59.88989338710689
  time_since_restore: 1304.2568163871765
  time_this_iter_s: 128.55576491355896
  time_total_s: 83436.69918656349
  timestamp: 1637595245
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 63360000
  training_iteration: 660
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    660 |          83436.7 | 63360000 |  1702.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 32.62
    apples_agent-1_min: 17
    apples_agent-2_max: 390
    apples_agent-2_mean: 350.71
    apples_agent-2_min: 192
    apples_agent-3_max: 268
    apples_agent-3_mean: 217.25
    apples_agent-3_min: 139
    apples_agent-4_max: 17
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 384.89
    apples_agent-5_min: 221
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 428.61
    cleaning_beam_agent-0_min: 381
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.36
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 2.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 27.87
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 459.15
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-36-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1828.0
  episode_reward_mean: 1701.96
  episode_reward_min: 1030.0
  episodes_this_iter: 96
  episodes_total: 63456
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11618.667
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36919519305229187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013420272152870893
        model: {}
        policy_loss: -0.0011383723467588425
        total_loss: 0.0017161490395665169
        vf_explained_var: 0.009406283497810364
        vf_loss: 35.04045104980469
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23106849193572998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008836386259645224
        model: {}
        policy_loss: -0.001611847896128893
        total_loss: 0.0010397122241556644
        vf_explained_var: 0.13592523336410522
        vf_loss: 30.580690383911133
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28062641620635986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011164332972839475
        model: {}
        policy_loss: -0.0017961286939680576
        total_loss: 0.005703052505850792
        vf_explained_var: 0.057209163904190063
        vf_loss: 79.92867279052734
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5365200042724609
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005263632629066706
        model: {}
        policy_loss: -0.0015454958193004131
        total_loss: 0.0056359535083174706
        vf_explained_var: 0.040812134742736816
        vf_loss: 81.25624084472656
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8740465044975281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013888797257095575
        model: {}
        policy_loss: -0.002019000705331564
        total_loss: -0.00047696568071842194
        vf_explained_var: 0.05393682420253754
        vf_loss: 30.8009033203125
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2641264498233795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013206095900386572
        model: {}
        policy_loss: -0.002000880893319845
        total_loss: 0.0003245054977014661
        vf_explained_var: 0.1419932246208191
        vf_loss: 27.899959564208984
    load_time_ms: 14621.026
    num_steps_sampled: 63456000
    num_steps_trained: 63456000
    sample_time_ms: 101813.859
    update_time_ms: 14.141
  iterations_since_restore: 11
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.479234972677588
    ram_util_percent: 15.171038251366125
  pid: 28385
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 420.0
    agent-3: 420.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 240.945
    agent-1: 240.945
    agent-2: 379.805
    agent-3: 379.805
    agent-4: 230.23
    agent-5: 230.23
  policy_reward_min:
    agent-0: 139.5
    agent-1: 139.5
    agent-2: 236.5
    agent-3: 236.5
    agent-4: 139.0
    agent-5: 139.0
  sampler_perf:
    mean_env_wait_ms: 26.74056780933368
    mean_inference_ms: 13.094303285682454
    mean_processing_ms: 59.86582591610284
  time_since_restore: 1431.41557264328
  time_this_iter_s: 127.15875625610352
  time_total_s: 83563.8579428196
  timestamp: 1637595374
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 63456000
  training_iteration: 661
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    661 |          83563.9 | 63456000 |  1701.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 30.89
    apples_agent-1_min: 19
    apples_agent-2_max: 432
    apples_agent-2_mean: 351.62
    apples_agent-2_min: 228
    apples_agent-3_max: 259
    apples_agent-3_mean: 210.47
    apples_agent-3_min: 135
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 458
    apples_agent-5_mean: 383.27
    apples_agent-5_min: 245
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 418.72
    cleaning_beam_agent-0_min: 301
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.48
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 2.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 28.77
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 454.81
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-38-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1826.0
  episode_reward_mean: 1695.75
  episode_reward_min: 1129.0
  episodes_this_iter: 96
  episodes_total: 63552
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11613.752
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3648138642311096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012514526024460793
        model: {}
        policy_loss: -0.0013652705820277333
        total_loss: 0.001339165959507227
        vf_explained_var: 0.04212625324726105
        vf_loss: 33.46385955810547
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23285144567489624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011374569730833173
        model: {}
        policy_loss: -0.0019373511895537376
        total_loss: 0.0006640984793193638
        vf_explained_var: 0.13749602437019348
        vf_loss: 30.11159896850586
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2849768400192261
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006940911407582462
        model: {}
        policy_loss: -0.0016326941549777985
        total_loss: 0.006120722740888596
        vf_explained_var: 0.07124865055084229
        vf_loss: 82.54910278320312
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5426549911499023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013142609968781471
        model: {}
        policy_loss: -0.0015700077638030052
        total_loss: 0.005939443130046129
        vf_explained_var: 0.047918111085891724
        vf_loss: 84.64398956298828
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8676397204399109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030896798707544804
        model: {}
        policy_loss: -0.002414482645690441
        total_loss: -0.0007657110691070557
        vf_explained_var: 0.04017829895019531
        vf_loss: 31.755146026611328
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2605787217617035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009958338923752308
        model: {}
        policy_loss: -0.0016972152516245842
        total_loss: 0.0007107809651643038
        vf_explained_var: 0.13829781115055084
        vf_loss: 28.665206909179688
    load_time_ms: 14591.972
    num_steps_sampled: 63552000
    num_steps_trained: 63552000
    sample_time_ms: 101881.069
    update_time_ms: 14.423
  iterations_since_restore: 12
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.29340659340659
    ram_util_percent: 15.119780219780221
  pid: 28385
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 239.99
    agent-1: 239.99
    agent-2: 378.83
    agent-3: 378.83
    agent-4: 229.055
    agent-5: 229.055
  policy_reward_min:
    agent-0: 163.5
    agent-1: 163.5
    agent-2: 241.0
    agent-3: 241.0
    agent-4: 156.0
    agent-5: 156.0
  sampler_perf:
    mean_env_wait_ms: 26.726791360531937
    mean_inference_ms: 13.091604188658193
    mean_processing_ms: 59.84277164653959
  time_since_restore: 1559.3178448677063
  time_this_iter_s: 127.90227222442627
  time_total_s: 83691.76021504402
  timestamp: 1637595502
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 63552000
  training_iteration: 662
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    662 |          83691.8 | 63552000 |  1695.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 30.96
    apples_agent-1_min: 16
    apples_agent-2_max: 413
    apples_agent-2_mean: 353.5
    apples_agent-2_min: 237
    apples_agent-3_max: 270
    apples_agent-3_mean: 215.69
    apples_agent-3_min: 135
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 385.7
    apples_agent-5_min: 235
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 418.28
    cleaning_beam_agent-0_min: 374
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 26.91
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 577
    cleaning_beam_agent-4_mean: 472.6
    cleaning_beam_agent-4_min: 421
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 2.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-40-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1835.0
  episode_reward_mean: 1708.48
  episode_reward_min: 1039.0
  episodes_this_iter: 96
  episodes_total: 63648
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.127
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36467844247817993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011064214631915092
        model: {}
        policy_loss: -0.0013225984293967485
        total_loss: 0.001475891680456698
        vf_explained_var: 0.027668267488479614
        vf_loss: 34.402740478515625
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22975699603557587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010819281451404095
        model: {}
        policy_loss: -0.0019081931095570326
        total_loss: 0.0006962988991290331
        vf_explained_var: 0.15025751292705536
        vf_loss: 30.0881290435791
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2825639545917511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010915701277554035
        model: {}
        policy_loss: -0.0019338924903422594
        total_loss: 0.005945505574345589
        vf_explained_var: 0.07404835522174835
        vf_loss: 83.7666015625
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5398897528648376
        entropy_coeff: 0.0017600000137463212
        kl: 0.001715080114081502
        model: {}
        policy_loss: -0.0015836460515856743
        total_loss: 0.006135927978903055
        vf_explained_var: 0.04186718165874481
        vf_loss: 86.69696044921875
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8692773580551147
        entropy_coeff: 0.0017600000137463212
        kl: 0.001972906058654189
        model: {}
        policy_loss: -0.002280410612002015
        total_loss: -0.0005243556224741042
        vf_explained_var: 0.015792056918144226
        vf_loss: 32.858909606933594
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25548702478408813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007704140152782202
        model: {}
        policy_loss: -0.0017486173892393708
        total_loss: 0.0006091130198910832
        vf_explained_var: 0.1584780067205429
        vf_loss: 28.073528289794922
    load_time_ms: 14559.267
    num_steps_sampled: 63648000
    num_steps_trained: 63648000
    sample_time_ms: 101812.235
    update_time_ms: 14.323
  iterations_since_restore: 13
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.11381215469613
    ram_util_percent: 15.149171270718236
  pid: 28385
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 425.0
    agent-3: 425.0
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 241.215
    agent-1: 241.215
    agent-2: 381.845
    agent-3: 381.845
    agent-4: 231.18
    agent-5: 231.18
  policy_reward_min:
    agent-0: 144.0
    agent-1: 144.0
    agent-2: 243.0
    agent-3: 243.0
    agent-4: 132.5
    agent-5: 132.5
  sampler_perf:
    mean_env_wait_ms: 26.722212693697863
    mean_inference_ms: 13.080859900887738
    mean_processing_ms: 59.83251508134286
  time_since_restore: 1686.422688961029
  time_this_iter_s: 127.10484409332275
  time_total_s: 83818.86505913734
  timestamp: 1637595629
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 63648000
  training_iteration: 663
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    663 |          83818.9 | 63648000 |  1708.48 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 31.11
    apples_agent-1_min: 5
    apples_agent-2_max: 428
    apples_agent-2_mean: 352.82
    apples_agent-2_min: 54
    apples_agent-3_max: 267
    apples_agent-3_mean: 211.95
    apples_agent-3_min: 25
    apples_agent-4_max: 18
    apples_agent-4_mean: 0.19
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 389.73
    apples_agent-5_min: 64
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 414.42
    cleaning_beam_agent-0_min: 381
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.71
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 27.94
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 485.3
    cleaning_beam_agent-4_min: 423
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-42-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1855.0
  episode_reward_mean: 1708.21
  episode_reward_min: 275.0
  episodes_this_iter: 96
  episodes_total: 63744
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11606.673
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3592865765094757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019021827029064298
        model: {}
        policy_loss: -0.0016437550075352192
        total_loss: 0.0010846005752682686
        vf_explained_var: 0.06733906269073486
        vf_loss: 33.60661315917969
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2318270206451416
        entropy_coeff: 0.0017600000137463212
        kl: 0.000648308196105063
        model: {}
        policy_loss: -0.0015925201587378979
        total_loss: 0.0011311837006360292
        vf_explained_var: 0.13126108050346375
        vf_loss: 31.31707000732422
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2799180746078491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012347002048045397
        model: {}
        policy_loss: -0.0019600163213908672
        total_loss: 0.005880614742636681
        vf_explained_var: 0.07924780249595642
        vf_loss: 83.33260345458984
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5466595888137817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012405000161379576
        model: {}
        policy_loss: -0.0019771663937717676
        total_loss: 0.005670174956321716
        vf_explained_var: 0.04896742105484009
        vf_loss: 86.09437561035156
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8845086097717285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023325677029788494
        model: {}
        policy_loss: -0.002113992115482688
        total_loss: -0.00033801631070673466
        vf_explained_var: 0.015420541167259216
        vf_loss: 33.32658386230469
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2601584792137146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007113269530236721
        model: {}
        policy_loss: -0.0019682319834828377
        total_loss: 0.0004345150664448738
        vf_explained_var: 0.15218986570835114
        vf_loss: 28.606117248535156
    load_time_ms: 14549.854
    num_steps_sampled: 63744000
    num_steps_trained: 63744000
    sample_time_ms: 101712.183
    update_time_ms: 14.143
  iterations_since_restore: 14
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.441530054644804
    ram_util_percent: 15.166666666666666
  pid: 28385
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 420.5
    agent-3: 420.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 240.785
    agent-1: 240.785
    agent-2: 381.68
    agent-3: 381.68
    agent-4: 231.64
    agent-5: 231.64
  policy_reward_min:
    agent-0: 45.0
    agent-1: 45.0
    agent-2: 59.5
    agent-3: 59.5
    agent-4: 33.0
    agent-5: 33.0
  sampler_perf:
    mean_env_wait_ms: 26.72956420933404
    mean_inference_ms: 13.075987324120314
    mean_processing_ms: 59.82062340871617
  time_since_restore: 1814.2860531806946
  time_this_iter_s: 127.86336421966553
  time_total_s: 83946.72842335701
  timestamp: 1637595757
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 63744000
  training_iteration: 664
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    664 |          83946.7 | 63744000 |  1708.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.18
    apples_agent-1_min: 19
    apples_agent-2_max: 417
    apples_agent-2_mean: 356.01
    apples_agent-2_min: 300
    apples_agent-3_max: 274
    apples_agent-3_mean: 217.2
    apples_agent-3_min: 152
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 383.77
    apples_agent-5_min: 322
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 412.46
    cleaning_beam_agent-0_min: 383
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 2.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 26.56
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 496.78
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 2.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-44-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1838.0
  episode_reward_mean: 1716.74
  episode_reward_min: 1442.0
  episodes_this_iter: 96
  episodes_total: 63840
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11604.993
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3596995174884796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013108828570693731
        model: {}
        policy_loss: -0.0013001598417758942
        total_loss: 0.0014116261154413223
        vf_explained_var: 0.007467687129974365
        vf_loss: 33.44845199584961
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22858618199825287
        entropy_coeff: 0.0017600000137463212
        kl: 0.001239180564880371
        model: {}
        policy_loss: -0.0017711883410811424
        total_loss: 0.0007951939478516579
        vf_explained_var: 0.11965647339820862
        vf_loss: 29.68679428100586
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2771579623222351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013816653518006206
        model: {}
        policy_loss: -0.00185375043656677
        total_loss: 0.005529666319489479
        vf_explained_var: 0.042980730533599854
        vf_loss: 78.71197509765625
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5306662917137146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005671926774084568
        model: {}
        policy_loss: -0.0015630587004125118
        total_loss: 0.005339442752301693
        vf_explained_var: 0.04864996671676636
        vf_loss: 78.36470794677734
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9010879993438721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018525999039411545
        model: {}
        policy_loss: -0.001939147710800171
        total_loss: -0.0004549846053123474
        vf_explained_var: 0.010688945651054382
        vf_loss: 30.700586318969727
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2547653317451477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007858764147385955
        model: {}
        policy_loss: -0.0018216636963188648
        total_loss: 0.00033866148442029953
        vf_explained_var: 0.15755446255207062
        vf_loss: 26.087059020996094
    load_time_ms: 14534.248
    num_steps_sampled: 63840000
    num_steps_trained: 63840000
    sample_time_ms: 101648.285
    update_time_ms: 14.103
  iterations_since_restore: 15
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.162637362637366
    ram_util_percent: 15.201098901098904
  pid: 28385
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 415.0
    agent-3: 415.0
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 241.19
    agent-1: 241.19
    agent-2: 385.435
    agent-3: 385.435
    agent-4: 231.745
    agent-5: 231.745
  policy_reward_min:
    agent-0: 184.5
    agent-1: 184.5
    agent-2: 311.0
    agent-3: 311.0
    agent-4: 192.0
    agent-5: 192.0
  sampler_perf:
    mean_env_wait_ms: 26.73746850390933
    mean_inference_ms: 13.07005410934998
    mean_processing_ms: 59.81840570063162
  time_since_restore: 1942.298553943634
  time_this_iter_s: 128.01250076293945
  time_total_s: 84074.74092411995
  timestamp: 1637595885
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 63840000
  training_iteration: 665
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    665 |          84074.7 | 63840000 |  1716.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.85
    apples_agent-1_min: 16
    apples_agent-2_max: 423
    apples_agent-2_mean: 352.57
    apples_agent-2_min: 190
    apples_agent-3_max: 265
    apples_agent-3_mean: 213.18
    apples_agent-3_min: 86
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.22
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 384.57
    apples_agent-5_min: 227
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 411.24
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.47
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 34.03
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 486.41
    cleaning_beam_agent-4_min: 430
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 3.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-46-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1853.0
  episode_reward_mean: 1706.26
  episode_reward_min: 981.0
  episodes_this_iter: 96
  episodes_total: 63936
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11605.177
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3605600595474243
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011195794213563204
        model: {}
        policy_loss: -0.0013853139244019985
        total_loss: 0.0014830478467047215
        vf_explained_var: 0.02092692255973816
        vf_loss: 35.02939987182617
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22678905725479126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012973228003829718
        model: {}
        policy_loss: -0.0019981106743216515
        total_loss: 0.0006982176564633846
        vf_explained_var: 0.1359064131975174
        vf_loss: 30.954696655273438
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2813800573348999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009187270770780742
        model: {}
        policy_loss: -0.0018602091586217284
        total_loss: 0.005973495543003082
        vf_explained_var: 0.07009470462799072
        vf_loss: 83.28931427001953
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5425412654876709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010165143758058548
        model: {}
        policy_loss: -0.0017141159623861313
        total_loss: 0.005857239011675119
        vf_explained_var: 0.048094987869262695
        vf_loss: 85.26220703125
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8798094391822815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017249773954972625
        model: {}
        policy_loss: -0.0021000804845243692
        total_loss: -0.00043221795931458473
        vf_explained_var: 0.02799472212791443
        vf_loss: 32.16321563720703
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2538069486618042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009164997027255595
        model: {}
        policy_loss: -0.002033031079918146
        total_loss: 0.00027418695390224457
        vf_explained_var: 0.16717077791690826
        vf_loss: 27.539167404174805
    load_time_ms: 14509.453
    num_steps_sampled: 63936000
    num_steps_trained: 63936000
    sample_time_ms: 101671.234
    update_time_ms: 13.812
  iterations_since_restore: 16
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.948901098901096
    ram_util_percent: 15.100000000000001
  pid: 28385
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 419.0
    agent-3: 419.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 241.78
    agent-1: 241.78
    agent-2: 381.08
    agent-3: 381.08
    agent-4: 230.27
    agent-5: 230.27
  policy_reward_min:
    agent-0: 133.5
    agent-1: 133.5
    agent-2: 218.5
    agent-3: 218.5
    agent-4: 138.5
    agent-5: 138.5
  sampler_perf:
    mean_env_wait_ms: 26.752547581234776
    mean_inference_ms: 13.063065587650222
    mean_processing_ms: 59.829591325304065
  time_since_restore: 2070.2897984981537
  time_this_iter_s: 127.99124455451965
  time_total_s: 84202.73216867447
  timestamp: 1637596013
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 63936000
  training_iteration: 666
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    666 |          84202.7 | 63936000 |  1706.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 30.95
    apples_agent-1_min: 6
    apples_agent-2_max: 445
    apples_agent-2_mean: 358.09
    apples_agent-2_min: 305
    apples_agent-3_max: 276
    apples_agent-3_mean: 211.16
    apples_agent-3_min: 143
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 391.46
    apples_agent-5_min: 336
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 415.25
    cleaning_beam_agent-0_min: 387
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.38
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 32.66
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 496.92
    cleaning_beam_agent-4_min: 419
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-49-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1849.0
  episode_reward_mean: 1718.52
  episode_reward_min: 1496.0
  episodes_this_iter: 96
  episodes_total: 64032
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.556
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36130982637405396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010352113749831915
        model: {}
        policy_loss: -0.0013183415867388248
        total_loss: 0.0017301738262176514
        vf_explained_var: -0.0015534758567810059
        vf_loss: 36.84418487548828
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2241145372390747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011491430923342705
        model: {}
        policy_loss: -0.002135921735316515
        total_loss: 0.0006912434473633766
        vf_explained_var: 0.12726052105426788
        vf_loss: 32.216060638427734
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27157503366470337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009905364131554961
        model: {}
        policy_loss: -0.001572378445416689
        total_loss: 0.005737531930208206
        vf_explained_var: 0.040071189403533936
        vf_loss: 77.87876892089844
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.546966552734375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016543172532692552
        model: {}
        policy_loss: -0.0016991933807730675
        total_loss: 0.0050499276258051395
        vf_explained_var: 0.04829812049865723
        vf_loss: 77.11782836914062
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8708433508872986
        entropy_coeff: 0.0017600000137463212
        kl: 0.002280917251482606
        model: {}
        policy_loss: -0.0020223851315677166
        total_loss: -0.0005550235509872437
        vf_explained_var: 0.03282409906387329
        vf_loss: 30.000446319580078
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24925610423088074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007932423613965511
        model: {}
        policy_loss: -0.0017421776428818703
        total_loss: 0.00038542365655303
        vf_explained_var: 0.16405253112316132
        vf_loss: 25.662900924682617
    load_time_ms: 14393.774
    num_steps_sampled: 64032000
    num_steps_trained: 64032000
    sample_time_ms: 101728.248
    update_time_ms: 13.681
  iterations_since_restore: 17
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.194505494505492
    ram_util_percent: 15.176373626373634
  pid: 28385
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 438.0
    agent-3: 438.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 243.88
    agent-1: 243.88
    agent-2: 380.93
    agent-3: 380.93
    agent-4: 234.45
    agent-5: 234.45
  policy_reward_min:
    agent-0: 60.0
    agent-1: 60.0
    agent-2: 312.0
    agent-3: 312.0
    agent-4: 200.5
    agent-5: 200.5
  sampler_perf:
    mean_env_wait_ms: 26.763574994641445
    mean_inference_ms: 13.05637779729207
    mean_processing_ms: 59.821011871650164
  time_since_restore: 2197.6880645751953
  time_this_iter_s: 127.39826607704163
  time_total_s: 84330.13043475151
  timestamp: 1637596141
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 64032000
  training_iteration: 667
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    667 |          84330.1 | 64032000 |  1718.52 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.13
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.62
    apples_agent-1_min: 18
    apples_agent-2_max: 396
    apples_agent-2_mean: 350.2
    apples_agent-2_min: 277
    apples_agent-3_max: 258
    apples_agent-3_mean: 212.16
    apples_agent-3_min: 139
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 449
    apples_agent-5_mean: 389.03
    apples_agent-5_min: 314
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 411.07
    cleaning_beam_agent-0_min: 365
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.49
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 31.28
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 515.29
    cleaning_beam_agent-4_min: 458
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-51-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1831.0
  episode_reward_mean: 1712.24
  episode_reward_min: 1341.0
  episodes_this_iter: 96
  episodes_total: 64128
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11606.079
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35983359813690186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014536925591528416
        model: {}
        policy_loss: -0.0012856079265475273
        total_loss: 0.0014390721917152405
        vf_explained_var: 0.021473467350006104
        vf_loss: 33.57985305786133
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2274026870727539
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011978785041719675
        model: {}
        policy_loss: -0.001798893790692091
        total_loss: 0.0008191969245672226
        vf_explained_var: 0.12433576583862305
        vf_loss: 30.183202743530273
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28060516715049744
        entropy_coeff: 0.0017600000137463212
        kl: 0.001388470409438014
        model: {}
        policy_loss: -0.0018355008214712143
        total_loss: 0.00564391165971756
        vf_explained_var: 0.056081414222717285
        vf_loss: 79.7327880859375
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.547914981842041
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012714008335024118
        model: {}
        policy_loss: -0.0016884278738871217
        total_loss: 0.005289985798299313
        vf_explained_var: 0.06025266647338867
        vf_loss: 79.42743682861328
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.866071879863739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025777616538107395
        model: {}
        policy_loss: -0.0021768175065517426
        total_loss: -0.0006461292505264282
        vf_explained_var: 0.02972996234893799
        vf_loss: 30.549720764160156
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24696755409240723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008917046943679452
        model: {}
        policy_loss: -0.001959513872861862
        total_loss: 0.0001777620054781437
        vf_explained_var: 0.17917576432228088
        vf_loss: 25.71938705444336
    load_time_ms: 14394.942
    num_steps_sampled: 64128000
    num_steps_trained: 64128000
    sample_time_ms: 101777.727
    update_time_ms: 13.926
  iterations_since_restore: 18
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.48415300546448
    ram_util_percent: 15.180327868852464
  pid: 28385
  policy_reward_max:
    agent-0: 268.5
    agent-1: 268.5
    agent-2: 419.5
    agent-3: 419.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 242.71
    agent-1: 242.71
    agent-2: 381.39
    agent-3: 381.39
    agent-4: 232.02
    agent-5: 232.02
  policy_reward_min:
    agent-0: 175.5
    agent-1: 175.5
    agent-2: 289.0
    agent-3: 289.0
    agent-4: 185.0
    agent-5: 185.0
  sampler_perf:
    mean_env_wait_ms: 26.768402565245488
    mean_inference_ms: 13.047366079197053
    mean_processing_ms: 59.801119362380085
  time_since_restore: 2325.938499212265
  time_this_iter_s: 128.2504346370697
  time_total_s: 84458.38086938858
  timestamp: 1637596269
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 64128000
  training_iteration: 668
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    668 |          84458.4 | 64128000 |  1712.24 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 0.25
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.49
    apples_agent-1_min: 11
    apples_agent-2_max: 431
    apples_agent-2_mean: 353.36
    apples_agent-2_min: 180
    apples_agent-3_max: 261
    apples_agent-3_mean: 211.64
    apples_agent-3_min: 95
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 381.02
    apples_agent-5_min: 212
    cleaning_beam_agent-0_max: 448
    cleaning_beam_agent-0_mean: 415.18
    cleaning_beam_agent-0_min: 368
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.7
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.11
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 35.13
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 611
    cleaning_beam_agent-4_mean: 516.62
    cleaning_beam_agent-4_min: 449
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-53-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1822.0
  episode_reward_mean: 1706.91
  episode_reward_min: 870.0
  episodes_this_iter: 96
  episodes_total: 64224
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.56
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36321306228637695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014057174557819963
        model: {}
        policy_loss: -0.0011905564460903406
        total_loss: 0.0014893568586558104
        vf_explained_var: 0.041520923376083374
        vf_loss: 33.19167709350586
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22900302708148956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008251534309238195
        model: {}
        policy_loss: -0.0018046232871711254
        total_loss: 0.0008855718187987804
        vf_explained_var: 0.10795770585536957
        vf_loss: 30.932409286499023
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28279823064804077
        entropy_coeff: 0.0017600000137463212
        kl: 0.001556644681841135
        model: {}
        policy_loss: -0.002232198603451252
        total_loss: 0.005507203284651041
        vf_explained_var: 0.05791720747947693
        vf_loss: 82.37127685546875
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5714442133903503
        entropy_coeff: 0.0017600000137463212
        kl: 0.002034194068983197
        model: {}
        policy_loss: -0.001792491297237575
        total_loss: 0.005588608793914318
        vf_explained_var: 0.0445886105298996
        vf_loss: 83.868408203125
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8742860555648804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014155919197946787
        model: {}
        policy_loss: -0.001890784245915711
        total_loss: -0.00036425114376470447
        vf_explained_var: 0.00797414779663086
        vf_loss: 30.652729034423828
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25117403268814087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007386151701211929
        model: {}
        policy_loss: -0.001778493169695139
        total_loss: 0.00041220756247639656
        vf_explained_var: 0.15266287326812744
        vf_loss: 26.327638626098633
    load_time_ms: 14412.835
    num_steps_sampled: 64224000
    num_steps_trained: 64224000
    sample_time_ms: 101640.642
    update_time_ms: 13.949
  iterations_since_restore: 19
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.67624309392265
    ram_util_percent: 15.115469613259672
  pid: 28385
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 428.5
    agent-3: 428.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 242.025
    agent-1: 242.025
    agent-2: 383.69
    agent-3: 383.69
    agent-4: 227.74
    agent-5: 227.74
  policy_reward_min:
    agent-0: 115.0
    agent-1: 115.0
    agent-2: 196.0
    agent-3: 196.0
    agent-4: 124.0
    agent-5: 124.0
  sampler_perf:
    mean_env_wait_ms: 26.775372432587428
    mean_inference_ms: 13.039761167055842
    mean_processing_ms: 59.78963863408099
  time_since_restore: 2453.2801580429077
  time_this_iter_s: 127.3416588306427
  time_total_s: 84585.72252821922
  timestamp: 1637596396
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 64224000
  training_iteration: 669
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    669 |          84585.7 | 64224000 |  1706.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 30.78
    apples_agent-1_min: 17
    apples_agent-2_max: 421
    apples_agent-2_mean: 349.58
    apples_agent-2_min: 207
    apples_agent-3_max: 263
    apples_agent-3_mean: 216.02
    apples_agent-3_min: 109
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 380.97
    apples_agent-5_min: 101
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 413.42
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.35
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 1.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 37.47
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 608
    cleaning_beam_agent-4_mean: 515.52
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-55-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1829.0
  episode_reward_mean: 1707.34
  episode_reward_min: 828.0
  episodes_this_iter: 96
  episodes_total: 64320
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.126
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36454418301582336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014378400519490242
        model: {}
        policy_loss: -0.0015707574784755707
        total_loss: 0.0013141599483788013
        vf_explained_var: 0.040142059326171875
        vf_loss: 35.26514434814453
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22707664966583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011804730165749788
        model: {}
        policy_loss: -0.0022418354637920856
        total_loss: 0.0005516912788152695
        vf_explained_var: 0.13049152493476868
        vf_loss: 31.93181610107422
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.281059205532074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010187284788116813
        model: {}
        policy_loss: -0.0017363550141453743
        total_loss: 0.006059104576706886
        vf_explained_var: 0.07153993844985962
        vf_loss: 82.9012222290039
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.569696843624115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009269575821235776
        model: {}
        policy_loss: -0.0016564022516831756
        total_loss: 0.005799943581223488
        vf_explained_var: 0.053875118494033813
        vf_loss: 84.59014892578125
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8848835229873657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021986346691846848
        model: {}
        policy_loss: -0.0018025059252977371
        total_loss: -3.7064310163259506e-05
        vf_explained_var: 0.02617698907852173
        vf_loss: 33.22835159301758
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2516195774078369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009374944493174553
        model: {}
        policy_loss: -0.002124028280377388
        total_loss: 0.0003758389502763748
        vf_explained_var: 0.1390978842973709
        vf_loss: 29.427181243896484
    load_time_ms: 14410.0
    num_steps_sampled: 64320000
    num_steps_trained: 64320000
    sample_time_ms: 101612.401
    update_time_ms: 14.312
  iterations_since_restore: 20
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.96120218579235
    ram_util_percent: 15.184153005464484
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 254.0
    agent-5: 254.0
  policy_reward_mean:
    agent-0: 241.725
    agent-1: 241.725
    agent-2: 383.96
    agent-3: 383.96
    agent-4: 227.985
    agent-5: 227.985
  policy_reward_min:
    agent-0: 125.5
    agent-1: 125.5
    agent-2: 218.5
    agent-3: 218.5
    agent-4: 57.5
    agent-5: 57.5
  sampler_perf:
    mean_env_wait_ms: 26.786358334961154
    mean_inference_ms: 13.038040411444456
    mean_processing_ms: 59.798019314532254
  time_since_restore: 2581.5709886550903
  time_this_iter_s: 128.29083061218262
  time_total_s: 84714.0133588314
  timestamp: 1637596525
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 64320000
  training_iteration: 670
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    670 |            84714 | 64320000 |  1707.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 30.89
    apples_agent-1_min: 17
    apples_agent-2_max: 407
    apples_agent-2_mean: 352.41
    apples_agent-2_min: 164
    apples_agent-3_max: 262
    apples_agent-3_mean: 213.27
    apples_agent-3_min: 97
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 431
    apples_agent-5_mean: 377.4
    apples_agent-5_min: 146
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 411.45
    cleaning_beam_agent-0_min: 368
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 2.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 34.77
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 502.77
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-57-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1829.0
  episode_reward_mean: 1704.39
  episode_reward_min: 726.0
  episodes_this_iter: 96
  episodes_total: 64416
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11609.511
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3603581190109253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018904340686276555
        model: {}
        policy_loss: -0.001423815032467246
        total_loss: 0.0013271273346617818
        vf_explained_var: 0.04380427300930023
        vf_loss: 33.85172653198242
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23055483400821686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008712021517567337
        model: {}
        policy_loss: -0.0018504359759390354
        total_loss: 0.00081091676838696
        vf_explained_var: 0.13311855494976044
        vf_loss: 30.6712589263916
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2864636778831482
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014491996262222528
        model: {}
        policy_loss: -0.001815655967220664
        total_loss: 0.005815222393721342
        vf_explained_var: 0.06364329159259796
        vf_loss: 81.35053253173828
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5678418874740601
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009652362205088139
        model: {}
        policy_loss: -0.0015941653400659561
        total_loss: 0.005677306093275547
        vf_explained_var: 0.04914620518684387
        vf_loss: 82.7087631225586
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8847751617431641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017394759925082326
        model: {}
        policy_loss: -0.002008649520576
        total_loss: -0.00037209363654255867
        vf_explained_var: 0.004500433802604675
        vf_loss: 31.93761444091797
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25546419620513916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011410311562940478
        model: {}
        policy_loss: -0.002121357247233391
        total_loss: 0.00012174155563116074
        vf_explained_var: 0.15899282693862915
        vf_loss: 26.927154541015625
    load_time_ms: 14390.943
    num_steps_sampled: 64416000
    num_steps_trained: 64416000
    sample_time_ms: 101724.045
    update_time_ms: 14.53
  iterations_since_restore: 21
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.897282608695654
    ram_util_percent: 15.138043478260872
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 242.05
    agent-1: 242.05
    agent-2: 381.675
    agent-3: 381.675
    agent-4: 228.47
    agent-5: 228.47
  policy_reward_min:
    agent-0: 98.0
    agent-1: 98.0
    agent-2: 174.5
    agent-3: 174.5
    agent-4: 90.5
    agent-5: 90.5
  sampler_perf:
    mean_env_wait_ms: 26.79659719571152
    mean_inference_ms: 13.03875915891657
    mean_processing_ms: 59.81876607490024
  time_since_restore: 2709.644862651825
  time_this_iter_s: 128.07387399673462
  time_total_s: 84842.08723282814
  timestamp: 1637596654
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 64416000
  training_iteration: 671
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    671 |          84842.1 | 64416000 |  1704.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.78
    apples_agent-1_min: 13
    apples_agent-2_max: 438
    apples_agent-2_mean: 356.6
    apples_agent-2_min: 282
    apples_agent-3_max: 259
    apples_agent-3_mean: 213.59
    apples_agent-3_min: 149
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 388.66
    apples_agent-5_min: 295
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 404.03
    cleaning_beam_agent-0_min: 372
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.55
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 35.82
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 600
    cleaning_beam_agent-4_mean: 501.83
    cleaning_beam_agent-4_min: 410
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 2.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_10-59-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1881.0
  episode_reward_mean: 1719.23
  episode_reward_min: 1379.0
  episodes_this_iter: 96
  episodes_total: 64512
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.693
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3494742512702942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016630496829748154
        model: {}
        policy_loss: -0.0017701983451843262
        total_loss: 0.0011846115812659264
        vf_explained_var: -0.006374523043632507
        vf_loss: 35.698814392089844
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2263226956129074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011863668914884329
        model: {}
        policy_loss: -0.001900452421978116
        total_loss: 0.0007886460516601801
        vf_explained_var: 0.12937486171722412
        vf_loss: 30.87425994873047
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2787078022956848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011410671286284924
        model: {}
        policy_loss: -0.0016666669398546219
        total_loss: 0.006100107915699482
        vf_explained_var: 0.04623810946941376
        vf_loss: 82.5730209350586
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.565087080001831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004630923504009843
        model: {}
        policy_loss: -0.0013588881120085716
        total_loss: 0.005951744504272938
        vf_explained_var: 0.0427643358707428
        vf_loss: 83.05181884765625
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8796899914741516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017663154285401106
        model: {}
        policy_loss: -0.0019633849151432514
        total_loss: -0.00044396333396434784
        vf_explained_var: 0.03930032253265381
        vf_loss: 30.676794052124023
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24820998311042786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011983881704509258
        model: {}
        policy_loss: -0.0018626698292791843
        total_loss: 0.0005083716241642833
        vf_explained_var: 0.11358514428138733
        vf_loss: 28.07891082763672
    load_time_ms: 14401.186
    num_steps_sampled: 64512000
    num_steps_trained: 64512000
    sample_time_ms: 101658.133
    update_time_ms: 14.363
  iterations_since_restore: 22
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.795604395604393
    ram_util_percent: 15.143956043956045
  pid: 28385
  policy_reward_max:
    agent-0: 266.0
    agent-1: 266.0
    agent-2: 434.0
    agent-3: 434.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 242.365
    agent-1: 242.365
    agent-2: 383.54
    agent-3: 383.54
    agent-4: 233.71
    agent-5: 233.71
  policy_reward_min:
    agent-0: 148.0
    agent-1: 148.0
    agent-2: 319.0
    agent-3: 319.0
    agent-4: 180.0
    agent-5: 180.0
  sampler_perf:
    mean_env_wait_ms: 26.800798423525574
    mean_inference_ms: 13.034065989803562
    mean_processing_ms: 59.815929175064674
  time_since_restore: 2837.0105605125427
  time_this_iter_s: 127.36569786071777
  time_total_s: 84969.45293068886
  timestamp: 1637596782
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 64512000
  training_iteration: 672
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    672 |          84969.5 | 64512000 |  1719.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 29.7
    apples_agent-1_min: 16
    apples_agent-2_max: 421
    apples_agent-2_mean: 357.22
    apples_agent-2_min: 282
    apples_agent-3_max: 272
    apples_agent-3_mean: 212.71
    apples_agent-3_min: 157
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 387.56
    apples_agent-5_min: 296
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 401.57
    cleaning_beam_agent-0_min: 350
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.79
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 37.66
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 595
    cleaning_beam_agent-4_mean: 506.16
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 2.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-01-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1822.0
  episode_reward_mean: 1722.57
  episode_reward_min: 1359.0
  episodes_this_iter: 96
  episodes_total: 64608
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11608.746
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3528438210487366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012292525498196483
        model: {}
        policy_loss: -0.0011848005233332515
        total_loss: 0.001508741988800466
        vf_explained_var: 0.018682807683944702
        vf_loss: 33.14549255371094
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22304587066173553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008532432839274406
        model: {}
        policy_loss: -0.0015274197794497013
        total_loss: 0.000992417219094932
        vf_explained_var: 0.13872897624969482
        vf_loss: 29.12399673461914
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2803153097629547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009112301049754024
        model: {}
        policy_loss: -0.0016897316090762615
        total_loss: 0.00574300205335021
        vf_explained_var: 0.04579740762710571
        vf_loss: 79.2608871459961
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5773562788963318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009210866410285234
        model: {}
        policy_loss: -0.0017786743119359016
        total_loss: 0.005251433700323105
        vf_explained_var: 0.03485296666622162
        vf_loss: 80.46253204345703
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8918101191520691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018200937192887068
        model: {}
        policy_loss: -0.0019669278990477324
        total_loss: -0.0005737256724387407
        vf_explained_var: 0.032118648290634155
        vf_loss: 29.627843856811523
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24974778294563293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008223482291214168
        model: {}
        policy_loss: -0.0018860669806599617
        total_loss: 0.00039271963760256767
        vf_explained_var: 0.10879670083522797
        vf_loss: 27.183420181274414
    load_time_ms: 14405.793
    num_steps_sampled: 64608000
    num_steps_trained: 64608000
    sample_time_ms: 101826.81
    update_time_ms: 14.391
  iterations_since_restore: 23
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.094535519125678
    ram_util_percent: 15.177049180327872
  pid: 28385
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 426.0
    agent-3: 426.0
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 243.725
    agent-1: 243.725
    agent-2: 384.075
    agent-3: 384.075
    agent-4: 233.485
    agent-5: 233.485
  policy_reward_min:
    agent-0: 177.5
    agent-1: 177.5
    agent-2: 318.0
    agent-3: 318.0
    agent-4: 184.0
    agent-5: 184.0
  sampler_perf:
    mean_env_wait_ms: 26.814154855776493
    mean_inference_ms: 13.033155616678851
    mean_processing_ms: 59.844974788828004
  time_since_restore: 2965.821667432785
  time_this_iter_s: 128.8111069202423
  time_total_s: 85098.2640376091
  timestamp: 1637596911
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 64608000
  training_iteration: 673
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    673 |          85098.3 | 64608000 |  1722.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.12
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.97
    apples_agent-1_min: 10
    apples_agent-2_max: 424
    apples_agent-2_mean: 355.63
    apples_agent-2_min: 132
    apples_agent-3_max: 274
    apples_agent-3_mean: 207.22
    apples_agent-3_min: 55
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 383.15
    apples_agent-5_min: 127
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 398.17
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 2.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.11
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 41.94
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 595
    cleaning_beam_agent-4_mean: 505.0
    cleaning_beam_agent-4_min: 435
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 2.96
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-03-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1842.0
  episode_reward_mean: 1704.76
  episode_reward_min: 580.0
  episodes_this_iter: 96
  episodes_total: 64704
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11596.786
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3610229194164276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020199897699058056
        model: {}
        policy_loss: -0.0016587155405431986
        total_loss: 0.0012254948960617185
        vf_explained_var: 0.07477664947509766
        vf_loss: 35.196075439453125
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23001189529895782
        entropy_coeff: 0.0017600000137463212
        kl: 0.00046214094618335366
        model: {}
        policy_loss: -0.001649640966206789
        total_loss: 0.0011120205745100975
        vf_explained_var: 0.1674242615699768
        vf_loss: 31.664794921875
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28656864166259766
        entropy_coeff: 0.0017600000137463212
        kl: 0.001295361202210188
        model: {}
        policy_loss: -0.002000137697905302
        total_loss: 0.006238625850528479
        vf_explained_var: 0.10521695017814636
        vf_loss: 87.43128204345703
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5685614347457886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009144644718617201
        model: {}
        policy_loss: -0.001855333335697651
        total_loss: 0.006361809559166431
        vf_explained_var: 0.05721306800842285
        vf_loss: 92.17813110351562
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.889885425567627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030201589688658714
        model: {}
        policy_loss: -0.0023048128932714462
        total_loss: -0.00043139129411429167
        vf_explained_var: 0.035797908902168274
        vf_loss: 34.39622497558594
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2603059411048889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008623232133686543
        model: {}
        policy_loss: -0.002309744246304035
        total_loss: 0.00024692248553037643
        vf_explained_var: 0.15649700164794922
        vf_loss: 30.148056030273438
    load_time_ms: 14389.529
    num_steps_sampled: 64704000
    num_steps_trained: 64704000
    sample_time_ms: 101894.921
    update_time_ms: 14.333
  iterations_since_restore: 24
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.02677595628415
    ram_util_percent: 15.146994535519129
  pid: 28385
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 429.0
    agent-3: 429.0
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 241.835
    agent-1: 241.835
    agent-2: 379.68
    agent-3: 379.68
    agent-4: 230.865
    agent-5: 230.865
  policy_reward_min:
    agent-0: 91.0
    agent-1: 91.0
    agent-2: 119.5
    agent-3: 119.5
    agent-4: 79.5
    agent-5: 79.5
  sampler_perf:
    mean_env_wait_ms: 26.82388692538692
    mean_inference_ms: 13.031775823008633
    mean_processing_ms: 59.85035690564667
  time_since_restore: 3094.0811202526093
  time_this_iter_s: 128.25945281982422
  time_total_s: 85226.52349042892
  timestamp: 1637597039
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 64704000
  training_iteration: 674
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    674 |          85226.5 | 64704000 |  1704.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.52
    apples_agent-1_min: 17
    apples_agent-2_max: 413
    apples_agent-2_mean: 354.55
    apples_agent-2_min: 299
    apples_agent-3_max: 281
    apples_agent-3_mean: 209.95
    apples_agent-3_min: 142
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 382.93
    apples_agent-5_min: 265
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 408.13
    cleaning_beam_agent-0_min: 367
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.33
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 36.07
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 504.75
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-06-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1852.0
  episode_reward_mean: 1720.52
  episode_reward_min: 1547.0
  episodes_this_iter: 96
  episodes_total: 64800
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11595.947
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35444289445877075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012202123180031776
        model: {}
        policy_loss: -0.0012604659423232079
        total_loss: 0.0014645522460341454
        vf_explained_var: 0.0010177642107009888
        vf_loss: 33.488407135009766
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22292907536029816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009915274567902088
        model: {}
        policy_loss: -0.001852413173764944
        total_loss: 0.0006445157341659069
        vf_explained_var: 0.13767854869365692
        vf_loss: 28.892839431762695
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2839024066925049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011018086224794388
        model: {}
        policy_loss: -0.0015224250964820385
        total_loss: 0.005894374568015337
        vf_explained_var: 0.043854862451553345
        vf_loss: 79.16466522216797
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5708648562431335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018488778732717037
        model: {}
        policy_loss: -0.0016710017807781696
        total_loss: 0.005271619185805321
        vf_explained_var: 0.042174309492111206
        vf_loss: 79.47345733642578
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8856245875358582
        entropy_coeff: 0.0017600000137463212
        kl: 0.002510853810235858
        model: {}
        policy_loss: -0.0020657265558838844
        total_loss: -0.0006458302959799767
        vf_explained_var: 0.029799968004226685
        vf_loss: 29.785953521728516
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25622111558914185
        entropy_coeff: 0.0017600000137463212
        kl: 0.001086260424926877
        model: {}
        policy_loss: -0.0018745828419923782
        total_loss: 0.0003941245377063751
        vf_explained_var: 0.11565877497196198
        vf_loss: 27.196571350097656
    load_time_ms: 14390.512
    num_steps_sampled: 64800000
    num_steps_trained: 64800000
    sample_time_ms: 101907.091
    update_time_ms: 14.349
  iterations_since_restore: 25
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.07158469945356
    ram_util_percent: 15.203825136612029
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 432.0
    agent-3: 432.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 243.33
    agent-1: 243.33
    agent-2: 385.205
    agent-3: 385.205
    agent-4: 231.725
    agent-5: 231.725
  policy_reward_min:
    agent-0: 190.0
    agent-1: 190.0
    agent-2: 328.0
    agent-3: 328.0
    agent-4: 153.5
    agent-5: 153.5
  sampler_perf:
    mean_env_wait_ms: 26.826960161226765
    mean_inference_ms: 13.032998800520716
    mean_processing_ms: 59.85447992418787
  time_since_restore: 3222.212607383728
  time_this_iter_s: 128.13148713111877
  time_total_s: 85354.65497756004
  timestamp: 1637597167
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 64800000
  training_iteration: 675
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    675 |          85354.7 | 64800000 |  1720.52 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 31.86
    apples_agent-1_min: 18
    apples_agent-2_max: 418
    apples_agent-2_mean: 355.47
    apples_agent-2_min: 248
    apples_agent-3_max: 269
    apples_agent-3_mean: 211.45
    apples_agent-3_min: 108
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 384.06
    apples_agent-5_min: 284
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 406.15
    cleaning_beam_agent-0_min: 341
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 35.99
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 522.22
    cleaning_beam_agent-4_min: 448
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-08-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1831.0
  episode_reward_mean: 1722.16
  episode_reward_min: 1278.0
  episodes_this_iter: 96
  episodes_total: 64896
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11604.734
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3517027199268341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015399944968521595
        model: {}
        policy_loss: -0.00154025643132627
        total_loss: 0.0011461757821962237
        vf_explained_var: 0.019690722227096558
        vf_loss: 33.0543098449707
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2196418046951294
        entropy_coeff: 0.0017600000137463212
        kl: 0.000957686803303659
        model: {}
        policy_loss: -0.0014480995014309883
        total_loss: 0.001144389738328755
        vf_explained_var: 0.11851349472999573
        vf_loss: 29.790571212768555
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2847329378128052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009571500704623759
        model: {}
        policy_loss: -0.0017940012039616704
        total_loss: 0.005521008744835854
        vf_explained_var: 0.04880443215370178
        vf_loss: 78.161376953125
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5667724609375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006805601296946406
        model: {}
        policy_loss: -0.0015609832480549812
        total_loss: 0.005443611182272434
        vf_explained_var: 0.026584267616271973
        vf_loss: 80.0211410522461
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.906520426273346
        entropy_coeff: 0.0017600000137463212
        kl: 0.002498432993888855
        model: {}
        policy_loss: -0.0022125570103526115
        total_loss: -0.00095400121062994
        vf_explained_var: 0.013586044311523438
        vf_loss: 28.540294647216797
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2548786401748657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008177484851330519
        model: {}
        policy_loss: -0.0019147330895066261
        total_loss: 6.919074803590775e-05
        vf_explained_var: 0.15907727181911469
        vf_loss: 24.32514190673828
    load_time_ms: 14388.18
    num_steps_sampled: 64896000
    num_steps_trained: 64896000
    sample_time_ms: 101889.165
    update_time_ms: 14.393
  iterations_since_restore: 26
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.61153846153846
    ram_util_percent: 15.1956043956044
  pid: 28385
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 422.0
    agent-3: 422.0
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 246.275
    agent-1: 246.275
    agent-2: 383.035
    agent-3: 383.035
    agent-4: 231.77
    agent-5: 231.77
  policy_reward_min:
    agent-0: 184.0
    agent-1: 184.0
    agent-2: 283.0
    agent-3: 283.0
    agent-4: 172.0
    agent-5: 172.0
  sampler_perf:
    mean_env_wait_ms: 26.830438752977944
    mean_inference_ms: 13.027859127610837
    mean_processing_ms: 59.83762260507807
  time_since_restore: 3350.0919091701508
  time_this_iter_s: 127.87930178642273
  time_total_s: 85482.53427934647
  timestamp: 1637597295
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 64896000
  training_iteration: 676
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    676 |          85482.5 | 64896000 |  1722.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.14
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.62
    apples_agent-1_min: 11
    apples_agent-2_max: 397
    apples_agent-2_mean: 350.2
    apples_agent-2_min: 43
    apples_agent-3_max: 270
    apples_agent-3_mean: 206.96
    apples_agent-3_min: 25
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 381.51
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 411.07
    cleaning_beam_agent-0_min: 318
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.92
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 36.33
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 505.28
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 2.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-10-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1850.0
  episode_reward_mean: 1690.47
  episode_reward_min: 156.0
  episodes_this_iter: 96
  episodes_total: 64992
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11605.144
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36306121945381165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016258260002359748
        model: {}
        policy_loss: -0.0016379808075726032
        total_loss: 0.0012436062097549438
        vf_explained_var: 0.11581262946128845
        vf_loss: 35.205684661865234
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22713324427604675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010199043899774551
        model: {}
        policy_loss: -0.0018529673106968403
        total_loss: 0.0009707608260214329
        vf_explained_var: 0.19196492433547974
        vf_loss: 32.23484420776367
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2932929992675781
        entropy_coeff: 0.0017600000137463212
        kl: 0.000691952183842659
        model: {}
        policy_loss: -0.0019409083761274815
        total_loss: 0.005996162537485361
        vf_explained_var: 0.14117379486560822
        vf_loss: 84.53266143798828
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.568848729133606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008366324473172426
        model: {}
        policy_loss: -0.0017789746634662151
        total_loss: 0.006544317118823528
        vf_explained_var: 0.052599042654037476
        vf_loss: 93.24468231201172
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9083815217018127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016227394808083773
        model: {}
        policy_loss: -0.0018379967659711838
        total_loss: 6.900494918227196e-05
        vf_explained_var: 0.023293614387512207
        vf_loss: 35.05755615234375
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2644309401512146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009650795836932957
        model: {}
        policy_loss: -0.0022446177899837494
        total_loss: 0.00017468200530856848
        vf_explained_var: 0.19579796493053436
        vf_loss: 28.84698486328125
    load_time_ms: 14407.07
    num_steps_sampled: 64992000
    num_steps_trained: 64992000
    sample_time_ms: 101922.923
    update_time_ms: 14.384
  iterations_since_restore: 27
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.018579234972677
    ram_util_percent: 15.112568306010928
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 421.0
    agent-3: 421.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 239.7
    agent-1: 239.7
    agent-2: 376.57
    agent-3: 376.57
    agent-4: 228.965
    agent-5: 228.965
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 54.5
    agent-3: 54.5
    agent-4: 23.5
    agent-5: 23.5
  sampler_perf:
    mean_env_wait_ms: 26.832815888991245
    mean_inference_ms: 13.025216250448638
    mean_processing_ms: 59.84182474314118
  time_since_restore: 3478.010675430298
  time_this_iter_s: 127.9187662601471
  time_total_s: 85610.45304560661
  timestamp: 1637597423
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 64992000
  training_iteration: 677
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    677 |          85610.5 | 64992000 |  1690.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 32.24
    apples_agent-1_min: 20
    apples_agent-2_max: 404
    apples_agent-2_mean: 361.48
    apples_agent-2_min: 266
    apples_agent-3_max: 269
    apples_agent-3_mean: 209.16
    apples_agent-3_min: 150
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 386.28
    apples_agent-5_min: 274
    cleaning_beam_agent-0_max: 448
    cleaning_beam_agent-0_mean: 417.47
    cleaning_beam_agent-0_min: 384
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.88
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 35.05
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 607
    cleaning_beam_agent-4_mean: 505.12
    cleaning_beam_agent-4_min: 441
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 2.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-12-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1848.0
  episode_reward_mean: 1723.27
  episode_reward_min: 1269.0
  episodes_this_iter: 96
  episodes_total: 65088
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11604.305
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3573340177536011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014049651799723506
        model: {}
        policy_loss: -0.0012385300360620022
        total_loss: 0.0014943459536880255
        vf_explained_var: 0.009686321020126343
        vf_loss: 33.617820739746094
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2223319709300995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012249995488673449
        model: {}
        policy_loss: -0.0018149070674553514
        total_loss: 0.0007116275373846292
        vf_explained_var: 0.1385539025068283
        vf_loss: 29.178417205810547
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27981722354888916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012636642204597592
        model: {}
        policy_loss: -0.0017107431776821613
        total_loss: 0.005409684963524342
        vf_explained_var: 0.051422834396362305
        vf_loss: 76.12906646728516
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5555272698402405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011739021865651011
        model: {}
        policy_loss: -0.001647184370085597
        total_loss: 0.005039167124778032
        vf_explained_var: 0.04584050178527832
        vf_loss: 76.64080810546875
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8942788243293762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021551295649260283
        model: {}
        policy_loss: -0.0020578959956765175
        total_loss: -0.0005444412236101925
        vf_explained_var: 0.01801249384880066
        vf_loss: 30.873865127563477
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2603013515472412
        entropy_coeff: 0.0017600000137463212
        kl: 0.000649956171400845
        model: {}
        policy_loss: -0.0016223462298512459
        total_loss: 0.0005046832375228405
        vf_explained_var: 0.17759455740451813
        vf_loss: 25.851604461669922
    load_time_ms: 14377.802
    num_steps_sampled: 65088000
    num_steps_trained: 65088000
    sample_time_ms: 101906.758
    update_time_ms: 14.138
  iterations_since_restore: 28
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.729120879120885
    ram_util_percent: 15.165384615384612
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 417.0
    agent-3: 417.0
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 245.025
    agent-1: 245.025
    agent-2: 384.81
    agent-3: 384.81
    agent-4: 231.8
    agent-5: 231.8
  policy_reward_min:
    agent-0: 172.0
    agent-1: 172.0
    agent-2: 292.0
    agent-3: 292.0
    agent-4: 170.5
    agent-5: 170.5
  sampler_perf:
    mean_env_wait_ms: 26.836394275343373
    mean_inference_ms: 13.024177455097304
    mean_processing_ms: 59.835222960771254
  time_since_restore: 3605.7954988479614
  time_this_iter_s: 127.78482341766357
  time_total_s: 85738.23786902428
  timestamp: 1637597551
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 65088000
  training_iteration: 678
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    678 |          85738.2 | 65088000 |  1723.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 32.08
    apples_agent-1_min: 17
    apples_agent-2_max: 416
    apples_agent-2_mean: 360.87
    apples_agent-2_min: 294
    apples_agent-3_max: 261
    apples_agent-3_mean: 214.93
    apples_agent-3_min: 150
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 385.41
    apples_agent-5_min: 344
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 423.07
    cleaning_beam_agent-0_min: 384
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.41
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 32.88
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 507.86
    cleaning_beam_agent-4_min: 441
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-14-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1870.0
  episode_reward_mean: 1731.07
  episode_reward_min: 1582.0
  episodes_this_iter: 96
  episodes_total: 65184
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11611.132
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36063507199287415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014302509371191263
        model: {}
        policy_loss: -0.0009563682251609862
        total_loss: 0.0016198488883674145
        vf_explained_var: -0.00996457040309906
        vf_loss: 32.10934066772461
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21878525614738464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006149131222628057
        model: {}
        policy_loss: -0.001468574395403266
        total_loss: 0.0009394506923854351
        vf_explained_var: 0.1196441650390625
        vf_loss: 27.930883407592773
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27806806564331055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008679144666530192
        model: {}
        policy_loss: -0.0017511583864688873
        total_loss: 0.005665520206093788
        vf_explained_var: 0.03633938729763031
        vf_loss: 79.0607681274414
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5405170321464539
        entropy_coeff: 0.0017600000137463212
        kl: 0.002473995089530945
        model: {}
        policy_loss: -0.0018320339731872082
        total_loss: 0.0051048146560788155
        vf_explained_var: 0.04061053693294525
        vf_loss: 78.88158416748047
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9040641784667969
        entropy_coeff: 0.0017600000137463212
        kl: 0.002543182112276554
        model: {}
        policy_loss: -0.0020454339683055878
        total_loss: -0.000786864198744297
        vf_explained_var: 0.02076689898967743
        vf_loss: 28.497270584106445
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26023173332214355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005721801426261663
        model: {}
        policy_loss: -0.0013811725657433271
        total_loss: 0.0007753006648272276
        vf_explained_var: 0.1046290397644043
        vf_loss: 26.144824981689453
    load_time_ms: 14381.603
    num_steps_sampled: 65184000
    num_steps_trained: 65184000
    sample_time_ms: 101887.832
    update_time_ms: 13.994
  iterations_since_restore: 29
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.125966850828735
    ram_util_percent: 15.15801104972376
  pid: 28385
  policy_reward_max:
    agent-0: 283.0
    agent-1: 283.0
    agent-2: 422.0
    agent-3: 422.0
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 245.785
    agent-1: 245.785
    agent-2: 388.15
    agent-3: 388.15
    agent-4: 231.6
    agent-5: 231.6
  policy_reward_min:
    agent-0: 220.5
    agent-1: 220.5
    agent-2: 342.5
    agent-3: 342.5
    agent-4: 203.0
    agent-5: 203.0
  sampler_perf:
    mean_env_wait_ms: 26.835905838629827
    mean_inference_ms: 13.019426942374997
    mean_processing_ms: 59.82564883709341
  time_since_restore: 3733.050843000412
  time_this_iter_s: 127.25534415245056
  time_total_s: 85865.49321317673
  timestamp: 1637597679
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 65184000
  training_iteration: 679
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    679 |          85865.5 | 65184000 |  1731.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 30.73
    apples_agent-1_min: 11
    apples_agent-2_max: 435
    apples_agent-2_mean: 357.44
    apples_agent-2_min: 257
    apples_agent-3_max: 280
    apples_agent-3_mean: 216.32
    apples_agent-3_min: 153
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 383.01
    apples_agent-5_min: 308
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 417.65
    cleaning_beam_agent-0_min: 388
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.39
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 32.75
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 619
    cleaning_beam_agent-4_mean: 491.94
    cleaning_beam_agent-4_min: 403
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-16-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1831.0
  episode_reward_mean: 1715.77
  episode_reward_min: 1351.0
  episodes_this_iter: 96
  episodes_total: 65280
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11608.513
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36987361311912537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013354136608541012
        model: {}
        policy_loss: -0.0011877063661813736
        total_loss: 0.001447821967303753
        vf_explained_var: 0.004417985677719116
        vf_loss: 32.86506271362305
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2208513468503952
        entropy_coeff: 0.0017600000137463212
        kl: 0.001009243307635188
        model: {}
        policy_loss: -0.0017030173912644386
        total_loss: 0.0008574631065130234
        vf_explained_var: 0.10782144963741302
        vf_loss: 29.49177360534668
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2838451564311981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013743513263761997
        model: {}
        policy_loss: -0.0017893514595925808
        total_loss: 0.005752819590270519
        vf_explained_var: 0.05190667510032654
        vf_loss: 80.41737365722656
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5347976684570312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010313937673345208
        model: {}
        policy_loss: -0.001391278114169836
        total_loss: 0.005698631051927805
        vf_explained_var: 0.053556472063064575
        vf_loss: 80.31153869628906
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9070144891738892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011264161439612508
        model: {}
        policy_loss: -0.0015696723712608218
        total_loss: -0.00012230919674038887
        vf_explained_var: 0.022864341735839844
        vf_loss: 30.437103271484375
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26182568073272705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006914560799486935
        model: {}
        policy_loss: -0.001744083478115499
        total_loss: 0.00046921096509322524
        vf_explained_var: 0.14180415868759155
        vf_loss: 26.741065979003906
    load_time_ms: 14391.903
    num_steps_sampled: 65280000
    num_steps_trained: 65280000
    sample_time_ms: 101764.848
    update_time_ms: 13.78
  iterations_since_restore: 30
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.271270718232046
    ram_util_percent: 15.202209944751383
  pid: 28385
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 424.0
    agent-3: 424.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 243.085
    agent-1: 243.085
    agent-2: 384.735
    agent-3: 384.735
    agent-4: 230.065
    agent-5: 230.065
  policy_reward_min:
    agent-0: 187.0
    agent-1: 187.0
    agent-2: 299.0
    agent-3: 299.0
    agent-4: 189.5
    agent-5: 189.5
  sampler_perf:
    mean_env_wait_ms: 26.837690634208762
    mean_inference_ms: 13.014493117317919
    mean_processing_ms: 59.814302865037604
  time_since_restore: 3860.1466903686523
  time_this_iter_s: 127.09584736824036
  time_total_s: 85992.58906054497
  timestamp: 1637597806
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 65280000
  training_iteration: 680
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    680 |          85992.6 | 65280000 |  1715.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 31.19
    apples_agent-1_min: 7
    apples_agent-2_max: 404
    apples_agent-2_mean: 354.78
    apples_agent-2_min: 87
    apples_agent-3_max: 280
    apples_agent-3_mean: 221.41
    apples_agent-3_min: 57
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 385.54
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 423.08
    cleaning_beam_agent-0_min: 381
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.45
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 29.43
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 499.28
    cleaning_beam_agent-4_min: 424
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 2.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-18-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1847.0
  episode_reward_mean: 1723.44
  episode_reward_min: 393.0
  episodes_this_iter: 96
  episodes_total: 65376
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11605.782
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3821108937263489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017300918698310852
        model: {}
        policy_loss: -0.001363792922347784
        total_loss: 0.0012748348526656628
        vf_explained_var: 0.044693633913993835
        vf_loss: 33.11144256591797
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22355493903160095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010619678068906069
        model: {}
        policy_loss: -0.0017704566707834601
        total_loss: 0.0008454423514194787
        vf_explained_var: 0.1318303346633911
        vf_loss: 30.093576431274414
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2868838906288147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010492443107068539
        model: {}
        policy_loss: -0.0018758196383714676
        total_loss: 0.0060886116698384285
        vf_explained_var: 0.07918258011341095
        vf_loss: 84.69345092773438
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5325734615325928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008549904450774193
        model: {}
        policy_loss: -0.0015434448141604662
        total_loss: 0.006360176019370556
        vf_explained_var: 0.042954251170158386
        vf_loss: 88.40949249267578
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9037251472473145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021925102919340134
        model: {}
        policy_loss: -0.0019509481498971581
        total_loss: -0.0002354263560846448
        vf_explained_var: 0.0016179680824279785
        vf_loss: 33.0607795715332
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25888246297836304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007181887631304562
        model: {}
        policy_loss: -0.002027291338890791
        total_loss: 0.0003367704339325428
        vf_explained_var: 0.14849096536636353
        vf_loss: 28.196937561035156
    load_time_ms: 14396.742
    num_steps_sampled: 65376000
    num_steps_trained: 65376000
    sample_time_ms: 101694.039
    update_time_ms: 13.743
  iterations_since_restore: 31
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.986885245901643
    ram_util_percent: 15.081420765027325
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 428.5
    agent-3: 428.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 243.55
    agent-1: 243.55
    agent-2: 386.845
    agent-3: 386.845
    agent-4: 231.325
    agent-5: 231.325
  policy_reward_min:
    agent-0: 64.5
    agent-1: 64.5
    agent-2: 89.0
    agent-3: 89.0
    agent-4: 43.0
    agent-5: 43.0
  sampler_perf:
    mean_env_wait_ms: 26.841371435225245
    mean_inference_ms: 13.013370935595487
    mean_processing_ms: 59.81026154013933
  time_since_restore: 3987.527095556259
  time_this_iter_s: 127.38040518760681
  time_total_s: 86119.96946573257
  timestamp: 1637597935
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 65376000
  training_iteration: 681
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    681 |            86120 | 65376000 |  1723.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 32.35
    apples_agent-1_min: 0
    apples_agent-2_max: 398
    apples_agent-2_mean: 352.86
    apples_agent-2_min: 7
    apples_agent-3_max: 280
    apples_agent-3_mean: 214.17
    apples_agent-3_min: 6
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 384.15
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 456
    cleaning_beam_agent-0_mean: 414.83
    cleaning_beam_agent-0_min: 373
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 2.55
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 30.49
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 483.33
    cleaning_beam_agent-4_min: 402
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 3.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-21-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1856.0
  episode_reward_mean: 1710.1
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 65472
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11617.445
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3885045051574707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018283065874129534
        model: {}
        policy_loss: -0.001638171263039112
        total_loss: 0.0011784015223383904
        vf_explained_var: 0.09212680160999298
        vf_loss: 35.00341796875
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2300107181072235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012116667348891497
        model: {}
        policy_loss: -0.002082027494907379
        total_loss: 0.0007399908499792218
        vf_explained_var: 0.16310995817184448
        vf_loss: 32.26836395263672
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28724727034568787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015132126864045858
        model: {}
        policy_loss: -0.0019939723424613476
        total_loss: 0.005721472203731537
        vf_explained_var: 0.11336885392665863
        vf_loss: 82.20999145507812
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5467081069946289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006347621674649417
        model: {}
        policy_loss: -0.001669599674642086
        total_loss: 0.006471596658229828
        vf_explained_var: 0.017988786101341248
        vf_loss: 91.0340347290039
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9065635204315186
        entropy_coeff: 0.0017600000137463212
        kl: 0.002728370949625969
        model: {}
        policy_loss: -0.0017824189271777868
        total_loss: -6.0138991102576256e-05
        vf_explained_var: 0.01574084162712097
        vf_loss: 33.17832946777344
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2668890058994293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007492496515624225
        model: {}
        policy_loss: -0.002107350854203105
        total_loss: 0.0001754937693476677
        vf_explained_var: 0.18226219713687897
        vf_loss: 27.525712966918945
    load_time_ms: 14409.617
    num_steps_sampled: 65472000
    num_steps_trained: 65472000
    sample_time_ms: 101682.626
    update_time_ms: 13.556
  iterations_since_restore: 32
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.697802197802197
    ram_util_percent: 15.164835164835164
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 438.0
    agent-3: 438.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 242.95
    agent-1: 242.95
    agent-2: 380.99
    agent-3: 380.99
    agent-4: 231.11
    agent-5: 231.11
  policy_reward_min:
    agent-0: 9.5
    agent-1: 9.5
    agent-2: 13.5
    agent-3: 13.5
    agent-4: 14.0
    agent-5: 14.0
  sampler_perf:
    mean_env_wait_ms: 26.839005363002183
    mean_inference_ms: 13.010917374252536
    mean_processing_ms: 59.805091037215476
  time_since_restore: 4114.981006860733
  time_this_iter_s: 127.45391130447388
  time_total_s: 86247.42337703705
  timestamp: 1637598062
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 65472000
  training_iteration: 682
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    682 |          86247.4 | 65472000 |   1710.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 30.11
    apples_agent-1_min: 2
    apples_agent-2_max: 401
    apples_agent-2_mean: 349.68
    apples_agent-2_min: 4
    apples_agent-3_max: 276
    apples_agent-3_mean: 211.74
    apples_agent-3_min: 3
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 377.3
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 404.97
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 2.0
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 68
    cleaning_beam_agent-2_mean: 3.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 31.7
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 599
    cleaning_beam_agent-4_mean: 489.13
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 3.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-23-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1817.0
  episode_reward_mean: 1688.43
  episode_reward_min: 45.0
  episodes_this_iter: 96
  episodes_total: 65568
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11615.704
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3825090527534485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020226985216140747
        model: {}
        policy_loss: -0.0016365423798561096
        total_loss: 0.0014403359964489937
        vf_explained_var: 0.12024226784706116
        vf_loss: 37.50093078613281
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23620754480361938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013571034651249647
        model: {}
        policy_loss: -0.002160851377993822
        total_loss: 0.0008170206565409899
        vf_explained_var: 0.20181365311145782
        vf_loss: 33.93594741821289
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2907654941082001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001160432118922472
        model: {}
        policy_loss: -0.0021779388189315796
        total_loss: 0.006383867003023624
        vf_explained_var: 0.16777834296226501
        vf_loss: 90.73553466796875
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5476117134094238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009045428014360368
        model: {}
        policy_loss: -0.0018481584265828133
        total_loss: 0.007448533549904823
        vf_explained_var: 0.05942922830581665
        vf_loss: 102.6048812866211
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9154715538024902
        entropy_coeff: 0.0017600000137463212
        kl: 0.001759161939844489
        model: {}
        policy_loss: -0.0016907271929085255
        total_loss: 0.000545641640201211
        vf_explained_var: -0.00010979175567626953
        vf_loss: 38.47600555419922
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27196523547172546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010442069033160806
        model: {}
        policy_loss: -0.00237787002697587
        total_loss: 0.00018689059652388096
        vf_explained_var: 0.19982095062732697
        vf_loss: 30.434200286865234
    load_time_ms: 14403.448
    num_steps_sampled: 65568000
    num_steps_trained: 65568000
    sample_time_ms: 101529.268
    update_time_ms: 13.5
  iterations_since_restore: 33
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.241758241758237
    ram_util_percent: 15.182417582417587
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 427.0
    agent-3: 427.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 239.265
    agent-1: 239.265
    agent-2: 379.255
    agent-3: 379.255
    agent-4: 225.695
    agent-5: 225.695
  policy_reward_min:
    agent-0: 5.5
    agent-1: 5.5
    agent-2: 9.0
    agent-3: 9.0
    agent-4: 8.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 26.832002821893393
    mean_inference_ms: 13.007453522711014
    mean_processing_ms: 59.79305377985543
  time_since_restore: 4242.182963609695
  time_this_iter_s: 127.2019567489624
  time_total_s: 86374.62533378601
  timestamp: 1637598190
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 65568000
  training_iteration: 683
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    683 |          86374.6 | 65568000 |  1688.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.0
    apples_agent-1_min: 11
    apples_agent-2_max: 415
    apples_agent-2_mean: 357.36
    apples_agent-2_min: 139
    apples_agent-3_max: 262
    apples_agent-3_mean: 213.13
    apples_agent-3_min: 112
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 383.0
    apples_agent-5_min: 150
    cleaning_beam_agent-0_max: 439
    cleaning_beam_agent-0_mean: 411.59
    cleaning_beam_agent-0_min: 325
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 2.01
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 2.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 34.76
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 619
    cleaning_beam_agent-4_mean: 482.58
    cleaning_beam_agent-4_min: 392
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 2.67
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-25-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1845.0
  episode_reward_mean: 1709.71
  episode_reward_min: 736.0
  episodes_this_iter: 96
  episodes_total: 65664
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11617.439
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37697118520736694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014329537516459823
        model: {}
        policy_loss: -0.0016019450267776847
        total_loss: 0.0011112052015960217
        vf_explained_var: 0.03760725259780884
        vf_loss: 33.76622009277344
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22687163949012756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010436928132548928
        model: {}
        policy_loss: -0.0015903072198852897
        total_loss: 0.0010602041147649288
        vf_explained_var: 0.13090907037258148
        vf_loss: 30.49805450439453
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2808637022972107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008326692623086274
        model: {}
        policy_loss: -0.0017782794311642647
        total_loss: 0.005769651383161545
        vf_explained_var: 0.06923636794090271
        vf_loss: 80.42249298095703
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.540237545967102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010231954511255026
        model: {}
        policy_loss: -0.0014907359145581722
        total_loss: 0.005947519559413195
        vf_explained_var: 0.029635131359100342
        vf_loss: 83.8907470703125
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9289523959159851
        entropy_coeff: 0.0017600000137463212
        kl: 0.001735449186526239
        model: {}
        policy_loss: -0.0019539049826562405
        total_loss: -0.0003194850869476795
        vf_explained_var: 0.009528681635856628
        vf_loss: 32.69377517700195
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2653650641441345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010496516479179263
        model: {}
        policy_loss: -0.0018419092521071434
        total_loss: 0.0004176059737801552
        vf_explained_var: 0.17340724170207977
        vf_loss: 27.265552520751953
    load_time_ms: 14390.625
    num_steps_sampled: 65664000
    num_steps_trained: 65664000
    sample_time_ms: 101435.055
    update_time_ms: 13.626
  iterations_since_restore: 34
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.28674033149171
    ram_util_percent: 15.1353591160221
  pid: 28385
  policy_reward_max:
    agent-0: 279.0
    agent-1: 279.0
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 242.19
    agent-1: 242.19
    agent-2: 383.47
    agent-3: 383.47
    agent-4: 229.195
    agent-5: 229.195
  policy_reward_min:
    agent-0: 99.0
    agent-1: 99.0
    agent-2: 176.0
    agent-3: 176.0
    agent-4: 93.0
    agent-5: 93.0
  sampler_perf:
    mean_env_wait_ms: 26.827846606034417
    mean_inference_ms: 13.006088432319508
    mean_processing_ms: 59.78731933205233
  time_since_restore: 4369.33786034584
  time_this_iter_s: 127.15489673614502
  time_total_s: 86501.78023052216
  timestamp: 1637598317
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 65664000
  training_iteration: 684
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    684 |          86501.8 | 65664000 |  1709.71 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.11
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 30.4
    apples_agent-1_min: 7
    apples_agent-2_max: 416
    apples_agent-2_mean: 353.62
    apples_agent-2_min: 20
    apples_agent-3_max: 270
    apples_agent-3_mean: 209.39
    apples_agent-3_min: 12
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 377.69
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 403.17
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.79
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 2.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 33.26
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 618
    cleaning_beam_agent-4_mean: 482.83
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 3.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-27-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1858.0
  episode_reward_mean: 1691.88
  episode_reward_min: 106.0
  episodes_this_iter: 96
  episodes_total: 65760
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.863
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37772834300994873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012915758416056633
        model: {}
        policy_loss: -0.0014229989610612392
        total_loss: 0.0015399447875097394
        vf_explained_var: 0.11082775890827179
        vf_loss: 36.27748489379883
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2333928346633911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011729579418897629
        model: {}
        policy_loss: -0.002302055014297366
        total_loss: 0.00066197948763147
        vf_explained_var: 0.17237414419651031
        vf_loss: 33.74806594848633
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2864559292793274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006898188148625195
        model: {}
        policy_loss: -0.001812643138691783
        total_loss: 0.006612817756831646
        vf_explained_var: 0.1314769685268402
        vf_loss: 89.29621887207031
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5471823811531067
        entropy_coeff: 0.0017600000137463212
        kl: 0.001276201568543911
        model: {}
        policy_loss: -0.001746184192597866
        total_loss: 0.0072041889652609825
        vf_explained_var: 0.035495951771736145
        vf_loss: 99.13414764404297
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9153022766113281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020584524609148502
        model: {}
        policy_loss: -0.0017609342467039824
        total_loss: 0.0004052629228681326
        vf_explained_var: 0.011695712804794312
        vf_loss: 37.771278381347656
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27086663246154785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007714516832493246
        model: {}
        policy_loss: -0.002281883265823126
        total_loss: 0.00030318787321448326
        vf_explained_var: 0.1974756419658661
        vf_loss: 30.61798858642578
    load_time_ms: 14390.722
    num_steps_sampled: 65760000
    num_steps_trained: 65760000
    sample_time_ms: 101453.956
    update_time_ms: 13.442
  iterations_since_restore: 35
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.895081967213116
    ram_util_percent: 15.181420765027324
  pid: 28385
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 419.5
    agent-3: 419.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 239.045
    agent-1: 239.045
    agent-2: 379.89
    agent-3: 379.89
    agent-4: 227.005
    agent-5: 227.005
  policy_reward_min:
    agent-0: 14.5
    agent-1: 14.5
    agent-2: 22.0
    agent-3: 22.0
    agent-4: 16.5
    agent-5: 16.5
  sampler_perf:
    mean_env_wait_ms: 26.828110713957454
    mean_inference_ms: 13.006508616733475
    mean_processing_ms: 59.80115868546292
  time_since_restore: 4497.700833797455
  time_this_iter_s: 128.36297345161438
  time_total_s: 86630.14320397377
  timestamp: 1637598445
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 65760000
  training_iteration: 685
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    685 |          86630.1 | 65760000 |  1691.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 31.77
    apples_agent-1_min: 15
    apples_agent-2_max: 418
    apples_agent-2_mean: 354.19
    apples_agent-2_min: 247
    apples_agent-3_max: 265
    apples_agent-3_mean: 214.9
    apples_agent-3_min: 156
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 382.75
    apples_agent-5_min: 267
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 403.9
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.63
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 34.49
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 610
    cleaning_beam_agent-4_mean: 493.53
    cleaning_beam_agent-4_min: 403
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-29-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1830.0
  episode_reward_mean: 1707.93
  episode_reward_min: 1206.0
  episodes_this_iter: 96
  episodes_total: 65856
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.732
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37630075216293335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017484811833128333
        model: {}
        policy_loss: -0.0013830717653036118
        total_loss: 0.0014271917752921581
        vf_explained_var: 0.02002999186515808
        vf_loss: 34.72551727294922
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23041090369224548
        entropy_coeff: 0.0017600000137463212
        kl: 0.001307314378209412
        model: {}
        policy_loss: -0.0018835398368537426
        total_loss: 0.0007914621382951736
        vf_explained_var: 0.13020792603492737
        vf_loss: 30.805252075195312
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2869708240032196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009981280891224742
        model: {}
        policy_loss: -0.0018919119611382484
        total_loss: 0.0056572044268250465
        vf_explained_var: 0.06647033989429474
        vf_loss: 80.5418472290039
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5531260371208191
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005652701365761459
        model: {}
        policy_loss: -0.0015102437464520335
        total_loss: 0.00580190122127533
        vf_explained_var: 0.040478140115737915
        vf_loss: 82.85646057128906
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9104129076004028
        entropy_coeff: 0.0017600000137463212
        kl: 0.001287340186536312
        model: {}
        policy_loss: -0.001890045590698719
        total_loss: -0.0005233804695308208
        vf_explained_var: 0.0072494447231292725
        vf_loss: 29.689950942993164
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2618812620639801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006555925356224179
        model: {}
        policy_loss: -0.0016571776941418648
        total_loss: 0.0004361635074019432
        vf_explained_var: 0.14738067984580994
        vf_loss: 25.542503356933594
    load_time_ms: 14374.668
    num_steps_sampled: 65856000
    num_steps_trained: 65856000
    sample_time_ms: 101427.658
    update_time_ms: 13.583
  iterations_since_restore: 36
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.523756906077345
    ram_util_percent: 15.206629834254146
  pid: 28385
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 241.105
    agent-1: 241.105
    agent-2: 382.35
    agent-3: 382.35
    agent-4: 230.51
    agent-5: 230.51
  policy_reward_min:
    agent-0: 171.0
    agent-1: 171.0
    agent-2: 278.5
    agent-3: 278.5
    agent-4: 153.5
    agent-5: 153.5
  sampler_perf:
    mean_env_wait_ms: 26.827360867111125
    mean_inference_ms: 13.006381442266138
    mean_processing_ms: 59.79907166625244
  time_since_restore: 4625.059942722321
  time_this_iter_s: 127.35910892486572
  time_total_s: 86757.50231289864
  timestamp: 1637598573
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 65856000
  training_iteration: 686
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    686 |          86757.5 | 65856000 |  1707.93 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 30.56
    apples_agent-1_min: 15
    apples_agent-2_max: 412
    apples_agent-2_mean: 352.12
    apples_agent-2_min: 294
    apples_agent-3_max: 263
    apples_agent-3_mean: 211.05
    apples_agent-3_min: 119
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 383.28
    apples_agent-5_min: 314
    cleaning_beam_agent-0_max: 439
    cleaning_beam_agent-0_mean: 410.89
    cleaning_beam_agent-0_min: 375
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.65
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 2.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 32.4
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 492.37
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-31-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1849.0
  episode_reward_mean: 1708.29
  episode_reward_min: 1350.0
  episodes_this_iter: 96
  episodes_total: 65952
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11608.855
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3744223713874817
        entropy_coeff: 0.0017600000137463212
        kl: 0.001337518566288054
        model: {}
        policy_loss: -0.0012582866474986076
        total_loss: 0.001371611375361681
        vf_explained_var: 0.012850821018218994
        vf_loss: 32.888832092285156
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22772735357284546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005654139677062631
        model: {}
        policy_loss: -0.0015409472398459911
        total_loss: 0.000980336219072342
        vf_explained_var: 0.12216240167617798
        vf_loss: 29.220809936523438
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28580567240715027
        entropy_coeff: 0.0017600000137463212
        kl: 0.001044975477270782
        model: {}
        policy_loss: -0.0019510251004248857
        total_loss: 0.005251280032098293
        vf_explained_var: 0.06144608557224274
        vf_loss: 77.05322265625
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5559870600700378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006675568874925375
        model: {}
        policy_loss: -0.0013968474231660366
        total_loss: 0.005401418544352055
        vf_explained_var: 0.05319778621196747
        vf_loss: 77.76803588867188
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9328455328941345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030326927080750465
        model: {}
        policy_loss: -0.0020754735451191664
        total_loss: -0.0007035256130620837
        vf_explained_var: 0.01635269820690155
        vf_loss: 30.137590408325195
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2630273699760437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009028493077494204
        model: {}
        policy_loss: -0.001831089612096548
        total_loss: 0.0003057178109884262
        vf_explained_var: 0.15654997527599335
        vf_loss: 25.99736785888672
    load_time_ms: 14368.166
    num_steps_sampled: 65952000
    num_steps_trained: 65952000
    sample_time_ms: 101411.515
    update_time_ms: 13.611
  iterations_since_restore: 37
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.55659340659341
    ram_util_percent: 15.16538461538462
  pid: 28385
  policy_reward_max:
    agent-0: 284.5
    agent-1: 284.5
    agent-2: 434.5
    agent-3: 434.5
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 242.06
    agent-1: 242.06
    agent-2: 380.655
    agent-3: 380.655
    agent-4: 231.43
    agent-5: 231.43
  policy_reward_min:
    agent-0: 191.5
    agent-1: 191.5
    agent-2: 298.5
    agent-3: 298.5
    agent-4: 185.0
    agent-5: 185.0
  sampler_perf:
    mean_env_wait_ms: 26.82775075908117
    mean_inference_ms: 13.00362479103445
    mean_processing_ms: 59.79540594829322
  time_since_restore: 4752.775904178619
  time_this_iter_s: 127.71596145629883
  time_total_s: 86885.21827435493
  timestamp: 1637598701
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 65952000
  training_iteration: 687
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    687 |          86885.2 | 65952000 |  1708.29 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.24
    apples_agent-1_min: 11
    apples_agent-2_max: 397
    apples_agent-2_mean: 348.17
    apples_agent-2_min: 103
    apples_agent-3_max: 272
    apples_agent-3_mean: 214.76
    apples_agent-3_min: 61
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.22
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 380.1
    apples_agent-5_min: 120
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 409.7
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 2.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 35.09
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 479.99
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 3.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-33-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1832.0
  episode_reward_mean: 1699.98
  episode_reward_min: 497.0
  episodes_this_iter: 96
  episodes_total: 66048
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.177
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3773089051246643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019691884517669678
        model: {}
        policy_loss: -0.0016152748139575124
        total_loss: 0.0010513279121369123
        vf_explained_var: 0.060095012187957764
        vf_loss: 33.306640625
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22892232239246368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008591159712523222
        model: {}
        policy_loss: -0.001609272789210081
        total_loss: 0.0010366099886596203
        vf_explained_var: 0.13905468583106995
        vf_loss: 30.487855911254883
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29011741280555725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010896134190261364
        model: {}
        policy_loss: -0.0019340603612363338
        total_loss: 0.005939996801316738
        vf_explained_var: 0.08617228269577026
        vf_loss: 83.84661102294922
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.547088623046875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018059026915580034
        model: {}
        policy_loss: -0.0018266122788190842
        total_loss: 0.005965418182313442
        vf_explained_var: 0.04545840620994568
        vf_loss: 87.54904174804688
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9267363548278809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018117596628144383
        model: {}
        policy_loss: -0.002354491502046585
        total_loss: -0.0008022752008400857
        vf_explained_var: 0.003856852650642395
        vf_loss: 31.832721710205078
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2689777612686157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006315892678685486
        model: {}
        policy_loss: -0.0016955751925706863
        total_loss: 0.0005421247333288193
        vf_explained_var: 0.1541525274515152
        vf_loss: 27.11104965209961
    load_time_ms: 14370.986
    num_steps_sampled: 66048000
    num_steps_trained: 66048000
    sample_time_ms: 101416.929
    update_time_ms: 13.763
  iterations_since_restore: 38
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.95274725274725
    ram_util_percent: 15.174175824175828
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 423.5
    agent-3: 423.5
    agent-4: 266.0
    agent-5: 266.0
  policy_reward_mean:
    agent-0: 240.235
    agent-1: 240.235
    agent-2: 380.24
    agent-3: 380.24
    agent-4: 229.515
    agent-5: 229.515
  policy_reward_min:
    agent-0: 75.5
    agent-1: 75.5
    agent-2: 102.0
    agent-3: 102.0
    agent-4: 71.0
    agent-5: 71.0
  sampler_perf:
    mean_env_wait_ms: 26.824173222411027
    mean_inference_ms: 13.002657805393456
    mean_processing_ms: 59.797341061830465
  time_since_restore: 4880.630401611328
  time_this_iter_s: 127.85449743270874
  time_total_s: 87013.07277178764
  timestamp: 1637598829
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 66048000
  training_iteration: 688
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    688 |          87013.1 | 66048000 |  1699.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 32.53
    apples_agent-1_min: 0
    apples_agent-2_max: 516
    apples_agent-2_mean: 355.23
    apples_agent-2_min: 282
    apples_agent-3_max: 286
    apples_agent-3_mean: 213.01
    apples_agent-3_min: 161
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.27
    apples_agent-4_min: 0
    apples_agent-5_max: 490
    apples_agent-5_mean: 383.74
    apples_agent-5_min: 295
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 399.2
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 29
    cleaning_beam_agent-1_mean: 2.54
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 36.12
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 472.93
    cleaning_beam_agent-4_min: 381
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-35-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1862.0
  episode_reward_mean: 1701.8
  episode_reward_min: 1387.0
  episodes_this_iter: 96
  episodes_total: 66144
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11601.165
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3613296151161194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017571891658008099
        model: {}
        policy_loss: -0.001200545346364379
        total_loss: 0.0019207476871088147
        vf_explained_var: 0.00985795259475708
        vf_loss: 37.57233810424805
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23854674398899078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005438681109808385
        model: {}
        policy_loss: -0.002093682996928692
        total_loss: 0.0006717036012560129
        vf_explained_var: 0.16043463349342346
        vf_loss: 31.852272033691406
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2874297499656677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009580187033861876
        model: {}
        policy_loss: -0.0020737024024128914
        total_loss: 0.005520761013031006
        vf_explained_var: 0.054906442761421204
        vf_loss: 81.00341796875
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.557862401008606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006796382367610931
        model: {}
        policy_loss: -0.0017050700262188911
        total_loss: 0.00545453280210495
        vf_explained_var: 0.050971224904060364
        vf_loss: 81.4144287109375
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9219193458557129
        entropy_coeff: 0.0017600000137463212
        kl: 0.003292204113677144
        model: {}
        policy_loss: -0.002090320223942399
        total_loss: -0.0006069440860301256
        vf_explained_var: 0.01034662127494812
        vf_loss: 31.05953598022461
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2690834105014801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013168161967769265
        model: {}
        policy_loss: -0.0018929163925349712
        total_loss: 0.00033247051760554314
        vf_explained_var: 0.13920331001281738
        vf_loss: 26.98972511291504
    load_time_ms: 14366.013
    num_steps_sampled: 66144000
    num_steps_trained: 66144000
    sample_time_ms: 101524.809
    update_time_ms: 13.636
  iterations_since_restore: 39
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.222950819672132
    ram_util_percent: 15.179234972677596
  pid: 28385
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 467.0
    agent-3: 467.0
    agent-4: 285.5
    agent-5: 285.5
  policy_reward_mean:
    agent-0: 238.6
    agent-1: 238.6
    agent-2: 381.815
    agent-3: 381.815
    agent-4: 230.485
    agent-5: 230.485
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 302.0
    agent-3: 302.0
    agent-4: 179.0
    agent-5: 179.0
  sampler_perf:
    mean_env_wait_ms: 26.82469090629879
    mean_inference_ms: 13.002433286026344
    mean_processing_ms: 59.803260302836904
  time_since_restore: 5008.866034030914
  time_this_iter_s: 128.23563241958618
  time_total_s: 87141.30840420723
  timestamp: 1637598957
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 66144000
  training_iteration: 689
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    689 |          87141.3 | 66144000 |   1701.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 30.95
    apples_agent-1_min: 16
    apples_agent-2_max: 412
    apples_agent-2_mean: 354.24
    apples_agent-2_min: 289
    apples_agent-3_max: 269
    apples_agent-3_mean: 217.39
    apples_agent-3_min: 159
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.33
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 382.4
    apples_agent-5_min: 298
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 393.63
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 2.54
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 33.92
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 472.02
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-38-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1865.0
  episode_reward_mean: 1718.54
  episode_reward_min: 1357.0
  episodes_this_iter: 96
  episodes_total: 66240
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11601.864
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3527792692184448
        entropy_coeff: 0.0017600000137463212
        kl: 0.00138760672416538
        model: {}
        policy_loss: -0.0014242164324969053
        total_loss: 0.001298174960538745
        vf_explained_var: 0.004735395312309265
        vf_loss: 33.432857513427734
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23193956911563873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010821220930665731
        model: {}
        policy_loss: -0.0016615968197584152
        total_loss: 0.0009114891290664673
        vf_explained_var: 0.11128243803977966
        vf_loss: 29.813003540039062
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2846965789794922
        entropy_coeff: 0.0017600000137463212
        kl: 0.000885533809196204
        model: {}
        policy_loss: -0.0016659032553434372
        total_loss: 0.005628181155771017
        vf_explained_var: 0.043576374650001526
        vf_loss: 77.95150756835938
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5464969873428345
        entropy_coeff: 0.0017600000137463212
        kl: 0.000767057528719306
        model: {}
        policy_loss: -0.00156116159632802
        total_loss: 0.005345806013792753
        vf_explained_var: 0.03829416632652283
        vf_loss: 78.68801879882812
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9182676672935486
        entropy_coeff: 0.0017600000137463212
        kl: 0.002408525673672557
        model: {}
        policy_loss: -0.002110319212079048
        total_loss: -0.0006864990573376417
        vf_explained_var: 0.02569928765296936
        vf_loss: 30.39974594116211
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2724125385284424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008803372620604932
        model: {}
        policy_loss: -0.0019788278732448816
        total_loss: 0.00010160659439861774
        vf_explained_var: 0.1791089028120041
        vf_loss: 25.598806381225586
    load_time_ms: 14339.179
    num_steps_sampled: 66240000
    num_steps_trained: 66240000
    sample_time_ms: 101642.425
    update_time_ms: 13.807
  iterations_since_restore: 40
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.555737704918037
    ram_util_percent: 15.18633879781421
  pid: 28385
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 422.0
    agent-3: 422.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 242.6
    agent-1: 242.6
    agent-2: 385.875
    agent-3: 385.875
    agent-4: 230.795
    agent-5: 230.795
  policy_reward_min:
    agent-0: 184.5
    agent-1: 184.5
    agent-2: 312.0
    agent-3: 312.0
    agent-4: 182.0
    agent-5: 182.0
  sampler_perf:
    mean_env_wait_ms: 26.821950528518077
    mean_inference_ms: 13.002088605181733
    mean_processing_ms: 59.80575204418597
  time_since_restore: 5136.92245054245
  time_this_iter_s: 128.05641651153564
  time_total_s: 87269.36482071877
  timestamp: 1637599085
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 66240000
  training_iteration: 690
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    690 |          87269.4 | 66240000 |  1718.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.35
    apples_agent-1_min: 19
    apples_agent-2_max: 400
    apples_agent-2_mean: 357.52
    apples_agent-2_min: 249
    apples_agent-3_max: 275
    apples_agent-3_mean: 214.75
    apples_agent-3_min: 127
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.28
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 382.93
    apples_agent-5_min: 225
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 391.97
    cleaning_beam_agent-0_min: 272
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 38.89
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 482.79
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-40-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1838.0
  episode_reward_mean: 1716.33
  episode_reward_min: 1150.0
  episodes_this_iter: 96
  episodes_total: 66336
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11604.348
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3560142517089844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016086047980934381
        model: {}
        policy_loss: -0.0013697531539946795
        total_loss: 0.0013004251522943377
        vf_explained_var: 0.020957618951797485
        vf_loss: 32.96761703491211
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22428496181964874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009795394726097584
        model: {}
        policy_loss: -0.001702012843452394
        total_loss: 0.0009361589327454567
        vf_explained_var: 0.10228106379508972
        vf_loss: 30.329111099243164
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28446948528289795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007055640453472733
        model: {}
        policy_loss: -0.001695884857326746
        total_loss: 0.005520285107195377
        vf_explained_var: 0.05824929475784302
        vf_loss: 77.16836547851562
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5527321696281433
        entropy_coeff: 0.0017600000137463212
        kl: 0.000538521446287632
        model: {}
        policy_loss: -0.0015420839190483093
        total_loss: 0.0053716013208031654
        vf_explained_var: 0.03908625245094299
        vf_loss: 78.86490631103516
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9202442169189453
        entropy_coeff: 0.0017600000137463212
        kl: 0.002312123542651534
        model: {}
        policy_loss: -0.0019743521697819233
        total_loss: -0.0005715680308640003
        vf_explained_var: 0.01093532145023346
        vf_loss: 30.2241153717041
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.274772584438324
        entropy_coeff: 0.0017600000137463212
        kl: 0.00112040841486305
        model: {}
        policy_loss: -0.00196493836119771
        total_loss: 0.0002982323057949543
        vf_explained_var: 0.10034924745559692
        vf_loss: 27.46768569946289
    load_time_ms: 14355.37
    num_steps_sampled: 66336000
    num_steps_trained: 66336000
    sample_time_ms: 101594.414
    update_time_ms: 13.662
  iterations_since_restore: 41
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.099999999999994
    ram_util_percent: 15.207650273224049
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 428.5
    agent-3: 428.5
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 243.965
    agent-1: 243.965
    agent-2: 384.785
    agent-3: 384.785
    agent-4: 229.415
    agent-5: 229.415
  policy_reward_min:
    agent-0: 154.5
    agent-1: 154.5
    agent-2: 276.0
    agent-3: 276.0
    agent-4: 144.5
    agent-5: 144.5
  sampler_perf:
    mean_env_wait_ms: 26.81764817868373
    mean_inference_ms: 13.000920717132555
    mean_processing_ms: 59.801792407076334
  time_since_restore: 5264.014038324356
  time_this_iter_s: 127.09158778190613
  time_total_s: 87396.45640850067
  timestamp: 1637599214
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 66336000
  training_iteration: 691
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    691 |          87396.5 | 66336000 |  1716.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 0.27
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.02
    apples_agent-1_min: 9
    apples_agent-2_max: 408
    apples_agent-2_mean: 354.27
    apples_agent-2_min: 115
    apples_agent-3_max: 267
    apples_agent-3_mean: 219.11
    apples_agent-3_min: 89
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 459
    apples_agent-5_mean: 384.31
    apples_agent-5_min: 134
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 402.67
    cleaning_beam_agent-0_min: 344
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.91
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 38.6
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 489.0
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 3.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-42-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1837.0
  episode_reward_mean: 1698.55
  episode_reward_min: 615.0
  episodes_this_iter: 96
  episodes_total: 66432
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11592.623
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36754342913627625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017581610009074211
        model: {}
        policy_loss: -0.0017448102589696646
        total_loss: 0.0011599261779338121
        vf_explained_var: 0.07456514239311218
        vf_loss: 35.51610565185547
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2297523468732834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007433319697156549
        model: {}
        policy_loss: -0.002023800276219845
        total_loss: 0.0008896715007722378
        vf_explained_var: 0.13558344542980194
        vf_loss: 33.1783332824707
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28439587354660034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007357309223152697
        model: {}
        policy_loss: -0.0017941473051905632
        total_loss: 0.005988952703773975
        vf_explained_var: 0.10234969854354858
        vf_loss: 82.83638763427734
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5587917566299438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019965022802352905
        model: {}
        policy_loss: -0.0016261441633105278
        total_loss: 0.006305333226919174
        vf_explained_var: 0.03356744349002838
        vf_loss: 89.14952850341797
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9383069276809692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030197873711586
        model: {}
        policy_loss: -0.0025503828655928373
        total_loss: -0.0009492100216448307
        vf_explained_var: 0.006592541933059692
        vf_loss: 32.52595520019531
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28370946645736694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010125563712790608
        model: {}
        policy_loss: -0.0019812937825918198
        total_loss: 0.0002835211344063282
        vf_explained_var: 0.15668024122714996
        vf_loss: 27.641422271728516
    load_time_ms: 14341.722
    num_steps_sampled: 66432000
    num_steps_trained: 66432000
    sample_time_ms: 101621.599
    update_time_ms: 13.726
  iterations_since_restore: 42
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.397802197802196
    ram_util_percent: 15.152747252747254
  pid: 28385
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 240.425
    agent-1: 240.425
    agent-2: 379.87
    agent-3: 379.87
    agent-4: 228.98
    agent-5: 228.98
  policy_reward_min:
    agent-0: 86.0
    agent-1: 86.0
    agent-2: 145.0
    agent-3: 145.0
    agent-4: 76.5
    agent-5: 76.5
  sampler_perf:
    mean_env_wait_ms: 26.818514623869063
    mean_inference_ms: 12.999884546046346
    mean_processing_ms: 59.802638294661975
  time_since_restore: 5391.486462831497
  time_this_iter_s: 127.47242450714111
  time_total_s: 87523.92883300781
  timestamp: 1637599342
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 66432000
  training_iteration: 692
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    692 |          87523.9 | 66432000 |  1698.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.56
    apples_agent-1_min: 17
    apples_agent-2_max: 407
    apples_agent-2_mean: 355.43
    apples_agent-2_min: 268
    apples_agent-3_max: 274
    apples_agent-3_mean: 216.34
    apples_agent-3_min: 124
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 381.19
    apples_agent-5_min: 280
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 399.07
    cleaning_beam_agent-0_min: 313
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.96
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 1.69
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 37.63
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 467.31
    cleaning_beam_agent-4_min: 387
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-44-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1813.0
  episode_reward_mean: 1713.01
  episode_reward_min: 1248.0
  episodes_this_iter: 96
  episodes_total: 66528
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11594.211
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36893370747566223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018435592064633965
        model: {}
        policy_loss: -0.0013783262111246586
        total_loss: 0.0012760451063513756
        vf_explained_var: 0.007674813270568848
        vf_loss: 33.03690719604492
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2250698357820511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009348193416371942
        model: {}
        policy_loss: -0.0018410831689834595
        total_loss: 0.0006836708635091782
        vf_explained_var: 0.12265320122241974
        vf_loss: 29.208812713623047
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28014835715293884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011526885209605098
        model: {}
        policy_loss: -0.0017736898735165596
        total_loss: 0.006013122852891684
        vf_explained_var: 0.05476158857345581
        vf_loss: 82.79878234863281
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5646691918373108
        entropy_coeff: 0.0017600000137463212
        kl: 0.001020205905660987
        model: {}
        policy_loss: -0.00145779550075531
        total_loss: 0.005895830690860748
        vf_explained_var: 0.04740326106548309
        vf_loss: 83.47443389892578
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9256494045257568
        entropy_coeff: 0.0017600000137463212
        kl: 0.002565446775406599
        model: {}
        policy_loss: -0.0022755670361220837
        total_loss: -0.0009555728174746037
        vf_explained_var: 0.01815071702003479
        vf_loss: 29.49135971069336
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27757200598716736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009623492369428277
        model: {}
        policy_loss: -0.0018093083053827286
        total_loss: 0.00039715925231575966
        vf_explained_var: 0.10120663046836853
        vf_loss: 26.949926376342773
    load_time_ms: 14348.768
    num_steps_sampled: 66528000
    num_steps_trained: 66528000
    sample_time_ms: 101636.367
    update_time_ms: 13.618
  iterations_since_restore: 43
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.01436464088398
    ram_util_percent: 15.174033149171276
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 440.0
    agent-3: 440.0
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 243.185
    agent-1: 243.185
    agent-2: 384.07
    agent-3: 384.07
    agent-4: 229.25
    agent-5: 229.25
  policy_reward_min:
    agent-0: 177.0
    agent-1: 177.0
    agent-2: 275.0
    agent-3: 275.0
    agent-4: 172.0
    agent-5: 172.0
  sampler_perf:
    mean_env_wait_ms: 26.812938642271234
    mean_inference_ms: 13.000030263492569
    mean_processing_ms: 59.80274855409003
  time_since_restore: 5518.989687919617
  time_this_iter_s: 127.5032250881195
  time_total_s: 87651.43205809593
  timestamp: 1637599469
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 66528000
  training_iteration: 693
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    693 |          87651.4 | 66528000 |  1713.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 31.57
    apples_agent-1_min: 13
    apples_agent-2_max: 403
    apples_agent-2_mean: 354.48
    apples_agent-2_min: 295
    apples_agent-3_max: 288
    apples_agent-3_mean: 211.82
    apples_agent-3_min: 140
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 385.33
    apples_agent-5_min: 279
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 397.02
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.83
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 106
    cleaning_beam_agent-3_mean: 40.85
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 482.7
    cleaning_beam_agent-4_min: 419
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-46-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1846.0
  episode_reward_mean: 1717.81
  episode_reward_min: 1363.0
  episodes_this_iter: 96
  episodes_total: 66624
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11593.338
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37277472019195557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008256131550297141
        model: {}
        policy_loss: -0.0013969261199235916
        total_loss: 0.0014129402115941048
        vf_explained_var: -0.00251789391040802
        vf_loss: 34.659507751464844
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2268172949552536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009648512350395322
        model: {}
        policy_loss: -0.0018134003039449453
        total_loss: 0.0007586979772895575
        vf_explained_var: 0.14035385847091675
        vf_loss: 29.712947845458984
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27678588032722473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009090638486668468
        model: {}
        policy_loss: -0.0018542289035394788
        total_loss: 0.0054463050328195095
        vf_explained_var: 0.04790566861629486
        vf_loss: 77.87677764892578
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5721484422683716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006255552871152759
        model: {}
        policy_loss: -0.00139614287763834
        total_loss: 0.005351908504962921
        vf_explained_var: 0.05130811035633087
        vf_loss: 77.5503158569336
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9375504851341248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030409255996346474
        model: {}
        policy_loss: -0.0020815394818782806
        total_loss: -0.0006905947811901569
        vf_explained_var: 0.039151355624198914
        vf_loss: 30.410320281982422
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2632628083229065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012526526115834713
        model: {}
        policy_loss: -0.00196089968085289
        total_loss: 0.00019452255219221115
        vf_explained_var: 0.1681428700685501
        vf_loss: 26.187625885009766
    load_time_ms: 14336.731
    num_steps_sampled: 66624000
    num_steps_trained: 66624000
    sample_time_ms: 101650.274
    update_time_ms: 13.643
  iterations_since_restore: 44
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.741208791208788
    ram_util_percent: 15.20659340659341
  pid: 28385
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 427.0
    agent-3: 427.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 244.88
    agent-1: 244.88
    agent-2: 381.32
    agent-3: 381.32
    agent-4: 232.705
    agent-5: 232.705
  policy_reward_min:
    agent-0: 196.5
    agent-1: 196.5
    agent-2: 310.0
    agent-3: 310.0
    agent-4: 175.0
    agent-5: 175.0
  sampler_perf:
    mean_env_wait_ms: 26.80896118173049
    mean_inference_ms: 13.00015115174541
    mean_processing_ms: 59.79722584920014
  time_since_restore: 5646.152285337448
  time_this_iter_s: 127.16259741783142
  time_total_s: 87778.59465551376
  timestamp: 1637599596
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 66624000
  training_iteration: 694
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    694 |          87778.6 | 66624000 |  1717.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 32.49
    apples_agent-1_min: 8
    apples_agent-2_max: 446
    apples_agent-2_mean: 354.57
    apples_agent-2_min: 78
    apples_agent-3_max: 288
    apples_agent-3_mean: 216.32
    apples_agent-3_min: 42
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 379.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 390.48
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.66
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 37.12
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 627
    cleaning_beam_agent-4_mean: 489.46
    cleaning_beam_agent-4_min: 416
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-48-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1837.0
  episode_reward_mean: 1700.87
  episode_reward_min: 246.0
  episodes_this_iter: 96
  episodes_total: 66720
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11597.559
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36838239431381226
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012924035545438528
        model: {}
        policy_loss: -0.0012356197694316506
        total_loss: 0.0016393056139349937
        vf_explained_var: 0.06410087645053864
        vf_loss: 35.23278045654297
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2285664677619934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008869342273101211
        model: {}
        policy_loss: -0.0019667185842990875
        total_loss: 0.0007163216359913349
        vf_explained_var: 0.17947639524936676
        vf_loss: 30.853166580200195
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28234434127807617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006796028465032578
        model: {}
        policy_loss: -0.0017512650229036808
        total_loss: 0.0061073871329426765
        vf_explained_var: 0.09294702112674713
        vf_loss: 83.5557861328125
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5714223980903625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010592681355774403
        model: {}
        policy_loss: -0.001921861432492733
        total_loss: 0.005841617006808519
        vf_explained_var: 0.048202306032180786
        vf_loss: 87.69180297851562
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9219633340835571
        entropy_coeff: 0.0017600000137463212
        kl: 0.002497968729585409
        model: {}
        policy_loss: -0.0021242457441985607
        total_loss: -0.0001448432740289718
        vf_explained_var: 0.027705147862434387
        vf_loss: 36.02056121826172
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2689942717552185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006880259024910629
        model: {}
        policy_loss: -0.001983810681849718
        total_loss: 0.0007942691445350647
        vf_explained_var: 0.12264913320541382
        vf_loss: 32.515113830566406
    load_time_ms: 14340.664
    num_steps_sampled: 66720000
    num_steps_trained: 66720000
    sample_time_ms: 101556.736
    update_time_ms: 13.576
  iterations_since_restore: 45
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.331491712707184
    ram_util_percent: 15.146408839779005
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 426.5
    agent-3: 426.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 238.325
    agent-1: 238.325
    agent-2: 383.87
    agent-3: 383.87
    agent-4: 228.24
    agent-5: 228.24
  policy_reward_min:
    agent-0: 55.5
    agent-1: 55.5
    agent-2: 92.5
    agent-3: 92.5
    agent-4: -25.0
    agent-5: -25.0
  sampler_perf:
    mean_env_wait_ms: 26.806683466837207
    mean_inference_ms: 13.000093714435087
    mean_processing_ms: 59.79434817103833
  time_since_restore: 5773.625032186508
  time_this_iter_s: 127.47274684906006
  time_total_s: 87906.06740236282
  timestamp: 1637599724
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 66720000
  training_iteration: 695
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    695 |          87906.1 | 66720000 |  1700.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 31.72
    apples_agent-1_min: 15
    apples_agent-2_max: 414
    apples_agent-2_mean: 362.18
    apples_agent-2_min: 161
    apples_agent-3_max: 255
    apples_agent-3_mean: 211.82
    apples_agent-3_min: 96
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 429
    apples_agent-5_mean: 385.74
    apples_agent-5_min: 219
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 391.54
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 2.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.31
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 44.0
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 512.44
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-50-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1852.0
  episode_reward_mean: 1717.01
  episode_reward_min: 792.0
  episodes_this_iter: 96
  episodes_total: 66816
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11598.308
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36888283491134644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016405251808464527
        model: {}
        policy_loss: -0.0015902332961559296
        total_loss: 0.0012116702273488045
        vf_explained_var: 0.02492716908454895
        vf_loss: 34.51137161254883
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22704669833183289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011835034238174558
        model: {}
        policy_loss: -0.0019819610752165318
        total_loss: 0.0007550788577646017
        vf_explained_var: 0.11276736855506897
        vf_loss: 31.366405487060547
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27593886852264404
        entropy_coeff: 0.0017600000137463212
        kl: 0.001307590981014073
        model: {}
        policy_loss: -0.0015480616129934788
        total_loss: 0.006127890665084124
        vf_explained_var: 0.058677300810813904
        vf_loss: 81.61605834960938
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5793883800506592
        entropy_coeff: 0.0017600000137463212
        kl: 0.000989578547887504
        model: {}
        policy_loss: -0.0017125913873314857
        total_loss: 0.00553442956879735
        vf_explained_var: 0.0464852899312973
        vf_loss: 82.66747283935547
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9234183430671692
        entropy_coeff: 0.0017600000137463212
        kl: 0.002051697578281164
        model: {}
        policy_loss: -0.001890021376311779
        total_loss: -0.00041736848652362823
        vf_explained_var: 0.004143044352531433
        vf_loss: 30.97867202758789
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2701139450073242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009112115949392319
        model: {}
        policy_loss: -0.0016615847125649452
        total_loss: 0.00040945596992969513
        vf_explained_var: 0.17901262640953064
        vf_loss: 25.464384078979492
    load_time_ms: 14377.794
    num_steps_sampled: 66816000
    num_steps_trained: 66816000
    sample_time_ms: 101664.244
    update_time_ms: 13.7
  iterations_since_restore: 46
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.905434782608697
    ram_util_percent: 15.213043478260872
  pid: 28385
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 418.0
    agent-3: 418.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 243.565
    agent-1: 243.565
    agent-2: 383.875
    agent-3: 383.875
    agent-4: 231.065
    agent-5: 231.065
  policy_reward_min:
    agent-0: 120.0
    agent-1: 120.0
    agent-2: 168.5
    agent-3: 168.5
    agent-4: 107.5
    agent-5: 107.5
  sampler_perf:
    mean_env_wait_ms: 26.80965828535933
    mean_inference_ms: 13.002737873914318
    mean_processing_ms: 59.80275284678725
  time_since_restore: 5902.519627571106
  time_this_iter_s: 128.89459538459778
  time_total_s: 88034.96199774742
  timestamp: 1637599853
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 66816000
  training_iteration: 696
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    696 |            88035 | 66816000 |  1717.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 32.82
    apples_agent-1_min: 4
    apples_agent-2_max: 425
    apples_agent-2_mean: 355.84
    apples_agent-2_min: 13
    apples_agent-3_max: 284
    apples_agent-3_mean: 217.43
    apples_agent-3_min: 15
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 380.28
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 392.81
    cleaning_beam_agent-0_min: 360
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 1.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 1.9
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 36.65
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 609
    cleaning_beam_agent-4_mean: 511.01
    cleaning_beam_agent-4_min: 430
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 3.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-53-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1917.0
  episode_reward_mean: 1707.66
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 66912
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11598.794
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3663313090801239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010232977801933885
        model: {}
        policy_loss: -0.0016564601100981236
        total_loss: 0.0011918521486222744
        vf_explained_var: 0.08348749577999115
        vf_loss: 34.93058776855469
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22773012518882751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007702799048274755
        model: {}
        policy_loss: -0.0017369820270687342
        total_loss: 0.001024492084980011
        vf_explained_var: 0.17095287144184113
        vf_loss: 31.622760772705078
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2801845669746399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009897614363580942
        model: {}
        policy_loss: -0.001750006340444088
        total_loss: 0.005980124231427908
        vf_explained_var: 0.11101900041103363
        vf_loss: 82.23253631591797
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5609332323074341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006724480772390962
        model: {}
        policy_loss: -0.0016419622115790844
        total_loss: 0.006199294701218605
        vf_explained_var: 0.04679296910762787
        vf_loss: 88.28497314453125
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9095048904418945
        entropy_coeff: 0.0017600000137463212
        kl: 0.002540906425565481
        model: {}
        policy_loss: -0.002205892000347376
        total_loss: -0.000523202121257782
        vf_explained_var: 0.03583279252052307
        vf_loss: 32.83417510986328
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2720388174057007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011605890467762947
        model: {}
        policy_loss: -0.0023298957385122776
        total_loss: 4.5855995267629623e-05
        vf_explained_var: 0.16046424210071564
        vf_loss: 28.545381546020508
    load_time_ms: 14357.536
    num_steps_sampled: 66912000
    num_steps_trained: 66912000
    sample_time_ms: 101687.228
    update_time_ms: 13.738
  iterations_since_restore: 47
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.554945054945055
    ram_util_percent: 15.185714285714289
  pid: 28385
  policy_reward_max:
    agent-0: 282.5
    agent-1: 282.5
    agent-2: 432.0
    agent-3: 432.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 241.265
    agent-1: 241.265
    agent-2: 384.475
    agent-3: 384.475
    agent-4: 228.09
    agent-5: 228.09
  policy_reward_min:
    agent-0: 8.0
    agent-1: 8.0
    agent-2: 19.5
    agent-3: 19.5
    agent-4: 12.0
    agent-5: 12.0
  sampler_perf:
    mean_env_wait_ms: 26.8113801381004
    mean_inference_ms: 13.00451670670109
    mean_processing_ms: 59.80825566100599
  time_since_restore: 6030.2226893901825
  time_this_iter_s: 127.70306181907654
  time_total_s: 88162.6650595665
  timestamp: 1637599981
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 66912000
  training_iteration: 697
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    697 |          88162.7 | 66912000 |  1707.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.59
    apples_agent-1_min: 16
    apples_agent-2_max: 428
    apples_agent-2_mean: 362.8
    apples_agent-2_min: 269
    apples_agent-3_max: 263
    apples_agent-3_mean: 215.99
    apples_agent-3_min: 103
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.21
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 382.38
    apples_agent-5_min: 279
    cleaning_beam_agent-0_max: 416
    cleaning_beam_agent-0_mean: 391.21
    cleaning_beam_agent-0_min: 360
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.67
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 33.79
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 613
    cleaning_beam_agent-4_mean: 512.17
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-55-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1882.0
  episode_reward_mean: 1723.81
  episode_reward_min: 1195.0
  episodes_this_iter: 96
  episodes_total: 67008
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11601.415
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3530896008014679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020387086551636457
        model: {}
        policy_loss: -0.0013484549708664417
        total_loss: 0.0013303929008543491
        vf_explained_var: 0.0042142122983932495
        vf_loss: 33.00285339355469
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22583521902561188
        entropy_coeff: 0.0017600000137463212
        kl: 0.001108589000068605
        model: {}
        policy_loss: -0.0019047297537326813
        total_loss: 0.000718727707862854
        vf_explained_var: 0.08775913715362549
        vf_loss: 30.209266662597656
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27917301654815674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008144911262206733
        model: {}
        policy_loss: -0.001717531937174499
        total_loss: 0.006011064164340496
        vf_explained_var: 0.047348231077194214
        vf_loss: 82.19941711425781
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5559000968933105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005404216353781521
        model: {}
        policy_loss: -0.0016970420256257057
        total_loss: 0.005791779607534409
        vf_explained_var: 0.02315106987953186
        vf_loss: 84.67202758789062
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9107500910758972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012414540397003293
        model: {}
        policy_loss: -0.0017641638405621052
        total_loss: -0.000452454318292439
        vf_explained_var: 0.05557413399219513
        vf_loss: 29.146263122558594
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2674010396003723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006989334942772985
        model: {}
        policy_loss: -0.001650866586714983
        total_loss: 0.0004450692795217037
        vf_explained_var: 0.16251669824123383
        vf_loss: 25.66564178466797
    load_time_ms: 14354.45
    num_steps_sampled: 67008000
    num_steps_trained: 67008000
    sample_time_ms: 101653.203
    update_time_ms: 13.538
  iterations_since_restore: 48
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.762637362637367
    ram_util_percent: 15.176923076923082
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 424.0
    agent-3: 424.0
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 242.635
    agent-1: 242.635
    agent-2: 388.005
    agent-3: 388.005
    agent-4: 231.265
    agent-5: 231.265
  policy_reward_min:
    agent-0: 152.0
    agent-1: 152.0
    agent-2: 280.5
    agent-3: 280.5
    agent-4: 165.0
    agent-5: 165.0
  sampler_perf:
    mean_env_wait_ms: 26.814447432196786
    mean_inference_ms: 13.00406604142254
    mean_processing_ms: 59.80642877913701
  time_since_restore: 6157.7345588207245
  time_this_iter_s: 127.51186943054199
  time_total_s: 88290.17692899704
  timestamp: 1637600109
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 67008000
  training_iteration: 698
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    698 |          88290.2 | 67008000 |  1723.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.07
    apples_agent-1_min: 12
    apples_agent-2_max: 413
    apples_agent-2_mean: 356.79
    apples_agent-2_min: 107
    apples_agent-3_max: 272
    apples_agent-3_mean: 212.65
    apples_agent-3_min: 76
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 459
    apples_agent-5_mean: 380.29
    apples_agent-5_min: 124
    cleaning_beam_agent-0_max: 408
    cleaning_beam_agent-0_mean: 380.96
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.86
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 2.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 38.26
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 626
    cleaning_beam_agent-4_mean: 507.03
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 2.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-57-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1840.0
  episode_reward_mean: 1707.88
  episode_reward_min: 515.0
  episodes_this_iter: 96
  episodes_total: 67104
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11597.816
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34886687994003296
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014824161771684885
        model: {}
        policy_loss: -0.0013365579070523381
        total_loss: 0.0015744578558951616
        vf_explained_var: 0.0319741815328598
        vf_loss: 35.25021743774414
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23172315955162048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010669954353943467
        model: {}
        policy_loss: -0.0019113291054964066
        total_loss: 0.0008158539421856403
        vf_explained_var: 0.13884493708610535
        vf_loss: 31.35016632080078
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2810334861278534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011893854243680835
        model: {}
        policy_loss: -0.0018321371171623468
        total_loss: 0.006071503274142742
        vf_explained_var: 0.07398074865341187
        vf_loss: 83.98255920410156
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5648513436317444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009375131339766085
        model: {}
        policy_loss: -0.0017063511768355966
        total_loss: 0.006122898310422897
        vf_explained_var: 0.02808581292629242
        vf_loss: 88.23387908935547
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9097344875335693
        entropy_coeff: 0.0017600000137463212
        kl: 0.001879702671431005
        model: {}
        policy_loss: -0.0021891845390200615
        total_loss: -0.0005802176892757416
        vf_explained_var: 0.06114780902862549
        vf_loss: 32.10101318359375
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27284878492355347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007640669355168939
        model: {}
        policy_loss: -0.002137353178113699
        total_loss: 0.00023647258058190346
        vf_explained_var: 0.1677083671092987
        vf_loss: 28.540424346923828
    load_time_ms: 14358.057
    num_steps_sampled: 67104000
    num_steps_trained: 67104000
    sample_time_ms: 101576.268
    update_time_ms: 13.666
  iterations_since_restore: 49
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.32582417582417
    ram_util_percent: 15.157142857142858
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 251.0
    agent-5: 251.0
  policy_reward_mean:
    agent-0: 241.73
    agent-1: 241.73
    agent-2: 383.055
    agent-3: 383.055
    agent-4: 229.155
    agent-5: 229.155
  policy_reward_min:
    agent-0: 71.5
    agent-1: 71.5
    agent-2: 101.0
    agent-3: 101.0
    agent-4: 85.0
    agent-5: 85.0
  sampler_perf:
    mean_env_wait_ms: 26.811109149466642
    mean_inference_ms: 13.004530887789379
    mean_processing_ms: 59.80917559215945
  time_since_restore: 6285.229147672653
  time_this_iter_s: 127.49458885192871
  time_total_s: 88417.67151784897
  timestamp: 1637600236
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 67104000
  training_iteration: 699
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    699 |          88417.7 | 67104000 |  1707.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 31.12
    apples_agent-1_min: 3
    apples_agent-2_max: 404
    apples_agent-2_mean: 355.17
    apples_agent-2_min: 33
    apples_agent-3_max: 260
    apples_agent-3_mean: 213.57
    apples_agent-3_min: 9
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 378.02
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 417
    cleaning_beam_agent-0_mean: 385.67
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 2.29
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 32.7
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 580
    cleaning_beam_agent-4_mean: 504.13
    cleaning_beam_agent-4_min: 415
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 3.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_11-59-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1826.0
  episode_reward_mean: 1705.63
  episode_reward_min: 134.0
  episodes_this_iter: 96
  episodes_total: 67200
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11596.579
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3503785729408264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014914266066625714
        model: {}
        policy_loss: -0.001389678567647934
        total_loss: 0.0014187968336045742
        vf_explained_var: 0.09770789742469788
        vf_loss: 34.25138854980469
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23533779382705688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008527052123099566
        model: {}
        policy_loss: -0.0017375289462506771
        total_loss: 0.00094192149117589
        vf_explained_var: 0.18547546863555908
        vf_loss: 30.936464309692383
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28329187631607056
        entropy_coeff: 0.0017600000137463212
        kl: 0.001052803942002356
        model: {}
        policy_loss: -0.001882316661067307
        total_loss: 0.006032983772456646
        vf_explained_var: 0.12007889151573181
        vf_loss: 84.13897705078125
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.549464225769043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008528206963092089
        model: {}
        policy_loss: -0.0014704505447298288
        total_loss: 0.0067776949144899845
        vf_explained_var: 0.036528363823890686
        vf_loss: 92.15203857421875
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9137364029884338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013983985409140587
        model: {}
        policy_loss: -0.001826319145038724
        total_loss: -8.348224218934774e-05
        vf_explained_var: 0.015528887510299683
        vf_loss: 33.510108947753906
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2758404016494751
        entropy_coeff: 0.0017600000137463212
        kl: 0.000566152622923255
        model: {}
        policy_loss: -0.0021288227289915085
        total_loss: 0.00016229785978794098
        vf_explained_var: 0.1847994327545166
        vf_loss: 27.766002655029297
    load_time_ms: 14363.382
    num_steps_sampled: 67200000
    num_steps_trained: 67200000
    sample_time_ms: 101515.78
    update_time_ms: 13.397
  iterations_since_restore: 50
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.021546961325964
    ram_util_percent: 15.178453038674038
  pid: 28385
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 442.5
    agent-3: 442.5
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 240.19
    agent-1: 240.19
    agent-2: 384.43
    agent-3: 384.43
    agent-4: 228.195
    agent-5: 228.195
  policy_reward_min:
    agent-0: 18.5
    agent-1: 18.5
    agent-2: 30.5
    agent-3: 30.5
    agent-4: 18.0
    agent-5: 18.0
  sampler_perf:
    mean_env_wait_ms: 26.81103003823811
    mean_inference_ms: 13.004740034413757
    mean_processing_ms: 59.80609496229834
  time_since_restore: 6412.66624879837
  time_this_iter_s: 127.43710112571716
  time_total_s: 88545.10861897469
  timestamp: 1637600364
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 67200000
  training_iteration: 700
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    700 |          88545.1 | 67200000 |  1705.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 31.7
    apples_agent-1_min: 13
    apples_agent-2_max: 409
    apples_agent-2_mean: 357.62
    apples_agent-2_min: 186
    apples_agent-3_max: 270
    apples_agent-3_mean: 213.56
    apples_agent-3_min: 108
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 384.63
    apples_agent-5_min: 252
    cleaning_beam_agent-0_max: 411
    cleaning_beam_agent-0_mean: 382.11
    cleaning_beam_agent-0_min: 345
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 2.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 33.2
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 500.68
    cleaning_beam_agent-4_min: 411
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-01-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1865.0
  episode_reward_mean: 1726.1
  episode_reward_min: 1000.0
  episodes_this_iter: 96
  episodes_total: 67296
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11598.052
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3411218225955963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012621888890862465
        model: {}
        policy_loss: -0.0013886860106140375
        total_loss: 0.001335421809926629
        vf_explained_var: 0.03205382823944092
        vf_loss: 33.24481201171875
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2261527180671692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011990821221843362
        model: {}
        policy_loss: -0.0017659533768892288
        total_loss: 0.0008117174729704857
        vf_explained_var: 0.1367160677909851
        vf_loss: 29.756980895996094
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27563410997390747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006747005390934646
        model: {}
        policy_loss: -0.0016449240501970053
        total_loss: 0.006123990751802921
        vf_explained_var: 0.08029693365097046
        vf_loss: 82.540283203125
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5514537692070007
        entropy_coeff: 0.0017600000137463212
        kl: 0.001092810183763504
        model: {}
        policy_loss: -0.0016137654893100262
        total_loss: 0.005922191310673952
        vf_explained_var: 0.05239194631576538
        vf_loss: 85.06517028808594
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9046630263328552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009700221125967801
        model: {}
        policy_loss: -0.0018056833650916815
        total_loss: -0.00028599356301128864
        vf_explained_var: 0.02830110490322113
        vf_loss: 31.11897087097168
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26179975271224976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006957676378078759
        model: {}
        policy_loss: -0.0017817565239965916
        total_loss: 0.0005069705657660961
        vf_explained_var: 0.13639242947101593
        vf_loss: 27.494930267333984
    load_time_ms: 14350.393
    num_steps_sampled: 67296000
    num_steps_trained: 67296000
    sample_time_ms: 101606.506
    update_time_ms: 13.588
  iterations_since_restore: 51
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.76358695652174
    ram_util_percent: 15.205434782608696
  pid: 28385
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 428.0
    agent-3: 428.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 246.35
    agent-1: 246.35
    agent-2: 385.03
    agent-3: 385.03
    agent-4: 231.67
    agent-5: 231.67
  policy_reward_min:
    agent-0: 150.5
    agent-1: 150.5
    agent-2: 203.5
    agent-3: 203.5
    agent-4: 146.0
    agent-5: 146.0
  sampler_perf:
    mean_env_wait_ms: 26.810750589707727
    mean_inference_ms: 13.0047559635079
    mean_processing_ms: 59.80833718646986
  time_since_restore: 6540.549125909805
  time_this_iter_s: 127.88287711143494
  time_total_s: 88672.99149608612
  timestamp: 1637600493
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 67296000
  training_iteration: 701
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    701 |            88673 | 67296000 |   1726.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 30.96
    apples_agent-1_min: 12
    apples_agent-2_max: 408
    apples_agent-2_mean: 353.02
    apples_agent-2_min: 202
    apples_agent-3_max: 271
    apples_agent-3_mean: 209.89
    apples_agent-3_min: 121
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 466
    apples_agent-5_mean: 385.31
    apples_agent-5_min: 238
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 385.42
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 2.05
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 1.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 33.81
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 500.01
    cleaning_beam_agent-4_min: 430
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 3.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-03-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1839.0
  episode_reward_mean: 1710.59
  episode_reward_min: 988.0
  episodes_this_iter: 96
  episodes_total: 67392
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11610.704
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3412231206893921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008643683395348489
        model: {}
        policy_loss: -0.0013648532330989838
        total_loss: 0.001573466695845127
        vf_explained_var: 0.05724705755710602
        vf_loss: 35.388702392578125
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23085972666740417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009674226166680455
        model: {}
        policy_loss: -0.001859885174781084
        total_loss: 0.0009057233110070229
        vf_explained_var: 0.15464335680007935
        vf_loss: 31.719192504882812
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2775610685348511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008807166595943272
        model: {}
        policy_loss: -0.0016903462819755077
        total_loss: 0.00584240211173892
        vf_explained_var: 0.1054200679063797
        vf_loss: 80.21253204345703
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5596011877059937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007008215179666877
        model: {}
        policy_loss: -0.0016432586126029491
        total_loss: 0.005903960205614567
        vf_explained_var: 0.04988420009613037
        vf_loss: 85.32115936279297
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9046085476875305
        entropy_coeff: 0.0017600000137463212
        kl: 0.002615025732666254
        model: {}
        policy_loss: -0.0020904075354337692
        total_loss: -0.0005075722001492977
        vf_explained_var: 0.047994986176490784
        vf_loss: 31.74946403503418
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2721177935600281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005231577670201659
        model: {}
        policy_loss: -0.0017553383950144053
        total_loss: 0.0007582888938486576
        vf_explained_var: 0.09748843312263489
        vf_loss: 29.925556182861328
    load_time_ms: 14371.028
    num_steps_sampled: 67392000
    num_steps_trained: 67392000
    sample_time_ms: 101601.335
    update_time_ms: 13.543
  iterations_since_restore: 52
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.81483516483517
    ram_util_percent: 15.180219780219783
  pid: 28385
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 424.0
    agent-3: 424.0
    agent-4: 270.5
    agent-5: 270.5
  policy_reward_mean:
    agent-0: 241.04
    agent-1: 241.04
    agent-2: 381.885
    agent-3: 381.885
    agent-4: 232.37
    agent-5: 232.37
  policy_reward_min:
    agent-0: 127.5
    agent-1: 127.5
    agent-2: 220.5
    agent-3: 220.5
    agent-4: 146.0
    agent-5: 146.0
  sampler_perf:
    mean_env_wait_ms: 26.80852736156022
    mean_inference_ms: 13.004053834908813
    mean_processing_ms: 59.8044606040065
  time_since_restore: 6668.352555274963
  time_this_iter_s: 127.80342936515808
  time_total_s: 88800.79492545128
  timestamp: 1637600621
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 67392000
  training_iteration: 702
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    702 |          88800.8 | 67392000 |  1710.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 0.29
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 30.14
    apples_agent-1_min: 3
    apples_agent-2_max: 414
    apples_agent-2_mean: 358.97
    apples_agent-2_min: 17
    apples_agent-3_max: 262
    apples_agent-3_mean: 210.18
    apples_agent-3_min: 8
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 381.73
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 385.24
    cleaning_beam_agent-0_min: 332
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 2.2
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.39
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 35.79
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 493.57
    cleaning_beam_agent-4_min: 406
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.23
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-05-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1834.0
  episode_reward_mean: 1715.61
  episode_reward_min: 107.0
  episodes_this_iter: 96
  episodes_total: 67488
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11611.382
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3502647876739502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018991883844137192
        model: {}
        policy_loss: -0.0015014607924968004
        total_loss: 0.0012083909241482615
        vf_explained_var: 0.08118663728237152
        vf_loss: 33.263179779052734
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22842222452163696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009906081249937415
        model: {}
        policy_loss: -0.0017111399210989475
        total_loss: 0.0009924850892275572
        vf_explained_var: 0.14288559556007385
        vf_loss: 31.056488037109375
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27694177627563477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012028578203171492
        model: {}
        policy_loss: -0.0018900190480053425
        total_loss: 0.005879473406821489
        vf_explained_var: 0.12428063154220581
        vf_loss: 82.569091796875
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5609652996063232
        entropy_coeff: 0.0017600000137463212
        kl: 0.00044897786574438214
        model: {}
        policy_loss: -0.0015100708696991205
        total_loss: 0.006438601762056351
        vf_explained_var: 0.05345597863197327
        vf_loss: 89.3597183227539
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9120928049087524
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025390377268195152
        model: {}
        policy_loss: -0.0019635623320937157
        total_loss: -0.00010243715951219201
        vf_explained_var: 0.010054171085357666
        vf_loss: 34.66410446166992
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2670982778072357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012778553646057844
        model: {}
        policy_loss: -0.002046233508735895
        total_loss: 0.000348587054759264
        vf_explained_var: 0.17995843291282654
        vf_loss: 28.649147033691406
    load_time_ms: 14362.788
    num_steps_sampled: 67488000
    num_steps_trained: 67488000
    sample_time_ms: 101614.798
    update_time_ms: 13.803
  iterations_since_restore: 53
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.428571428571427
    ram_util_percent: 15.185164835164834
  pid: 28385
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 434.0
    agent-3: 434.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 240.92
    agent-1: 240.92
    agent-2: 385.795
    agent-3: 385.795
    agent-4: 231.09
    agent-5: 231.09
  policy_reward_min:
    agent-0: 15.5
    agent-1: 15.5
    agent-2: 22.0
    agent-3: 22.0
    agent-4: 16.0
    agent-5: 16.0
  sampler_perf:
    mean_env_wait_ms: 26.808240745661628
    mean_inference_ms: 13.003963259979452
    mean_processing_ms: 59.806433381555415
  time_since_restore: 6795.8435661792755
  time_this_iter_s: 127.49101090431213
  time_total_s: 88928.28593635559
  timestamp: 1637600749
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 67488000
  training_iteration: 703
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    703 |          88928.3 | 67488000 |  1715.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.1
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.17
    apples_agent-1_min: 1
    apples_agent-2_max: 412
    apples_agent-2_mean: 355.0
    apples_agent-2_min: 21
    apples_agent-3_max: 303
    apples_agent-3_mean: 218.2
    apples_agent-3_min: 3
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 378.89
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 389.23
    cleaning_beam_agent-0_min: 350
    cleaning_beam_agent-1_max: 26
    cleaning_beam_agent-1_mean: 2.1
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 2.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 28.82
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 497.81
    cleaning_beam_agent-4_min: 422
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 2.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-07-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1840.0
  episode_reward_mean: 1725.97
  episode_reward_min: 145.0
  episodes_this_iter: 96
  episodes_total: 67584
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11609.905
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3589707016944885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013635912910103798
        model: {}
        policy_loss: -0.0013102078810334206
        total_loss: 0.0015895580872893333
        vf_explained_var: 0.08358733355998993
        vf_loss: 35.315555572509766
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2257651835680008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011155568063259125
        model: {}
        policy_loss: -0.0018731881864368916
        total_loss: 0.0009750993922352791
        vf_explained_var: 0.1617007553577423
        vf_loss: 32.45631790161133
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27658531069755554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009254055330529809
        model: {}
        policy_loss: -0.0017073622439056635
        total_loss: 0.006275709718465805
        vf_explained_var: 0.12973901629447937
        vf_loss: 84.69857788085938
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5643140077590942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014874867629259825
        model: {}
        policy_loss: -0.0015776250511407852
        total_loss: 0.006677260156720877
        vf_explained_var: 0.047457873821258545
        vf_loss: 92.48080444335938
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9260024428367615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017410402651876211
        model: {}
        policy_loss: -0.0018441334832459688
        total_loss: -4.6432833187282085e-05
        vf_explained_var: 0.007937058806419373
        vf_loss: 34.274654388427734
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26746073365211487
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009733022889122367
        model: {}
        policy_loss: -0.0020490819588303566
        total_loss: 0.00039934992673806846
        vf_explained_var: 0.15569953620433807
        vf_loss: 29.191617965698242
    load_time_ms: 14374.239
    num_steps_sampled: 67584000
    num_steps_trained: 67584000
    sample_time_ms: 101638.856
    update_time_ms: 13.825
  iterations_since_restore: 54
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.729670329670334
    ram_util_percent: 15.158241758241761
  pid: 28385
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 433.5
    agent-3: 433.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 243.975
    agent-1: 243.975
    agent-2: 389.59
    agent-3: 389.59
    agent-4: 229.42
    agent-5: 229.42
  policy_reward_min:
    agent-0: 24.5
    agent-1: 24.5
    agent-2: 26.5
    agent-3: 26.5
    agent-4: 21.5
    agent-5: 21.5
  sampler_perf:
    mean_env_wait_ms: 26.808504829117783
    mean_inference_ms: 13.003121639910272
    mean_processing_ms: 59.80430008362142
  time_since_restore: 6923.340938329697
  time_this_iter_s: 127.49737215042114
  time_total_s: 89055.78330850601
  timestamp: 1637600876
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 67584000
  training_iteration: 704
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    704 |          89055.8 | 67584000 |  1725.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 31.52
    apples_agent-1_min: 14
    apples_agent-2_max: 397
    apples_agent-2_mean: 360.3
    apples_agent-2_min: 299
    apples_agent-3_max: 272
    apples_agent-3_mean: 217.47
    apples_agent-3_min: 144
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 430
    apples_agent-5_mean: 386.09
    apples_agent-5_min: 331
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 397.54
    cleaning_beam_agent-0_min: 369
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 2.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 2.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 29.88
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 490.14
    cleaning_beam_agent-4_min: 422
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-10-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1839.0
  episode_reward_mean: 1740.51
  episode_reward_min: 1504.0
  episodes_this_iter: 96
  episodes_total: 67680
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11617.007
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3570808470249176
        entropy_coeff: 0.0017600000137463212
        kl: 0.002538938308134675
        model: {}
        policy_loss: -0.0013990122824907303
        total_loss: 0.0012899953871965408
        vf_explained_var: -0.0028259605169296265
        vf_loss: 33.1746826171875
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22105062007904053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010755700059235096
        model: {}
        policy_loss: -0.0016506286337971687
        total_loss: 0.0009720069356262684
        vf_explained_var: 0.09016327559947968
        vf_loss: 30.116867065429688
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2687404453754425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010058749467134476
        model: {}
        policy_loss: -0.0015491275116801262
        total_loss: 0.006103045307099819
        vf_explained_var: 0.040510207414627075
        vf_loss: 81.25154113769531
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5597637891769409
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008830669103190303
        model: {}
        policy_loss: -0.001465433044359088
        total_loss: 0.00562968710437417
        vf_explained_var: 0.04533550143241882
        vf_loss: 80.80303192138672
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9342158436775208
        entropy_coeff: 0.0017600000137463212
        kl: 0.002493683947250247
        model: {}
        policy_loss: -0.0018315515480935574
        total_loss: -0.000622614286839962
        vf_explained_var: 0.024635732173919678
        vf_loss: 28.531551361083984
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25389909744262695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006817069370299578
        model: {}
        policy_loss: -0.0015064808540046215
        total_loss: 0.0006148554384708405
        vf_explained_var: 0.12036006152629852
        vf_loss: 25.68199348449707
    load_time_ms: 14405.114
    num_steps_sampled: 67680000
    num_steps_trained: 67680000
    sample_time_ms: 101656.58
    update_time_ms: 13.731
  iterations_since_restore: 55
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.68241758241758
    ram_util_percent: 15.202747252747253
  pid: 28385
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 433.0
    agent-3: 433.0
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 246.03
    agent-1: 246.03
    agent-2: 390.83
    agent-3: 390.83
    agent-4: 233.395
    agent-5: 233.395
  policy_reward_min:
    agent-0: 206.0
    agent-1: 206.0
    agent-2: 331.5
    agent-3: 331.5
    agent-4: 204.5
    agent-5: 204.5
  sampler_perf:
    mean_env_wait_ms: 26.80607635254198
    mean_inference_ms: 13.002754310007107
    mean_processing_ms: 59.80247010477128
  time_since_restore: 7051.44104218483
  time_this_iter_s: 128.10010385513306
  time_total_s: 89183.88341236115
  timestamp: 1637601004
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 67680000
  training_iteration: 705
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    705 |          89183.9 | 67680000 |  1740.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 31.28
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 360.19
    apples_agent-2_min: 261
    apples_agent-3_max: 279
    apples_agent-3_mean: 214.15
    apples_agent-3_min: 134
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.3
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 384.37
    apples_agent-5_min: 282
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 405.09
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.59
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 31.14
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 473.22
    cleaning_beam_agent-4_min: 391
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-12-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1815.0
  episode_reward_mean: 1728.49
  episode_reward_min: 1226.0
  episodes_this_iter: 96
  episodes_total: 67776
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11618.351
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35521581768989563
        entropy_coeff: 0.0017600000137463212
        kl: 0.000772181898355484
        model: {}
        policy_loss: -0.001163652166724205
        total_loss: 0.0016244137659668922
        vf_explained_var: 0.005259603261947632
        vf_loss: 34.13248062133789
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22611577808856964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007262725266627967
        model: {}
        policy_loss: -0.0018510408699512482
        total_loss: 0.0007492033764719963
        vf_explained_var: 0.12604700028896332
        vf_loss: 29.982120513916016
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.273772656917572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009582723723724484
        model: {}
        policy_loss: -0.0017481925897300243
        total_loss: 0.005774762947112322
        vf_explained_var: 0.06003040075302124
        vf_loss: 80.04794311523438
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5727858543395996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007871356792747974
        model: {}
        policy_loss: -0.0013489797711372375
        total_loss: 0.005790982395410538
        vf_explained_var: 0.04351209104061127
        vf_loss: 81.4806900024414
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9438410997390747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011834664037451148
        model: {}
        policy_loss: -0.0019203934352844954
        total_loss: -0.0006494193803519011
        vf_explained_var: 0.044178396463394165
        vf_loss: 29.32134246826172
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25511133670806885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010708296904340386
        model: {}
        policy_loss: -0.0018469344358891249
        total_loss: 0.0002982055302709341
        vf_explained_var: 0.15401272475719452
        vf_loss: 25.94137954711914
    load_time_ms: 14384.312
    num_steps_sampled: 67776000
    num_steps_trained: 67776000
    sample_time_ms: 101589.238
    update_time_ms: 13.6
  iterations_since_restore: 56
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.26775956284153
    ram_util_percent: 15.161748633879784
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 426.0
    agent-3: 426.0
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 243.735
    agent-1: 243.735
    agent-2: 388.585
    agent-3: 388.585
    agent-4: 231.925
    agent-5: 231.925
  policy_reward_min:
    agent-0: 165.0
    agent-1: 165.0
    agent-2: 276.0
    agent-3: 276.0
    agent-4: 172.0
    agent-5: 172.0
  sampler_perf:
    mean_env_wait_ms: 26.8065758700561
    mean_inference_ms: 13.003575247780208
    mean_processing_ms: 59.806723212043835
  time_since_restore: 7179.391051769257
  time_this_iter_s: 127.95000958442688
  time_total_s: 89311.83342194557
  timestamp: 1637601133
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 67776000
  training_iteration: 706
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    706 |          89311.8 | 67776000 |  1728.49 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.65
    apples_agent-1_min: 20
    apples_agent-2_max: 431
    apples_agent-2_mean: 364.25
    apples_agent-2_min: 264
    apples_agent-3_max: 267
    apples_agent-3_mean: 216.6
    apples_agent-3_min: 131
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 384.33
    apples_agent-5_min: 229
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 403.74
    cleaning_beam_agent-0_min: 311
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.68
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 2.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 34.08
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 485.0
    cleaning_beam_agent-4_min: 410
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-14-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1846.0
  episode_reward_mean: 1744.31
  episode_reward_min: 1260.0
  episodes_this_iter: 96
  episodes_total: 67872
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11619.378
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3582785725593567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008269640384241939
        model: {}
        policy_loss: -0.0011329036206007004
        total_loss: 0.0015311893075704575
        vf_explained_var: 0.02381373941898346
        vf_loss: 32.94662094116211
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22587941586971283
        entropy_coeff: 0.0017600000137463212
        kl: 0.001026613637804985
        model: {}
        policy_loss: -0.0016886359080672264
        total_loss: 0.0008896356448531151
        vf_explained_var: 0.11819081008434296
        vf_loss: 29.758209228515625
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2682691812515259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010738964192569256
        model: {}
        policy_loss: -0.0017104572616517544
        total_loss: 0.006158270873129368
        vf_explained_var: 0.053248271346092224
        vf_loss: 83.4087905883789
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5587359666824341
        entropy_coeff: 0.0017600000137463212
        kl: 0.001622952288016677
        model: {}
        policy_loss: -0.0017354097217321396
        total_loss: 0.0056611960753798485
        vf_explained_var: 0.04907187819480896
        vf_loss: 83.79978942871094
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9230625033378601
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012483932077884674
        model: {}
        policy_loss: -0.0018069292418658733
        total_loss: -0.0002648495719768107
        vf_explained_var: 0.017444714903831482
        vf_loss: 31.66669464111328
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2536563575267792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008090939954854548
        model: {}
        policy_loss: -0.0017196009866893291
        total_loss: 0.0004974193871021271
        vf_explained_var: 0.17203471064567566
        vf_loss: 26.634613037109375
    load_time_ms: 14396.393
    num_steps_sampled: 67872000
    num_steps_trained: 67872000
    sample_time_ms: 101567.631
    update_time_ms: 13.747
  iterations_since_restore: 57
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.776923076923076
    ram_util_percent: 15.193406593406594
  pid: 28385
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 428.5
    agent-3: 428.5
    agent-4: 268.0
    agent-5: 268.0
  policy_reward_mean:
    agent-0: 245.42
    agent-1: 245.42
    agent-2: 392.99
    agent-3: 392.99
    agent-4: 233.745
    agent-5: 233.745
  policy_reward_min:
    agent-0: 190.0
    agent-1: 190.0
    agent-2: 294.5
    agent-3: 294.5
    agent-4: 145.5
    agent-5: 145.5
  sampler_perf:
    mean_env_wait_ms: 26.80416752195341
    mean_inference_ms: 13.003636806830954
    mean_processing_ms: 59.802400875913854
  time_since_restore: 7307.009197711945
  time_this_iter_s: 127.61814594268799
  time_total_s: 89439.45156788826
  timestamp: 1637601260
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 67872000
  training_iteration: 707
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    707 |          89439.5 | 67872000 |  1744.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 33.01
    apples_agent-1_min: 13
    apples_agent-2_max: 425
    apples_agent-2_mean: 362.56
    apples_agent-2_min: 271
    apples_agent-3_max: 275
    apples_agent-3_mean: 216.78
    apples_agent-3_min: 101
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 387.06
    apples_agent-5_min: 312
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 411.52
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 2.07
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.68
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 31.69
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 488.64
    cleaning_beam_agent-4_min: 427
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-16-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1827.0
  episode_reward_mean: 1734.4
  episode_reward_min: 1335.0
  episodes_this_iter: 96
  episodes_total: 67968
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.502
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35961267352104187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011676748981699347
        model: {}
        policy_loss: -0.0010472871363162994
        total_loss: 0.0016134215984493494
        vf_explained_var: -0.0021321773529052734
        vf_loss: 32.936256408691406
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.226938858628273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006479048170149326
        model: {}
        policy_loss: -0.0017447690479457378
        total_loss: 0.0008041616529226303
        vf_explained_var: 0.10339836776256561
        vf_loss: 29.483444213867188
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26930561661720276
        entropy_coeff: 0.0017600000137463212
        kl: 0.001029627863317728
        model: {}
        policy_loss: -0.0017218389548361301
        total_loss: 0.005947659257799387
        vf_explained_var: 0.03776553273200989
        vf_loss: 81.43476104736328
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5488830208778381
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013342701131477952
        model: {}
        policy_loss: -0.0016952857840806246
        total_loss: 0.005511158145964146
        vf_explained_var: 0.034751832485198975
        vf_loss: 81.72478485107422
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9250742793083191
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018495596013963223
        model: {}
        policy_loss: -0.0016923584043979645
        total_loss: -0.00024015642702579498
        vf_explained_var: 0.012736126780509949
        vf_loss: 30.803302764892578
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.258881151676178
        entropy_coeff: 0.0017600000137463212
        kl: 0.00079264648957178
        model: {}
        policy_loss: -0.0015016114339232445
        total_loss: 0.0008630231022834778
        vf_explained_var: 0.09994876384735107
        vf_loss: 28.20266342163086
    load_time_ms: 14408.727
    num_steps_sampled: 67968000
    num_steps_trained: 67968000
    sample_time_ms: 101653.967
    update_time_ms: 13.837
  iterations_since_restore: 58
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.272677595628412
    ram_util_percent: 15.189617486338802
  pid: 28385
  policy_reward_max:
    agent-0: 277.0
    agent-1: 277.0
    agent-2: 428.0
    agent-3: 428.0
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 243.185
    agent-1: 243.185
    agent-2: 390.445
    agent-3: 390.445
    agent-4: 233.57
    agent-5: 233.57
  policy_reward_min:
    agent-0: 187.5
    agent-1: 187.5
    agent-2: 285.0
    agent-3: 285.0
    agent-4: 195.0
    agent-5: 195.0
  sampler_perf:
    mean_env_wait_ms: 26.805342696333437
    mean_inference_ms: 13.002543272214258
    mean_processing_ms: 59.80817327699882
  time_since_restore: 7435.474258184433
  time_this_iter_s: 128.4650604724884
  time_total_s: 89567.91662836075
  timestamp: 1637601389
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 67968000
  training_iteration: 708
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    708 |          89567.9 | 67968000 |   1734.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.66
    apples_agent-1_min: 15
    apples_agent-2_max: 411
    apples_agent-2_mean: 363.34
    apples_agent-2_min: 317
    apples_agent-3_max: 276
    apples_agent-3_mean: 216.44
    apples_agent-3_min: 138
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 386.27
    apples_agent-5_min: 321
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 408.06
    cleaning_beam_agent-0_min: 367
    cleaning_beam_agent-1_max: 21
    cleaning_beam_agent-1_mean: 2.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 31.19
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 468.84
    cleaning_beam_agent-4_min: 393
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-18-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1848.0
  episode_reward_mean: 1739.13
  episode_reward_min: 1566.0
  episodes_this_iter: 96
  episodes_total: 68064
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11623.751
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3665769696235657
        entropy_coeff: 0.0017600000137463212
        kl: 0.001163677079603076
        model: {}
        policy_loss: -0.0013897186145186424
        total_loss: 0.001235818024724722
        vf_explained_var: 0.006543517112731934
        vf_loss: 32.7071418762207
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22442583739757538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008391886949539185
        model: {}
        policy_loss: -0.0015705517726019025
        total_loss: 0.0008986045140773058
        vf_explained_var: 0.13095051050186157
        vf_loss: 28.6414737701416
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2701442241668701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010987109271809459
        model: {}
        policy_loss: -0.0017468133009970188
        total_loss: 0.005909767001867294
        vf_explained_var: 0.04068668186664581
        vf_loss: 81.32035827636719
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5413706302642822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007449871627613902
        model: {}
        policy_loss: -0.0015759654343128204
        total_loss: 0.005528814159333706
        vf_explained_var: 0.051040083169937134
        vf_loss: 80.57588195800781
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9370386600494385
        entropy_coeff: 0.0017600000137463212
        kl: 0.001748142996802926
        model: {}
        policy_loss: -0.00165786431171
        total_loss: -0.0002654904965311289
        vf_explained_var: 0.023565158247947693
        vf_loss: 30.41560173034668
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2529507279396057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010408367961645126
        model: {}
        policy_loss: -0.0019170688465237617
        total_loss: 0.0003273673355579376
        vf_explained_var: 0.14528411626815796
        vf_loss: 26.896316528320312
    load_time_ms: 14418.244
    num_steps_sampled: 68064000
    num_steps_trained: 68064000
    sample_time_ms: 101658.363
    update_time_ms: 13.858
  iterations_since_restore: 59
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.86263736263736
    ram_util_percent: 15.19505494505495
  pid: 28385
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 427.5
    agent-3: 427.5
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 244.41
    agent-1: 244.41
    agent-2: 391.52
    agent-3: 391.52
    agent-4: 233.635
    agent-5: 233.635
  policy_reward_min:
    agent-0: 201.5
    agent-1: 201.5
    agent-2: 359.5
    agent-3: 359.5
    agent-4: 199.5
    agent-5: 199.5
  sampler_perf:
    mean_env_wait_ms: 26.803646395658628
    mean_inference_ms: 13.0011471879384
    mean_processing_ms: 59.81164755333014
  time_since_restore: 7563.134309053421
  time_this_iter_s: 127.66005086898804
  time_total_s: 89695.57667922974
  timestamp: 1637601517
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 68064000
  training_iteration: 709
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    709 |          89695.6 | 68064000 |  1739.13 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 31.02
    apples_agent-1_min: 17
    apples_agent-2_max: 409
    apples_agent-2_mean: 355.37
    apples_agent-2_min: 291
    apples_agent-3_max: 267
    apples_agent-3_mean: 219.9
    apples_agent-3_min: 148
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 378.57
    apples_agent-5_min: 315
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 405.75
    cleaning_beam_agent-0_min: 382
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.37
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 28.6
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 458.36
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-20-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1859.0
  episode_reward_mean: 1723.56
  episode_reward_min: 1433.0
  episodes_this_iter: 96
  episodes_total: 68160
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11624.846
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3724838197231293
        entropy_coeff: 0.0017600000137463212
        kl: 0.001587362727150321
        model: {}
        policy_loss: -0.0013414730783551931
        total_loss: 0.0012761105317622423
        vf_explained_var: 0.0014971941709518433
        vf_loss: 32.73157501220703
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22591252624988556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009033032692968845
        model: {}
        policy_loss: -0.0014879852533340454
        total_loss: 0.000949799083173275
        vf_explained_var: 0.13469426333904266
        vf_loss: 28.35392951965332
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27667081356048584
        entropy_coeff: 0.0017600000137463212
        kl: 0.00131623400375247
        model: {}
        policy_loss: -0.0019455718575045466
        total_loss: 0.005533992778509855
        vf_explained_var: 0.057554543018341064
        vf_loss: 79.6650390625
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5445007085800171
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005509660695679486
        model: {}
        policy_loss: -0.0013492258731275797
        total_loss: 0.005673495586961508
        vf_explained_var: 0.05616988241672516
        vf_loss: 79.81043243408203
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9392125010490417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018848374020308256
        model: {}
        policy_loss: -0.0016810803208500147
        total_loss: -0.00028989301063120365
        vf_explained_var: 0.026274576783180237
        vf_loss: 30.442026138305664
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2616026997566223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009988563833758235
        model: {}
        policy_loss: -0.0019522072980180383
        total_loss: 0.0002628779038786888
        vf_explained_var: 0.14576873183250427
        vf_loss: 26.755062103271484
    load_time_ms: 14418.254
    num_steps_sampled: 68160000
    num_steps_trained: 68160000
    sample_time_ms: 101554.94
    update_time_ms: 13.833
  iterations_since_restore: 60
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.02722222222222
    ram_util_percent: 15.168333333333337
  pid: 28385
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 440.0
    agent-3: 440.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 242.895
    agent-1: 242.895
    agent-2: 388.41
    agent-3: 388.41
    agent-4: 230.475
    agent-5: 230.475
  policy_reward_min:
    agent-0: 205.0
    agent-1: 205.0
    agent-2: 303.0
    agent-3: 303.0
    agent-4: 189.5
    agent-5: 189.5
  sampler_perf:
    mean_env_wait_ms: 26.79801555022275
    mean_inference_ms: 12.999641612909134
    mean_processing_ms: 59.80523630845962
  time_since_restore: 7689.5487768650055
  time_this_iter_s: 126.41446781158447
  time_total_s: 89821.99114704132
  timestamp: 1637601643
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 68160000
  training_iteration: 710
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    710 |            89822 | 68160000 |  1723.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 32.22
    apples_agent-1_min: 16
    apples_agent-2_max: 413
    apples_agent-2_mean: 358.55
    apples_agent-2_min: 267
    apples_agent-3_max: 277
    apples_agent-3_mean: 217.57
    apples_agent-3_min: 143
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 382.15
    apples_agent-5_min: 247
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 403.59
    cleaning_beam_agent-0_min: 373
    cleaning_beam_agent-1_max: 24
    cleaning_beam_agent-1_mean: 2.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 2.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 27.4
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 451.3
    cleaning_beam_agent-4_min: 333
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-22-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1850.0
  episode_reward_mean: 1725.02
  episode_reward_min: 1267.0
  episodes_this_iter: 96
  episodes_total: 68256
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11626.062
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3752708435058594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017624406609684229
        model: {}
        policy_loss: -0.0015713977627456188
        total_loss: 0.001036288682371378
        vf_explained_var: -0.012214258313179016
        vf_loss: 32.68159484863281
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2286168336868286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011806745314970613
        model: {}
        policy_loss: -0.0016116267070174217
        total_loss: 0.0008471333421766758
        vf_explained_var: 0.11422525346279144
        vf_loss: 28.611278533935547
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2760854661464691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010616204235702753
        model: {}
        policy_loss: -0.0015811233315616846
        total_loss: 0.006365871988236904
        vf_explained_var: 0.050949230790138245
        vf_loss: 84.3290786743164
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5410162210464478
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010306932963430882
        model: {}
        policy_loss: -0.001518279779702425
        total_loss: 0.005951237864792347
        vf_explained_var: 0.05249902606010437
        vf_loss: 84.2170639038086
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9250105619430542
        entropy_coeff: 0.0017600000137463212
        kl: 0.002739114686846733
        model: {}
        policy_loss: -0.002451359760016203
        total_loss: -0.001093439757823944
        vf_explained_var: 0.03656959533691406
        vf_loss: 29.859375
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26401278376579285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012118850136175752
        model: {}
        policy_loss: -0.0017822985537350178
        total_loss: 0.00036807148717343807
        vf_explained_var: 0.15836456418037415
        vf_loss: 26.15032386779785
    load_time_ms: 14444.633
    num_steps_sampled: 68256000
    num_steps_trained: 68256000
    sample_time_ms: 101592.12
    update_time_ms: 13.692
  iterations_since_restore: 61
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.6
    ram_util_percent: 15.21783783783784
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 242.88
    agent-1: 242.88
    agent-2: 388.86
    agent-3: 388.86
    agent-4: 230.77
    agent-5: 230.77
  policy_reward_min:
    agent-0: 183.0
    agent-1: 183.0
    agent-2: 291.0
    agent-3: 291.0
    agent-4: 159.5
    agent-5: 159.5
  sampler_perf:
    mean_env_wait_ms: 26.79504390585859
    mean_inference_ms: 12.998145543746464
    mean_processing_ms: 59.80547490977116
  time_since_restore: 7818.076506853104
  time_this_iter_s: 128.52772998809814
  time_total_s: 89950.51887702942
  timestamp: 1637601773
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 68256000
  training_iteration: 711
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    711 |          89950.5 | 68256000 |  1725.02 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.81
    apples_agent-1_min: 15
    apples_agent-2_max: 410
    apples_agent-2_mean: 356.54
    apples_agent-2_min: 263
    apples_agent-3_max: 276
    apples_agent-3_mean: 214.06
    apples_agent-3_min: 146
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 379.79
    apples_agent-5_min: 294
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 398.6
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.37
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 27.38
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 464.31
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-25-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1818.0
  episode_reward_mean: 1723.96
  episode_reward_min: 1262.0
  episodes_this_iter: 96
  episodes_total: 68352
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11620.522
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3779635429382324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012711983872577548
        model: {}
        policy_loss: -0.001304806675761938
        total_loss: 0.0013785110786557198
        vf_explained_var: 0.008652821183204651
        vf_loss: 33.48533630371094
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22417211532592773
        entropy_coeff: 0.0017600000137463212
        kl: 0.000701284734532237
        model: {}
        policy_loss: -0.0018134387210011482
        total_loss: 0.0007993443869054317
        vf_explained_var: 0.11108960211277008
        vf_loss: 30.073299407958984
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27402564883232117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009658541530370712
        model: {}
        policy_loss: -0.001630650833249092
        total_loss: 0.005676561035215855
        vf_explained_var: 0.05788567662239075
        vf_loss: 77.89492797851562
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5339223146438599
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010863655479624867
        model: {}
        policy_loss: -0.0016747182235121727
        total_loss: 0.005271444097161293
        vf_explained_var: 0.04683545231819153
        vf_loss: 78.8586654663086
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9253658056259155
        entropy_coeff: 0.0017600000137463212
        kl: 0.002216800581663847
        model: {}
        policy_loss: -0.0019395984709262848
        total_loss: -0.0006290692836046219
        vf_explained_var: 0.01758912205696106
        vf_loss: 29.391742706298828
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26825040578842163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006941121537238359
        model: {}
        policy_loss: -0.0017731082625687122
        total_loss: 0.00039002904668450356
        vf_explained_var: 0.1198425143957138
        vf_loss: 26.35254669189453
    load_time_ms: 14454.643
    num_steps_sampled: 68352000
    num_steps_trained: 68352000
    sample_time_ms: 101542.917
    update_time_ms: 13.784
  iterations_since_restore: 62
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.49171270718232
    ram_util_percent: 15.171823204419887
  pid: 28385
  policy_reward_max:
    agent-0: 267.0
    agent-1: 267.0
    agent-2: 418.5
    agent-3: 418.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 242.965
    agent-1: 242.965
    agent-2: 388.16
    agent-3: 388.16
    agent-4: 230.855
    agent-5: 230.855
  policy_reward_min:
    agent-0: 175.5
    agent-1: 175.5
    agent-2: 291.5
    agent-3: 291.5
    agent-4: 164.0
    agent-5: 164.0
  sampler_perf:
    mean_env_wait_ms: 26.791475514705798
    mean_inference_ms: 12.997082044204346
    mean_processing_ms: 59.799869291914085
  time_since_restore: 7945.449736833572
  time_this_iter_s: 127.37322998046875
  time_total_s: 90077.89210700989
  timestamp: 1637601901
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 68352000
  training_iteration: 712
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    712 |          90077.9 | 68352000 |  1723.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 31.3
    apples_agent-1_min: 1
    apples_agent-2_max: 409
    apples_agent-2_mean: 350.19
    apples_agent-2_min: 9
    apples_agent-3_max: 277
    apples_agent-3_mean: 211.46
    apples_agent-3_min: 15
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 458
    apples_agent-5_mean: 377.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 403.07
    cleaning_beam_agent-0_min: 271
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 26.42
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 471.87
    cleaning_beam_agent-4_min: 404
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 3.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-27-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1832.0
  episode_reward_mean: 1694.77
  episode_reward_min: 103.0
  episodes_this_iter: 96
  episodes_total: 68448
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11619.668
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39019960165023804
        entropy_coeff: 0.0017600000137463212
        kl: 0.001583749894052744
        model: {}
        policy_loss: -0.0015771938487887383
        total_loss: 0.0013153022155165672
        vf_explained_var: 0.09400299191474915
        vf_loss: 35.79249954223633
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2341105043888092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013059130869805813
        model: {}
        policy_loss: -0.0021376803051680326
        total_loss: 0.0007112711318768561
        vf_explained_var: 0.18262217938899994
        vf_loss: 32.60987091064453
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28175535798072815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010762936435639858
        model: {}
        policy_loss: -0.0019308258779346943
        total_loss: 0.006207856349647045
        vf_explained_var: 0.13998347520828247
        vf_loss: 86.3457260131836
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5379007458686829
        entropy_coeff: 0.0017600000137463212
        kl: 0.001060410519130528
        model: {}
        policy_loss: -0.0016693531069904566
        total_loss: 0.006857973523437977
        vf_explained_var: 0.05718235671520233
        vf_loss: 94.74031066894531
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9346732497215271
        entropy_coeff: 0.0017600000137463212
        kl: 0.002093345858156681
        model: {}
        policy_loss: -0.002195522654801607
        total_loss: 0.00011257256846874952
        vf_explained_var: 0.031086936593055725
        vf_loss: 39.53118133544922
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27230313420295715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009928698418661952
        model: {}
        policy_loss: -0.0023253243416547775
        total_loss: 0.0006906306371092796
        vf_explained_var: 0.14101018011569977
        vf_loss: 34.95206832885742
    load_time_ms: 14481.615
    num_steps_sampled: 68448000
    num_steps_trained: 68448000
    sample_time_ms: 101554.253
    update_time_ms: 13.764
  iterations_since_restore: 63
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.404918032786888
    ram_util_percent: 15.173224043715848
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 427.0
    agent-3: 427.0
    agent-4: 268.0
    agent-5: 268.0
  policy_reward_mean:
    agent-0: 236.745
    agent-1: 236.745
    agent-2: 381.7
    agent-3: 381.7
    agent-4: 228.94
    agent-5: 228.94
  policy_reward_min:
    agent-0: 10.5
    agent-1: 10.5
    agent-2: 20.5
    agent-3: 20.5
    agent-4: -25.0
    agent-5: -25.0
  sampler_perf:
    mean_env_wait_ms: 26.78753412719829
    mean_inference_ms: 12.996061506704491
    mean_processing_ms: 59.79657601574847
  time_since_restore: 8073.31657576561
  time_this_iter_s: 127.86683893203735
  time_total_s: 90205.75894594193
  timestamp: 1637602029
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 68448000
  training_iteration: 713
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    713 |          90205.8 | 68448000 |  1694.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 30.7
    apples_agent-1_min: 8
    apples_agent-2_max: 405
    apples_agent-2_mean: 348.71
    apples_agent-2_min: 94
    apples_agent-3_max: 288
    apples_agent-3_mean: 213.03
    apples_agent-3_min: 48
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 375.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 405.59
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.69
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 2.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 27.07
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 465.96
    cleaning_beam_agent-4_min: 394
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-29-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1849.0
  episode_reward_mean: 1693.4
  episode_reward_min: 400.0
  episodes_this_iter: 96
  episodes_total: 68544
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11620.433
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38694214820861816
        entropy_coeff: 0.0017600000137463212
        kl: 0.001781898201443255
        model: {}
        policy_loss: -0.0015421032439917326
        total_loss: 0.0012923218309879303
        vf_explained_var: 0.0593419224023819
        vf_loss: 35.154415130615234
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23110973834991455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006866534240543842
        model: {}
        policy_loss: -0.0019456716254353523
        total_loss: 0.0008566025644540787
        vf_explained_var: 0.14197726547718048
        vf_loss: 32.09027099609375
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2795870006084442
        entropy_coeff: 0.0017600000137463212
        kl: 0.000909815717022866
        model: {}
        policy_loss: -0.0016797366552054882
        total_loss: 0.006673265248537064
        vf_explained_var: 0.09631885588169098
        vf_loss: 88.45075225830078
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5441688299179077
        entropy_coeff: 0.0017600000137463212
        kl: 0.001551477937027812
        model: {}
        policy_loss: -0.0015702610835433006
        total_loss: 0.0068842824548482895
        vf_explained_var: 0.03843894600868225
        vf_loss: 94.12283325195312
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9272240996360779
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021448791958391666
        model: {}
        policy_loss: -0.0023475950583815575
        total_loss: -0.0005604894831776619
        vf_explained_var: 0.015174955129623413
        vf_loss: 34.190185546875
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27231544256210327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009893069509416819
        model: {}
        policy_loss: -0.001828983542509377
        total_loss: 0.0008016339270398021
        vf_explained_var: 0.10530441999435425
        vf_loss: 31.098918914794922
    load_time_ms: 14502.609
    num_steps_sampled: 68544000
    num_steps_trained: 68544000
    sample_time_ms: 101464.963
    update_time_ms: 13.649
  iterations_since_restore: 64
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.580662983425416
    ram_util_percent: 15.187845303867409
  pid: 28385
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 432.5
    agent-3: 432.5
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 238.065
    agent-1: 238.065
    agent-2: 381.045
    agent-3: 381.045
    agent-4: 227.59
    agent-5: 227.59
  policy_reward_min:
    agent-0: 70.5
    agent-1: 70.5
    agent-2: 109.5
    agent-3: 109.5
    agent-4: -25.0
    agent-5: -25.0
  sampler_perf:
    mean_env_wait_ms: 26.78195287161368
    mean_inference_ms: 12.994880412579981
    mean_processing_ms: 59.79110323013465
  time_since_restore: 8200.182074308395
  time_this_iter_s: 126.86549854278564
  time_total_s: 90332.62444448471
  timestamp: 1637602156
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 68544000
  training_iteration: 714
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    714 |          90332.6 | 68544000 |   1693.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 31.29
    apples_agent-1_min: 10
    apples_agent-2_max: 419
    apples_agent-2_mean: 353.36
    apples_agent-2_min: 100
    apples_agent-3_max: 264
    apples_agent-3_mean: 213.77
    apples_agent-3_min: 22
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 377.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 404.2
    cleaning_beam_agent-0_min: 247
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 2.66
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 30.13
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 475.42
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 4.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-31-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1863.0
  episode_reward_mean: 1712.04
  episode_reward_min: 419.0
  episodes_this_iter: 96
  episodes_total: 68640
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11617.897
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38127413392066956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009695218759588897
        model: {}
        policy_loss: -0.0014156721299514174
        total_loss: 0.0015609186375513673
        vf_explained_var: 0.07224759459495544
        vf_loss: 36.476322174072266
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2291397750377655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007545810658484697
        model: {}
        policy_loss: -0.0017137785907834768
        total_loss: 0.0011613143142312765
        vf_explained_var: 0.1662587970495224
        vf_loss: 32.783790588378906
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27894654870033264
        entropy_coeff: 0.0017600000137463212
        kl: 0.001215364784002304
        model: {}
        policy_loss: -0.002023790031671524
        total_loss: 0.006382194347679615
        vf_explained_var: 0.10688529908657074
        vf_loss: 88.96932983398438
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5330888032913208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010135723277926445
        model: {}
        policy_loss: -0.0015762533294036984
        total_loss: 0.0072860438376665115
        vf_explained_var: 0.01587650179862976
        vf_loss: 98.00533294677734
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9350142478942871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031230696476995945
        model: {}
        policy_loss: -0.0019815543200820684
        total_loss: 0.00015434663509950042
        vf_explained_var: 0.023674964904785156
        vf_loss: 37.81526565551758
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2730596363544464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005123199080117047
        model: {}
        policy_loss: -0.002097790827974677
        total_loss: 0.0008634552359580994
        vf_explained_var: 0.1134246438741684
        vf_loss: 34.418304443359375
    load_time_ms: 14490.68
    num_steps_sampled: 68640000
    num_steps_trained: 68640000
    sample_time_ms: 101488.674
    update_time_ms: 13.855
  iterations_since_restore: 65
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.131147540983605
    ram_util_percent: 15.225136612021862
  pid: 28385
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 430.0
    agent-3: 430.0
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 240.985
    agent-1: 240.985
    agent-2: 387.68
    agent-3: 387.68
    agent-4: 227.355
    agent-5: 227.355
  policy_reward_min:
    agent-0: 64.0
    agent-1: 64.0
    agent-2: 89.5
    agent-3: 89.5
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 26.777481832318568
    mean_inference_ms: 12.99376006922564
    mean_processing_ms: 59.78283933277916
  time_since_restore: 8328.29777264595
  time_this_iter_s: 128.11569833755493
  time_total_s: 90460.74014282227
  timestamp: 1637602284
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 68640000
  training_iteration: 715
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    715 |          90460.7 | 68640000 |  1712.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.83
    apples_agent-1_min: 8
    apples_agent-2_max: 418
    apples_agent-2_mean: 359.44
    apples_agent-2_min: 118
    apples_agent-3_max: 279
    apples_agent-3_mean: 213.66
    apples_agent-3_min: 63
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 383.17
    apples_agent-5_min: 136
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 406.75
    cleaning_beam_agent-0_min: 350
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 1.64
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 31.7
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 453.29
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 4.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-33-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1868.0
  episode_reward_mean: 1711.53
  episode_reward_min: 622.0
  episodes_this_iter: 96
  episodes_total: 68736
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11614.778
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3778938949108124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012834228109568357
        model: {}
        policy_loss: -0.0015409139450639486
        total_loss: 0.0012209482956677675
        vf_explained_var: 0.04286150634288788
        vf_loss: 34.269554138183594
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22511477768421173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006313167396001518
        model: {}
        policy_loss: -0.0018608374521136284
        total_loss: 0.0007858565077185631
        vf_explained_var: 0.14996662735939026
        vf_loss: 30.428964614868164
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27500757575035095
        entropy_coeff: 0.0017600000137463212
        kl: 0.00125576788559556
        model: {}
        policy_loss: -0.001998043619096279
        total_loss: 0.005937008187174797
        vf_explained_var: 0.11094032227993011
        vf_loss: 84.19062805175781
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5453506112098694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008658437291160226
        model: {}
        policy_loss: -0.0014158934354782104
        total_loss: 0.006543559487909079
        vf_explained_var: 0.058636829257011414
        vf_loss: 89.19268798828125
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.944048285484314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020022825337946415
        model: {}
        policy_loss: -0.0018365392461419106
        total_loss: -0.0001699700951576233
        vf_explained_var: -0.0039637088775634766
        vf_loss: 33.28093338012695
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26435548067092896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009385532466694713
        model: {}
        policy_loss: -0.0018722494132816792
        total_loss: 0.0006464212201535702
        vf_explained_var: 0.10198086500167847
        vf_loss: 29.839374542236328
    load_time_ms: 14480.655
    num_steps_sampled: 68736000
    num_steps_trained: 68736000
    sample_time_ms: 101384.216
    update_time_ms: 13.956
  iterations_since_restore: 66
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.569444444444443
    ram_util_percent: 15.142777777777782
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 239.525
    agent-1: 239.525
    agent-2: 385.855
    agent-3: 385.855
    agent-4: 230.385
    agent-5: 230.385
  policy_reward_min:
    agent-0: 97.5
    agent-1: 97.5
    agent-2: 128.5
    agent-3: 128.5
    agent-4: 85.0
    agent-5: 85.0
  sampler_perf:
    mean_env_wait_ms: 26.773821453521936
    mean_inference_ms: 12.993603839561054
    mean_processing_ms: 59.77966908365521
  time_since_restore: 8455.071643590927
  time_this_iter_s: 126.7738709449768
  time_total_s: 90587.51401376724
  timestamp: 1637602411
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 68736000
  training_iteration: 716
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    716 |          90587.5 | 68736000 |  1711.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 32.12
    apples_agent-1_min: 14
    apples_agent-2_max: 416
    apples_agent-2_mean: 357.15
    apples_agent-2_min: 177
    apples_agent-3_max: 273
    apples_agent-3_mean: 216.95
    apples_agent-3_min: 102
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 387.13
    apples_agent-5_min: 191
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 403.08
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 32.56
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 516
    cleaning_beam_agent-4_mean: 449.36
    cleaning_beam_agent-4_min: 373
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-35-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1868.0
  episode_reward_mean: 1731.65
  episode_reward_min: 902.0
  episodes_this_iter: 96
  episodes_total: 68832
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11621.104
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37230294942855835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019857981242239475
        model: {}
        policy_loss: -0.001694181002676487
        total_loss: 0.0009907856583595276
        vf_explained_var: 0.029445305466651917
        vf_loss: 33.402198791503906
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22576743364334106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009465840412303805
        model: {}
        policy_loss: -0.001689685508608818
        total_loss: 0.000891116913408041
        vf_explained_var: 0.13488154113292694
        vf_loss: 29.781545639038086
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27466151118278503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014516672817990184
        model: {}
        policy_loss: -0.0018876867834478617
        total_loss: 0.00571066327393055
        vf_explained_var: 0.0772479921579361
        vf_loss: 80.81753540039062
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5360310077667236
        entropy_coeff: 0.0017600000137463212
        kl: 0.001399386441335082
        model: {}
        policy_loss: -0.0017691110260784626
        total_loss: 0.005437323823571205
        vf_explained_var: 0.06915949285030365
        vf_loss: 81.49850463867188
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9553481340408325
        entropy_coeff: 0.0017600000137463212
        kl: 0.004199113231152296
        model: {}
        policy_loss: -0.0025122989900410175
        total_loss: -0.001033490989357233
        vf_explained_var: 0.004235297441482544
        vf_loss: 31.602209091186523
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2595043182373047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009378609829582274
        model: {}
        policy_loss: -0.001699675340205431
        total_loss: 0.0008181184530258179
        vf_explained_var: 0.060357749462127686
        vf_loss: 29.74523162841797
    load_time_ms: 14513.059
    num_steps_sampled: 68832000
    num_steps_trained: 68832000
    sample_time_ms: 101284.771
    update_time_ms: 13.674
  iterations_since_restore: 67
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.893922651933696
    ram_util_percent: 15.146408839779008
  pid: 28385
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 425.0
    agent-3: 425.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 243.125
    agent-1: 243.125
    agent-2: 388.075
    agent-3: 388.075
    agent-4: 234.625
    agent-5: 234.625
  policy_reward_min:
    agent-0: 124.5
    agent-1: 124.5
    agent-2: 200.5
    agent-3: 200.5
    agent-4: 126.0
    agent-5: 126.0
  sampler_perf:
    mean_env_wait_ms: 26.77060285087727
    mean_inference_ms: 12.991812068642163
    mean_processing_ms: 59.774281519962315
  time_since_restore: 8582.125783920288
  time_this_iter_s: 127.05414032936096
  time_total_s: 90714.5681540966
  timestamp: 1637602538
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 68832000
  training_iteration: 717
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    717 |          90714.6 | 68832000 |  1731.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 0.26
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 33.08
    apples_agent-1_min: 15
    apples_agent-2_max: 423
    apples_agent-2_mean: 357.86
    apples_agent-2_min: 294
    apples_agent-3_max: 273
    apples_agent-3_mean: 215.08
    apples_agent-3_min: 144
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.24
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 383.36
    apples_agent-5_min: 312
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 402.63
    cleaning_beam_agent-0_min: 357
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 2.03
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 28.37
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 430.99
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-37-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1836.0
  episode_reward_mean: 1722.51
  episode_reward_min: 1395.0
  episodes_this_iter: 96
  episodes_total: 68928
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11622.792
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3724214732646942
        entropy_coeff: 0.0017600000137463212
        kl: 0.001820009434595704
        model: {}
        policy_loss: -0.001537257805466652
        total_loss: 0.001030782237648964
        vf_explained_var: 0.003519013524055481
        vf_loss: 32.235023498535156
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22737881541252136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010110540315508842
        model: {}
        policy_loss: -0.0016846845392137766
        total_loss: 0.000786285731010139
        vf_explained_var: 0.11202515661716461
        vf_loss: 28.71158218383789
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2776462733745575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011674894485622644
        model: {}
        policy_loss: -0.0016040317714214325
        total_loss: 0.0058226631954312325
        vf_explained_var: 0.04549197852611542
        vf_loss: 79.15352630615234
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5339843034744263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006707636639475822
        model: {}
        policy_loss: -0.0014057760126888752
        total_loss: 0.005588741973042488
        vf_explained_var: 0.043991297483444214
        vf_loss: 79.34333801269531
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9586272835731506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023361011408269405
        model: {}
        policy_loss: -0.00209396006539464
        total_loss: -0.000829694326967001
        vf_explained_var: 0.018894702196121216
        vf_loss: 29.5145320892334
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26637011766433716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007985199918039143
        model: {}
        policy_loss: -0.0017337221652269363
        total_loss: 0.0008326978422701359
        vf_explained_var: -0.003247976303100586
        vf_loss: 30.352298736572266
    load_time_ms: 14508.527
    num_steps_sampled: 68928000
    num_steps_trained: 68928000
    sample_time_ms: 101247.898
    update_time_ms: 13.804
  iterations_since_restore: 68
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.287978142076508
    ram_util_percent: 15.180327868852462
  pid: 28385
  policy_reward_max:
    agent-0: 283.5
    agent-1: 283.5
    agent-2: 428.0
    agent-3: 428.0
    agent-4: 267.5
    agent-5: 267.5
  policy_reward_mean:
    agent-0: 242.23
    agent-1: 242.23
    agent-2: 387.725
    agent-3: 387.725
    agent-4: 231.3
    agent-5: 231.3
  policy_reward_min:
    agent-0: 189.0
    agent-1: 189.0
    agent-2: 309.5
    agent-3: 309.5
    agent-4: 197.0
    agent-5: 197.0
  sampler_perf:
    mean_env_wait_ms: 26.766807342173262
    mean_inference_ms: 12.99039833250376
    mean_processing_ms: 59.77254212876318
  time_since_restore: 8710.198752641678
  time_this_iter_s: 128.07296872138977
  time_total_s: 90842.641122818
  timestamp: 1637602666
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 68928000
  training_iteration: 718
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    718 |          90842.6 | 68928000 |  1722.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 0.13
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.27
    apples_agent-1_min: 4
    apples_agent-2_max: 420
    apples_agent-2_mean: 354.99
    apples_agent-2_min: 32
    apples_agent-3_max: 266
    apples_agent-3_mean: 212.32
    apples_agent-3_min: 18
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 378.98
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 411.08
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 2.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 30.57
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 408.11
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 3.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-39-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1871.0
  episode_reward_mean: 1708.53
  episode_reward_min: 149.0
  episodes_this_iter: 96
  episodes_total: 69024
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11624.999
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38364529609680176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015350361354649067
        model: {}
        policy_loss: -0.0018596736481413245
        total_loss: 0.0009199557825922966
        vf_explained_var: 0.07623562216758728
        vf_loss: 34.5484504699707
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2324358969926834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001065190532244742
        model: {}
        policy_loss: -0.0021259747445583344
        total_loss: 0.0006353864446282387
        vf_explained_var: 0.15497298538684845
        vf_loss: 31.704479217529297
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28297415375709534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011551715433597565
        model: {}
        policy_loss: -0.0018438334809616208
        total_loss: 0.005997008644044399
        vf_explained_var: 0.0890909731388092
        vf_loss: 83.38877868652344
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5394832491874695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008658026345074177
        model: {}
        policy_loss: -0.001720253610983491
        total_loss: 0.006060478743165731
        vf_explained_var: 0.04529377818107605
        vf_loss: 87.30226135253906
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9616430997848511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015328828012570739
        model: {}
        policy_loss: -0.002118893899023533
        total_loss: -0.0005220302846282721
        vf_explained_var: 0.013508394360542297
        vf_loss: 32.89353942871094
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27000653743743896
        entropy_coeff: 0.0017600000137463212
        kl: 0.000887298199813813
        model: {}
        policy_loss: -0.002015086356550455
        total_loss: 0.0007760128937661648
        vf_explained_var: 0.02922263741493225
        vf_loss: 32.66307830810547
    load_time_ms: 14516.884
    num_steps_sampled: 69024000
    num_steps_trained: 69024000
    sample_time_ms: 101215.686
    update_time_ms: 13.765
  iterations_since_restore: 69
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.519780219780223
    ram_util_percent: 15.113186813186815
  pid: 28385
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 432.5
    agent-3: 432.5
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 240.8
    agent-1: 240.8
    agent-2: 384.245
    agent-3: 384.245
    agent-4: 229.22
    agent-5: 229.22
  policy_reward_min:
    agent-0: 18.0
    agent-1: 18.0
    agent-2: 37.0
    agent-3: 37.0
    agent-4: 19.5
    agent-5: 19.5
  sampler_perf:
    mean_env_wait_ms: 26.75885140748288
    mean_inference_ms: 12.990128563866921
    mean_processing_ms: 59.77208618726141
  time_since_restore: 8837.731192350388
  time_this_iter_s: 127.53243970870972
  time_total_s: 90970.1735625267
  timestamp: 1637602794
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 69024000
  training_iteration: 719
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    719 |          90970.2 | 69024000 |  1708.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.64
    apples_agent-1_min: 21
    apples_agent-2_max: 406
    apples_agent-2_mean: 352.26
    apples_agent-2_min: 302
    apples_agent-3_max: 282
    apples_agent-3_mean: 214.1
    apples_agent-3_min: 135
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 377.52
    apples_agent-5_min: 315
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 404.77
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.9
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.86
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 31.01
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 408.35
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-42-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1850.0
  episode_reward_mean: 1713.15
  episode_reward_min: 1519.0
  episodes_this_iter: 96
  episodes_total: 69120
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11625.08
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3824383020401001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001875095535069704
        model: {}
        policy_loss: -0.0014997441321611404
        total_loss: 0.0009318199008703232
        vf_explained_var: 0.004679396748542786
        vf_loss: 31.046566009521484
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23139505088329315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013700728304684162
        model: {}
        policy_loss: -0.0018970086239278316
        total_loss: 0.0005769478157162666
        vf_explained_var: 0.07854799926280975
        vf_loss: 28.812152862548828
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2798827290534973
        entropy_coeff: 0.0017600000137463212
        kl: 0.000849529926199466
        model: {}
        policy_loss: -0.0016995840705931187
        total_loss: 0.005714408587664366
        vf_explained_var: 0.03038369119167328
        vf_loss: 79.06585693359375
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.537600576877594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011523989960551262
        model: {}
        policy_loss: -0.0015053837560117245
        total_loss: 0.005369601771235466
        vf_explained_var: 0.039476633071899414
        vf_loss: 78.21162414550781
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9642643332481384
        entropy_coeff: 0.0017600000137463212
        kl: 0.001036281231790781
        model: {}
        policy_loss: -0.0017330599948763847
        total_loss: -0.0005231774412095547
        vf_explained_var: 0.006731584668159485
        vf_loss: 29.069873809814453
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26821035146713257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011774981394410133
        model: {}
        policy_loss: -0.0017962702549993992
        total_loss: 0.000668572261929512
        vf_explained_var: -0.001426801085472107
        vf_loss: 29.368940353393555
    load_time_ms: 14529.119
    num_steps_sampled: 69120000
    num_steps_trained: 69120000
    sample_time_ms: 101469.024
    update_time_ms: 14.038
  iterations_since_restore: 70
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.77880434782609
    ram_util_percent: 15.192391304347824
  pid: 28385
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 421.5
    agent-3: 421.5
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 241.965
    agent-1: 241.965
    agent-2: 385.595
    agent-3: 385.595
    agent-4: 229.015
    agent-5: 229.015
  policy_reward_min:
    agent-0: 207.0
    agent-1: 207.0
    agent-2: 328.5
    agent-3: 328.5
    agent-4: 203.5
    agent-5: 203.5
  sampler_perf:
    mean_env_wait_ms: 26.755292228415797
    mean_inference_ms: 12.990178630261507
    mean_processing_ms: 59.77440881106898
  time_since_restore: 8966.803168773651
  time_this_iter_s: 129.07197642326355
  time_total_s: 91099.24553894997
  timestamp: 1637602923
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 69120000
  training_iteration: 720
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    720 |          91099.2 | 69120000 |  1713.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.33
    apples_agent-1_min: 17
    apples_agent-2_max: 401
    apples_agent-2_mean: 348.41
    apples_agent-2_min: 200
    apples_agent-3_max: 261
    apples_agent-3_mean: 213.02
    apples_agent-3_min: 102
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 419
    apples_agent-5_mean: 374.8
    apples_agent-5_min: 265
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 413.17
    cleaning_beam_agent-0_min: 346
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.37
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 29.33
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 408.24
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-44-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1836.0
  episode_reward_mean: 1702.0
  episode_reward_min: 940.0
  episodes_this_iter: 96
  episodes_total: 69216
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11619.744
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3951576352119446
        entropy_coeff: 0.0017600000137463212
        kl: 0.001517313183285296
        model: {}
        policy_loss: -0.0013979561626911163
        total_loss: 0.001322934404015541
        vf_explained_var: 0.02498970925807953
        vf_loss: 34.163658142089844
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23588749766349792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008064891444519162
        model: {}
        policy_loss: -0.001601079711690545
        total_loss: 0.001079697860404849
        vf_explained_var: 0.11882176995277405
        vf_loss: 30.959396362304688
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2788245975971222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012352227931842208
        model: {}
        policy_loss: -0.001986752264201641
        total_loss: 0.0057491580955684185
        vf_explained_var: 0.08215974271297455
        vf_loss: 82.26641845703125
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5322626829147339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006857673288322985
        model: {}
        policy_loss: -0.001877065747976303
        total_loss: 0.0057256147265434265
        vf_explained_var: 0.04653985798358917
        vf_loss: 85.39466094970703
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.952692985534668
        entropy_coeff: 0.0017600000137463212
        kl: 0.001738855498842895
        model: {}
        policy_loss: -0.0022993143647909164
        total_loss: -0.0007453702855855227
        vf_explained_var: 0.008214235305786133
        vf_loss: 32.30685043334961
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27367711067199707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008918688399717212
        model: {}
        policy_loss: -0.0018285191617906094
        total_loss: 0.0007696459069848061
        vf_explained_var: 0.05694723129272461
        vf_loss: 30.798377990722656
    load_time_ms: 14524.203
    num_steps_sampled: 69216000
    num_steps_trained: 69216000
    sample_time_ms: 101294.558
    update_time_ms: 13.892
  iterations_since_restore: 71
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.126373626373628
    ram_util_percent: 15.21868131868132
  pid: 28385
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 414.0
    agent-3: 414.0
    agent-4: 253.0
    agent-5: 253.0
  policy_reward_mean:
    agent-0: 239.385
    agent-1: 239.385
    agent-2: 384.235
    agent-3: 384.235
    agent-4: 227.38
    agent-5: 227.38
  policy_reward_min:
    agent-0: 134.0
    agent-1: 134.0
    agent-2: 215.0
    agent-3: 215.0
    agent-4: 121.0
    agent-5: 121.0
  sampler_perf:
    mean_env_wait_ms: 26.746508210434154
    mean_inference_ms: 12.988596635035105
    mean_processing_ms: 59.76686143193521
  time_since_restore: 9093.593059539795
  time_this_iter_s: 126.7898907661438
  time_total_s: 91226.03542971611
  timestamp: 1637603051
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 69216000
  training_iteration: 721
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    721 |            91226 | 69216000 |     1702 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 32.47
    apples_agent-1_min: 17
    apples_agent-2_max: 392
    apples_agent-2_mean: 353.83
    apples_agent-2_min: 286
    apples_agent-3_max: 259
    apples_agent-3_mean: 215.81
    apples_agent-3_min: 130
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 422
    apples_agent-5_mean: 379.66
    apples_agent-5_min: 322
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 410.22
    cleaning_beam_agent-0_min: 349
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.9
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 3.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 28.77
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 420.92
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-46-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1827.0
  episode_reward_mean: 1718.8
  episode_reward_min: 1477.0
  episodes_this_iter: 96
  episodes_total: 69312
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11613.037
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3836003243923187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012449710629880428
        model: {}
        policy_loss: -0.0014582241419702768
        total_loss: 0.0011823705863207579
        vf_explained_var: 0.010125398635864258
        vf_loss: 33.15729904174805
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2330741286277771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007396513829007745
        model: {}
        policy_loss: -0.0019720103591680527
        total_loss: 0.0005569589557126164
        vf_explained_var: 0.12226937711238861
        vf_loss: 29.39179229736328
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27494725584983826
        entropy_coeff: 0.0017600000137463212
        kl: 0.001265723374672234
        model: {}
        policy_loss: -0.0017348816618323326
        total_loss: 0.005681334529072046
        vf_explained_var: 0.04126742482185364
        vf_loss: 79.00123596191406
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.534932553768158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014758709585294127
        model: {}
        policy_loss: -0.0015243720263242722
        total_loss: 0.005354123190045357
        vf_explained_var: 0.05123010277748108
        vf_loss: 78.19976806640625
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9471380114555359
        entropy_coeff: 0.0017600000137463212
        kl: 0.002831437159329653
        model: {}
        policy_loss: -0.0019993381574749947
        total_loss: -0.000750475563108921
        vf_explained_var: 0.010793238878250122
        vf_loss: 29.158233642578125
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2695639133453369
        entropy_coeff: 0.0017600000137463212
        kl: 0.000920048332773149
        model: {}
        policy_loss: -0.00166994403116405
        total_loss: 0.0006540457252413034
        vf_explained_var: 0.0502990186214447
        vf_loss: 27.98421859741211
    load_time_ms: 14511.435
    num_steps_sampled: 69312000
    num_steps_trained: 69312000
    sample_time_ms: 101476.893
    update_time_ms: 14.063
  iterations_since_restore: 72
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.29836956521739
    ram_util_percent: 15.153804347826085
  pid: 28385
  policy_reward_max:
    agent-0: 262.0
    agent-1: 262.0
    agent-2: 424.0
    agent-3: 424.0
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 239.975
    agent-1: 239.975
    agent-2: 388.62
    agent-3: 388.62
    agent-4: 230.805
    agent-5: 230.805
  policy_reward_min:
    agent-0: 127.5
    agent-1: 127.5
    agent-2: 352.0
    agent-3: 352.0
    agent-4: 195.0
    agent-5: 195.0
  sampler_perf:
    mean_env_wait_ms: 26.7452135038724
    mean_inference_ms: 12.988786751375834
    mean_processing_ms: 59.771978577151096
  time_since_restore: 9222.530262470245
  time_this_iter_s: 128.93720293045044
  time_total_s: 91354.97263264656
  timestamp: 1637603180
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 69312000
  training_iteration: 722
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    722 |            91355 | 69312000 |   1718.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.44
    apples_agent-1_min: 19
    apples_agent-2_max: 401
    apples_agent-2_mean: 358.57
    apples_agent-2_min: 277
    apples_agent-3_max: 279
    apples_agent-3_mean: 222.85
    apples_agent-3_min: 149
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 431
    apples_agent-5_mean: 381.9
    apples_agent-5_min: 257
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 419.27
    cleaning_beam_agent-0_min: 375
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.71
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.39
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 30.33
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 442.77
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-48-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1843.0
  episode_reward_mean: 1739.25
  episode_reward_min: 1506.0
  episodes_this_iter: 96
  episodes_total: 69408
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11614.718
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3852001428604126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011466359719634056
        model: {}
        policy_loss: -0.0011545978486537933
        total_loss: 0.001481375191360712
        vf_explained_var: -0.011908620595932007
        vf_loss: 33.13925552368164
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22674770653247833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015275931218639016
        model: {}
        policy_loss: -0.0017924789572134614
        total_loss: 0.0007947016856633127
        vf_explained_var: 0.09866988658905029
        vf_loss: 29.862581253051758
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2747785747051239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007677196408621967
        model: {}
        policy_loss: -0.001657234039157629
        total_loss: 0.005903642158955336
        vf_explained_var: 0.027029260993003845
        vf_loss: 80.44490814208984
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5231562852859497
        entropy_coeff: 0.0017600000137463212
        kl: 0.001476457342505455
        model: {}
        policy_loss: -0.0016856230795383453
        total_loss: 0.005276728421449661
        vf_explained_var: 0.04862646758556366
        vf_loss: 78.83102416992188
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9432432651519775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012789744650945067
        model: {}
        policy_loss: -0.0018288388382643461
        total_loss: -0.00048186839558184147
        vf_explained_var: -0.011920809745788574
        vf_loss: 30.070781707763672
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2680431008338928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009687087149359286
        model: {}
        policy_loss: -0.0016392627730965614
        total_loss: 0.0006245900876820087
        vf_explained_var: 0.0909624993801117
        vf_loss: 27.356101989746094
    load_time_ms: 14509.119
    num_steps_sampled: 69408000
    num_steps_trained: 69408000
    sample_time_ms: 101396.093
    update_time_ms: 14.334
  iterations_since_restore: 73
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.982320441988957
    ram_util_percent: 15.187845303867404
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 435.0
    agent-3: 435.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 244.6
    agent-1: 244.6
    agent-2: 393.065
    agent-3: 393.065
    agent-4: 231.96
    agent-5: 231.96
  policy_reward_min:
    agent-0: 181.0
    agent-1: 181.0
    agent-2: 326.0
    agent-3: 326.0
    agent-4: 158.0
    agent-5: 158.0
  sampler_perf:
    mean_env_wait_ms: 26.740474696194493
    mean_inference_ms: 12.988816549954045
    mean_processing_ms: 59.7671196349863
  time_since_restore: 9349.63705086708
  time_this_iter_s: 127.10678839683533
  time_total_s: 91482.0794210434
  timestamp: 1637603308
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 69408000
  training_iteration: 723
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    723 |          91482.1 | 69408000 |  1739.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 31.86
    apples_agent-1_min: 16
    apples_agent-2_max: 416
    apples_agent-2_mean: 355.69
    apples_agent-2_min: 250
    apples_agent-3_max: 289
    apples_agent-3_mean: 211.95
    apples_agent-3_min: 138
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 378.56
    apples_agent-5_min: 256
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 416.84
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 2.09
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.69
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 32.31
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 452.86
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-50-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1828.0
  episode_reward_mean: 1714.3
  episode_reward_min: 1151.0
  episodes_this_iter: 96
  episodes_total: 69504
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11622.542
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39185085892677307
        entropy_coeff: 0.0017600000137463212
        kl: 0.00125770247541368
        model: {}
        policy_loss: -0.0014234479749575257
        total_loss: 0.0012792542111128569
        vf_explained_var: 0.033145710825920105
        vf_loss: 33.923606872558594
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23472215235233307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008196276030503213
        model: {}
        policy_loss: -0.0017309030517935753
        total_loss: 0.0009220819920301437
        vf_explained_var: 0.12553314864635468
        vf_loss: 30.660938262939453
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27509579062461853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011744746007025242
        model: {}
        policy_loss: -0.001732013886794448
        total_loss: 0.00579632306471467
        vf_explained_var: 0.0768178403377533
        vf_loss: 80.12504577636719
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.54644775390625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013738328125327826
        model: {}
        policy_loss: -0.001566277351230383
        total_loss: 0.005503475666046143
        vf_explained_var: 0.07435019314289093
        vf_loss: 80.31503295898438
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9354640245437622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031778677366673946
        model: {}
        policy_loss: -0.002219009678810835
        total_loss: -0.0007051955908536911
        vf_explained_var: 0.01088462769985199
        vf_loss: 31.602333068847656
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27629512548446655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009298574295826256
        model: {}
        policy_loss: -0.0016631502658128738
        total_loss: 0.0006324928253889084
        vf_explained_var: 0.1503884643316269
        vf_loss: 27.81922721862793
    load_time_ms: 14537.158
    num_steps_sampled: 69504000
    num_steps_trained: 69504000
    sample_time_ms: 101578.123
    update_time_ms: 14.432
  iterations_since_restore: 74
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.32826086956522
    ram_util_percent: 15.13532608695652
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 422.5
    agent-3: 422.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 241.935
    agent-1: 241.935
    agent-2: 384.89
    agent-3: 384.89
    agent-4: 230.325
    agent-5: 230.325
  policy_reward_min:
    agent-0: 155.0
    agent-1: 155.0
    agent-2: 266.0
    agent-3: 266.0
    agent-4: 154.5
    agent-5: 154.5
  sampler_perf:
    mean_env_wait_ms: 26.73861871742224
    mean_inference_ms: 12.988642226104812
    mean_processing_ms: 59.76944215167457
  time_since_restore: 9478.70307803154
  time_this_iter_s: 129.06602716445923
  time_total_s: 91611.14544820786
  timestamp: 1637603437
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 69504000
  training_iteration: 724
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    724 |          91611.1 | 69504000 |   1714.3 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 31.89
    apples_agent-1_min: 15
    apples_agent-2_max: 434
    apples_agent-2_mean: 360.55
    apples_agent-2_min: 299
    apples_agent-3_max: 288
    apples_agent-3_mean: 209.42
    apples_agent-3_min: 145
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.17
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 385.53
    apples_agent-5_min: 329
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 411.34
    cleaning_beam_agent-0_min: 375
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.91
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 2.84
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 33.73
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 471.68
    cleaning_beam_agent-4_min: 412
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-52-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1836.0
  episode_reward_mean: 1739.21
  episode_reward_min: 1577.0
  episodes_this_iter: 96
  episodes_total: 69600
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11615.745
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3788035213947296
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011677620932459831
        model: {}
        policy_loss: -0.0012510757660493255
        total_loss: 0.00127762695774436
        vf_explained_var: -0.007241547107696533
        vf_loss: 31.953975677490234
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.230285182595253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007461790228262544
        model: {}
        policy_loss: -0.0015497570857405663
        total_loss: 0.0008863834664225578
        vf_explained_var: 0.1020757257938385
        vf_loss: 28.4144229888916
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26939934492111206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013864553766325116
        model: {}
        policy_loss: -0.0017759089823812246
        total_loss: 0.005743944086134434
        vf_explained_var: 0.03950819373130798
        vf_loss: 79.93995666503906
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5354287624359131
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011232028482481837
        model: {}
        policy_loss: -0.0014906672295182943
        total_loss: 0.0052864206954836845
        vf_explained_var: 0.07210123538970947
        vf_loss: 77.1944351196289
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9174157977104187
        entropy_coeff: 0.0017600000137463212
        kl: 0.003665601834654808
        model: {}
        policy_loss: -0.002182047814130783
        total_loss: -0.0005982270231470466
        vf_explained_var: -0.01752611994743347
        vf_loss: 31.9847412109375
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2689344584941864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009629607084207237
        model: {}
        policy_loss: -0.0016238605603575706
        total_loss: 0.0007401490584015846
        vf_explained_var: 0.14085590839385986
        vf_loss: 28.373336791992188
    load_time_ms: 14510.168
    num_steps_sampled: 69600000
    num_steps_trained: 69600000
    sample_time_ms: 101633.923
    update_time_ms: 14.601
  iterations_since_restore: 75
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.233333333333338
    ram_util_percent: 15.187978142076501
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 427.0
    agent-3: 427.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 245.735
    agent-1: 245.735
    agent-2: 389.23
    agent-3: 389.23
    agent-4: 234.64
    agent-5: 234.64
  policy_reward_min:
    agent-0: 219.0
    agent-1: 219.0
    agent-2: 315.5
    agent-3: 315.5
    agent-4: 203.0
    agent-5: 203.0
  sampler_perf:
    mean_env_wait_ms: 26.738501283695424
    mean_inference_ms: 12.988071576246243
    mean_processing_ms: 59.77021095992635
  time_since_restore: 9607.040309429169
  time_this_iter_s: 128.33723139762878
  time_total_s: 91739.48267960548
  timestamp: 1637603565
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 69600000
  training_iteration: 725
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    725 |          91739.5 | 69600000 |  1739.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 0.74
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.54
    apples_agent-1_min: 10
    apples_agent-2_max: 426
    apples_agent-2_mean: 355.69
    apples_agent-2_min: 63
    apples_agent-3_max: 289
    apples_agent-3_mean: 213.94
    apples_agent-3_min: 47
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 376.06
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 410.36
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 2.07
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 2.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 32.49
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 469.52
    cleaning_beam_agent-4_min: 413
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 5.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-54-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1875.0
  episode_reward_mean: 1709.99
  episode_reward_min: 296.0
  episodes_this_iter: 96
  episodes_total: 69696
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11618.034
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.386078804731369
        entropy_coeff: 0.0017600000137463212
        kl: 0.002118036150932312
        model: {}
        policy_loss: -0.0019693048670887947
        total_loss: 0.0007845927029848099
        vf_explained_var: 0.07558511197566986
        vf_loss: 34.333919525146484
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2380748987197876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009983188938349485
        model: {}
        policy_loss: -0.0020362078212201595
        total_loss: 0.0006742335972376168
        vf_explained_var: 0.15709665417671204
        vf_loss: 31.294509887695312
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27297067642211914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011120144044980407
        model: {}
        policy_loss: -0.0017525022849440575
        total_loss: 0.006760906428098679
        vf_explained_var: 0.11618392169475555
        vf_loss: 89.93836975097656
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5482665300369263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015738982474431396
        model: {}
        policy_loss: -0.0018872774671763182
        total_loss: 0.006703516468405724
        vf_explained_var: 0.06239922344684601
        vf_loss: 95.55743408203125
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9448064565658569
        entropy_coeff: 0.0017600000137463212
        kl: 0.002662423299625516
        model: {}
        policy_loss: -0.002289359923452139
        total_loss: -0.0006081545725464821
        vf_explained_var: 0.0011233538389205933
        vf_loss: 33.44065856933594
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28665900230407715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012033750535920262
        model: {}
        policy_loss: -0.0020724290516227484
        total_loss: 0.00026711937971413136
        vf_explained_var: 0.1534663885831833
        vf_loss: 28.44066619873047
    load_time_ms: 14526.418
    num_steps_sampled: 69696000
    num_steps_trained: 69696000
    sample_time_ms: 101641.659
    update_time_ms: 14.56
  iterations_since_restore: 76
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.4
    ram_util_percent: 15.174585635359122
  pid: 28385
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 437.5
    agent-3: 437.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 240.785
    agent-1: 240.785
    agent-2: 384.925
    agent-3: 384.925
    agent-4: 229.285
    agent-5: 229.285
  policy_reward_min:
    agent-0: 43.5
    agent-1: 43.5
    agent-2: 52.0
    agent-3: 52.0
    agent-4: 52.5
    agent-5: 52.5
  sampler_perf:
    mean_env_wait_ms: 26.736854390198495
    mean_inference_ms: 12.987032111943417
    mean_processing_ms: 59.7666174856387
  time_since_restore: 9734.116481304169
  time_this_iter_s: 127.076171875
  time_total_s: 91866.55885148048
  timestamp: 1637603692
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 69696000
  training_iteration: 726
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    726 |          91866.6 | 69696000 |  1709.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 0.71
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.97
    apples_agent-1_min: 10
    apples_agent-2_max: 398
    apples_agent-2_mean: 352.23
    apples_agent-2_min: 63
    apples_agent-3_max: 260
    apples_agent-3_mean: 212.6
    apples_agent-3_min: 47
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 466
    apples_agent-5_mean: 377.74
    apples_agent-5_min: 76
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 410.43
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 33.25
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 457.51
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 4.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-57-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1880.0
  episode_reward_mean: 1712.02
  episode_reward_min: 296.0
  episodes_this_iter: 96
  episodes_total: 69792
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11610.032
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3674785792827606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016799730947241187
        model: {}
        policy_loss: -0.0015633191214874387
        total_loss: 0.0011775920866057277
        vf_explained_var: -0.0015720278024673462
        vf_loss: 33.87674331665039
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2291787564754486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006935630808584392
        model: {}
        policy_loss: -0.0016204542480409145
        total_loss: 0.000987690407782793
        vf_explained_var: 0.11001482605934143
        vf_loss: 30.11500358581543
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27288827300071716
        entropy_coeff: 0.0017600000137463212
        kl: 0.001164554851129651
        model: {}
        policy_loss: -0.0018812196794897318
        total_loss: 0.005538162775337696
        vf_explained_var: 0.054330840706825256
        vf_loss: 78.9966812133789
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5459998250007629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006022774032317102
        model: {}
        policy_loss: -0.0016279048286378384
        total_loss: 0.005407568532973528
        vf_explained_var: 0.04198350012302399
        vf_loss: 79.96435546875
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9418779611587524
        entropy_coeff: 0.0017600000137463212
        kl: 0.002281873021274805
        model: {}
        policy_loss: -0.0022488723043352365
        total_loss: -0.0008098047692328691
        vf_explained_var: 0.009681850671768188
        vf_loss: 30.967708587646484
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28447413444519043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012071233941242099
        model: {}
        policy_loss: -0.00197169603779912
        total_loss: 0.0002959412522614002
        vf_explained_var: 0.1265667974948883
        vf_loss: 27.6830997467041
    load_time_ms: 14503.52
    num_steps_sampled: 69792000
    num_steps_trained: 69792000
    sample_time_ms: 101785.61
    update_time_ms: 14.916
  iterations_since_restore: 77
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.42349726775956
    ram_util_percent: 15.18579234972678
  pid: 28385
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 420.0
    agent-3: 420.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 242.675
    agent-1: 242.675
    agent-2: 383.955
    agent-3: 383.955
    agent-4: 229.38
    agent-5: 229.38
  policy_reward_min:
    agent-0: 43.5
    agent-1: 43.5
    agent-2: 52.0
    agent-3: 52.0
    agent-4: 52.5
    agent-5: 52.5
  sampler_perf:
    mean_env_wait_ms: 26.736280221346853
    mean_inference_ms: 12.986028724955702
    mean_processing_ms: 59.76284027207726
  time_since_restore: 9862.261517047882
  time_this_iter_s: 128.14503574371338
  time_total_s: 91994.7038872242
  timestamp: 1637603821
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 69792000
  training_iteration: 727
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    727 |          91994.7 | 69792000 |  1712.02 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 31.1
    apples_agent-1_min: 16
    apples_agent-2_max: 407
    apples_agent-2_mean: 356.29
    apples_agent-2_min: 231
    apples_agent-3_max: 270
    apples_agent-3_mean: 212.33
    apples_agent-3_min: 77
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 466
    apples_agent-5_mean: 380.36
    apples_agent-5_min: 250
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 402.71
    cleaning_beam_agent-0_min: 350
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.48
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 190
    cleaning_beam_agent-3_mean: 33.36
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 450.98
    cleaning_beam_agent-4_min: 392
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_12-59-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1848.0
  episode_reward_mean: 1717.27
  episode_reward_min: 1240.0
  episodes_this_iter: 96
  episodes_total: 69888
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11608.882
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36284518241882324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013003500644117594
        model: {}
        policy_loss: -0.0012770587345585227
        total_loss: 0.0014503846177831292
        vf_explained_var: 0.004557490348815918
        vf_loss: 33.6605110168457
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23229661583900452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005580278811976314
        model: {}
        policy_loss: -0.0016717193648219109
        total_loss: 0.0008803186938166618
        vf_explained_var: 0.12433643639087677
        vf_loss: 29.608787536621094
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2767137289047241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011798092164099216
        model: {}
        policy_loss: -0.001903034746646881
        total_loss: 0.005661363713443279
        vf_explained_var: 0.062152788043022156
        vf_loss: 80.51412963867188
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5395960211753845
        entropy_coeff: 0.0017600000137463212
        kl: 0.001002974109724164
        model: {}
        policy_loss: -0.0016008608508855104
        total_loss: 0.005500979721546173
        vf_explained_var: 0.06239296495914459
        vf_loss: 80.51529693603516
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9485985040664673
        entropy_coeff: 0.0017600000137463212
        kl: 0.001596553367562592
        model: {}
        policy_loss: -0.0021476121619343758
        total_loss: -0.0007711501093581319
        vf_explained_var: 0.032979145646095276
        vf_loss: 30.459943771362305
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2919774055480957
        entropy_coeff: 0.0017600000137463212
        kl: 0.001327431295067072
        model: {}
        policy_loss: -0.001797891454771161
        total_loss: 0.0004528580466285348
        vf_explained_var: 0.12965843081474304
        vf_loss: 27.646263122558594
    load_time_ms: 14508.26
    num_steps_sampled: 69888000
    num_steps_trained: 69888000
    sample_time_ms: 101676.092
    update_time_ms: 14.812
  iterations_since_restore: 78
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.715469613259668
    ram_util_percent: 15.154696132596685
  pid: 28385
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 418.5
    agent-3: 418.5
    agent-4: 284.0
    agent-5: 284.0
  policy_reward_mean:
    agent-0: 241.045
    agent-1: 241.045
    agent-2: 386.465
    agent-3: 386.465
    agent-4: 231.125
    agent-5: 231.125
  policy_reward_min:
    agent-0: 176.0
    agent-1: 176.0
    agent-2: 286.5
    agent-3: 286.5
    agent-4: 157.5
    agent-5: 157.5
  sampler_perf:
    mean_env_wait_ms: 26.733662443144357
    mean_inference_ms: 12.985009799305338
    mean_processing_ms: 59.756016130733975
  time_since_restore: 9989.273490667343
  time_this_iter_s: 127.01197361946106
  time_total_s: 92121.71586084366
  timestamp: 1637603948
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 69888000
  training_iteration: 728
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    728 |          92121.7 | 69888000 |  1717.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 0.08
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 30.69
    apples_agent-1_min: 7
    apples_agent-2_max: 412
    apples_agent-2_mean: 351.25
    apples_agent-2_min: 14
    apples_agent-3_max: 291
    apples_agent-3_mean: 217.72
    apples_agent-3_min: 17
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 424
    apples_agent-5_mean: 377.08
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 394.95
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 25
    cleaning_beam_agent-1_mean: 1.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 2.88
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 29.85
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 457.16
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 4.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-01-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1885.0
  episode_reward_mean: 1716.89
  episode_reward_min: 93.0
  episodes_this_iter: 96
  episodes_total: 69984
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.002
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37682023644447327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024969088844954967
        model: {}
        policy_loss: -0.0019432390108704567
        total_loss: 0.0007565966807305813
        vf_explained_var: 0.07241372764110565
        vf_loss: 33.63040542602539
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23230071365833282
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212565810419619
        model: {}
        policy_loss: -0.002097517717629671
        total_loss: 0.000618624035269022
        vf_explained_var: 0.14106135070323944
        vf_loss: 31.24990463256836
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2814273536205292
        entropy_coeff: 0.0017600000137463212
        kl: 0.001231985166668892
        model: {}
        policy_loss: -0.001889204839244485
        total_loss: 0.0062367976643145084
        vf_explained_var: 0.10388824343681335
        vf_loss: 86.21316528320312
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5510131120681763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012316703796386719
        model: {}
        policy_loss: -0.0015367165906354785
        total_loss: 0.006490661296993494
        vf_explained_var: 0.06430687010288239
        vf_loss: 89.97161865234375
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9387644529342651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013709956547245383
        model: {}
        policy_loss: -0.001797159668058157
        total_loss: -0.00012896163389086723
        vf_explained_var: 0.01028299331665039
        vf_loss: 33.20425033569336
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29795998334884644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006664696265943348
        model: {}
        policy_loss: -0.0022367381025105715
        total_loss: 0.00010847926023416221
        vf_explained_var: 0.14810603857040405
        vf_loss: 28.69624137878418
    load_time_ms: 14509.483
    num_steps_sampled: 69984000
    num_steps_trained: 69984000
    sample_time_ms: 101770.814
    update_time_ms: 14.993
  iterations_since_restore: 79
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.749726775956287
    ram_util_percent: 15.166666666666666
  pid: 28385
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 429.0
    agent-3: 429.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 243.085
    agent-1: 243.085
    agent-2: 386.665
    agent-3: 386.665
    agent-4: 228.695
    agent-5: 228.695
  policy_reward_min:
    agent-0: 16.0
    agent-1: 16.0
    agent-2: 21.0
    agent-3: 21.0
    agent-4: 9.5
    agent-5: 9.5
  sampler_perf:
    mean_env_wait_ms: 26.730565212556503
    mean_inference_ms: 12.9840380139837
    mean_processing_ms: 59.753060176456756
  time_since_restore: 10117.70789361
  time_this_iter_s: 128.43440294265747
  time_total_s: 92250.15026378632
  timestamp: 1637604076
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 69984000
  training_iteration: 729
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    729 |          92250.2 | 69984000 |  1716.89 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.94
    apples_agent-1_min: 2
    apples_agent-2_max: 395
    apples_agent-2_mean: 352.45
    apples_agent-2_min: 6
    apples_agent-3_max: 264
    apples_agent-3_mean: 216.74
    apples_agent-3_min: 9
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 377.36
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 407.3
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.78
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 2.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 32.35
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 461.7
    cleaning_beam_agent-4_min: 388
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 4.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-03-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1822.0
  episode_reward_mean: 1714.11
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 70080
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11610.173
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3812433183193207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012920201988890767
        model: {}
        policy_loss: -0.001354556530714035
        total_loss: 0.0012998348101973534
        vf_explained_var: 0.07614430785179138
        vf_loss: 33.25380325317383
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23172299563884735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009078899165615439
        model: {}
        policy_loss: -0.0018716426566243172
        total_loss: 0.0007918031187728047
        vf_explained_var: 0.1461428850889206
        vf_loss: 30.712779998779297
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27408337593078613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010752459056675434
        model: {}
        policy_loss: -0.0019368811044842005
        total_loss: 0.005809256806969643
        vf_explained_var: 0.12446986138820648
        vf_loss: 82.28524017333984
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5481268167495728
        entropy_coeff: 0.0017600000137463212
        kl: 0.002125273458659649
        model: {}
        policy_loss: -0.0019905685912817717
        total_loss: 0.0060091749764978886
        vf_explained_var: 0.04620116949081421
        vf_loss: 89.64441680908203
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9454458951950073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021030299831181765
        model: {}
        policy_loss: -0.002044825814664364
        total_loss: -0.00040729623287916183
        vf_explained_var: -0.010503560304641724
        vf_loss: 33.015159606933594
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2925605773925781
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012405929155647755
        model: {}
        policy_loss: -0.0020286329090595245
        total_loss: 0.0002420339733362198
        vf_explained_var: 0.14737185835838318
        vf_loss: 27.855737686157227
    load_time_ms: 14483.007
    num_steps_sampled: 70080000
    num_steps_trained: 70080000
    sample_time_ms: 101622.248
    update_time_ms: 14.956
  iterations_since_restore: 80
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.45659340659341
    ram_util_percent: 15.169780219780227
  pid: 28385
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 242.15
    agent-1: 242.15
    agent-2: 387.085
    agent-3: 387.085
    agent-4: 227.82
    agent-5: 227.82
  policy_reward_min:
    agent-0: 14.0
    agent-1: 14.0
    agent-2: 13.5
    agent-3: 13.5
    agent-4: 12.5
    agent-5: 12.5
  sampler_perf:
    mean_env_wait_ms: 26.727446355940813
    mean_inference_ms: 12.98270902703595
    mean_processing_ms: 59.75004937226488
  time_since_restore: 10245.062358617783
  time_this_iter_s: 127.35446500778198
  time_total_s: 92377.5047287941
  timestamp: 1637604204
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 70080000
  training_iteration: 730
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    730 |          92377.5 | 70080000 |  1714.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 30.14
    apples_agent-1_min: 15
    apples_agent-2_max: 408
    apples_agent-2_mean: 349.93
    apples_agent-2_min: 256
    apples_agent-3_max: 259
    apples_agent-3_mean: 214.37
    apples_agent-3_min: 98
    apples_agent-4_max: 20
    apples_agent-4_mean: 0.34
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 378.07
    apples_agent-5_min: 205
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 402.51
    cleaning_beam_agent-0_min: 370
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.39
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 31.82
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 453.04
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 4.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-05-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1828.0
  episode_reward_mean: 1714.05
  episode_reward_min: 1221.0
  episodes_this_iter: 96
  episodes_total: 70176
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11612.71
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37158864736557007
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122314493637532
        model: {}
        policy_loss: -0.0012322315014898777
        total_loss: 0.0014924872666597366
        vf_explained_var: 0.006354436278343201
        vf_loss: 33.78715515136719
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23139959573745728
        entropy_coeff: 0.0017600000137463212
        kl: 0.000685757608152926
        model: {}
        policy_loss: -0.0017158782575279474
        total_loss: 0.0008526465389877558
        vf_explained_var: 0.12508991360664368
        vf_loss: 29.757869720458984
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27421924471855164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010653858771547675
        model: {}
        policy_loss: -0.001779141603037715
        total_loss: 0.005883628968149424
        vf_explained_var: 0.06446273624897003
        vf_loss: 81.4539566040039
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5578998327255249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006279773078858852
        model: {}
        policy_loss: -0.0015577806625515223
        total_loss: 0.005749966949224472
        vf_explained_var: 0.048154085874557495
        vf_loss: 82.89654541015625
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9531772136688232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016410534735769033
        model: {}
        policy_loss: -0.0021184764336794615
        total_loss: -0.0007905126549303532
        vf_explained_var: 0.02739623188972473
        vf_loss: 30.0555419921875
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2942597270011902
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006400790298357606
        model: {}
        policy_loss: -0.0018375972285866737
        total_loss: 0.0004466986283659935
        vf_explained_var: 0.09134651720523834
        vf_loss: 28.02193832397461
    load_time_ms: 14502.073
    num_steps_sampled: 70176000
    num_steps_trained: 70176000
    sample_time_ms: 101714.364
    update_time_ms: 14.969
  iterations_since_restore: 81
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.595108695652176
    ram_util_percent: 15.16304347826087
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 433.5
    agent-3: 433.5
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 242.125
    agent-1: 242.125
    agent-2: 383.33
    agent-3: 383.33
    agent-4: 231.57
    agent-5: 231.57
  policy_reward_min:
    agent-0: 194.0
    agent-1: 194.0
    agent-2: 271.0
    agent-3: 271.0
    agent-4: 139.0
    agent-5: 139.0
  sampler_perf:
    mean_env_wait_ms: 26.724706999590424
    mean_inference_ms: 12.981634469423089
    mean_processing_ms: 59.75273264853056
  time_since_restore: 10372.942840337753
  time_this_iter_s: 127.8804817199707
  time_total_s: 92505.38521051407
  timestamp: 1637604333
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 70176000
  training_iteration: 731
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    731 |          92505.4 | 70176000 |  1714.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 32.0
    apples_agent-1_min: 17
    apples_agent-2_max: 405
    apples_agent-2_mean: 358.23
    apples_agent-2_min: 295
    apples_agent-3_max: 273
    apples_agent-3_mean: 212.65
    apples_agent-3_min: 144
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 384.56
    apples_agent-5_min: 293
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 398.14
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 2.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 40.14
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 451.83
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-07-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1845.0
  episode_reward_mean: 1734.73
  episode_reward_min: 1546.0
  episodes_this_iter: 96
  episodes_total: 70272
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11622.516
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3705689609050751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010008553508669138
        model: {}
        policy_loss: -0.0012838405091315508
        total_loss: 0.0013969028368592262
        vf_explained_var: -0.00910237431526184
        vf_loss: 33.329471588134766
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22828622162342072
        entropy_coeff: 0.0017600000137463212
        kl: 0.001294206129387021
        model: {}
        policy_loss: -0.0016388734802603722
        total_loss: 0.0008897408843040466
        vf_explained_var: 0.11271007359027863
        vf_loss: 29.303997039794922
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2711482346057892
        entropy_coeff: 0.0017600000137463212
        kl: 0.001210369635373354
        model: {}
        policy_loss: -0.0015489929355680943
        total_loss: 0.005904204677790403
        vf_explained_var: 0.042604804039001465
        vf_loss: 79.30419921875
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5674930810928345
        entropy_coeff: 0.0017600000137463212
        kl: 0.001717209699563682
        model: {}
        policy_loss: -0.0018825358711183071
        total_loss: 0.005106767639517784
        vf_explained_var: 0.034138426184654236
        vf_loss: 79.88087463378906
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9547662138938904
        entropy_coeff: 0.0017600000137463212
        kl: 0.001899836352095008
        model: {}
        policy_loss: -0.001978795975446701
        total_loss: -0.0004372266121208668
        vf_explained_var: 0.026242956519126892
        vf_loss: 32.21957778930664
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28725916147232056
        entropy_coeff: 0.0017600000137463212
        kl: 0.001015456859022379
        model: {}
        policy_loss: -0.002044473774731159
        total_loss: 0.0004499211208894849
        vf_explained_var: 0.0919010192155838
        vf_loss: 29.99974250793457
    load_time_ms: 14509.91
    num_steps_sampled: 70272000
    num_steps_trained: 70272000
    sample_time_ms: 101598.209
    update_time_ms: 14.828
  iterations_since_restore: 82
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.053551912568306
    ram_util_percent: 15.115846994535524
  pid: 28385
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 435.5
    agent-3: 435.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 245.45
    agent-1: 245.45
    agent-2: 388.65
    agent-3: 388.65
    agent-4: 233.265
    agent-5: 233.265
  policy_reward_min:
    agent-0: 210.5
    agent-1: 210.5
    agent-2: 350.0
    agent-3: 350.0
    agent-4: 166.0
    agent-5: 166.0
  sampler_perf:
    mean_env_wait_ms: 26.72476110683426
    mean_inference_ms: 12.981550831246034
    mean_processing_ms: 59.75474956992941
  time_since_restore: 10500.89169716835
  time_this_iter_s: 127.94885683059692
  time_total_s: 92633.33406734467
  timestamp: 1637604462
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 70272000
  training_iteration: 732
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    732 |          92633.3 | 70272000 |  1734.73 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 30.67
    apples_agent-1_min: 19
    apples_agent-2_max: 415
    apples_agent-2_mean: 357.61
    apples_agent-2_min: 306
    apples_agent-3_max: 275
    apples_agent-3_mean: 214.29
    apples_agent-3_min: 124
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 384.66
    apples_agent-5_min: 302
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 396.75
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 27
    cleaning_beam_agent-1_mean: 2.78
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.39
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 37.28
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 531
    cleaning_beam_agent-4_mean: 426.18
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 4.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-09-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1867.0
  episode_reward_mean: 1728.1
  episode_reward_min: 1439.0
  episodes_this_iter: 96
  episodes_total: 70368
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11617.782
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3726111650466919
        entropy_coeff: 0.0017600000137463212
        kl: 0.001397685264237225
        model: {}
        policy_loss: -0.001380979549139738
        total_loss: 0.001229665707796812
        vf_explained_var: -0.013870984315872192
        vf_loss: 32.66438674926758
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2310079038143158
        entropy_coeff: 0.0017600000137463212
        kl: 0.00045078364200890064
        model: {}
        policy_loss: -0.0015998206799849868
        total_loss: 0.0008681552717462182
        vf_explained_var: 0.11027507483959198
        vf_loss: 28.74550437927246
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26966533064842224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010721561266109347
        model: {}
        policy_loss: -0.001550221350044012
        total_loss: 0.00569617236033082
        vf_explained_var: 0.047035038471221924
        vf_loss: 77.21006774902344
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5582739114761353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011963952565565705
        model: {}
        policy_loss: -0.0017748279497027397
        total_loss: 0.004789023660123348
        vf_explained_var: 0.0683523416519165
        vf_loss: 75.46415710449219
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.962590217590332
        entropy_coeff: 0.0017600000137463212
        kl: 0.002000032225623727
        model: {}
        policy_loss: -0.0018863086588680744
        total_loss: -0.0005405806005001068
        vf_explained_var: 0.009074762463569641
        vf_loss: 30.398841857910156
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2853449285030365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008400032529607415
        model: {}
        policy_loss: -0.001744713168591261
        total_loss: 0.0005577011033892632
        vf_explained_var: 0.08559954166412354
        vf_loss: 28.046205520629883
    load_time_ms: 14494.631
    num_steps_sampled: 70368000
    num_steps_trained: 70368000
    sample_time_ms: 101582.733
    update_time_ms: 14.512
  iterations_since_restore: 83
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.57777777777778
    ram_util_percent: 15.180000000000003
  pid: 28385
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 424.0
    agent-3: 424.0
    agent-4: 279.5
    agent-5: 279.5
  policy_reward_mean:
    agent-0: 243.94
    agent-1: 243.94
    agent-2: 387.37
    agent-3: 387.37
    agent-4: 232.74
    agent-5: 232.74
  policy_reward_min:
    agent-0: 178.5
    agent-1: 178.5
    agent-2: 333.0
    agent-3: 333.0
    agent-4: 186.0
    agent-5: 186.0
  sampler_perf:
    mean_env_wait_ms: 26.720274923255793
    mean_inference_ms: 12.98074697733874
    mean_processing_ms: 59.74824865465653
  time_since_restore: 10627.590258359909
  time_this_iter_s: 126.69856119155884
  time_total_s: 92760.03262853622
  timestamp: 1637604588
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 70368000
  training_iteration: 733
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    733 |            92760 | 70368000 |   1728.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 31.92
    apples_agent-1_min: 17
    apples_agent-2_max: 433
    apples_agent-2_mean: 355.91
    apples_agent-2_min: 297
    apples_agent-3_max: 273
    apples_agent-3_mean: 214.57
    apples_agent-3_min: 130
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 378.8
    apples_agent-5_min: 328
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 396.49
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 40.2
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 431.91
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-11-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1848.0
  episode_reward_mean: 1711.95
  episode_reward_min: 1525.0
  episodes_this_iter: 96
  episodes_total: 70464
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11611.745
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3730867803096771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007515026372857392
        model: {}
        policy_loss: -0.001096420455724001
        total_loss: 0.0014479421079158783
        vf_explained_var: 0.0019313842058181763
        vf_loss: 32.009979248046875
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23059700429439545
        entropy_coeff: 0.0017600000137463212
        kl: 0.000689838663674891
        model: {}
        policy_loss: -0.0016388501971960068
        total_loss: 0.0008393586613237858
        vf_explained_var: 0.09900319576263428
        vf_loss: 28.84060287475586
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27619582414627075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012937623541802168
        model: {}
        policy_loss: -0.0020533129572868347
        total_loss: 0.0055270083248615265
        vf_explained_var: 0.039921730756759644
        vf_loss: 80.6642837524414
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5531561374664307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005618614377453923
        model: {}
        policy_loss: -0.0015363353304564953
        total_loss: 0.005500342231243849
        vf_explained_var: 0.04695631563663483
        vf_loss: 80.10233306884766
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9700629711151123
        entropy_coeff: 0.0017600000137463212
        kl: 0.002559876535087824
        model: {}
        policy_loss: -0.0020352869760245085
        total_loss: -0.000829517375677824
        vf_explained_var: 0.014102727174758911
        vf_loss: 29.13079833984375
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2938793897628784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007338369614444673
        model: {}
        policy_loss: -0.002045926172286272
        total_loss: 0.00012845080345869064
        vf_explained_var: 0.09036144614219666
        vf_loss: 26.916040420532227
    load_time_ms: 14467.089
    num_steps_sampled: 70464000
    num_steps_trained: 70464000
    sample_time_ms: 101481.15
    update_time_ms: 14.506
  iterations_since_restore: 84
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.32912087912088
    ram_util_percent: 15.184065934065936
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 427.5
    agent-3: 427.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 242.565
    agent-1: 242.565
    agent-2: 385.085
    agent-3: 385.085
    agent-4: 228.325
    agent-5: 228.325
  policy_reward_min:
    agent-0: 209.0
    agent-1: 209.0
    agent-2: 337.0
    agent-3: 337.0
    agent-4: 198.0
    agent-5: 198.0
  sampler_perf:
    mean_env_wait_ms: 26.715493150647575
    mean_inference_ms: 12.980076482702005
    mean_processing_ms: 59.74225274986321
  time_since_restore: 10755.242322444916
  time_this_iter_s: 127.65206408500671
  time_total_s: 92887.68469262123
  timestamp: 1637604716
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 70464000
  training_iteration: 734
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    734 |          92887.7 | 70464000 |  1711.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 30.69
    apples_agent-1_min: 19
    apples_agent-2_max: 424
    apples_agent-2_mean: 353.29
    apples_agent-2_min: 210
    apples_agent-3_max: 275
    apples_agent-3_mean: 211.6
    apples_agent-3_min: 123
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 379.13
    apples_agent-5_min: 209
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 399.46
    cleaning_beam_agent-0_min: 368
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.69
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 37.86
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 521
    cleaning_beam_agent-4_mean: 418.13
    cleaning_beam_agent-4_min: 350
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 4.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-14-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1848.0
  episode_reward_mean: 1705.72
  episode_reward_min: 921.0
  episodes_this_iter: 96
  episodes_total: 70560
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.238
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3688012361526489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013391576940193772
        model: {}
        policy_loss: -0.0013095940230414271
        total_loss: 0.0014904217096045613
        vf_explained_var: 0.026868164539337158
        vf_loss: 34.49107360839844
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2369268387556076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007776404963806272
        model: {}
        policy_loss: -0.0018408941105008125
        total_loss: 0.000944807194173336
        vf_explained_var: 0.09956866502761841
        vf_loss: 32.02695083618164
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27305349707603455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011903414269909263
        model: {}
        policy_loss: -0.0020224973559379578
        total_loss: 0.005594905931502581
        vf_explained_var: 0.08799144625663757
        vf_loss: 80.97977447509766
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5641462802886963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007260559359565377
        model: {}
        policy_loss: -0.0015485722105950117
        total_loss: 0.005896612070500851
        vf_explained_var: 0.05109274387359619
        vf_loss: 84.38079071044922
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9597413539886475
        entropy_coeff: 0.0017600000137463212
        kl: 0.002220276277512312
        model: {}
        policy_loss: -0.002007429488003254
        total_loss: -0.0005910638719797134
        vf_explained_var: 0.0284145325422287
        vf_loss: 31.055070877075195
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2836129367351532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021517788991332054
        model: {}
        policy_loss: -0.0024584452621638775
        total_loss: -0.00014145765453577042
        vf_explained_var: 0.11893539130687714
        vf_loss: 28.16146469116211
    load_time_ms: 14476.322
    num_steps_sampled: 70560000
    num_steps_trained: 70560000
    sample_time_ms: 101282.956
    update_time_ms: 14.292
  iterations_since_restore: 85
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.64388888888889
    ram_util_percent: 15.195555555555558
  pid: 28385
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 421.5
    agent-3: 421.5
    agent-4: 268.0
    agent-5: 268.0
  policy_reward_mean:
    agent-0: 240.515
    agent-1: 240.515
    agent-2: 382.21
    agent-3: 382.21
    agent-4: 230.135
    agent-5: 230.135
  policy_reward_min:
    agent-0: 121.5
    agent-1: 121.5
    agent-2: 218.0
    agent-3: 218.0
    agent-4: 121.0
    agent-5: 121.0
  sampler_perf:
    mean_env_wait_ms: 26.71202476106595
    mean_inference_ms: 12.979317591353864
    mean_processing_ms: 59.73756740124345
  time_since_restore: 10881.759455442429
  time_this_iter_s: 126.51713299751282
  time_total_s: 93014.20182561874
  timestamp: 1637604843
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 70560000
  training_iteration: 735
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    735 |          93014.2 | 70560000 |  1705.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 30.92
    apples_agent-1_min: 17
    apples_agent-2_max: 404
    apples_agent-2_mean: 359.95
    apples_agent-2_min: 284
    apples_agent-3_max: 264
    apples_agent-3_mean: 218.49
    apples_agent-3_min: 145
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 384.95
    apples_agent-5_min: 332
    cleaning_beam_agent-0_max: 430
    cleaning_beam_agent-0_mean: 393.3
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 23
    cleaning_beam_agent-1_mean: 1.61
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 2.59
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 41.3
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 517
    cleaning_beam_agent-4_mean: 440.24
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 4.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-16-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1846.0
  episode_reward_mean: 1733.93
  episode_reward_min: 1498.0
  episodes_this_iter: 96
  episodes_total: 70656
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.942
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3665670156478882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013782107271254063
        model: {}
        policy_loss: -0.0014192744856700301
        total_loss: 0.0013265718007460237
        vf_explained_var: -0.0019367486238479614
        vf_loss: 33.91002655029297
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2330232560634613
        entropy_coeff: 0.0017600000137463212
        kl: 0.001119435066357255
        model: {}
        policy_loss: -0.0020471415482461452
        total_loss: 0.0006482426542788744
        vf_explained_var: 0.08282484114170074
        vf_loss: 31.055072784423828
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2636716365814209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012518786825239658
        model: {}
        policy_loss: -0.0017121139680966735
        total_loss: 0.005608247593045235
        vf_explained_var: 0.047205761075019836
        vf_loss: 77.84423828125
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5497469902038574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008173738024197519
        model: {}
        policy_loss: -0.0014110896736383438
        total_loss: 0.005466007627546787
        vf_explained_var: 0.03975348174571991
        vf_loss: 78.44654846191406
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9523194432258606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020137205719947815
        model: {}
        policy_loss: -0.002067379653453827
        total_loss: -0.0006659270729869604
        vf_explained_var: -0.005071356892585754
        vf_loss: 30.77532196044922
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26929983496665955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009638523915782571
        model: {}
        policy_loss: -0.0014536406379193068
        total_loss: 0.0007838411256670952
        vf_explained_var: 0.10680347681045532
        vf_loss: 27.114492416381836
    load_time_ms: 14482.768
    num_steps_sampled: 70656000
    num_steps_trained: 70656000
    sample_time_ms: 101384.963
    update_time_ms: 14.338
  iterations_since_restore: 86
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.5344262295082
    ram_util_percent: 15.19071038251366
  pid: 28385
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 242.495
    agent-1: 242.495
    agent-2: 390.025
    agent-3: 390.025
    agent-4: 234.445
    agent-5: 234.445
  policy_reward_min:
    agent-0: 158.0
    agent-1: 158.0
    agent-2: 326.0
    agent-3: 326.0
    agent-4: 201.0
    agent-5: 201.0
  sampler_perf:
    mean_env_wait_ms: 26.709354134341776
    mean_inference_ms: 12.978925130242324
    mean_processing_ms: 59.736441174363826
  time_since_restore: 11009.893084287643
  time_this_iter_s: 128.13362884521484
  time_total_s: 93142.33545446396
  timestamp: 1637604971
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 70656000
  training_iteration: 736
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    736 |          93142.3 | 70656000 |  1733.93 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.96
    apples_agent-1_min: 5
    apples_agent-2_max: 410
    apples_agent-2_mean: 353.84
    apples_agent-2_min: 11
    apples_agent-3_max: 265
    apples_agent-3_mean: 217.15
    apples_agent-3_min: 14
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 377.44
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 392.49
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.41
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 2.45
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 43.86
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 450.92
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 3.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-18-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1863.0
  episode_reward_mean: 1710.78
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 70752
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.396
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36717817187309265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015933978138491511
        model: {}
        policy_loss: -0.0014812164008617401
        total_loss: 0.0012838318943977356
        vf_explained_var: 0.08637361228466034
        vf_loss: 34.11281204223633
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23591282963752747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010188971646130085
        model: {}
        policy_loss: -0.0018971932586282492
        total_loss: 0.0008689718088135123
        vf_explained_var: 0.15274083614349365
        vf_loss: 31.81369972229004
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27073147892951965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011242965701967478
        model: {}
        policy_loss: -0.00216023251414299
        total_loss: 0.005904423072934151
        vf_explained_var: 0.1094723641872406
        vf_loss: 85.41146850585938
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5537446141242981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015498390421271324
        model: {}
        policy_loss: -0.0018716215854510665
        total_loss: 0.006240291055291891
        vf_explained_var: 0.05337291955947876
        vf_loss: 90.86502838134766
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9550104737281799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021658928599208593
        model: {}
        policy_loss: -0.0019033493008464575
        total_loss: -6.265565752983093e-05
        vf_explained_var: 0.0025434792041778564
        vf_loss: 35.215110778808594
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27976465225219727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008345574606209993
        model: {}
        policy_loss: -0.00194576196372509
        total_loss: 0.000509968027472496
        vf_explained_var: 0.16427654027938843
        vf_loss: 29.481163024902344
    load_time_ms: 14496.922
    num_steps_sampled: 70752000
    num_steps_trained: 70752000
    sample_time_ms: 101297.203
    update_time_ms: 14.102
  iterations_since_restore: 87
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.0967032967033
    ram_util_percent: 15.126373626373631
  pid: 28385
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 431.0
    agent-3: 431.0
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 241.27
    agent-1: 241.27
    agent-2: 384.67
    agent-3: 384.67
    agent-4: 229.45
    agent-5: 229.45
  policy_reward_min:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 12.5
    agent-3: 12.5
    agent-4: 8.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 26.70863395103023
    mean_inference_ms: 12.978540319685514
    mean_processing_ms: 59.73485902420426
  time_since_restore: 11137.337349891663
  time_this_iter_s: 127.44426560401917
  time_total_s: 93269.77972006798
  timestamp: 1637605099
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 70752000
  training_iteration: 737
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    737 |          93269.8 | 70752000 |  1710.78 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.77
    apples_agent-1_min: 15
    apples_agent-2_max: 416
    apples_agent-2_mean: 361.32
    apples_agent-2_min: 303
    apples_agent-3_max: 267
    apples_agent-3_mean: 218.98
    apples_agent-3_min: 159
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 384.65
    apples_agent-5_min: 321
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 389.04
    cleaning_beam_agent-0_min: 357
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.92
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 37.32
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 457.31
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-20-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1845.0
  episode_reward_mean: 1738.19
  episode_reward_min: 1513.0
  episodes_this_iter: 96
  episodes_total: 70848
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11625.424
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35507506132125854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013150661252439022
        model: {}
        policy_loss: -0.0011178844142705202
        total_loss: 0.001506156986579299
        vf_explained_var: -0.006268501281738281
        vf_loss: 32.48974609375
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23149779438972473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008076300146058202
        model: {}
        policy_loss: -0.0016192160546779633
        total_loss: 0.0008681020699441433
        vf_explained_var: 0.10364292562007904
        vf_loss: 28.947561264038086
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2637941837310791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013306110631674528
        model: {}
        policy_loss: -0.0017797863110899925
        total_loss: 0.005608411971479654
        vf_explained_var: 0.03262859582901001
        vf_loss: 78.5247573852539
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5285648703575134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012706995476037264
        model: {}
        policy_loss: -0.0017849165014922619
        total_loss: 0.0049681756645441055
        vf_explained_var: 0.05336026847362518
        vf_loss: 76.83370208740234
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9611223936080933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015245357062667608
        model: {}
        policy_loss: -0.0015495221596211195
        total_loss: -0.00023761135526001453
        vf_explained_var: 0.014504268765449524
        vf_loss: 30.034875869750977
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2712915241718292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006415352690964937
        model: {}
        policy_loss: -0.0017040595412254333
        total_loss: 0.0005122919101268053
        vf_explained_var: 0.12292292714118958
        vf_loss: 26.93824005126953
    load_time_ms: 14510.428
    num_steps_sampled: 70848000
    num_steps_trained: 70848000
    sample_time_ms: 101446.147
    update_time_ms: 14.089
  iterations_since_restore: 88
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.580327868852454
    ram_util_percent: 15.155737704918034
  pid: 28385
  policy_reward_max:
    agent-0: 268.5
    agent-1: 268.5
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 244.505
    agent-1: 244.505
    agent-2: 391.31
    agent-3: 391.31
    agent-4: 233.28
    agent-5: 233.28
  policy_reward_min:
    agent-0: 200.5
    agent-1: 200.5
    agent-2: 339.0
    agent-3: 339.0
    agent-4: 195.0
    agent-5: 195.0
  sampler_perf:
    mean_env_wait_ms: 26.70684538928613
    mean_inference_ms: 12.977812345737973
    mean_processing_ms: 59.73286527176279
  time_since_restore: 11266.107142686844
  time_this_iter_s: 128.76979279518127
  time_total_s: 93398.54951286316
  timestamp: 1637605228
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 70848000
  training_iteration: 738
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    738 |          93398.5 | 70848000 |  1738.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.08
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 31.49
    apples_agent-1_min: 4
    apples_agent-2_max: 418
    apples_agent-2_mean: 354.93
    apples_agent-2_min: 38
    apples_agent-3_max: 279
    apples_agent-3_mean: 215.66
    apples_agent-3_min: 10
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 377.65
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 420
    cleaning_beam_agent-0_mean: 391.44
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.75
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 2.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 37.44
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 456.48
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 5.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-22-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1858.0
  episode_reward_mean: 1711.77
  episode_reward_min: 125.0
  episodes_this_iter: 96
  episodes_total: 70944
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11622.156
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37044522166252136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010376737918704748
        model: {}
        policy_loss: -0.001237969845533371
        total_loss: 0.0016158507205545902
        vf_explained_var: 0.10990026593208313
        vf_loss: 35.05805206298828
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.236476331949234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010700849816203117
        model: {}
        policy_loss: -0.0018929112702608109
        total_loss: 0.0011184820905327797
        vf_explained_var: 0.1345970332622528
        vf_loss: 34.275936126708984
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27329397201538086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011658378643915057
        model: {}
        policy_loss: -0.0020278887823224068
        total_loss: 0.006321531720459461
        vf_explained_var: 0.1403311938047409
        vf_loss: 88.30418395996094
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5276632308959961
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017074489733204246
        model: {}
        policy_loss: -0.0019981670193374157
        total_loss: 0.006751662585884333
        vf_explained_var: 0.057667702436447144
        vf_loss: 96.78515625
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9613316655158997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016122229862958193
        model: {}
        policy_loss: -0.0020303381606936455
        total_loss: -4.8226211220026016e-05
        vf_explained_var: 0.017548859119415283
        vf_loss: 36.74056625366211
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2882104814052582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006122196791693568
        model: {}
        policy_loss: -0.0019802965689450502
        total_loss: 0.0005396385095082223
        vf_explained_var: 0.18981805443763733
        vf_loss: 30.271854400634766
    load_time_ms: 14494.71
    num_steps_sampled: 70944000
    num_steps_trained: 70944000
    sample_time_ms: 101346.999
    update_time_ms: 13.903
  iterations_since_restore: 89
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.373076923076923
    ram_util_percent: 15.212087912087915
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 240.925
    agent-1: 240.925
    agent-2: 385.445
    agent-3: 385.445
    agent-4: 229.515
    agent-5: 229.515
  policy_reward_min:
    agent-0: 22.5
    agent-1: 22.5
    agent-2: 23.0
    agent-3: 23.0
    agent-4: 17.0
    agent-5: 17.0
  sampler_perf:
    mean_env_wait_ms: 26.705073965618475
    mean_inference_ms: 12.977183916763764
    mean_processing_ms: 59.72983092549719
  time_since_restore: 11393.312982797623
  time_this_iter_s: 127.20584011077881
  time_total_s: 93525.75535297394
  timestamp: 1637605355
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 70944000
  training_iteration: 739
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    739 |          93525.8 | 70944000 |  1711.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 30.95
    apples_agent-1_min: 19
    apples_agent-2_max: 420
    apples_agent-2_mean: 360.28
    apples_agent-2_min: 304
    apples_agent-3_max: 309
    apples_agent-3_mean: 221.01
    apples_agent-3_min: 155
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 381.14
    apples_agent-5_min: 318
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 398.81
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 1.73
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 33.66
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 440.14
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 4.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-24-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1864.0
  episode_reward_mean: 1737.27
  episode_reward_min: 1567.0
  episodes_this_iter: 96
  episodes_total: 71040
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11626.327
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3561910092830658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014971948694437742
        model: {}
        policy_loss: -0.0011183060705661774
        total_loss: 0.0014602018054574728
        vf_explained_var: -0.0018328875303268433
        vf_loss: 32.054046630859375
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22777855396270752
        entropy_coeff: 0.0017600000137463212
        kl: 0.001258681295439601
        model: {}
        policy_loss: -0.0016136192716658115
        total_loss: 0.0008571469224989414
        vf_explained_var: 0.10163870453834534
        vf_loss: 28.716564178466797
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26598045229911804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009541658218950033
        model: {}
        policy_loss: -0.0016605162527412176
        total_loss: 0.005886130500584841
        vf_explained_var: 0.049068957567214966
        vf_loss: 80.14775848388672
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5141139626502991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016075305175036192
        model: {}
        policy_loss: -0.00161751639097929
        total_loss: 0.005495430901646614
        vf_explained_var: 0.049511462450027466
        vf_loss: 80.17787170410156
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9551461935043335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021400204859673977
        model: {}
        policy_loss: -0.0020293425768613815
        total_loss: -0.0007904428639449179
        vf_explained_var: 0.02474217116832733
        vf_loss: 29.199569702148438
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27151453495025635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010638890089467168
        model: {}
        policy_loss: -0.0017604324966669083
        total_loss: 0.0004968130961060524
        vf_explained_var: 0.08657915890216827
        vf_loss: 27.35109519958496
    load_time_ms: 14551.005
    num_steps_sampled: 71040000
    num_steps_trained: 71040000
    sample_time_ms: 101288.281
    update_time_ms: 13.795
  iterations_since_restore: 90
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.985082872928174
    ram_util_percent: 15.13425414364641
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 434.5
    agent-3: 434.5
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 245.29
    agent-1: 245.29
    agent-2: 391.84
    agent-3: 391.84
    agent-4: 231.505
    agent-5: 231.505
  policy_reward_min:
    agent-0: 219.0
    agent-1: 219.0
    agent-2: 337.0
    agent-3: 337.0
    agent-4: 194.5
    agent-5: 194.5
  sampler_perf:
    mean_env_wait_ms: 26.70300391408618
    mean_inference_ms: 12.976770113951954
    mean_processing_ms: 59.72641555279657
  time_since_restore: 11520.746376514435
  time_this_iter_s: 127.43339371681213
  time_total_s: 93653.18874669075
  timestamp: 1637605482
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 71040000
  training_iteration: 740
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    740 |          93653.2 | 71040000 |  1737.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.75
    apples_agent-1_min: 19
    apples_agent-2_max: 422
    apples_agent-2_mean: 356.45
    apples_agent-2_min: 194
    apples_agent-3_max: 279
    apples_agent-3_mean: 220.85
    apples_agent-3_min: 112
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 382.24
    apples_agent-5_min: 209
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 399.68
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.48
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 31.48
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 445.47
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 4.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-26-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1867.0
  episode_reward_mean: 1726.65
  episode_reward_min: 991.0
  episodes_this_iter: 96
  episodes_total: 71136
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11626.085
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35125231742858887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010666361777111888
        model: {}
        policy_loss: -0.0011089015752077103
        total_loss: 0.0017437913920730352
        vf_explained_var: 0.00284421443939209
        vf_loss: 34.70896530151367
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2311985045671463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009805464651435614
        model: {}
        policy_loss: -0.0020410879515111446
        total_loss: 0.0006554294377565384
        vf_explained_var: 0.10852351784706116
        vf_loss: 31.034286499023438
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26915213465690613
        entropy_coeff: 0.0017600000137463212
        kl: 0.001216180040501058
        model: {}
        policy_loss: -0.0019631884060800076
        total_loss: 0.005988346878439188
        vf_explained_var: 0.08003368973731995
        vf_loss: 84.25243377685547
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5197356343269348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015955287963151932
        model: {}
        policy_loss: -0.0016592815518379211
        total_loss: 0.0060308268293738365
        vf_explained_var: 0.06056980788707733
        vf_loss: 86.04844665527344
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9520336985588074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019346107728779316
        model: {}
        policy_loss: -0.0019646859727799892
        total_loss: -0.0006324867717921734
        vf_explained_var: 0.05903905630111694
        vf_loss: 30.077774047851562
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27362313866615295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006938658189028502
        model: {}
        policy_loss: -0.0018145751673728228
        total_loss: 0.0004903522785753012
        vf_explained_var: 0.1368894726037979
        vf_loss: 27.865062713623047
    load_time_ms: 14501.528
    num_steps_sampled: 71136000
    num_steps_trained: 71136000
    sample_time_ms: 101330.183
    update_time_ms: 13.887
  iterations_since_restore: 91
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.054347826086957
    ram_util_percent: 15.21032608695652
  pid: 28385
  policy_reward_max:
    agent-0: 280.5
    agent-1: 280.5
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 241.06
    agent-1: 241.06
    agent-2: 389.385
    agent-3: 389.385
    agent-4: 232.88
    agent-5: 232.88
  policy_reward_min:
    agent-0: 128.5
    agent-1: 128.5
    agent-2: 225.0
    agent-3: 225.0
    agent-4: 142.0
    agent-5: 142.0
  sampler_perf:
    mean_env_wait_ms: 26.701195590620998
    mean_inference_ms: 12.976651899534978
    mean_processing_ms: 59.72553495023882
  time_since_restore: 11648.489419460297
  time_this_iter_s: 127.74304294586182
  time_total_s: 93780.93178963661
  timestamp: 1637605612
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 71136000
  training_iteration: 741
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    741 |          93780.9 | 71136000 |  1726.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 31.13
    apples_agent-1_min: 14
    apples_agent-2_max: 408
    apples_agent-2_mean: 355.79
    apples_agent-2_min: 220
    apples_agent-3_max: 272
    apples_agent-3_mean: 221.18
    apples_agent-3_min: 108
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.18
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 379.12
    apples_agent-5_min: 232
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 399.74
    cleaning_beam_agent-0_min: 339
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 31.36
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 452.0
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.4
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-28-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1853.0
  episode_reward_mean: 1718.87
  episode_reward_min: 998.0
  episodes_this_iter: 96
  episodes_total: 71232
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11615.977
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3542954623699188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012864945456385612
        model: {}
        policy_loss: -0.0012775505892932415
        total_loss: 0.0014547253958880901
        vf_explained_var: 0.03886111080646515
        vf_loss: 33.55836486816406
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23330242931842804
        entropy_coeff: 0.0017600000137463212
        kl: 0.001367317046970129
        model: {}
        policy_loss: -0.001805417239665985
        total_loss: 0.0007943562231957912
        vf_explained_var: 0.13846547901630402
        vf_loss: 30.103851318359375
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26663073897361755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008068436291068792
        model: {}
        policy_loss: -0.001654488267377019
        total_loss: 0.006072442978620529
        vf_explained_var: 0.08545657992362976
        vf_loss: 81.96204376220703
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5145657062530518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0003716631617862731
        model: {}
        policy_loss: -0.0013342322781682014
        total_loss: 0.006365050561726093
        vf_explained_var: 0.04042255878448486
        vf_loss: 86.04918670654297
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9483246803283691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014524752041324973
        model: {}
        policy_loss: -0.0016900287009775639
        total_loss: -0.00021409743931144476
        vf_explained_var: 0.02179691195487976
        vf_loss: 31.449844360351562
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27661705017089844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006505016353912652
        model: {}
        policy_loss: -0.0017820866778492928
        total_loss: 0.000521484762430191
        vf_explained_var: 0.13227657973766327
        vf_loss: 27.904178619384766
    load_time_ms: 14478.801
    num_steps_sampled: 71232000
    num_steps_trained: 71232000
    sample_time_ms: 101313.891
    update_time_ms: 13.854
  iterations_since_restore: 92
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.146703296703297
    ram_util_percent: 15.121978021978022
  pid: 28385
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 433.5
    agent-3: 433.5
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 241.1
    agent-1: 241.1
    agent-2: 387.655
    agent-3: 387.655
    agent-4: 230.68
    agent-5: 230.68
  policy_reward_min:
    agent-0: 131.5
    agent-1: 131.5
    agent-2: 221.0
    agent-3: 221.0
    agent-4: 146.5
    agent-5: 146.5
  sampler_perf:
    mean_env_wait_ms: 26.699940637107517
    mean_inference_ms: 12.976549773620862
    mean_processing_ms: 59.72750949307098
  time_since_restore: 11775.978512525558
  time_this_iter_s: 127.48909306526184
  time_total_s: 93908.42088270187
  timestamp: 1637605739
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 71232000
  training_iteration: 742
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    742 |          93908.4 | 71232000 |  1718.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 32.28
    apples_agent-1_min: 14
    apples_agent-2_max: 434
    apples_agent-2_mean: 358.86
    apples_agent-2_min: 215
    apples_agent-3_max: 271
    apples_agent-3_mean: 215.4
    apples_agent-3_min: 125
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 460
    apples_agent-5_mean: 381.23
    apples_agent-5_min: 244
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 398.49
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.3
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 35.28
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 446.13
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 21
    cleaning_beam_agent-5_mean: 4.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-31-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1887.0
  episode_reward_mean: 1716.5
  episode_reward_min: 1093.0
  episodes_this_iter: 96
  episodes_total: 71328
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11627.641
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35025134682655334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007233274518512189
        model: {}
        policy_loss: -0.0010962611995637417
        total_loss: 0.0020008040592074394
        vf_explained_var: -0.008012652397155762
        vf_loss: 37.13505935668945
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23340456187725067
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008743311045691371
        model: {}
        policy_loss: -0.0020702406764030457
        total_loss: 0.000748894177377224
        vf_explained_var: 0.125749871134758
        vf_loss: 32.29926300048828
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26185110211372375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009896409465000033
        model: {}
        policy_loss: -0.0018768622539937496
        total_loss: 0.006135529838502407
        vf_explained_var: 0.04870292544364929
        vf_loss: 84.73252868652344
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5178350210189819
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007009725668467581
        model: {}
        policy_loss: -0.001731685595586896
        total_loss: 0.005486117675900459
        vf_explained_var: 0.08712035417556763
        vf_loss: 81.29194641113281
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9531933069229126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009854233358055353
        model: {}
        policy_loss: -0.0018673799932003021
        total_loss: -0.0004147021099925041
        vf_explained_var: 0.02321772277355194
        vf_loss: 31.302976608276367
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28176063299179077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007207673043012619
        model: {}
        policy_loss: -0.001737179234623909
        total_loss: 0.000616828037891537
        vf_explained_var: 0.11065740883350372
        vf_loss: 28.49903678894043
    load_time_ms: 14493.729
    num_steps_sampled: 71328000
    num_steps_trained: 71328000
    sample_time_ms: 101381.638
    update_time_ms: 13.799
  iterations_since_restore: 93
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.28076923076923
    ram_util_percent: 15.25329670329671
  pid: 28385
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 448.5
    agent-3: 448.5
    agent-4: 279.5
    agent-5: 279.5
  policy_reward_mean:
    agent-0: 239.55
    agent-1: 239.55
    agent-2: 387.685
    agent-3: 387.685
    agent-4: 231.015
    agent-5: 231.015
  policy_reward_min:
    agent-0: 83.0
    agent-1: 83.0
    agent-2: 246.5
    agent-3: 246.5
    agent-4: 160.5
    agent-5: 160.5
  sampler_perf:
    mean_env_wait_ms: 26.695702917274897
    mean_inference_ms: 12.975741793516498
    mean_processing_ms: 59.72224608577368
  time_since_restore: 11903.683056592941
  time_this_iter_s: 127.70454406738281
  time_total_s: 94036.12542676926
  timestamp: 1637605867
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 71328000
  training_iteration: 743
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    743 |          94036.1 | 71328000 |   1716.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.19
    apples_agent-1_min: 12
    apples_agent-2_max: 422
    apples_agent-2_mean: 359.88
    apples_agent-2_min: 278
    apples_agent-3_max: 285
    apples_agent-3_mean: 218.91
    apples_agent-3_min: 172
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 383.84
    apples_agent-5_min: 291
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 401.37
    cleaning_beam_agent-0_min: 369
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.69
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 2.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 29.17
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 449.03
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 4.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-33-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1843.0
  episode_reward_mean: 1728.11
  episode_reward_min: 1345.0
  episodes_this_iter: 96
  episodes_total: 71424
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11628.26
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34965980052948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016108406707644463
        model: {}
        policy_loss: -0.001260894350707531
        total_loss: 0.0014437874779105186
        vf_explained_var: -0.0023659616708755493
        vf_loss: 33.20086669921875
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22677835822105408
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009667993290349841
        model: {}
        policy_loss: -0.001679660752415657
        total_loss: 0.0009648660197854042
        vf_explained_var: 0.0783766657114029
        vf_loss: 30.436561584472656
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26673460006713867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011183508904650807
        model: {}
        policy_loss: -0.0015723668038845062
        total_loss: 0.005830650217831135
        vf_explained_var: 0.03501404821872711
        vf_loss: 78.7247314453125
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.514076292514801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012900601141154766
        model: {}
        policy_loss: -0.0017274769488722086
        total_loss: 0.0050726113840937614
        vf_explained_var: 0.057704851031303406
        vf_loss: 77.04865264892578
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9622352123260498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0043359617702662945
        model: {}
        policy_loss: -0.0023951861076056957
        total_loss: -0.0011758403852581978
        vf_explained_var: 0.012603402137756348
        vf_loss: 29.128793716430664
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2733968496322632
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012393498327583075
        model: {}
        policy_loss: -0.00190828088670969
        total_loss: 0.000403539277613163
        vf_explained_var: 0.0533953458070755
        vf_loss: 27.930004119873047
    load_time_ms: 14472.807
    num_steps_sampled: 71424000
    num_steps_trained: 71424000
    sample_time_ms: 101471.839
    update_time_ms: 13.844
  iterations_since_restore: 94
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.33169398907104
    ram_util_percent: 15.227322404371586
  pid: 28385
  policy_reward_max:
    agent-0: 263.0
    agent-1: 263.0
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 242.56
    agent-1: 242.56
    agent-2: 389.58
    agent-3: 389.58
    agent-4: 231.915
    agent-5: 231.915
  policy_reward_min:
    agent-0: 192.5
    agent-1: 192.5
    agent-2: 296.5
    agent-3: 296.5
    agent-4: 183.5
    agent-5: 183.5
  sampler_perf:
    mean_env_wait_ms: 26.695532408398545
    mean_inference_ms: 12.97608901936338
    mean_processing_ms: 59.723214960806956
  time_since_restore: 12032.031346082687
  time_this_iter_s: 128.3482894897461
  time_total_s: 94164.473716259
  timestamp: 1637605996
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 71424000
  training_iteration: 744
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    744 |          94164.5 | 71424000 |  1728.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.53
    apples_agent-1_min: 15
    apples_agent-2_max: 404
    apples_agent-2_mean: 359.75
    apples_agent-2_min: 270
    apples_agent-3_max: 304
    apples_agent-3_mean: 219.86
    apples_agent-3_min: 146
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 381.89
    apples_agent-5_min: 301
    cleaning_beam_agent-0_max: 439
    cleaning_beam_agent-0_mean: 394.49
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.1
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 3.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 28.95
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 453.56
    cleaning_beam_agent-4_min: 388
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 4.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-35-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1892.0
  episode_reward_mean: 1728.91
  episode_reward_min: 1329.0
  episodes_this_iter: 96
  episodes_total: 71520
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11630.608
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3489779233932495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013506277464330196
        model: {}
        policy_loss: -0.0013260040432214737
        total_loss: 0.0014184497995302081
        vf_explained_var: 0.01158083975315094
        vf_loss: 33.5865592956543
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23021693527698517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008747761603444815
        model: {}
        policy_loss: -0.001774939359165728
        total_loss: 0.0008755616145208478
        vf_explained_var: 0.1009531170129776
        vf_loss: 30.55682945251465
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2691189646720886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011437175562605262
        model: {}
        policy_loss: -0.0018068128265440464
        total_loss: 0.006190474145114422
        vf_explained_var: 0.05767133831977844
        vf_loss: 84.70938873291016
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5188015699386597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007592620677314699
        model: {}
        policy_loss: -0.0014097366947680712
        total_loss: 0.0060926033183932304
        vf_explained_var: 0.062028899788856506
        vf_loss: 84.15432739257812
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9573571085929871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014772008871659636
        model: {}
        policy_loss: -0.0016500204801559448
        total_loss: -0.0003817584365606308
        vf_explained_var: 0.03058798611164093
        vf_loss: 29.532085418701172
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27226996421813965
        entropy_coeff: 0.0017600000137463212
        kl: 0.001125394948758185
        model: {}
        policy_loss: -0.001907477155327797
        total_loss: 0.0004535866901278496
        vf_explained_var: 0.06882995367050171
        vf_loss: 28.40262222290039
    load_time_ms: 14491.665
    num_steps_sampled: 71520000
    num_steps_trained: 71520000
    sample_time_ms: 101563.872
    update_time_ms: 13.859
  iterations_since_restore: 95
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.32692307692308
    ram_util_percent: 15.153846153846153
  pid: 28385
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 434.0
    agent-3: 434.0
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 242.035
    agent-1: 242.035
    agent-2: 392.22
    agent-3: 392.22
    agent-4: 230.2
    agent-5: 230.2
  policy_reward_min:
    agent-0: 185.5
    agent-1: 185.5
    agent-2: 299.5
    agent-3: 299.5
    agent-4: 179.5
    agent-5: 179.5
  sampler_perf:
    mean_env_wait_ms: 26.693918886594794
    mean_inference_ms: 12.97530063362454
    mean_processing_ms: 59.71996166785308
  time_since_restore: 12159.731073856354
  time_this_iter_s: 127.69972777366638
  time_total_s: 94292.17344403267
  timestamp: 1637606123
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 71520000
  training_iteration: 745
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    745 |          94292.2 | 71520000 |  1728.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 31.77
    apples_agent-1_min: 17
    apples_agent-2_max: 418
    apples_agent-2_mean: 361.2
    apples_agent-2_min: 285
    apples_agent-3_max: 276
    apples_agent-3_mean: 222.47
    apples_agent-3_min: 175
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 389.16
    apples_agent-5_min: 325
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 400.27
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.38
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.67
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 30.06
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 452.25
    cleaning_beam_agent-4_min: 389
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-37-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1889.0
  episode_reward_mean: 1743.9
  episode_reward_min: 1447.0
  episodes_this_iter: 96
  episodes_total: 71616
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11631.399
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3470495939254761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014705162029713392
        model: {}
        policy_loss: -0.0014855796471238136
        total_loss: 0.0013418160378932953
        vf_explained_var: 0.010042265057563782
        vf_loss: 34.38203811645508
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23010794818401337
        entropy_coeff: 0.0017600000137463212
        kl: 0.000513230450451374
        model: {}
        policy_loss: -0.0015482804737985134
        total_loss: 0.0010957405902445316
        vf_explained_var: 0.1214858740568161
        vf_loss: 30.490129470825195
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26464158296585083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009403460426256061
        model: {}
        policy_loss: -0.0015801466070115566
        total_loss: 0.0060136523097753525
        vf_explained_var: 0.04651197791099548
        vf_loss: 80.59569549560547
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5123758316040039
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011240402236580849
        model: {}
        policy_loss: -0.0015170173719525337
        total_loss: 0.005533352494239807
        vf_explained_var: 0.05814257264137268
        vf_loss: 79.52153015136719
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9539865255355835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020697552245110273
        model: {}
        policy_loss: -0.0018604027573019266
        total_loss: -0.00053413026034832
        vf_explained_var: 0.01886637508869171
        vf_loss: 30.05289649963379
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2612627148628235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007810360984876752
        model: {}
        policy_loss: -0.0017340825870633125
        total_loss: 0.0005720686167478561
        vf_explained_var: 0.09406715631484985
        vf_loss: 27.659719467163086
    load_time_ms: 14499.603
    num_steps_sampled: 71616000
    num_steps_trained: 71616000
    sample_time_ms: 101541.027
    update_time_ms: 13.904
  iterations_since_restore: 96
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.076373626373623
    ram_util_percent: 15.190109890109893
  pid: 28385
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 437.0
    agent-3: 437.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 243.475
    agent-1: 243.475
    agent-2: 393.79
    agent-3: 393.79
    agent-4: 234.685
    agent-5: 234.685
  policy_reward_min:
    agent-0: 122.0
    agent-1: 122.0
    agent-2: 329.0
    agent-3: 329.0
    agent-4: 185.0
    agent-5: 185.0
  sampler_perf:
    mean_env_wait_ms: 26.691254343680548
    mean_inference_ms: 12.975041421320954
    mean_processing_ms: 59.714725751607986
  time_since_restore: 12287.72726893425
  time_this_iter_s: 127.99619507789612
  time_total_s: 94420.16963911057
  timestamp: 1637606252
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 71616000
  training_iteration: 746
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    746 |          94420.2 | 71616000 |   1743.9 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 32.3
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 361.62
    apples_agent-2_min: 281
    apples_agent-3_max: 275
    apples_agent-3_mean: 226.47
    apples_agent-3_min: 156
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 388.73
    apples_agent-5_min: 276
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 395.29
    cleaning_beam_agent-0_min: 327
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.41
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 30.6
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 442.58
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-39-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1848.0
  episode_reward_mean: 1744.35
  episode_reward_min: 1241.0
  episodes_this_iter: 96
  episodes_total: 71712
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11633.361
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3437708020210266
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015555073041468859
        model: {}
        policy_loss: -0.0013528913259506226
        total_loss: 0.0013864059001207352
        vf_explained_var: 0.00397944450378418
        vf_loss: 33.44335174560547
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22382190823554993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006562104681506753
        model: {}
        policy_loss: -0.0016172099858522415
        total_loss: 0.0010010208934545517
        vf_explained_var: 0.10319766402244568
        vf_loss: 30.121597290039062
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2603161036968231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011787456460297108
        model: {}
        policy_loss: -0.0017528412863612175
        total_loss: 0.006117446348071098
        vf_explained_var: 0.04023882746696472
        vf_loss: 83.284423828125
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5120824575424194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006487066857516766
        model: {}
        policy_loss: -0.0012087700888514519
        total_loss: 0.006138589233160019
        vf_explained_var: 0.05035872757434845
        vf_loss: 82.48625183105469
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9549663066864014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012387868482619524
        model: {}
        policy_loss: -0.001580393873155117
        total_loss: -7.632048800587654e-05
        vf_explained_var: 0.004432663321495056
        vf_loss: 31.84816551208496
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25632402300834656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006760405958630145
        model: {}
        policy_loss: -0.001806015381589532
        total_loss: 0.0005752840079367161
        vf_explained_var: 0.11354708671569824
        vf_loss: 28.324295043945312
    load_time_ms: 14473.834
    num_steps_sampled: 71712000
    num_steps_trained: 71712000
    sample_time_ms: 101585.101
    update_time_ms: 13.879
  iterations_since_restore: 97
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.58736263736264
    ram_util_percent: 15.16428571428572
  pid: 28385
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 430.0
    agent-3: 430.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 244.385
    agent-1: 244.385
    agent-2: 393.315
    agent-3: 393.315
    agent-4: 234.475
    agent-5: 234.475
  policy_reward_min:
    agent-0: 181.0
    agent-1: 181.0
    agent-2: 276.5
    agent-3: 276.5
    agent-4: 163.0
    agent-5: 163.0
  sampler_perf:
    mean_env_wait_ms: 26.689893601800964
    mean_inference_ms: 12.97574293983646
    mean_processing_ms: 59.7185059514747
  time_since_restore: 12415.349413394928
  time_this_iter_s: 127.6221444606781
  time_total_s: 94547.79178357124
  timestamp: 1637606379
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 71712000
  training_iteration: 747
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    747 |          94547.8 | 71712000 |  1744.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 31.05
    apples_agent-1_min: 10
    apples_agent-2_max: 407
    apples_agent-2_mean: 358.35
    apples_agent-2_min: 228
    apples_agent-3_max: 273
    apples_agent-3_mean: 220.39
    apples_agent-3_min: 104
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 381.99
    apples_agent-5_min: 255
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 398.8
    cleaning_beam_agent-0_min: 341
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 2.13
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 2.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 26.06
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 435.95
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-41-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1847.0
  episode_reward_mean: 1731.37
  episode_reward_min: 1041.0
  episodes_this_iter: 96
  episodes_total: 71808
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11622.554
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.350879967212677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011866209097206593
        model: {}
        policy_loss: -0.001153741031885147
        total_loss: 0.001539853634312749
        vf_explained_var: 0.024556100368499756
        vf_loss: 33.11143493652344
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2277819812297821
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008247424848377705
        model: {}
        policy_loss: -0.001957955304533243
        total_loss: 0.0006262356182560325
        vf_explained_var: 0.12172235548496246
        vf_loss: 29.850858688354492
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2616208791732788
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007919382769614458
        model: {}
        policy_loss: -0.00155133125372231
        total_loss: 0.006416411604732275
        vf_explained_var: 0.04541155695915222
        vf_loss: 84.28195190429688
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5082252025604248
        entropy_coeff: 0.0017600000137463212
        kl: 0.000798296881839633
        model: {}
        policy_loss: -0.0016810453962534666
        total_loss: 0.005800086539238691
        vf_explained_var: 0.051789745688438416
        vf_loss: 83.75608825683594
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9570341110229492
        entropy_coeff: 0.0017600000137463212
        kl: 0.002008747076615691
        model: {}
        policy_loss: -0.002305814530700445
        total_loss: -0.0009683526586741209
        vf_explained_var: 0.018313735723495483
        vf_loss: 30.218416213989258
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26612234115600586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008567105396650732
        model: {}
        policy_loss: -0.0016308766789734364
        total_loss: 0.0007124324329197407
        vf_explained_var: 0.08838953077793121
        vf_loss: 28.11684799194336
    load_time_ms: 14455.321
    num_steps_sampled: 71808000
    num_steps_trained: 71808000
    sample_time_ms: 101440.77
    update_time_ms: 14.053
  iterations_since_restore: 98
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.80441988950276
    ram_util_percent: 15.190055248618787
  pid: 28385
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 426.5
    agent-3: 426.5
    agent-4: 251.0
    agent-5: 251.0
  policy_reward_mean:
    agent-0: 242.96
    agent-1: 242.96
    agent-2: 392.24
    agent-3: 392.24
    agent-4: 230.485
    agent-5: 230.485
  policy_reward_min:
    agent-0: 142.0
    agent-1: 142.0
    agent-2: 233.0
    agent-3: 233.0
    agent-4: 145.5
    agent-5: 145.5
  sampler_perf:
    mean_env_wait_ms: 26.68807225280393
    mean_inference_ms: 12.975843705485545
    mean_processing_ms: 59.71781478805338
  time_since_restore: 12542.391700744629
  time_this_iter_s: 127.04228734970093
  time_total_s: 94674.83407092094
  timestamp: 1637606506
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 71808000
  training_iteration: 748
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    748 |          94674.8 | 71808000 |  1731.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.89
    apples_agent-1_min: 19
    apples_agent-2_max: 425
    apples_agent-2_mean: 359.13
    apples_agent-2_min: 209
    apples_agent-3_max: 278
    apples_agent-3_mean: 221.65
    apples_agent-3_min: 150
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 386.2
    apples_agent-5_min: 222
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 395.83
    cleaning_beam_agent-0_min: 343
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 26.33
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 437.7
    cleaning_beam_agent-4_min: 376
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 3.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-43-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1884.0
  episode_reward_mean: 1736.62
  episode_reward_min: 1034.0
  episodes_this_iter: 96
  episodes_total: 71904
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11618.589
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3589334487915039
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011694971472024918
        model: {}
        policy_loss: -0.0014004725962877274
        total_loss: 0.001236283453181386
        vf_explained_var: 0.038842007517814636
        vf_loss: 32.68479919433594
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22312897443771362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010116774355992675
        model: {}
        policy_loss: -0.001623725751414895
        total_loss: 0.0008526425808668137
        vf_explained_var: 0.1557905673980713
        vf_loss: 28.690773010253906
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2631840705871582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006378520047292113
        model: {}
        policy_loss: -0.0017828668933361769
        total_loss: 0.006089852191507816
        vf_explained_var: 0.06102097034454346
        vf_loss: 83.35922241210938
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5052585601806641
        entropy_coeff: 0.0017600000137463212
        kl: 0.001074358238838613
        model: {}
        policy_loss: -0.0017201360315084457
        total_loss: 0.006071279291063547
        vf_explained_var: 0.022066280245780945
        vf_loss: 86.80671691894531
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9455803632736206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029002022929489613
        model: {}
        policy_loss: -0.0020698390435427427
        total_loss: -0.0005164272151887417
        vf_explained_var: 0.0015074312686920166
        vf_loss: 32.176353454589844
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2616560161113739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007470331038348377
        model: {}
        policy_loss: -0.0019888575188815594
        total_loss: 0.0004685570893343538
        vf_explained_var: 0.09360592067241669
        vf_loss: 29.179340362548828
    load_time_ms: 14443.794
    num_steps_sampled: 71904000
    num_steps_trained: 71904000
    sample_time_ms: 101511.196
    update_time_ms: 14.247
  iterations_since_restore: 99
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.031868131868134
    ram_util_percent: 15.171978021978022
  pid: 28385
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 434.0
    agent-3: 434.0
    agent-4: 268.0
    agent-5: 268.0
  policy_reward_mean:
    agent-0: 243.3
    agent-1: 243.3
    agent-2: 391.915
    agent-3: 391.915
    agent-4: 233.095
    agent-5: 233.095
  policy_reward_min:
    agent-0: 145.0
    agent-1: 145.0
    agent-2: 236.5
    agent-3: 236.5
    agent-4: 135.5
    agent-5: 135.5
  sampler_perf:
    mean_env_wait_ms: 26.68594697812262
    mean_inference_ms: 12.975528679835252
    mean_processing_ms: 59.716957308666714
  time_since_restore: 12670.148998737335
  time_this_iter_s: 127.7572979927063
  time_total_s: 94802.59136891365
  timestamp: 1637606634
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 71904000
  training_iteration: 749
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    749 |          94802.6 | 71904000 |  1736.62 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 31.7
    apples_agent-1_min: 19
    apples_agent-2_max: 407
    apples_agent-2_mean: 363.36
    apples_agent-2_min: 312
    apples_agent-3_max: 278
    apples_agent-3_mean: 223.03
    apples_agent-3_min: 142
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 386.16
    apples_agent-5_min: 330
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 403.24
    cleaning_beam_agent-0_min: 346
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 22.83
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 448.54
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-46-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1861.0
  episode_reward_mean: 1753.71
  episode_reward_min: 1611.0
  episodes_this_iter: 96
  episodes_total: 72000
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11613.127
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3553352952003479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011298730969429016
        model: {}
        policy_loss: -0.0010424049105495214
        total_loss: 0.0014716375153511763
        vf_explained_var: -0.008756190538406372
        vf_loss: 31.394332885742188
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.220224529504776
        entropy_coeff: 0.0017600000137463212
        kl: 0.001011870801448822
        model: {}
        policy_loss: -0.001971667632460594
        total_loss: 0.0004703979939222336
        vf_explained_var: 0.09560035169124603
        vf_loss: 28.29659652709961
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25582802295684814
        entropy_coeff: 0.0017600000137463212
        kl: 0.00105810328386724
        model: {}
        policy_loss: -0.0014849089784547687
        total_loss: 0.006530816666781902
        vf_explained_var: 0.01720002293586731
        vf_loss: 84.65980529785156
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.506493091583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013079075142741203
        model: {}
        policy_loss: -0.0013413762208074331
        total_loss: 0.006120467092841864
        vf_explained_var: 0.03346097469329834
        vf_loss: 83.53272247314453
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9480124115943909
        entropy_coeff: 0.0017600000137463212
        kl: 0.003023935714736581
        model: {}
        policy_loss: -0.0018626294331625104
        total_loss: -0.0005762309883721173
        vf_explained_var: -0.001533016562461853
        vf_loss: 29.54902458190918
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25751516222953796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009169012773782015
        model: {}
        policy_loss: -0.0016720937564969063
        total_loss: 0.0006498242728412151
        vf_explained_var: 0.05851978063583374
        vf_loss: 27.751441955566406
    load_time_ms: 14417.795
    num_steps_sampled: 72000000
    num_steps_trained: 72000000
    sample_time_ms: 101547.749
    update_time_ms: 14.254
  iterations_since_restore: 100
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.126373626373628
    ram_util_percent: 15.207692307692309
  pid: 28385
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 428.5
    agent-3: 428.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 247.205
    agent-1: 247.205
    agent-2: 395.045
    agent-3: 395.045
    agent-4: 234.605
    agent-5: 234.605
  policy_reward_min:
    agent-0: 211.0
    agent-1: 211.0
    agent-2: 344.0
    agent-3: 344.0
    agent-4: 210.5
    agent-5: 210.5
  sampler_perf:
    mean_env_wait_ms: 26.684615464547214
    mean_inference_ms: 12.975171696840743
    mean_processing_ms: 59.717234130414546
  time_since_restore: 12797.62409567833
  time_this_iter_s: 127.47509694099426
  time_total_s: 94930.06646585464
  timestamp: 1637606762
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 72000000
  training_iteration: 750
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    750 |          94930.1 | 72000000 |  1753.71 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 31.93
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 362.77
    apples_agent-2_min: 258
    apples_agent-3_max: 278
    apples_agent-3_mean: 224.33
    apples_agent-3_min: 160
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 384.2
    apples_agent-5_min: 269
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 399.6
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 2.09
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 23.93
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 443.23
    cleaning_beam_agent-4_min: 393
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-48-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1853.0
  episode_reward_mean: 1742.26
  episode_reward_min: 1264.0
  episodes_this_iter: 96
  episodes_total: 72096
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11613.52
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3601941466331482
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009486551862210035
        model: {}
        policy_loss: -0.0013412149855867028
        total_loss: 0.0013780522858723998
        vf_explained_var: 0.003330036997795105
        vf_loss: 33.5321044921875
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22008153796195984
        entropy_coeff: 0.0017600000137463212
        kl: 0.000821668014395982
        model: {}
        policy_loss: -0.0016824596095830202
        total_loss: 0.0008715420844964683
        vf_explained_var: 0.12730076909065247
        vf_loss: 29.41342544555664
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2543070912361145
        entropy_coeff: 0.0017600000137463212
        kl: 0.001171210897155106
        model: {}
        policy_loss: -0.0018454198725521564
        total_loss: 0.005973279941827059
        vf_explained_var: 0.032247334718704224
        vf_loss: 82.6628189086914
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5069832801818848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008267838275060058
        model: {}
        policy_loss: -0.0017744549550116062
        total_loss: 0.005421008914709091
        vf_explained_var: 0.05718161165714264
        vf_loss: 80.87754821777344
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9621978998184204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021282490342855453
        model: {}
        policy_loss: -0.0018294351175427437
        total_loss: -0.0005135419778525829
        vf_explained_var: 0.008405670523643494
        vf_loss: 30.093610763549805
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26108217239379883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006890164222568274
        model: {}
        policy_loss: -0.0018102601170539856
        total_loss: 0.0005632108077406883
        vf_explained_var: 0.06480167806148529
        vf_loss: 28.329761505126953
    load_time_ms: 14451.013
    num_steps_sampled: 72096000
    num_steps_trained: 72096000
    sample_time_ms: 101536.986
    update_time_ms: 14.242
  iterations_since_restore: 101
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.747826086956525
    ram_util_percent: 15.115217391304347
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 243.1
    agent-1: 243.1
    agent-2: 395.005
    agent-3: 395.005
    agent-4: 233.025
    agent-5: 233.025
  policy_reward_min:
    agent-0: 172.0
    agent-1: 172.0
    agent-2: 292.5
    agent-3: 292.5
    agent-4: 167.5
    agent-5: 167.5
  sampler_perf:
    mean_env_wait_ms: 26.68306310002931
    mean_inference_ms: 12.975022554943866
    mean_processing_ms: 59.715621126707546
  time_since_restore: 12925.593796014786
  time_this_iter_s: 127.9697003364563
  time_total_s: 95058.0361661911
  timestamp: 1637606891
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 72096000
  training_iteration: 751
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    751 |            95058 | 72096000 |  1742.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.13
    apples_agent-1_min: 19
    apples_agent-2_max: 419
    apples_agent-2_mean: 359.22
    apples_agent-2_min: 258
    apples_agent-3_max: 293
    apples_agent-3_mean: 218.18
    apples_agent-3_min: 160
    apples_agent-4_max: 22
    apples_agent-4_mean: 0.37
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 377.99
    apples_agent-5_min: 269
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 402.75
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 25.26
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 422.82
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-50-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1898.0
  episode_reward_mean: 1732.47
  episode_reward_min: 1264.0
  episodes_this_iter: 96
  episodes_total: 72192
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11615.913
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3627675771713257
        entropy_coeff: 0.0017600000137463212
        kl: 0.001513265655376017
        model: {}
        policy_loss: -0.0014510552864521742
        total_loss: 0.0012404215522110462
        vf_explained_var: 0.007657751441001892
        vf_loss: 33.29948806762695
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2200585901737213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006910940282978117
        model: {}
        policy_loss: -0.0015343371778726578
        total_loss: 0.0011454436462372541
        vf_explained_var: 0.08712349832057953
        vf_loss: 30.670825958251953
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25911396741867065
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212569186463952
        model: {}
        policy_loss: -0.001704996800981462
        total_loss: 0.005945624317973852
        vf_explained_var: 0.03735789656639099
        vf_loss: 81.0666275024414
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5233914256095886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014782990328967571
        model: {}
        policy_loss: -0.0015992550179362297
        total_loss: 0.005490587092936039
        vf_explained_var: 0.052980437874794006
        vf_loss: 80.11013793945312
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9647971391677856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015996948350220919
        model: {}
        policy_loss: -0.0019032973796129227
        total_loss: -0.0006553966086357832
        vf_explained_var: 0.009049087762832642
        vf_loss: 29.459457397460938
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2687283158302307
        entropy_coeff: 0.0017600000137463212
        kl: 0.000865190289914608
        model: {}
        policy_loss: -0.0017077764496207237
        total_loss: 0.0006086481735110283
        vf_explained_var: 0.06384195387363434
        vf_loss: 27.893884658813477
    load_time_ms: 14461.328
    num_steps_sampled: 72192000
    num_steps_trained: 72192000
    sample_time_ms: 101502.834
    update_time_ms: 14.392
  iterations_since_restore: 102
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.352197802197804
    ram_util_percent: 15.246153846153849
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 252.5
    agent-5: 252.5
  policy_reward_mean:
    agent-0: 244.31
    agent-1: 244.31
    agent-2: 392.395
    agent-3: 392.395
    agent-4: 229.53
    agent-5: 229.53
  policy_reward_min:
    agent-0: 172.0
    agent-1: 172.0
    agent-2: 292.5
    agent-3: 292.5
    agent-4: 167.5
    agent-5: 167.5
  sampler_perf:
    mean_env_wait_ms: 26.68078879554349
    mean_inference_ms: 12.974813686092343
    mean_processing_ms: 59.71663780490917
  time_since_restore: 13052.868789196014
  time_this_iter_s: 127.27499318122864
  time_total_s: 95185.31115937233
  timestamp: 1637607019
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 72192000
  training_iteration: 752
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    752 |          95185.3 | 72192000 |  1732.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 0.12
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.81
    apples_agent-1_min: 11
    apples_agent-2_max: 417
    apples_agent-2_mean: 353.3
    apples_agent-2_min: 126
    apples_agent-3_max: 267
    apples_agent-3_mean: 216.02
    apples_agent-3_min: 62
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.13
    apples_agent-4_min: 0
    apples_agent-5_max: 430
    apples_agent-5_mean: 376.71
    apples_agent-5_min: 125
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 394.15
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 2.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 24.95
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 516
    cleaning_beam_agent-4_mean: 414.29
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 4.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-52-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1855.0
  episode_reward_mean: 1703.1
  episode_reward_min: 592.0
  episodes_this_iter: 96
  episodes_total: 72288
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.757
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36975187063217163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011182223679497838
        model: {}
        policy_loss: -0.0012960804160684347
        total_loss: 0.001432341756299138
        vf_explained_var: 0.09290885925292969
        vf_loss: 33.79187774658203
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.229137122631073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010381004540249705
        model: {}
        policy_loss: -0.0020445603877305984
        total_loss: 0.0006643109954893589
        vf_explained_var: 0.16422003507614136
        vf_loss: 31.12154197692871
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25731462240219116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006854920648038387
        model: {}
        policy_loss: -0.0019288500770926476
        total_loss: 0.005972733721137047
        vf_explained_var: 0.10095636546611786
        vf_loss: 83.54458618164062
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5340856909751892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011041336692869663
        model: {}
        policy_loss: -0.0017117448151111603
        total_loss: 0.006339649204164743
        vf_explained_var: 0.03200992941856384
        vf_loss: 89.91387176513672
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9695319533348083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010958106722682714
        model: {}
        policy_loss: -0.0017387859988957644
        total_loss: -0.00012990552932024002
        vf_explained_var: 0.010317489504814148
        vf_loss: 33.152587890625
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27720069885253906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006690651061944664
        model: {}
        policy_loss: -0.0017948218155652285
        total_loss: 0.0007870018016546965
        vf_explained_var: 0.08419531583786011
        vf_loss: 30.696937561035156
    load_time_ms: 14463.19
    num_steps_sampled: 72288000
    num_steps_trained: 72288000
    sample_time_ms: 101482.327
    update_time_ms: 14.401
  iterations_since_restore: 103
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.19060773480663
    ram_util_percent: 15.1060773480663
  pid: 28385
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 256.5
    agent-5: 256.5
  policy_reward_mean:
    agent-0: 238.05
    agent-1: 238.05
    agent-2: 385.385
    agent-3: 385.385
    agent-4: 228.115
    agent-5: 228.115
  policy_reward_min:
    agent-0: 85.0
    agent-1: 85.0
    agent-2: 129.0
    agent-3: 129.0
    agent-4: 82.0
    agent-5: 82.0
  sampler_perf:
    mean_env_wait_ms: 26.677499485755515
    mean_inference_ms: 12.974529479858317
    mean_processing_ms: 59.718974098396465
  time_since_restore: 13180.374956607819
  time_this_iter_s: 127.5061674118042
  time_total_s: 95312.81732678413
  timestamp: 1637607147
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 72288000
  training_iteration: 753
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    753 |          95312.8 | 72288000 |   1703.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.15
    apples_agent-1_min: 4
    apples_agent-2_max: 408
    apples_agent-2_mean: 353.95
    apples_agent-2_min: 22
    apples_agent-3_max: 282
    apples_agent-3_mean: 216.56
    apples_agent-3_min: 19
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 376.77
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 392.36
    cleaning_beam_agent-0_min: 296
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.34
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 24.05
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 414.4
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 4.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-54-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1844.0
  episode_reward_mean: 1697.4
  episode_reward_min: 137.0
  episodes_this_iter: 96
  episodes_total: 72384
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11618.426
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36559274792671204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012922619935125113
        model: {}
        policy_loss: -0.0015608025714755058
        total_loss: 0.001134264748543501
        vf_explained_var: 0.08953210711479187
        vf_loss: 33.38509750366211
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22596105933189392
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009115372085943818
        model: {}
        policy_loss: -0.0020052825566381216
        total_loss: 0.0006372131174430251
        vf_explained_var: 0.16893357038497925
        vf_loss: 30.401885986328125
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2607133984565735
        entropy_coeff: 0.0017600000137463212
        kl: 0.001061593764461577
        model: {}
        policy_loss: -0.0016428874805569649
        total_loss: 0.006598854437470436
        vf_explained_var: 0.11150319874286652
        vf_loss: 87.00598907470703
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5190348625183105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007134912302717566
        model: {}
        policy_loss: -0.0016214579809457064
        total_loss: 0.006850140169262886
        vf_explained_var: 0.04167069494724274
        vf_loss: 93.85102081298828
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9592601656913757
        entropy_coeff: 0.0017600000137463212
        kl: 0.001655049156397581
        model: {}
        policy_loss: -0.002002655528485775
        total_loss: -0.0002770600840449333
        vf_explained_var: 0.01588861644268036
        vf_loss: 34.13892364501953
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27699247002601624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007704978343099356
        model: {}
        policy_loss: -0.0021443949081003666
        total_loss: 0.0005420495290309191
        vf_explained_var: 0.08641019463539124
        vf_loss: 31.739501953125
    load_time_ms: 14460.975
    num_steps_sampled: 72384000
    num_steps_trained: 72384000
    sample_time_ms: 101425.016
    update_time_ms: 14.418
  iterations_since_restore: 104
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.200000000000003
    ram_util_percent: 15.232417582417586
  pid: 28385
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 432.0
    agent-3: 432.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 236.415
    agent-1: 236.415
    agent-2: 384.625
    agent-3: 384.625
    agent-4: 227.66
    agent-5: 227.66
  policy_reward_min:
    agent-0: 19.5
    agent-1: 19.5
    agent-2: 31.0
    agent-3: 31.0
    agent-4: 18.0
    agent-5: 18.0
  sampler_perf:
    mean_env_wait_ms: 26.6740691711181
    mean_inference_ms: 12.974928153632932
    mean_processing_ms: 59.72071141220247
  time_since_restore: 13308.15025305748
  time_this_iter_s: 127.77529644966125
  time_total_s: 95440.5926232338
  timestamp: 1637607274
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 72384000
  training_iteration: 754
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    754 |          95440.6 | 72384000 |   1697.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 31.65
    apples_agent-1_min: 17
    apples_agent-2_max: 404
    apples_agent-2_mean: 357.53
    apples_agent-2_min: 275
    apples_agent-3_max: 277
    apples_agent-3_mean: 218.7
    apples_agent-3_min: 150
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 382.05
    apples_agent-5_min: 257
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 392.14
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.62
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 2.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 24.24
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 531
    cleaning_beam_agent-4_mean: 420.05
    cleaning_beam_agent-4_min: 327
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-56-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1856.0
  episode_reward_mean: 1730.04
  episode_reward_min: 1292.0
  episodes_this_iter: 96
  episodes_total: 72480
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11621.825
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3549075126647949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021547600626945496
        model: {}
        policy_loss: -0.0016043721698224545
        total_loss: 0.001046232646331191
        vf_explained_var: 0.02111978828907013
        vf_loss: 32.752418518066406
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2180883288383484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007103591342456639
        model: {}
        policy_loss: -0.001535286195576191
        total_loss: 0.0010023429058492184
        vf_explained_var: 0.131687730550766
        vf_loss: 29.21463966369629
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25808441638946533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007411757251247764
        model: {}
        policy_loss: -0.0017337938770651817
        total_loss: 0.005741246975958347
        vf_explained_var: 0.04953595995903015
        vf_loss: 79.29268646240234
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5127517580986023
        entropy_coeff: 0.0017600000137463212
        kl: 0.001529750763438642
        model: {}
        policy_loss: -0.0018048561178147793
        total_loss: 0.005184306297451258
        vf_explained_var: 0.05472695827484131
        vf_loss: 78.91605377197266
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9698734283447266
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013912385329604149
        model: {}
        policy_loss: -0.0017814645543694496
        total_loss: -0.0004240567795932293
        vf_explained_var: 0.002524837851524353
        vf_loss: 30.64382553100586
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26078343391418457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011281867045909166
        model: {}
        policy_loss: -0.0019500893540680408
        total_loss: 0.0004426704254001379
        vf_explained_var: 0.0697271078824997
        vf_loss: 28.51739501953125
    load_time_ms: 14464.257
    num_steps_sampled: 72480000
    num_steps_trained: 72480000
    sample_time_ms: 101436.841
    update_time_ms: 14.401
  iterations_since_restore: 105
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.468131868131866
    ram_util_percent: 15.12967032967033
  pid: 28385
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 428.5
    agent-3: 428.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 243.77
    agent-1: 243.77
    agent-2: 388.795
    agent-3: 388.795
    agent-4: 232.455
    agent-5: 232.455
  policy_reward_min:
    agent-0: 184.0
    agent-1: 184.0
    agent-2: 300.5
    agent-3: 300.5
    agent-4: 161.5
    agent-5: 161.5
  sampler_perf:
    mean_env_wait_ms: 26.672279611505232
    mean_inference_ms: 12.97486778603748
    mean_processing_ms: 59.72340343601384
  time_since_restore: 13436.02665424347
  time_this_iter_s: 127.87640118598938
  time_total_s: 95568.46902441978
  timestamp: 1637607402
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 72480000
  training_iteration: 755
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    755 |          95568.5 | 72480000 |  1730.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 32.11
    apples_agent-1_min: 15
    apples_agent-2_max: 415
    apples_agent-2_mean: 357.33
    apples_agent-2_min: 201
    apples_agent-3_max: 267
    apples_agent-3_mean: 219.51
    apples_agent-3_min: 110
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 379.94
    apples_agent-5_min: 216
    cleaning_beam_agent-0_max: 420
    cleaning_beam_agent-0_mean: 388.5
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.69
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 25.12
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 413.33
    cleaning_beam_agent-4_min: 342
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_13-58-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1889.0
  episode_reward_mean: 1717.21
  episode_reward_min: 871.0
  episodes_this_iter: 96
  episodes_total: 72576
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11621.952
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3574827313423157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016361626330763102
        model: {}
        policy_loss: -0.0015805572038516402
        total_loss: 0.0011875149793922901
        vf_explained_var: 0.053199201822280884
        vf_loss: 33.97242736816406
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2243344485759735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006270213052630424
        model: {}
        policy_loss: -0.001821149606257677
        total_loss: 0.0008829119615256786
        vf_explained_var: 0.13791272044181824
        vf_loss: 30.98891830444336
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26223573088645935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009633653098717332
        model: {}
        policy_loss: -0.0018929790239781141
        total_loss: 0.006173718720674515
        vf_explained_var: 0.07097823917865753
        vf_loss: 85.28230285644531
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5176619291305542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017538727261126041
        model: {}
        policy_loss: -0.001752374111674726
        total_loss: 0.006088136695325375
        vf_explained_var: 0.04639224708080292
        vf_loss: 87.51597595214844
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9560914039611816
        entropy_coeff: 0.0017600000137463212
        kl: 0.00182380061596632
        model: {}
        policy_loss: -0.0020870519801974297
        total_loss: -0.0006481753662228584
        vf_explained_var: 0.03209172189235687
        vf_loss: 31.216007232666016
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2633070945739746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005668411031365395
        model: {}
        policy_loss: -0.0017565216403454542
        total_loss: 0.0005973041988909245
        vf_explained_var: 0.1270410120487213
        vf_loss: 28.172454833984375
    load_time_ms: 14441.082
    num_steps_sampled: 72576000
    num_steps_trained: 72576000
    sample_time_ms: 101404.923
    update_time_ms: 14.393
  iterations_since_restore: 106
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.888461538461538
    ram_util_percent: 15.236813186813192
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 437.5
    agent-3: 437.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 240.77
    agent-1: 240.77
    agent-2: 387.15
    agent-3: 387.15
    agent-4: 230.685
    agent-5: 230.685
  policy_reward_min:
    agent-0: 116.5
    agent-1: 116.5
    agent-2: 192.5
    agent-3: 192.5
    agent-4: 126.5
    agent-5: 126.5
  sampler_perf:
    mean_env_wait_ms: 26.667467055928615
    mean_inference_ms: 12.974141203978402
    mean_processing_ms: 59.72375708830228
  time_since_restore: 13563.46245098114
  time_this_iter_s: 127.4357967376709
  time_total_s: 95695.90482115746
  timestamp: 1637607530
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 72576000
  training_iteration: 756
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    756 |          95695.9 | 72576000 |  1717.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 30.72
    apples_agent-1_min: 15
    apples_agent-2_max: 410
    apples_agent-2_mean: 360.68
    apples_agent-2_min: 311
    apples_agent-3_max: 273
    apples_agent-3_mean: 217.05
    apples_agent-3_min: 155
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 384.45
    apples_agent-5_min: 337
    cleaning_beam_agent-0_max: 439
    cleaning_beam_agent-0_mean: 391.75
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.74
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 23.96
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 421.12
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-00-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1836.0
  episode_reward_mean: 1737.7
  episode_reward_min: 1593.0
  episodes_this_iter: 96
  episodes_total: 72672
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.354
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35422760248184204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011554830707609653
        model: {}
        policy_loss: -0.001150685828179121
        total_loss: 0.001572678331285715
        vf_explained_var: 0.007616013288497925
        vf_loss: 33.4680290222168
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21872399747371674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008724036160856485
        model: {}
        policy_loss: -0.0015762043185532093
        total_loss: 0.00101666827686131
        vf_explained_var: 0.12060587108135223
        vf_loss: 29.778274536132812
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2595972418785095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011773371370509267
        model: {}
        policy_loss: -0.0018376070074737072
        total_loss: 0.005781159736216068
        vf_explained_var: 0.02811475098133087
        vf_loss: 80.7566146850586
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5262281894683838
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012076878920197487
        model: {}
        policy_loss: -0.0015704412944614887
        total_loss: 0.005366164725273848
        vf_explained_var: 0.05452241003513336
        vf_loss: 78.627685546875
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9501510858535767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030852733179926872
        model: {}
        policy_loss: -0.0020853581372648478
        total_loss: -0.0008582032169215381
        vf_explained_var: 0.001478925347328186
        vf_loss: 28.99419593811035
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25254660844802856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012497615534812212
        model: {}
        policy_loss: -0.0017282862681895494
        total_loss: 0.0004985681734979153
        vf_explained_var: 0.07727725803852081
        vf_loss: 26.713361740112305
    load_time_ms: 14456.74
    num_steps_sampled: 72672000
    num_steps_trained: 72672000
    sample_time_ms: 101255.82
    update_time_ms: 14.385
  iterations_since_restore: 107
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.53611111111111
    ram_util_percent: 15.197777777777775
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 423.0
    agent-3: 423.0
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 244.195
    agent-1: 244.195
    agent-2: 390.995
    agent-3: 390.995
    agent-4: 233.66
    agent-5: 233.66
  policy_reward_min:
    agent-0: 211.0
    agent-1: 211.0
    agent-2: 348.5
    agent-3: 348.5
    agent-4: 203.5
    agent-5: 203.5
  sampler_perf:
    mean_env_wait_ms: 26.6634616755479
    mean_inference_ms: 12.97321242716525
    mean_processing_ms: 59.720609287849236
  time_since_restore: 13689.674775123596
  time_this_iter_s: 126.21232414245605
  time_total_s: 95822.11714529991
  timestamp: 1637607656
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 72672000
  training_iteration: 757
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    757 |          95822.1 | 72672000 |   1737.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 33.26
    apples_agent-1_min: 15
    apples_agent-2_max: 409
    apples_agent-2_mean: 362.04
    apples_agent-2_min: 272
    apples_agent-3_max: 264
    apples_agent-3_mean: 222.48
    apples_agent-3_min: 162
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 429
    apples_agent-5_mean: 382.88
    apples_agent-5_min: 324
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 401.44
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.67
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 2.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 23.16
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 421.76
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-03-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1857.0
  episode_reward_mean: 1742.11
  episode_reward_min: 1532.0
  episodes_this_iter: 96
  episodes_total: 72768
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11619.709
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3555716276168823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013219669926911592
        model: {}
        policy_loss: -0.0011367416009306908
        total_loss: 0.0014472967013716698
        vf_explained_var: -0.0015814751386642456
        vf_loss: 32.09842300415039
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21694248914718628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008614169200882316
        model: {}
        policy_loss: -0.0016642352566123009
        total_loss: 0.0007887948304414749
        vf_explained_var: 0.11575907468795776
        vf_loss: 28.348468780517578
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2609255909919739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014086337760090828
        model: {}
        policy_loss: -0.0017638810677453876
        total_loss: 0.006118856370449066
        vf_explained_var: 0.016231387853622437
        vf_loss: 83.41966247558594
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5074516534805298
        entropy_coeff: 0.0017600000137463212
        kl: 0.001037496142089367
        model: {}
        policy_loss: -0.0014927019365131855
        total_loss: 0.005642963573336601
        vf_explained_var: 0.056616172194480896
        vf_loss: 80.28778076171875
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9395102262496948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017636377597227693
        model: {}
        policy_loss: -0.002239725086838007
        total_loss: -0.0008617853745818138
        vf_explained_var: -0.001939520239830017
        vf_loss: 30.314735412597656
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24946671724319458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008673344855196774
        model: {}
        policy_loss: -0.0017489069141447544
        total_loss: 0.0006264960393309593
        vf_explained_var: 0.07061126828193665
        vf_loss: 28.14468002319336
    load_time_ms: 14474.788
    num_steps_sampled: 72768000
    num_steps_trained: 72768000
    sample_time_ms: 101267.849
    update_time_ms: 14.148
  iterations_since_restore: 108
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.123626373626372
    ram_util_percent: 15.14120879120879
  pid: 28385
  policy_reward_max:
    agent-0: 265.0
    agent-1: 265.0
    agent-2: 427.0
    agent-3: 427.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 245.01
    agent-1: 245.01
    agent-2: 394.16
    agent-3: 394.16
    agent-4: 231.885
    agent-5: 231.885
  policy_reward_min:
    agent-0: 213.5
    agent-1: 213.5
    agent-2: 341.0
    agent-3: 341.0
    agent-4: 163.5
    agent-5: 163.5
  sampler_perf:
    mean_env_wait_ms: 26.66062412327689
    mean_inference_ms: 12.972224592103055
    mean_processing_ms: 59.720401049061294
  time_since_restore: 13817.035875082016
  time_this_iter_s: 127.3610999584198
  time_total_s: 95949.47824525833
  timestamp: 1637607784
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 72768000
  training_iteration: 758
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    758 |          95949.5 | 72768000 |  1742.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 30.2
    apples_agent-1_min: 18
    apples_agent-2_max: 422
    apples_agent-2_mean: 359.75
    apples_agent-2_min: 283
    apples_agent-3_max: 286
    apples_agent-3_mean: 228.07
    apples_agent-3_min: 164
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 429
    apples_agent-5_mean: 384.14
    apples_agent-5_min: 334
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 406.56
    cleaning_beam_agent-0_min: 375
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 1.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 2.32
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 20.93
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 502
    cleaning_beam_agent-4_mean: 427.7
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-05-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1857.0
  episode_reward_mean: 1750.39
  episode_reward_min: 1570.0
  episodes_this_iter: 96
  episodes_total: 72864
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11621.092
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3539409935474396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012371696066111326
        model: {}
        policy_loss: -0.0010682145366445184
        total_loss: 0.0014859699876978993
        vf_explained_var: 0.022681713104248047
        vf_loss: 31.77120590209961
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21505585312843323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007464787340722978
        model: {}
        policy_loss: -0.0017795686144381762
        total_loss: 0.0007458565523847938
        vf_explained_var: 0.1122303158044815
        vf_loss: 29.03924560546875
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26349759101867676
        entropy_coeff: 0.0017600000137463212
        kl: 0.001134218880906701
        model: {}
        policy_loss: -0.001737968996167183
        total_loss: 0.006026378367096186
        vf_explained_var: 0.008838415145874023
        vf_loss: 82.281005859375
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5044794082641602
        entropy_coeff: 0.0017600000137463212
        kl: 0.001080766785889864
        model: {}
        policy_loss: -0.0016029092948883772
        total_loss: 0.005290365777909756
        vf_explained_var: 0.06395071744918823
        vf_loss: 77.81159210205078
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9412732124328613
        entropy_coeff: 0.0017600000137463212
        kl: 0.003331537824124098
        model: {}
        policy_loss: -0.0017943356651812792
        total_loss: -0.0004709875211119652
        vf_explained_var: -0.0017672181129455566
        vf_loss: 29.799930572509766
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2529515027999878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008945107110776007
        model: {}
        policy_loss: -0.001622842624783516
        total_loss: 0.0007038740441203117
        vf_explained_var: 0.06890441477298737
        vf_loss: 27.719125747680664
    load_time_ms: 14486.672
    num_steps_sampled: 72864000
    num_steps_trained: 72864000
    sample_time_ms: 101271.303
    update_time_ms: 14.005
  iterations_since_restore: 109
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.084615384615383
    ram_util_percent: 15.195604395604395
  pid: 28385
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 433.0
    agent-3: 433.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 247.37
    agent-1: 247.37
    agent-2: 393.55
    agent-3: 393.55
    agent-4: 234.275
    agent-5: 234.275
  policy_reward_min:
    agent-0: 212.5
    agent-1: 212.5
    agent-2: 336.5
    agent-3: 336.5
    agent-4: 210.5
    agent-5: 210.5
  sampler_perf:
    mean_env_wait_ms: 26.65758630637642
    mean_inference_ms: 12.971483685221104
    mean_processing_ms: 59.72008814507182
  time_since_restore: 13944.959348917007
  time_this_iter_s: 127.92347383499146
  time_total_s: 96077.40171909332
  timestamp: 1637607912
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 72864000
  training_iteration: 759
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    759 |          96077.4 | 72864000 |  1750.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.75
    apples_agent-1_min: 17
    apples_agent-2_max: 417
    apples_agent-2_mean: 361.88
    apples_agent-2_min: 268
    apples_agent-3_max: 286
    apples_agent-3_mean: 222.58
    apples_agent-3_min: 168
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 381.83
    apples_agent-5_min: 260
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 409.04
    cleaning_beam_agent-0_min: 363
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.95
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 24.43
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 521
    cleaning_beam_agent-4_mean: 410.65
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-07-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1883.0
  episode_reward_mean: 1739.22
  episode_reward_min: 1336.0
  episodes_this_iter: 96
  episodes_total: 72960
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.523
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36048394441604614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014406070113182068
        model: {}
        policy_loss: -0.0013105529360473156
        total_loss: 0.0013713836669921875
        vf_explained_var: 0.02242770791053772
        vf_loss: 33.163856506347656
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21612288057804108
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011242759646847844
        model: {}
        policy_loss: -0.001594472210854292
        total_loss: 0.000985071063041687
        vf_explained_var: 0.13111910223960876
        vf_loss: 29.59916877746582
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26677650213241577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012717953650280833
        model: {}
        policy_loss: -0.0016625686548650265
        total_loss: 0.006287458352744579
        vf_explained_var: 0.029280945658683777
        vf_loss: 84.1955337524414
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5201840996742249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009685766417533159
        model: {}
        policy_loss: -0.0014726370573043823
        total_loss: 0.005830360576510429
        vf_explained_var: 0.0567602664232254
        vf_loss: 82.18522644042969
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9306680560112
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016133686294779181
        model: {}
        policy_loss: -0.0020038429647684097
        total_loss: -0.000505195464938879
        vf_explained_var: 5.066394805908203e-07
        vf_loss: 31.366226196289062
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25838422775268555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007868852699175477
        model: {}
        policy_loss: -0.0015156340086832643
        total_loss: 0.0009164437651634216
        vf_explained_var: 0.07876066863536835
        vf_loss: 28.86836814880371
    load_time_ms: 14485.994
    num_steps_sampled: 72960000
    num_steps_trained: 72960000
    sample_time_ms: 101169.767
    update_time_ms: 13.906
  iterations_since_restore: 110
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.98444444444445
    ram_util_percent: 15.195000000000002
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 436.0
    agent-3: 436.0
    agent-4: 254.5
    agent-5: 254.5
  policy_reward_mean:
    agent-0: 244.99
    agent-1: 244.99
    agent-2: 392.435
    agent-3: 392.435
    agent-4: 232.185
    agent-5: 232.185
  policy_reward_min:
    agent-0: 184.5
    agent-1: 184.5
    agent-2: 314.0
    agent-3: 314.0
    agent-4: 169.5
    agent-5: 169.5
  sampler_perf:
    mean_env_wait_ms: 26.654080598640817
    mean_inference_ms: 12.970849801084366
    mean_processing_ms: 59.71655805085869
  time_since_restore: 14071.355866670609
  time_this_iter_s: 126.39651775360107
  time_total_s: 96203.79823684692
  timestamp: 1637608038
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 72960000
  training_iteration: 760
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    760 |          96203.8 | 72960000 |  1739.22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 0.45
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 30.45
    apples_agent-1_min: 11
    apples_agent-2_max: 415
    apples_agent-2_mean: 355.41
    apples_agent-2_min: 123
    apples_agent-3_max: 291
    apples_agent-3_mean: 223.86
    apples_agent-3_min: 107
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 425
    apples_agent-5_mean: 377.88
    apples_agent-5_min: 153
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 402.43
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.4
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 25.92
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 413.83
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-09-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1849.0
  episode_reward_mean: 1722.97
  episode_reward_min: 671.0
  episodes_this_iter: 96
  episodes_total: 73056
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11617.886
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3597819209098816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014751119306311011
        model: {}
        policy_loss: -0.0014553291257470846
        total_loss: 0.0012070221127942204
        vf_explained_var: 0.08227726817131042
        vf_loss: 32.9556884765625
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21948519349098206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009335025679320097
        model: {}
        policy_loss: -0.0019370188238099217
        total_loss: 0.0007646239828318357
        vf_explained_var: 0.14107397198677063
        vf_loss: 30.879364013671875
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2680385112762451
        entropy_coeff: 0.0017600000137463212
        kl: 0.001093694823794067
        model: {}
        policy_loss: -0.0019653469789773226
        total_loss: 0.006049882620573044
        vf_explained_var: 0.08712701499462128
        vf_loss: 84.86980438232422
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5248731374740601
        entropy_coeff: 0.0017600000137463212
        kl: 0.001032318570651114
        model: {}
        policy_loss: -0.0016806353814899921
        total_loss: 0.006051284726709127
        vf_explained_var: 0.07338987290859222
        vf_loss: 86.55697631835938
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9331337213516235
        entropy_coeff: 0.0017600000137463212
        kl: 0.002818523906171322
        model: {}
        policy_loss: -0.0021111750975251198
        total_loss: -0.00041193640208803117
        vf_explained_var: -0.0016863197088241577
        vf_loss: 33.41557312011719
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2643600106239319
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010088100098073483
        model: {}
        policy_loss: -0.002032820601016283
        total_loss: 0.00042750127613544464
        vf_explained_var: 0.12262110412120819
        vf_loss: 29.255943298339844
    load_time_ms: 14468.892
    num_steps_sampled: 73056000
    num_steps_trained: 73056000
    sample_time_ms: 101120.603
    update_time_ms: 13.97
  iterations_since_restore: 111
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.311956521739127
    ram_util_percent: 15.17065217391304
  pid: 28385
  policy_reward_max:
    agent-0: 272.5
    agent-1: 272.5
    agent-2: 427.5
    agent-3: 427.5
    agent-4: 252.5
    agent-5: 252.5
  policy_reward_mean:
    agent-0: 242.84
    agent-1: 242.84
    agent-2: 389.0
    agent-3: 389.0
    agent-4: 229.645
    agent-5: 229.645
  policy_reward_min:
    agent-0: 97.5
    agent-1: 97.5
    agent-2: 145.5
    agent-3: 145.5
    agent-4: 92.5
    agent-5: 92.5
  sampler_perf:
    mean_env_wait_ms: 26.65116283354702
    mean_inference_ms: 12.970055262026268
    mean_processing_ms: 59.71466384297476
  time_since_restore: 14198.675958156586
  time_this_iter_s: 127.32009148597717
  time_total_s: 96331.1183283329
  timestamp: 1637608167
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 73056000
  training_iteration: 761
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    761 |          96331.1 | 73056000 |  1722.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 32.56
    apples_agent-1_min: 13
    apples_agent-2_max: 434
    apples_agent-2_mean: 359.75
    apples_agent-2_min: 258
    apples_agent-3_max: 282
    apples_agent-3_mean: 219.29
    apples_agent-3_min: 145
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 379.76
    apples_agent-5_min: 228
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 401.38
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.95
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 31
    cleaning_beam_agent-2_mean: 2.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 27.1
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 421.54
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-11-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1869.0
  episode_reward_mean: 1729.33
  episode_reward_min: 1050.0
  episodes_this_iter: 96
  episodes_total: 73152
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11613.424
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35296034812927246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017147704493254423
        model: {}
        policy_loss: -0.0013533004093915224
        total_loss: 0.0014337127795442939
        vf_explained_var: 0.01731686294078827
        vf_loss: 34.08226776123047
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22104772925376892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011040782555937767
        model: {}
        policy_loss: -0.0014959024265408516
        total_loss: 0.0010814974084496498
        vf_explained_var: 0.14678415656089783
        vf_loss: 29.664424896240234
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26387304067611694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009080719901248813
        model: {}
        policy_loss: -0.001827620784752071
        total_loss: 0.00603056512773037
        vf_explained_var: 0.05260361731052399
        vf_loss: 83.22601318359375
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5159167051315308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015222793444991112
        model: {}
        policy_loss: -0.0018555345013737679
        total_loss: 0.005732701625674963
        vf_explained_var: 0.03405439853668213
        vf_loss: 84.9625015258789
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9500543475151062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014219682198017836
        model: {}
        policy_loss: -0.0018393248319625854
        total_loss: -0.0004072706215083599
        vf_explained_var: 0.02727574110031128
        vf_loss: 31.0415096282959
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2643226087093353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011824140092357993
        model: {}
        policy_loss: -0.0017911617178469896
        total_loss: 0.0005668966914527118
        vf_explained_var: 0.11728774011135101
        vf_loss: 28.232662200927734
    load_time_ms: 14450.78
    num_steps_sampled: 73152000
    num_steps_trained: 73152000
    sample_time_ms: 101097.698
    update_time_ms: 13.993
  iterations_since_restore: 112
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.679444444444446
    ram_util_percent: 15.199444444444447
  pid: 28385
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 444.0
    agent-3: 444.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 243.3
    agent-1: 243.3
    agent-2: 390.66
    agent-3: 390.66
    agent-4: 230.705
    agent-5: 230.705
  policy_reward_min:
    agent-0: 142.0
    agent-1: 142.0
    agent-2: 240.0
    agent-3: 240.0
    agent-4: 143.0
    agent-5: 143.0
  sampler_perf:
    mean_env_wait_ms: 26.649040312941366
    mean_inference_ms: 12.96948715177005
    mean_processing_ms: 59.714287719118545
  time_since_restore: 14325.499367952347
  time_this_iter_s: 126.82340979576111
  time_total_s: 96457.94173812866
  timestamp: 1637608294
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 73152000
  training_iteration: 762
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    762 |          96457.9 | 73152000 |  1729.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.96
    apples_agent-1_min: 10
    apples_agent-2_max: 434
    apples_agent-2_mean: 356.09
    apples_agent-2_min: 118
    apples_agent-3_max: 289
    apples_agent-3_mean: 224.26
    apples_agent-3_min: 95
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 425
    apples_agent-5_mean: 378.78
    apples_agent-5_min: 145
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 406.05
    cleaning_beam_agent-0_min: 377
    cleaning_beam_agent-1_max: 19
    cleaning_beam_agent-1_mean: 2.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 26.09
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 500
    cleaning_beam_agent-4_mean: 414.14
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 4.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-13-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1870.0
  episode_reward_mean: 1729.64
  episode_reward_min: 675.0
  episodes_this_iter: 96
  episodes_total: 73248
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.847
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35830265283584595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013964911922812462
        model: {}
        policy_loss: -0.001521109021268785
        total_loss: 0.0011744234943762422
        vf_explained_var: 0.060983702540397644
        vf_loss: 33.261436462402344
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22096259891986847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004912030999548733
        model: {}
        policy_loss: -0.0018238129559904337
        total_loss: 0.0009010916110128164
        vf_explained_var: 0.1253097653388977
        vf_loss: 31.13800811767578
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.266230970621109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011509042233228683
        model: {}
        policy_loss: -0.0018517328426241875
        total_loss: 0.006447897292673588
        vf_explained_var: 0.08408568799495697
        vf_loss: 87.6819839477539
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5091452598571777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010728109627962112
        model: {}
        policy_loss: -0.0017001470550894737
        total_loss: 0.006410418078303337
        vf_explained_var: 0.06250596046447754
        vf_loss: 90.06661987304688
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9410408735275269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010635789949446917
        model: {}
        policy_loss: -0.0017931070178747177
        total_loss: -0.0003009103238582611
        vf_explained_var: 0.029679954051971436
        vf_loss: 31.484315872192383
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2708197832107544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009359227842651308
        model: {}
        policy_loss: -0.0019323863089084625
        total_loss: 0.000420532189309597
        vf_explained_var: 0.12890414893627167
        vf_loss: 28.29559326171875
    load_time_ms: 14440.157
    num_steps_sampled: 73248000
    num_steps_trained: 73248000
    sample_time_ms: 101118.015
    update_time_ms: 13.885
  iterations_since_restore: 113
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.02912087912088
    ram_util_percent: 15.176923076923076
  pid: 28385
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 444.0
    agent-3: 444.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 244.395
    agent-1: 244.395
    agent-2: 390.365
    agent-3: 390.365
    agent-4: 230.06
    agent-5: 230.06
  policy_reward_min:
    agent-0: 101.0
    agent-1: 101.0
    agent-2: 145.0
    agent-3: 145.0
    agent-4: 91.5
    agent-5: 91.5
  sampler_perf:
    mean_env_wait_ms: 26.645926086851958
    mean_inference_ms: 12.969388766808054
    mean_processing_ms: 59.71319460891989
  time_since_restore: 14453.043337583542
  time_this_iter_s: 127.54396963119507
  time_total_s: 96585.48570775986
  timestamp: 1637608422
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 73248000
  training_iteration: 763
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    763 |          96585.5 | 73248000 |  1729.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 0.26
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.33
    apples_agent-1_min: 7
    apples_agent-2_max: 426
    apples_agent-2_mean: 358.57
    apples_agent-2_min: 77
    apples_agent-3_max: 291
    apples_agent-3_mean: 217.08
    apples_agent-3_min: 49
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.12
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 376.99
    apples_agent-5_min: 102
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 407.64
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.91
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 2.81
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 26.14
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 417.09
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 32
    cleaning_beam_agent-5_mean: 4.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-15-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1861.0
  episode_reward_mean: 1708.31
  episode_reward_min: 431.0
  episodes_this_iter: 96
  episodes_total: 73344
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11604.165
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36066365242004395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015050370711833239
        model: {}
        policy_loss: -0.001455097459256649
        total_loss: 0.00135795958340168
        vf_explained_var: 0.11122529208660126
        vf_loss: 34.47821807861328
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22555102407932281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007984554395079613
        model: {}
        policy_loss: -0.001955260057002306
        total_loss: 0.0009359319228678942
        vf_explained_var: 0.15299449861049652
        vf_loss: 32.88159942626953
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2677696943283081
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007713934173807502
        model: {}
        policy_loss: -0.0018731653690338135
        total_loss: 0.006637096870690584
        vf_explained_var: 0.10757985711097717
        vf_loss: 89.8153305053711
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5171293020248413
        entropy_coeff: 0.0017600000137463212
        kl: 0.001329581718891859
        model: {}
        policy_loss: -0.0017078127712011337
        total_loss: 0.006973658688366413
        vf_explained_var: 0.0472879558801651
        vf_loss: 95.91618347167969
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9202375411987305
        entropy_coeff: 0.0017600000137463212
        kl: 0.003128614043816924
        model: {}
        policy_loss: -0.0020609754137694836
        total_loss: -3.3780408557504416e-05
        vf_explained_var: 0.014798745512962341
        vf_loss: 36.46810531616211
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.284822940826416
        entropy_coeff: 0.0017600000137463212
        kl: 0.00125791784375906
        model: {}
        policy_loss: -0.0022432496771216393
        total_loss: 0.0004497659392654896
        vf_explained_var: 0.13710960745811462
        vf_loss: 31.943031311035156
    load_time_ms: 14454.686
    num_steps_sampled: 73344000
    num_steps_trained: 73344000
    sample_time_ms: 101080.174
    update_time_ms: 13.893
  iterations_since_restore: 114
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.72417582417582
    ram_util_percent: 15.185714285714287
  pid: 28385
  policy_reward_max:
    agent-0: 269.5
    agent-1: 269.5
    agent-2: 435.0
    agent-3: 435.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 239.3
    agent-1: 239.3
    agent-2: 386.17
    agent-3: 386.17
    agent-4: 228.685
    agent-5: 228.685
  policy_reward_min:
    agent-0: 62.0
    agent-1: 62.0
    agent-2: 96.0
    agent-3: 96.0
    agent-4: 50.5
    agent-5: 50.5
  sampler_perf:
    mean_env_wait_ms: 26.643274777037785
    mean_inference_ms: 12.968806400281856
    mean_processing_ms: 59.71269391222248
  time_since_restore: 14580.551015377045
  time_this_iter_s: 127.50767779350281
  time_total_s: 96712.99338555336
  timestamp: 1637608550
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 73344000
  training_iteration: 764
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    764 |            96713 | 73344000 |  1708.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 33.84
    apples_agent-1_min: 17
    apples_agent-2_max: 404
    apples_agent-2_mean: 362.52
    apples_agent-2_min: 302
    apples_agent-3_max: 261
    apples_agent-3_mean: 222.59
    apples_agent-3_min: 141
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 386.68
    apples_agent-5_min: 341
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 403.1
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 28
    cleaning_beam_agent-1_mean: 2.29
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 22.73
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 448.69
    cleaning_beam_agent-4_min: 388
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-17-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1855.0
  episode_reward_mean: 1753.77
  episode_reward_min: 1474.0
  episodes_this_iter: 96
  episodes_total: 73440
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11595.21
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34744715690612793
        entropy_coeff: 0.0017600000137463212
        kl: 0.001653648796491325
        model: {}
        policy_loss: -0.0012701703235507011
        total_loss: 0.0014721076004207134
        vf_explained_var: 0.00695173442363739
        vf_loss: 33.537845611572266
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22033482789993286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004799144226126373
        model: {}
        policy_loss: -0.0013743792660534382
        total_loss: 0.001197567442432046
        vf_explained_var: 0.12393338978290558
        vf_loss: 29.59735679626465
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26200252771377563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012010758509859443
        model: {}
        policy_loss: -0.001873709261417389
        total_loss: 0.005940385162830353
        vf_explained_var: 0.030948102474212646
        vf_loss: 82.75221252441406
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5146775841712952
        entropy_coeff: 0.0017600000137463212
        kl: 0.000563451845664531
        model: {}
        policy_loss: -0.0013460852205753326
        total_loss: 0.005887780338525772
        vf_explained_var: 0.051254451274871826
        vf_loss: 81.39697265625
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9199944734573364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019175868947058916
        model: {}
        policy_loss: -0.002068727742880583
        total_loss: -0.0006846226751804352
        vf_explained_var: 0.013841688632965088
        vf_loss: 30.03296661376953
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2694745361804962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006189742125570774
        model: {}
        policy_loss: -0.0016101454384624958
        total_loss: 0.0007435909938067198
        vf_explained_var: 0.07932174205780029
        vf_loss: 28.28009796142578
    load_time_ms: 14425.115
    num_steps_sampled: 73440000
    num_steps_trained: 73440000
    sample_time_ms: 101095.145
    update_time_ms: 13.979
  iterations_since_restore: 115
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.36153846153847
    ram_util_percent: 15.233516483516485
  pid: 28385
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 429.0
    agent-3: 429.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 246.935
    agent-1: 246.935
    agent-2: 395.33
    agent-3: 395.33
    agent-4: 234.62
    agent-5: 234.62
  policy_reward_min:
    agent-0: 202.5
    agent-1: 202.5
    agent-2: 323.0
    agent-3: 323.0
    agent-4: 201.5
    agent-5: 201.5
  sampler_perf:
    mean_env_wait_ms: 26.64353120703852
    mean_inference_ms: 12.969339714842983
    mean_processing_ms: 59.71569873259281
  time_since_restore: 14708.16905760765
  time_this_iter_s: 127.61804223060608
  time_total_s: 96840.61142778397
  timestamp: 1637608677
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 73440000
  training_iteration: 765
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    765 |          96840.6 | 73440000 |  1753.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 32.78
    apples_agent-1_min: 16
    apples_agent-2_max: 420
    apples_agent-2_mean: 360.8
    apples_agent-2_min: 0
    apples_agent-3_max: 281
    apples_agent-3_mean: 221.0
    apples_agent-3_min: 120
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 469
    apples_agent-5_mean: 388.45
    apples_agent-5_min: 338
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 400.98
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.01
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 24.26
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 449.54
    cleaning_beam_agent-4_min: 389
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-20-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1912.0
  episode_reward_mean: 1751.5
  episode_reward_min: 988.0
  episodes_this_iter: 96
  episodes_total: 73536
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11593.929
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3515979051589966
        entropy_coeff: 0.0017600000137463212
        kl: 0.001660353154875338
        model: {}
        policy_loss: -0.001123485853895545
        total_loss: 0.0015493816463276744
        vf_explained_var: 0.02413998544216156
        vf_loss: 32.91681671142578
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21778568625450134
        entropy_coeff: 0.0017600000137463212
        kl: 0.000939672056119889
        model: {}
        policy_loss: -0.0018463976448401809
        total_loss: 0.000762182055041194
        vf_explained_var: 0.11463290452957153
        vf_loss: 29.9188232421875
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2650916874408722
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014565617311745882
        model: {}
        policy_loss: -0.0020400257781147957
        total_loss: 0.006146120838820934
        vf_explained_var: 0.056638047099113464
        vf_loss: 86.52705383300781
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.519619882106781
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009690325241535902
        model: {}
        policy_loss: -0.001871015876531601
        total_loss: 0.005950634367763996
        vf_explained_var: 0.05100499093532562
        vf_loss: 87.36181640625
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9102717041969299
        entropy_coeff: 0.0017600000137463212
        kl: 0.002146054757758975
        model: {}
        policy_loss: -0.0017771334387362003
        total_loss: -0.00034227222204208374
        vf_explained_var: 0.0036244988441467285
        vf_loss: 30.369388580322266
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2722257375717163
        entropy_coeff: 0.0017600000137463212
        kl: 0.001131089753471315
        model: {}
        policy_loss: -0.0016907928511500359
        total_loss: 0.0006190640851855278
        vf_explained_var: 0.09327200055122375
        vf_loss: 27.889789581298828
    load_time_ms: 14422.548
    num_steps_sampled: 73536000
    num_steps_trained: 73536000
    sample_time_ms: 101234.152
    update_time_ms: 13.88
  iterations_since_restore: 116
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.119565217391305
    ram_util_percent: 15.193478260869567
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 439.0
    agent-3: 439.0
    agent-4: 276.5
    agent-5: 276.5
  policy_reward_mean:
    agent-0: 245.955
    agent-1: 245.955
    agent-2: 394.54
    agent-3: 394.54
    agent-4: 235.255
    agent-5: 235.255
  policy_reward_min:
    agent-0: 201.0
    agent-1: 201.0
    agent-2: 85.0
    agent-3: 85.0
    agent-4: 208.0
    agent-5: 208.0
  sampler_perf:
    mean_env_wait_ms: 26.64333124009322
    mean_inference_ms: 12.969201006719508
    mean_processing_ms: 59.71965665479398
  time_since_restore: 14836.952852487564
  time_this_iter_s: 128.78379487991333
  time_total_s: 96969.39522266388
  timestamp: 1637608806
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 73536000
  training_iteration: 766
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    766 |          96969.4 | 73536000 |   1751.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.59
    apples_agent-1_min: 16
    apples_agent-2_max: 415
    apples_agent-2_mean: 358.77
    apples_agent-2_min: 282
    apples_agent-3_max: 281
    apples_agent-3_mean: 223.09
    apples_agent-3_min: 160
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 386.94
    apples_agent-5_min: 324
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 396.09
    cleaning_beam_agent-0_min: 363
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.58
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 2.48
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 23.89
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 459.56
    cleaning_beam_agent-4_min: 387
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 3.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-22-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1889.0
  episode_reward_mean: 1750.44
  episode_reward_min: 1467.0
  episodes_this_iter: 96
  episodes_total: 73632
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11602.79
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3504798114299774
        entropy_coeff: 0.0017600000137463212
        kl: 0.00154920294880867
        model: {}
        policy_loss: -0.0014279538299888372
        total_loss: 0.0011664447374641895
        vf_explained_var: 0.02749265730381012
        vf_loss: 32.112403869628906
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22000601887702942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011305082589387894
        model: {}
        policy_loss: -0.0018238453194499016
        total_loss: 0.0007512075826525688
        vf_explained_var: 0.10379837453365326
        vf_loss: 29.622636795043945
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2662510275840759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009083944605663419
        model: {}
        policy_loss: -0.0019307718612253666
        total_loss: 0.006031443830579519
        vf_explained_var: 0.06363816559314728
        vf_loss: 84.30818176269531
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5255379676818848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009560534963384271
        model: {}
        policy_loss: -0.001416670624166727
        total_loss: 0.00605729129165411
        vf_explained_var: 0.06705889105796814
        vf_loss: 83.98909759521484
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9256927967071533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025318926200270653
        model: {}
        policy_loss: -0.0022262432612478733
        total_loss: -0.0007836329750716686
        vf_explained_var: 0.043777525424957275
        vf_loss: 30.718311309814453
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27297210693359375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012566595105454326
        model: {}
        policy_loss: -0.001892030704766512
        total_loss: 0.0004534450126811862
        vf_explained_var: 0.13105551898479462
        vf_loss: 28.25905418395996
    load_time_ms: 14435.527
    num_steps_sampled: 73632000
    num_steps_trained: 73632000
    sample_time_ms: 101325.729
    update_time_ms: 13.723
  iterations_since_restore: 117
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.02707182320442
    ram_util_percent: 15.219889502762435
  pid: 28385
  policy_reward_max:
    agent-0: 268.0
    agent-1: 268.0
    agent-2: 433.0
    agent-3: 433.0
    agent-4: 270.5
    agent-5: 270.5
  policy_reward_mean:
    agent-0: 244.915
    agent-1: 244.915
    agent-2: 395.225
    agent-3: 395.225
    agent-4: 235.08
    agent-5: 235.08
  policy_reward_min:
    agent-0: 208.0
    agent-1: 208.0
    agent-2: 328.0
    agent-3: 328.0
    agent-4: 195.5
    agent-5: 195.5
  sampler_perf:
    mean_env_wait_ms: 26.64253093726559
    mean_inference_ms: 12.968495486949696
    mean_processing_ms: 59.717312757580224
  time_since_restore: 14964.343094825745
  time_this_iter_s: 127.39024233818054
  time_total_s: 97096.78546500206
  timestamp: 1637608934
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 73632000
  training_iteration: 767
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    767 |          97096.8 | 73632000 |  1750.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 30.89
    apples_agent-1_min: 15
    apples_agent-2_max: 424
    apples_agent-2_mean: 359.39
    apples_agent-2_min: 298
    apples_agent-3_max: 271
    apples_agent-3_mean: 223.54
    apples_agent-3_min: 176
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 382.81
    apples_agent-5_min: 299
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 390.56
    cleaning_beam_agent-0_min: 363
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 2.18
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 24.89
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 449.83
    cleaning_beam_agent-4_min: 356
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-24-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1857.0
  episode_reward_mean: 1744.8
  episode_reward_min: 1493.0
  episodes_this_iter: 96
  episodes_total: 73728
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11602.77
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3473741412162781
        entropy_coeff: 0.0017600000137463212
        kl: 0.001677005784586072
        model: {}
        policy_loss: -0.0013134870678186417
        total_loss: 0.0012296056374907494
        vf_explained_var: 0.024013131856918335
        vf_loss: 31.5446834564209
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22272317111492157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006717554642818868
        model: {}
        policy_loss: -0.001555530820041895
        total_loss: 0.0008023357950150967
        vf_explained_var: 0.15100394189357758
        vf_loss: 27.49858856201172
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.262931227684021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010297112166881561
        model: {}
        policy_loss: -0.0017130998894572258
        total_loss: 0.006175420247018337
        vf_explained_var: 0.03582793474197388
        vf_loss: 83.51277923583984
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5176169872283936
        entropy_coeff: 0.0017600000137463212
        kl: 0.000927723478525877
        model: {}
        policy_loss: -0.00146332080475986
        total_loss: 0.005856947973370552
        vf_explained_var: 0.04868696630001068
        vf_loss: 82.31273651123047
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9342690706253052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015134562272578478
        model: {}
        policy_loss: -0.0019711609929800034
        total_loss: -0.0005586855113506317
        vf_explained_var: 0.02063162624835968
        vf_loss: 30.567882537841797
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2731805443763733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008130156202241778
        model: {}
        policy_loss: -0.0017414623871445656
        total_loss: 0.0005863453261554241
        vf_explained_var: 0.10006088018417358
        vf_loss: 28.086071014404297
    load_time_ms: 14394.109
    num_steps_sampled: 73728000
    num_steps_trained: 73728000
    sample_time_ms: 101405.207
    update_time_ms: 13.825
  iterations_since_restore: 118
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.57087912087912
    ram_util_percent: 15.176373626373628
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 435.0
    agent-3: 435.0
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 242.725
    agent-1: 242.725
    agent-2: 396.805
    agent-3: 396.805
    agent-4: 232.87
    agent-5: 232.87
  policy_reward_min:
    agent-0: 194.5
    agent-1: 194.5
    agent-2: 337.0
    agent-3: 337.0
    agent-4: 191.0
    agent-5: 191.0
  sampler_perf:
    mean_env_wait_ms: 26.641457521421735
    mean_inference_ms: 12.968518874928852
    mean_processing_ms: 59.71652884088264
  time_since_restore: 15092.045221567154
  time_this_iter_s: 127.7021267414093
  time_total_s: 97224.48759174347
  timestamp: 1637609062
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 73728000
  training_iteration: 768
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    768 |          97224.5 | 73728000 |   1744.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.74
    apples_agent-1_min: 17
    apples_agent-2_max: 403
    apples_agent-2_mean: 359.65
    apples_agent-2_min: 282
    apples_agent-3_max: 278
    apples_agent-3_mean: 224.12
    apples_agent-3_min: 153
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 387.44
    apples_agent-5_min: 336
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 401.64
    cleaning_beam_agent-0_min: 350
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 2.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 1.59
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 24.28
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 445.0
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-26-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1861.0
  episode_reward_mean: 1754.51
  episode_reward_min: 1517.0
  episodes_this_iter: 96
  episodes_total: 73824
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11600.971
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3474121689796448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017010716255754232
        model: {}
        policy_loss: -0.0012623383663594723
        total_loss: 0.0013735690154135227
        vf_explained_var: 0.023074328899383545
        vf_loss: 32.473541259765625
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22120755910873413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006894386606290936
        model: {}
        policy_loss: -0.0016114098252728581
        total_loss: 0.000956196803599596
        vf_explained_var: 0.11305315792560577
        vf_loss: 29.56932830810547
      agent-2:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26347801089286804
        entropy_coeff: 0.0017600000137463212
        kl: 0.001272966735996306
        model: {}
        policy_loss: -0.0019006937509402633
        total_loss: 0.005697541404515505
        vf_explained_var: 0.03842206299304962
        vf_loss: 80.61955261230469
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5216836929321289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005891785258427262
        model: {}
        policy_loss: -0.0012971386313438416
        total_loss: 0.005628590472042561
        vf_explained_var: 0.0643245130777359
        vf_loss: 78.43893432617188
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9378941655158997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015094524715095758
        model: {}
        policy_loss: -0.0020098392851650715
        total_loss: -0.0006814943626523018
        vf_explained_var: 0.023580700159072876
        vf_loss: 29.79041290283203
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26665621995925903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005732248537242413
        model: {}
        policy_loss: -0.0017730679828673601
        total_loss: 0.0004947814159095287
        vf_explained_var: 0.10482114553451538
        vf_loss: 27.371639251708984
    load_time_ms: 14389.509
    num_steps_sampled: 73824000
    num_steps_trained: 73824000
    sample_time_ms: 101312.461
    update_time_ms: 14.021
  iterations_since_restore: 119
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.330939226519334
    ram_util_percent: 15.245303867403319
  pid: 28385
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 269.0
    agent-5: 269.0
  policy_reward_mean:
    agent-0: 246.375
    agent-1: 246.375
    agent-2: 396.435
    agent-3: 396.435
    agent-4: 234.445
    agent-5: 234.445
  policy_reward_min:
    agent-0: 205.0
    agent-1: 205.0
    agent-2: 331.0
    agent-3: 331.0
    agent-4: 202.0
    agent-5: 202.0
  sampler_perf:
    mean_env_wait_ms: 26.6405515615116
    mean_inference_ms: 12.968563911252572
    mean_processing_ms: 59.714940699235974
  time_since_restore: 15219.02092885971
  time_this_iter_s: 126.97570729255676
  time_total_s: 97351.46329903603
  timestamp: 1637609189
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 73824000
  training_iteration: 769
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    769 |          97351.5 | 73824000 |  1754.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 32.01
    apples_agent-1_min: 19
    apples_agent-2_max: 408
    apples_agent-2_mean: 357.5
    apples_agent-2_min: 257
    apples_agent-3_max: 275
    apples_agent-3_mean: 222.33
    apples_agent-3_min: 150
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 382.85
    apples_agent-5_min: 264
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 398.16
    cleaning_beam_agent-0_min: 363
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.6
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 24.48
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 440.82
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-28-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1866.0
  episode_reward_mean: 1746.81
  episode_reward_min: 1296.0
  episodes_this_iter: 96
  episodes_total: 73920
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11603.064
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34924566745758057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019180676899850368
        model: {}
        policy_loss: -0.0012469231151044369
        total_loss: 0.0013912366703152657
        vf_explained_var: 0.0262555330991745
        vf_loss: 32.528343200683594
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21684055030345917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012187896063551307
        model: {}
        policy_loss: -0.0016965833492577076
        total_loss: 0.0008549185004085302
        vf_explained_var: 0.12322971224784851
        vf_loss: 29.33139991760254
      agent-2:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26660144329071045
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014524280559271574
        model: {}
        policy_loss: -0.0018835233058780432
        total_loss: 0.005739850457757711
        vf_explained_var: 0.04423978924751282
        vf_loss: 80.92594146728516
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5216818451881409
        entropy_coeff: 0.0017600000137463212
        kl: 0.000963997736107558
        model: {}
        policy_loss: -0.0015033748932182789
        total_loss: 0.00555115332826972
        vf_explained_var: 0.058932557702064514
        vf_loss: 79.72688293457031
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9551607370376587
        entropy_coeff: 0.0017600000137463212
        kl: 0.002882129978388548
        model: {}
        policy_loss: -0.0019811687525361776
        total_loss: -0.0007200620602816343
        vf_explained_var: 0.02121397852897644
        vf_loss: 29.421913146972656
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26739755272865295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013277156976982951
        model: {}
        policy_loss: -0.001832752488553524
        total_loss: 0.00043824315071105957
        vf_explained_var: 0.08861519396305084
        vf_loss: 27.41615104675293
    load_time_ms: 14400.708
    num_steps_sampled: 73920000
    num_steps_trained: 73920000
    sample_time_ms: 101506.947
    update_time_ms: 14.313
  iterations_since_restore: 120
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.588524590163928
    ram_util_percent: 15.224590163934433
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 245.645
    agent-1: 245.645
    agent-2: 394.99
    agent-3: 394.99
    agent-4: 232.77
    agent-5: 232.77
  policy_reward_min:
    agent-0: 186.5
    agent-1: 186.5
    agent-2: 289.0
    agent-3: 289.0
    agent-4: 166.0
    agent-5: 166.0
  sampler_perf:
    mean_env_wait_ms: 26.639513573500707
    mean_inference_ms: 12.967949699761594
    mean_processing_ms: 59.71458125446946
  time_since_restore: 15347.458042144775
  time_this_iter_s: 128.4371132850647
  time_total_s: 97479.90041232109
  timestamp: 1637609317
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 73920000
  training_iteration: 770
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    770 |          97479.9 | 73920000 |  1746.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.54
    apples_agent-1_min: 19
    apples_agent-2_max: 418
    apples_agent-2_mean: 361.44
    apples_agent-2_min: 295
    apples_agent-3_max: 270
    apples_agent-3_mean: 223.55
    apples_agent-3_min: 153
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 383.92
    apples_agent-5_min: 274
    cleaning_beam_agent-0_max: 423
    cleaning_beam_agent-0_mean: 397.95
    cleaning_beam_agent-0_min: 341
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 1.78
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 21.57
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 419.99
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-30-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1866.0
  episode_reward_mean: 1746.01
  episode_reward_min: 1396.0
  episodes_this_iter: 96
  episodes_total: 74016
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11599.644
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.347636878490448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018944089533761144
        model: {}
        policy_loss: -0.0012681789230555296
        total_loss: 0.001612314721569419
        vf_explained_var: 0.023122787475585938
        vf_loss: 34.92329406738281
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2205304652452469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007824368076398969
        model: {}
        policy_loss: -0.0018932688981294632
        total_loss: 0.0007854914292693138
        vf_explained_var: 0.14362703263759613
        vf_loss: 30.66893196105957
      agent-2:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2705942392349243
        entropy_coeff: 0.0017600000137463212
        kl: 0.001116005121730268
        model: {}
        policy_loss: -0.0017424225807189941
        total_loss: 0.006010235752910376
        vf_explained_var: 0.04364897310733795
        vf_loss: 82.28904724121094
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.522759199142456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009208472911268473
        model: {}
        policy_loss: -0.0015648407861590385
        total_loss: 0.005779131315648556
        vf_explained_var: 0.042295828461647034
        vf_loss: 82.6402816772461
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9487282633781433
        entropy_coeff: 0.0017600000137463212
        kl: 0.001431157230399549
        model: {}
        policy_loss: -0.0013827199582010508
        total_loss: -8.086208254098892e-05
        vf_explained_var: 0.031026139855384827
        vf_loss: 29.716148376464844
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.264151930809021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011896515497937799
        model: {}
        policy_loss: -0.001919180154800415
        total_loss: 0.0003000358119606972
        vf_explained_var: 0.12614671885967255
        vf_loss: 26.841228485107422
    load_time_ms: 14421.316
    num_steps_sampled: 74016000
    num_steps_trained: 74016000
    sample_time_ms: 101528.304
    update_time_ms: 14.152
  iterations_since_restore: 121
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.583152173913046
    ram_util_percent: 15.211413043478261
  pid: 28385
  policy_reward_max:
    agent-0: 286.0
    agent-1: 286.0
    agent-2: 431.5
    agent-3: 431.5
    agent-4: 257.0
    agent-5: 257.0
  policy_reward_mean:
    agent-0: 243.715
    agent-1: 243.715
    agent-2: 395.875
    agent-3: 395.875
    agent-4: 233.415
    agent-5: 233.415
  policy_reward_min:
    agent-0: 127.0
    agent-1: 127.0
    agent-2: 326.5
    agent-3: 326.5
    agent-4: 176.5
    agent-5: 176.5
  sampler_perf:
    mean_env_wait_ms: 26.636689548529574
    mean_inference_ms: 12.966946567491256
    mean_processing_ms: 59.714822404699206
  time_since_restore: 15475.192294836044
  time_this_iter_s: 127.73425269126892
  time_total_s: 97607.63466501236
  timestamp: 1637609447
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 74016000
  training_iteration: 771
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    771 |          97607.6 | 74016000 |  1746.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 31.61
    apples_agent-1_min: 15
    apples_agent-2_max: 408
    apples_agent-2_mean: 356.9
    apples_agent-2_min: 307
    apples_agent-3_max: 269
    apples_agent-3_mean: 224.8
    apples_agent-3_min: 146
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 388.42
    apples_agent-5_min: 343
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 400.64
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 2.01
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 1.87
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 24.25
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 410.84
    cleaning_beam_agent-4_min: 350
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-32-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1878.0
  episode_reward_mean: 1757.22
  episode_reward_min: 1565.0
  episodes_this_iter: 96
  episodes_total: 74112
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11598.379
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35101068019866943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012020859867334366
        model: {}
        policy_loss: -0.001325701829046011
        total_loss: 0.0013393161352723837
        vf_explained_var: 0.01880180835723877
        vf_loss: 32.82801055908203
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21781767904758453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010824992787092924
        model: {}
        policy_loss: -0.001691749319434166
        total_loss: 0.0007861302001401782
        vf_explained_var: 0.14525964856147766
        vf_loss: 28.612384796142578
      agent-2:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2674567401409149
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006884356262162328
        model: {}
        policy_loss: -0.0018982472829520702
        total_loss: 0.005749514326453209
        vf_explained_var: 0.029888108372688293
        vf_loss: 81.18487548828125
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5234012007713318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015066777123138309
        model: {}
        policy_loss: -0.0017933789640665054
        total_loss: 0.0051210857927799225
        vf_explained_var: 0.0651414692401886
        vf_loss: 78.35650634765625
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.95316481590271
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011931851040571928
        model: {}
        policy_loss: -0.001712005934678018
        total_loss: -0.00043364622979424894
        vf_explained_var: 0.009852766990661621
        vf_loss: 29.55929946899414
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2539839446544647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007805785280652344
        model: {}
        policy_loss: -0.0015993451233953238
        total_loss: 0.0005229832604527473
        vf_explained_var: 0.1403265744447708
        vf_loss: 25.693439483642578
    load_time_ms: 14437.067
    num_steps_sampled: 74112000
    num_steps_trained: 74112000
    sample_time_ms: 101646.124
    update_time_ms: 14.051
  iterations_since_restore: 122
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.093442622950818
    ram_util_percent: 15.137704918032785
  pid: 28385
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 426.0
    agent-3: 426.0
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 246.215
    agent-1: 246.215
    agent-2: 396.915
    agent-3: 396.915
    agent-4: 235.48
    agent-5: 235.48
  policy_reward_min:
    agent-0: 199.0
    agent-1: 199.0
    agent-2: 304.5
    agent-3: 304.5
    agent-4: 207.0
    agent-5: 207.0
  sampler_perf:
    mean_env_wait_ms: 26.63496310468832
    mean_inference_ms: 12.966624440728062
    mean_processing_ms: 59.7138915864521
  time_since_restore: 15603.303731918335
  time_this_iter_s: 128.11143708229065
  time_total_s: 97735.74610209465
  timestamp: 1637609575
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 74112000
  training_iteration: 772
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    772 |          97735.7 | 74112000 |  1757.22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 30.25
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 355.41
    apples_agent-2_min: 272
    apples_agent-3_max: 266
    apples_agent-3_mean: 221.5
    apples_agent-3_min: 158
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 386.6
    apples_agent-5_min: 315
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 402.58
    cleaning_beam_agent-0_min: 334
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 1.53
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 23.12
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 471
    cleaning_beam_agent-4_mean: 412.78
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-35-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1879.0
  episode_reward_mean: 1748.19
  episode_reward_min: 1436.0
  episodes_this_iter: 96
  episodes_total: 74208
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11591.295
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3536006212234497
        entropy_coeff: 0.0017600000137463212
        kl: 0.001485900254920125
        model: {}
        policy_loss: -0.0011925632134079933
        total_loss: 0.0014395394828170538
        vf_explained_var: 0.039516136050224304
        vf_loss: 32.54443359375
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2143733948469162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009137413580901921
        model: {}
        policy_loss: -0.001655873842537403
        total_loss: 0.0009679198265075684
        vf_explained_var: 0.1212935745716095
        vf_loss: 30.010913848876953
      agent-2:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26921188831329346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006893258541822433
        model: {}
        policy_loss: -0.0018834210932254791
        total_loss: 0.006027051713317633
        vf_explained_var: 0.030292019248008728
        vf_loss: 83.84286499023438
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5302441716194153
        entropy_coeff: 0.0017600000137463212
        kl: 0.001830613473430276
        model: {}
        policy_loss: -0.0019396182615309954
        total_loss: 0.005356007255613804
        vf_explained_var: 0.050192564725875854
        vf_loss: 82.28857421875
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9470881223678589
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026185393799096346
        model: {}
        policy_loss: -0.00191344297491014
        total_loss: -0.0004911478608846664
        vf_explained_var: -0.0016213208436965942
        vf_loss: 30.891712188720703
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25824469327926636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009061117889359593
        model: {}
        policy_loss: -0.0017096716910600662
        total_loss: 0.000597700010985136
        vf_explained_var: 0.10385763645172119
        vf_loss: 27.61880874633789
    load_time_ms: 14438.869
    num_steps_sampled: 74208000
    num_steps_trained: 74208000
    sample_time_ms: 101567.535
    update_time_ms: 14.14
  iterations_since_restore: 123
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.569444444444443
    ram_util_percent: 15.178888888888894
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 432.5
    agent-3: 432.5
    agent-4: 267.0
    agent-5: 267.0
  policy_reward_mean:
    agent-0: 245.74
    agent-1: 245.74
    agent-2: 393.58
    agent-3: 393.58
    agent-4: 234.775
    agent-5: 234.775
  policy_reward_min:
    agent-0: 190.0
    agent-1: 190.0
    agent-2: 323.5
    agent-3: 323.5
    agent-4: 198.0
    agent-5: 198.0
  sampler_perf:
    mean_env_wait_ms: 26.631545343142367
    mean_inference_ms: 12.966019913890923
    mean_processing_ms: 59.71048835712451
  time_since_restore: 15730.011611938477
  time_this_iter_s: 126.7078800201416
  time_total_s: 97862.45398211479
  timestamp: 1637609702
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 74208000
  training_iteration: 773
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    773 |          97862.5 | 74208000 |  1748.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 30.35
    apples_agent-1_min: 17
    apples_agent-2_max: 437
    apples_agent-2_mean: 355.97
    apples_agent-2_min: 236
    apples_agent-3_max: 274
    apples_agent-3_mean: 221.53
    apples_agent-3_min: 159
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 379.36
    apples_agent-5_min: 231
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 393.63
    cleaning_beam_agent-0_min: 358
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.05
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 1.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 23.83
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 411.35
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-37-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1871.0
  episode_reward_mean: 1737.19
  episode_reward_min: 1166.0
  episodes_this_iter: 96
  episodes_total: 74304
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11594.597
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34499645233154297
        entropy_coeff: 0.0017600000137463212
        kl: 0.001130761462263763
        model: {}
        policy_loss: -0.0012237979099154472
        total_loss: 0.001516642514616251
        vf_explained_var: 0.02826462686061859
        vf_loss: 33.476314544677734
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2175271213054657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008794388850219548
        model: {}
        policy_loss: -0.001707988791167736
        total_loss: 0.0009008543565869331
        vf_explained_var: 0.13294737040996552
        vf_loss: 29.916915893554688
      agent-2:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2699965238571167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010800778400152922
        model: {}
        policy_loss: -0.0018642260693013668
        total_loss: 0.006028135772794485
        vf_explained_var: 0.04289957880973816
        vf_loss: 83.67555236816406
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5169758796691895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012369563337415457
        model: {}
        policy_loss: -0.0016555385664105415
        total_loss: 0.005599685944616795
        vf_explained_var: 0.06598544120788574
        vf_loss: 81.6510238647461
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9496482610702515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028981384821236134
        model: {}
        policy_loss: -0.0022995006293058395
        total_loss: -0.0008285404765047133
        vf_explained_var: 0.01612544059753418
        vf_loss: 31.423410415649414
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26565197110176086
        entropy_coeff: 0.0017600000137463212
        kl: 0.001289887703023851
        model: {}
        policy_loss: -0.0020031295716762543
        total_loss: 0.0003004763275384903
        vf_explained_var: 0.13260014355182648
        vf_loss: 27.711509704589844
    load_time_ms: 14462.53
    num_steps_sampled: 74304000
    num_steps_trained: 74304000
    sample_time_ms: 101521.155
    update_time_ms: 14.125
  iterations_since_restore: 124
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.503296703296705
    ram_util_percent: 15.243956043956047
  pid: 28385
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 439.5
    agent-3: 439.5
    agent-4: 254.5
    agent-5: 254.5
  policy_reward_mean:
    agent-0: 244.03
    agent-1: 244.03
    agent-2: 393.555
    agent-3: 393.555
    agent-4: 231.01
    agent-5: 231.01
  policy_reward_min:
    agent-0: 160.5
    agent-1: 160.5
    agent-2: 267.5
    agent-3: 267.5
    agent-4: 155.0
    agent-5: 155.0
  sampler_perf:
    mean_env_wait_ms: 26.627938524571732
    mean_inference_ms: 12.965089506707416
    mean_processing_ms: 59.707303506398205
  time_since_restore: 15857.323364257812
  time_this_iter_s: 127.31175231933594
  time_total_s: 97989.76573443413
  timestamp: 1637609829
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 74304000
  training_iteration: 774
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    774 |          97989.8 | 74304000 |  1737.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 31.38
    apples_agent-1_min: 17
    apples_agent-2_max: 411
    apples_agent-2_mean: 358.98
    apples_agent-2_min: 290
    apples_agent-3_max: 274
    apples_agent-3_mean: 224.06
    apples_agent-3_min: 173
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 383.5
    apples_agent-5_min: 315
    cleaning_beam_agent-0_max: 431
    cleaning_beam_agent-0_mean: 390.2
    cleaning_beam_agent-0_min: 343
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.05
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 23.75
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 423.53
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-39-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1860.0
  episode_reward_mean: 1747.7
  episode_reward_min: 1481.0
  episodes_this_iter: 96
  episodes_total: 74400
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11596.013
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3422479033470154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013458330649882555
        model: {}
        policy_loss: -0.0011562025174498558
        total_loss: 0.0015942100435495377
        vf_explained_var: 0.012715905904769897
        vf_loss: 33.52770233154297
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21712853014469147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012496253475546837
        model: {}
        policy_loss: -0.0015932833775877953
        total_loss: 0.0010249647311866283
        vf_explained_var: 0.11799052357673645
        vf_loss: 30.003925323486328
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26717591285705566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006932106334716082
        model: {}
        policy_loss: -0.0016649267636239529
        total_loss: 0.0060635036788880825
        vf_explained_var: 0.02207884192466736
        vf_loss: 81.98663330078125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5274269580841064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013940841890871525
        model: {}
        policy_loss: -0.0014195251278579235
        total_loss: 0.005596775561571121
        vf_explained_var: 0.05381135642528534
        vf_loss: 79.44573211669922
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.940993070602417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020580983255058527
        model: {}
        policy_loss: -0.0018518157303333282
        total_loss: -0.00048699486069381237
        vf_explained_var: 0.0027005523443222046
        vf_loss: 30.209693908691406
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.258078396320343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007754778489470482
        model: {}
        policy_loss: -0.001776138786226511
        total_loss: 0.0005031395703554153
        vf_explained_var: 0.09792393445968628
        vf_loss: 27.3349609375
    load_time_ms: 14467.992
    num_steps_sampled: 74400000
    num_steps_trained: 74400000
    sample_time_ms: 101389.64
    update_time_ms: 14.122
  iterations_since_restore: 125
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.858333333333334
    ram_util_percent: 15.129999999999997
  pid: 28385
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 425.0
    agent-3: 425.0
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 245.845
    agent-1: 245.845
    agent-2: 395.03
    agent-3: 395.03
    agent-4: 232.975
    agent-5: 232.975
  policy_reward_min:
    agent-0: 204.5
    agent-1: 204.5
    agent-2: 326.5
    agent-3: 326.5
    agent-4: 197.5
    agent-5: 197.5
  sampler_perf:
    mean_env_wait_ms: 26.624586288791317
    mean_inference_ms: 12.965015501900645
    mean_processing_ms: 59.704098943774554
  time_since_restore: 15983.718722820282
  time_this_iter_s: 126.39535856246948
  time_total_s: 98116.1610929966
  timestamp: 1637609956
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 74400000
  training_iteration: 775
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    775 |          98116.2 | 74400000 |   1747.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.13
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.28
    apples_agent-1_min: 1
    apples_agent-2_max: 415
    apples_agent-2_mean: 353.87
    apples_agent-2_min: 26
    apples_agent-3_max: 281
    apples_agent-3_mean: 223.42
    apples_agent-3_min: 14
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 381.43
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 381.82
    cleaning_beam_agent-0_min: 357
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 2.05
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 24.41
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 438.39
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 4.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-41-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1849.0
  episode_reward_mean: 1735.37
  episode_reward_min: 129.0
  episodes_this_iter: 96
  episodes_total: 74496
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11603.631
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34861379861831665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007478559855371714
        model: {}
        policy_loss: -0.001263451762497425
        total_loss: 0.0015373928472399712
        vf_explained_var: 0.0934697687625885
        vf_loss: 34.144020080566406
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21631911396980286
        entropy_coeff: 0.0017600000137463212
        kl: 0.000872388482093811
        model: {}
        policy_loss: -0.0017301554325968027
        total_loss: 0.0010313085513189435
        vf_explained_var: 0.16529832780361176
        vf_loss: 31.421855926513672
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27475935220718384
        entropy_coeff: 0.0017600000137463212
        kl: 0.001150712138041854
        model: {}
        policy_loss: -0.0020464425906538963
        total_loss: 0.006161148194223642
        vf_explained_var: 0.08648400008678436
        vf_loss: 86.91165924072266
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5375044345855713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007203539134934545
        model: {}
        policy_loss: -0.0014954002108424902
        total_loss: 0.006574003957211971
        vf_explained_var: 0.05253317952156067
        vf_loss: 90.15411376953125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9394757151603699
        entropy_coeff: 0.0017600000137463212
        kl: 0.002016523154452443
        model: {}
        policy_loss: -0.0021540261805057526
        total_loss: -0.0004407074302434921
        vf_explained_var: 0.020569875836372375
        vf_loss: 33.66796112060547
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2625291645526886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006082667969167233
        model: {}
        policy_loss: -0.0019638403318822384
        total_loss: 0.0004817303270101547
        vf_explained_var: 0.15543586015701294
        vf_loss: 29.076210021972656
    load_time_ms: 14474.485
    num_steps_sampled: 74496000
    num_steps_trained: 74496000
    sample_time_ms: 101133.28
    update_time_ms: 14.068
  iterations_since_restore: 126
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.34
    ram_util_percent: 15.231666666666667
  pid: 28385
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 243.615
    agent-1: 243.615
    agent-2: 392.145
    agent-3: 392.145
    agent-4: 231.925
    agent-5: 231.925
  policy_reward_min:
    agent-0: 22.5
    agent-1: 22.5
    agent-2: 27.5
    agent-3: 27.5
    agent-4: 14.5
    agent-5: 14.5
  sampler_perf:
    mean_env_wait_ms: 26.621718056866175
    mean_inference_ms: 12.965187053039863
    mean_processing_ms: 59.701670296824624
  time_since_restore: 16110.125945806503
  time_this_iter_s: 126.40722298622131
  time_total_s: 98242.56831598282
  timestamp: 1637610082
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 74496000
  training_iteration: 776
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    776 |          98242.6 | 74496000 |  1735.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 31.11
    apples_agent-1_min: 16
    apples_agent-2_max: 412
    apples_agent-2_mean: 355.18
    apples_agent-2_min: 287
    apples_agent-3_max: 275
    apples_agent-3_mean: 226.73
    apples_agent-3_min: 171
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 382.7
    apples_agent-5_min: 321
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 383.27
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 18
    cleaning_beam_agent-1_mean: 2.01
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 2.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 23.43
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 440.34
    cleaning_beam_agent-4_min: 388
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-43-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1879.0
  episode_reward_mean: 1754.2
  episode_reward_min: 1543.0
  episodes_this_iter: 96
  episodes_total: 74592
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11598.993
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3367788791656494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014976065140217543
        model: {}
        policy_loss: -0.0012190842535346746
        total_loss: 0.0013919746270403266
        vf_explained_var: 0.027559801936149597
        vf_loss: 32.037906646728516
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2157304286956787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012234712485224009
        model: {}
        policy_loss: -0.0016566466074436903
        total_loss: 0.0009327648440375924
        vf_explained_var: 0.09992577135562897
        vf_loss: 29.691001892089844
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2722204327583313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009144739015027881
        model: {}
        policy_loss: -0.0017500254325568676
        total_loss: 0.006055221427232027
        vf_explained_var: 0.019471675157546997
        vf_loss: 82.84355926513672
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5244722366333008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017080340767279267
        model: {}
        policy_loss: -0.0018859202973544598
        total_loss: 0.005166623741388321
        vf_explained_var: 0.06026284396648407
        vf_loss: 79.75614929199219
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9524385333061218
        entropy_coeff: 0.0017600000137463212
        kl: 0.001725777518004179
        model: {}
        policy_loss: -0.0018947687931358814
        total_loss: -0.0005907486192882061
        vf_explained_var: 0.004310339689254761
        vf_loss: 29.8031063079834
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25492212176322937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008688429370522499
        model: {}
        policy_loss: -0.0016228047898039222
        total_loss: 0.0005349739803932607
        vf_explained_var: 0.13611648976802826
        vf_loss: 26.064376831054688
    load_time_ms: 14441.678
    num_steps_sampled: 74592000
    num_steps_trained: 74592000
    sample_time_ms: 101232.343
    update_time_ms: 14.389
  iterations_since_restore: 127
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.963387978142084
    ram_util_percent: 15.207103825136613
  pid: 28385
  policy_reward_max:
    agent-0: 267.5
    agent-1: 267.5
    agent-2: 443.5
    agent-3: 443.5
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 246.62
    agent-1: 246.62
    agent-2: 397.38
    agent-3: 397.38
    agent-4: 233.1
    agent-5: 233.1
  policy_reward_min:
    agent-0: 210.0
    agent-1: 210.0
    agent-2: 344.5
    agent-3: 344.5
    agent-4: 192.5
    agent-5: 192.5
  sampler_perf:
    mean_env_wait_ms: 26.62038803262965
    mean_inference_ms: 12.965038031557292
    mean_processing_ms: 59.70366223242274
  time_since_restore: 16238.08910870552
  time_this_iter_s: 127.96316289901733
  time_total_s: 98370.53147888184
  timestamp: 1637610210
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 74592000
  training_iteration: 777
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    777 |          98370.5 | 74592000 |   1754.2 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 32.15
    apples_agent-1_min: 18
    apples_agent-2_max: 401
    apples_agent-2_mean: 353.95
    apples_agent-2_min: 195
    apples_agent-3_max: 274
    apples_agent-3_mean: 224.0
    apples_agent-3_min: 130
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 385.01
    apples_agent-5_min: 212
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 376.32
    cleaning_beam_agent-0_min: 319
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 2.14
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 22.81
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 427.95
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 4.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-45-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1872.0
  episode_reward_mean: 1740.74
  episode_reward_min: 976.0
  episodes_this_iter: 96
  episodes_total: 74688
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11606.934
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33504608273506165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010434812866151333
        model: {}
        policy_loss: -0.0014015929773449898
        total_loss: 0.001253209076821804
        vf_explained_var: 0.029705017805099487
        vf_loss: 32.44483184814453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22007672488689423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010652593336999416
        model: {}
        policy_loss: -0.0018054144456982613
        total_loss: 0.00080059003084898
        vf_explained_var: 0.1060216873884201
        vf_loss: 29.933380126953125
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27742040157318115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007636199006810784
        model: {}
        policy_loss: -0.0016194386407732964
        total_loss: 0.00626008864492178
        vf_explained_var: 0.043574124574661255
        vf_loss: 83.67787170410156
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.540267825126648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010220054537057877
        model: {}
        policy_loss: -0.001567274797707796
        total_loss: 0.0057031880132853985
        vf_explained_var: 0.05931660532951355
        vf_loss: 82.21333312988281
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9378967881202698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012881630100309849
        model: {}
        policy_loss: -0.0016737822443246841
        total_loss: -0.0002485808217898011
        vf_explained_var: 0.02201111614704132
        vf_loss: 30.75897979736328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25624415278434753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011174245737493038
        model: {}
        policy_loss: -0.0018066009506583214
        total_loss: 0.0005363896489143372
        vf_explained_var: 0.12118609249591827
        vf_loss: 27.939796447753906
    load_time_ms: 14466.473
    num_steps_sampled: 74688000
    num_steps_trained: 74688000
    sample_time_ms: 101097.345
    update_time_ms: 14.243
  iterations_since_restore: 128
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.39777777777778
    ram_util_percent: 15.206111111111113
  pid: 28385
  policy_reward_max:
    agent-0: 268.5
    agent-1: 268.5
    agent-2: 436.5
    agent-3: 436.5
    agent-4: 267.5
    agent-5: 267.5
  policy_reward_mean:
    agent-0: 244.885
    agent-1: 244.885
    agent-2: 391.205
    agent-3: 391.205
    agent-4: 234.28
    agent-5: 234.28
  policy_reward_min:
    agent-0: 130.5
    agent-1: 130.5
    agent-2: 224.0
    agent-3: 224.0
    agent-4: 133.5
    agent-5: 133.5
  sampler_perf:
    mean_env_wait_ms: 26.616547150162827
    mean_inference_ms: 12.964453805444903
    mean_processing_ms: 59.699643944044325
  time_since_restore: 16364.77436542511
  time_this_iter_s: 126.68525671958923
  time_total_s: 98497.21673560143
  timestamp: 1637610337
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 74688000
  training_iteration: 778
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    778 |          98497.2 | 74688000 |  1740.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 31.08
    apples_agent-1_min: 17
    apples_agent-2_max: 409
    apples_agent-2_mean: 357.93
    apples_agent-2_min: 248
    apples_agent-3_max: 291
    apples_agent-3_mean: 230.48
    apples_agent-3_min: 166
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 382.5
    apples_agent-5_min: 271
    cleaning_beam_agent-0_max: 410
    cleaning_beam_agent-0_mean: 377.89
    cleaning_beam_agent-0_min: 346
    cleaning_beam_agent-1_max: 14
    cleaning_beam_agent-1_mean: 2.18
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 20.14
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 434.07
    cleaning_beam_agent-4_min: 387
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-47-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1854.0
  episode_reward_mean: 1747.69
  episode_reward_min: 1304.0
  episodes_this_iter: 96
  episodes_total: 74784
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11606.887
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3328785300254822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009323938866145909
        model: {}
        policy_loss: -0.0008863215334713459
        total_loss: 0.001717414939776063
        vf_explained_var: 0.024150505661964417
        vf_loss: 31.895999908447266
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2163037359714508
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010907547548413277
        model: {}
        policy_loss: -0.0018150466494262218
        total_loss: 0.0007940612267702818
        vf_explained_var: 0.08576425909996033
        vf_loss: 29.898027420043945
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2729331851005554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010381892789155245
        model: {}
        policy_loss: -0.0018470771610736847
        total_loss: 0.005949507933109999
        vf_explained_var: 0.014334708452224731
        vf_loss: 82.76947021484375
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5296804904937744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005578650161623955
        model: {}
        policy_loss: -0.0015550744719803333
        total_loss: 0.00558339711278677
        vf_explained_var: 0.04385392367839813
        vf_loss: 80.70707702636719
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9362961053848267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017898220103234053
        model: {}
        policy_loss: -0.0018604723736643791
        total_loss: -0.00048368179704993963
        vf_explained_var: -0.005008623003959656
        vf_loss: 30.246726989746094
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25426557660102844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007147390861064196
        model: {}
        policy_loss: -0.0015566148795187473
        total_loss: 0.0006150896660983562
        vf_explained_var: 0.1329944133758545
        vf_loss: 26.192113876342773
    load_time_ms: 14461.831
    num_steps_sampled: 74784000
    num_steps_trained: 74784000
    sample_time_ms: 101005.162
    update_time_ms: 13.991
  iterations_since_restore: 129
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.285555555555554
    ram_util_percent: 15.225555555555557
  pid: 28385
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 447.5
    agent-3: 447.5
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 245.36
    agent-1: 245.36
    agent-2: 395.41
    agent-3: 395.41
    agent-4: 233.075
    agent-5: 233.075
  policy_reward_min:
    agent-0: 179.5
    agent-1: 179.5
    agent-2: 297.5
    agent-3: 297.5
    agent-4: 175.0
    agent-5: 175.0
  sampler_perf:
    mean_env_wait_ms: 26.61315342820488
    mean_inference_ms: 12.963640967364668
    mean_processing_ms: 59.69667593447889
  time_since_restore: 16490.736428260803
  time_this_iter_s: 125.96206283569336
  time_total_s: 98623.17879843712
  timestamp: 1637610463
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 74784000
  training_iteration: 779
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    779 |          98623.2 | 74784000 |  1747.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.35
    apples_agent-1_min: 16
    apples_agent-2_max: 388
    apples_agent-2_mean: 354.62
    apples_agent-2_min: 303
    apples_agent-3_max: 280
    apples_agent-3_mean: 231.94
    apples_agent-3_min: 178
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 385.96
    apples_agent-5_min: 333
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 382.74
    cleaning_beam_agent-0_min: 354
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.45
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.31
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 19.77
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 524
    cleaning_beam_agent-4_mean: 432.44
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-49-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1862.0
  episode_reward_mean: 1756.43
  episode_reward_min: 1476.0
  episodes_this_iter: 96
  episodes_total: 74880
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11607.787
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3368571996688843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017456973437219858
        model: {}
        policy_loss: -0.0010641019325703382
        total_loss: 0.0014728358946740627
        vf_explained_var: 0.028469741344451904
        vf_loss: 31.298065185546875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2141025960445404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008979092235676944
        model: {}
        policy_loss: -0.001512689981609583
        total_loss: 0.0009116674773395061
        vf_explained_var: 0.12753893435001373
        vf_loss: 28.011798858642578
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2732205390930176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008316227467730641
        model: {}
        policy_loss: -0.0016213692724704742
        total_loss: 0.00603114627301693
        vf_explained_var: 0.021181613206863403
        vf_loss: 81.33384704589844
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5252493619918823
        entropy_coeff: 0.0017600000137463212
        kl: 0.002030655276030302
        model: {}
        policy_loss: -0.001807129941880703
        total_loss: 0.005026672035455704
        vf_explained_var: 0.07062384486198425
        vf_loss: 77.58241271972656
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9267863035202026
        entropy_coeff: 0.0017600000137463212
        kl: 0.003945007920265198
        model: {}
        policy_loss: -0.0021320299711078405
        total_loss: -0.0006403760053217411
        vf_explained_var: 0.004506096243858337
        vf_loss: 31.22797966003418
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24930830299854279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010221144184470177
        model: {}
        policy_loss: -0.0018680100329220295
        total_loss: 0.00042999815195798874
        vf_explained_var: 0.13648302853107452
        vf_loss: 27.3679141998291
    load_time_ms: 14441.976
    num_steps_sampled: 74880000
    num_steps_trained: 74880000
    sample_time_ms: 100890.576
    update_time_ms: 13.783
  iterations_since_restore: 130
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.55303867403315
    ram_util_percent: 15.172928176795581
  pid: 28385
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 428.0
    agent-3: 428.0
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 247.255
    agent-1: 247.255
    agent-2: 396.185
    agent-3: 396.185
    agent-4: 234.775
    agent-5: 234.775
  policy_reward_min:
    agent-0: 204.5
    agent-1: 204.5
    agent-2: 330.5
    agent-3: 330.5
    agent-4: 199.5
    agent-5: 199.5
  sampler_perf:
    mean_env_wait_ms: 26.611024786302306
    mean_inference_ms: 12.963828820684698
    mean_processing_ms: 59.69628417037802
  time_since_restore: 16617.834727287292
  time_this_iter_s: 127.09829902648926
  time_total_s: 98750.27709746361
  timestamp: 1637610590
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 74880000
  training_iteration: 780
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    780 |          98750.3 | 74880000 |  1756.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.08
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 31.3
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 355.57
    apples_agent-2_min: 108
    apples_agent-3_max: 271
    apples_agent-3_mean: 226.55
    apples_agent-3_min: 157
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 381.89
    apples_agent-5_min: 292
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 391.95
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.54
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 19.87
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 420.39
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-51-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1876.0
  episode_reward_mean: 1738.76
  episode_reward_min: 1305.0
  episodes_this_iter: 96
  episodes_total: 74976
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11609.227
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34296461939811707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015952716348692775
        model: {}
        policy_loss: -0.0012983661144971848
        total_loss: 0.0015286304987967014
        vf_explained_var: 0.020244672894477844
        vf_loss: 34.30612564086914
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21881185472011566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011395681649446487
        model: {}
        policy_loss: -0.0018233356531709433
        total_loss: 0.0008613299578428268
        vf_explained_var: 0.12905405461788177
        vf_loss: 30.697738647460938
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2734887897968292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009013033704832196
        model: {}
        policy_loss: -0.0019529415294528008
        total_loss: 0.006028924137353897
        vf_explained_var: 0.06111478805541992
        vf_loss: 84.63204956054688
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5215082764625549
        entropy_coeff: 0.0017600000137463212
        kl: 0.000530977500602603
        model: {}
        policy_loss: -0.0014458219520747662
        total_loss: 0.0060720667243003845
        vf_explained_var: 0.06393687427043915
        vf_loss: 84.35745239257812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9275085926055908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023791445419192314
        model: {}
        policy_loss: -0.0019021178595721722
        total_loss: -0.0005413363687694073
        vf_explained_var: 0.014853671193122864
        vf_loss: 29.931962966918945
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25174611806869507
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008037753868848085
        model: {}
        policy_loss: -0.00180909875780344
        total_loss: 0.0003921128809452057
        vf_explained_var: 0.12826602160930634
        vf_loss: 26.442867279052734
    load_time_ms: 14427.399
    num_steps_sampled: 74976000
    num_steps_trained: 74976000
    sample_time_ms: 100800.914
    update_time_ms: 13.8
  iterations_since_restore: 131
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.10655737704918
    ram_util_percent: 15.226775956284152
  pid: 28385
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 436.5
    agent-3: 436.5
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 243.015
    agent-1: 243.015
    agent-2: 393.945
    agent-3: 393.945
    agent-4: 232.42
    agent-5: 232.42
  policy_reward_min:
    agent-0: 148.5
    agent-1: 148.5
    agent-2: 214.0
    agent-3: 214.0
    agent-4: 178.5
    agent-5: 178.5
  sampler_perf:
    mean_env_wait_ms: 26.60764010083642
    mean_inference_ms: 12.963164880188124
    mean_processing_ms: 59.69330225667408
  time_since_restore: 16744.551915884018
  time_this_iter_s: 126.71718859672546
  time_total_s: 98876.99428606033
  timestamp: 1637610719
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 74976000
  training_iteration: 781
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    781 |            98877 | 74976000 |  1738.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 31.27
    apples_agent-1_min: 9
    apples_agent-2_max: 397
    apples_agent-2_mean: 349.93
    apples_agent-2_min: 65
    apples_agent-3_max: 272
    apples_agent-3_mean: 219.01
    apples_agent-3_min: 57
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 377.86
    apples_agent-5_min: 92
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 399.34
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 2.3
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 2.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 20.56
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 515
    cleaning_beam_agent-4_mean: 417.19
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 4.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-54-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1837.0
  episode_reward_mean: 1714.13
  episode_reward_min: 400.0
  episodes_this_iter: 96
  episodes_total: 75072
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11614.071
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34528863430023193
        entropy_coeff: 0.0017600000137463212
        kl: 0.001431977842003107
        model: {}
        policy_loss: -0.0012769545428454876
        total_loss: 0.0015697944909334183
        vf_explained_var: 0.07780539989471436
        vf_loss: 34.54457092285156
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22239145636558533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008225755300372839
        model: {}
        policy_loss: -0.0018761157989501953
        total_loss: 0.000994957867078483
        vf_explained_var: 0.1384282261133194
        vf_loss: 32.6248664855957
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2801569104194641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009829321643337607
        model: {}
        policy_loss: -0.002289840718731284
        total_loss: 0.005969103891402483
        vf_explained_var: 0.08404220640659332
        vf_loss: 87.5202407836914
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5329262614250183
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009391639032401145
        model: {}
        policy_loss: -0.0019434469286352396
        total_loss: 0.005994119681417942
        vf_explained_var: 0.07005240023136139
        vf_loss: 88.75518035888672
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9102871417999268
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020541162230074406
        model: {}
        policy_loss: -0.0023077738005667925
        total_loss: -0.0005565833416767418
        vf_explained_var: 0.017177432775497437
        vf_loss: 33.532962799072266
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26395905017852783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009742820402607322
        model: {}
        policy_loss: -0.002051878720521927
        total_loss: 0.00042937789112329483
        vf_explained_var: 0.13575246930122375
        vf_loss: 29.45824432373047
    load_time_ms: 14402.269
    num_steps_sampled: 75072000
    num_steps_trained: 75072000
    sample_time_ms: 100690.165
    update_time_ms: 13.625
  iterations_since_restore: 132
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.40773480662983
    ram_util_percent: 15.221546961325972
  pid: 28385
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 436.5
    agent-3: 436.5
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 240.34
    agent-1: 240.34
    agent-2: 386.55
    agent-3: 386.55
    agent-4: 230.175
    agent-5: 230.175
  policy_reward_min:
    agent-0: 65.5
    agent-1: 65.5
    agent-2: 83.0
    agent-3: 83.0
    agent-4: 51.5
    agent-5: 51.5
  sampler_perf:
    mean_env_wait_ms: 26.60608761758994
    mean_inference_ms: 12.963338658704561
    mean_processing_ms: 59.69177777999791
  time_since_restore: 16871.355457544327
  time_this_iter_s: 126.80354166030884
  time_total_s: 99003.79782772064
  timestamp: 1637610846
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 75072000
  training_iteration: 782
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    782 |          99003.8 | 75072000 |  1714.13 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.11
    apples_agent-1_min: 10
    apples_agent-2_max: 412
    apples_agent-2_mean: 354.68
    apples_agent-2_min: 83
    apples_agent-3_max: 280
    apples_agent-3_mean: 227.23
    apples_agent-3_min: 56
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 382.19
    apples_agent-5_min: 88
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 394.1
    cleaning_beam_agent-0_min: 309
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.66
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.3
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 20.8
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 425.42
    cleaning_beam_agent-4_min: 367
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-56-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1885.0
  episode_reward_mean: 1731.34
  episode_reward_min: 432.0
  episodes_this_iter: 96
  episodes_total: 75168
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11616.459
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3404639959335327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013332848902791739
        model: {}
        policy_loss: -0.001430707168765366
        total_loss: 0.001333905616775155
        vf_explained_var: 0.08733981847763062
        vf_loss: 33.63833236694336
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2209216058254242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010591173777356744
        model: {}
        policy_loss: -0.001977271866053343
        total_loss: 0.0007982430979609489
        vf_explained_var: 0.14406722784042358
        vf_loss: 31.643373489379883
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27700719237327576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010321345180273056
        model: {}
        policy_loss: -0.0020580184645950794
        total_loss: 0.006153164431452751
        vf_explained_var: 0.0843224823474884
        vf_loss: 86.98714447021484
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5248773097991943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011247643269598484
        model: {}
        policy_loss: -0.0015109435189515352
        total_loss: 0.006315568462014198
        vf_explained_var: 0.08019588887691498
        vf_loss: 87.50296020507812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9152352809906006
        entropy_coeff: 0.0017600000137463212
        kl: 0.002652746392413974
        model: {}
        policy_loss: -0.0018472580704838037
        total_loss: -3.8291094824671745e-05
        vf_explained_var: 0.0037328749895095825
        vf_loss: 34.19780731201172
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27173754572868347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009545239736326039
        model: {}
        policy_loss: -0.0020051165483891964
        total_loss: 0.0005487690214067698
        vf_explained_var: 0.12030087411403656
        vf_loss: 30.321426391601562
    load_time_ms: 14395.821
    num_steps_sampled: 75168000
    num_steps_trained: 75168000
    sample_time_ms: 100731.963
    update_time_ms: 13.681
  iterations_since_restore: 133
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.86519337016574
    ram_util_percent: 15.220441988950277
  pid: 28385
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 445.0
    agent-3: 445.0
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 241.88
    agent-1: 241.88
    agent-2: 393.7
    agent-3: 393.7
    agent-4: 230.09
    agent-5: 230.09
  policy_reward_min:
    agent-0: 58.5
    agent-1: 58.5
    agent-2: 101.0
    agent-3: 101.0
    agent-4: 56.5
    agent-5: 56.5
  sampler_perf:
    mean_env_wait_ms: 26.604266251341286
    mean_inference_ms: 12.96367648378957
    mean_processing_ms: 59.691506291067384
  time_since_restore: 16998.40467953682
  time_this_iter_s: 127.04922199249268
  time_total_s: 99130.84704971313
  timestamp: 1637610973
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 75168000
  training_iteration: 783
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    783 |          99130.8 | 75168000 |  1731.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 31.2
    apples_agent-1_min: 4
    apples_agent-2_max: 403
    apples_agent-2_mean: 348.11
    apples_agent-2_min: 67
    apples_agent-3_max: 280
    apples_agent-3_mean: 223.98
    apples_agent-3_min: 45
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 377.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 393.31
    cleaning_beam_agent-0_min: 165
    cleaning_beam_agent-1_max: 15
    cleaning_beam_agent-1_mean: 1.83
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 2.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 19.28
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 432.49
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 3.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_14-58-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1847.0
  episode_reward_mean: 1718.31
  episode_reward_min: 274.0
  episodes_this_iter: 96
  episodes_total: 75264
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11626.345
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34101182222366333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013195924693718553
        model: {}
        policy_loss: -0.0013004799839109182
        total_loss: 0.0015351916663348675
        vf_explained_var: 0.06932874023914337
        vf_loss: 34.35854721069336
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21831786632537842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008761867647990584
        model: {}
        policy_loss: -0.0018024379387497902
        total_loss: 0.0009872815571725368
        vf_explained_var: 0.13962365686893463
        vf_loss: 31.73958396911621
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2747402787208557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010313078528270125
        model: {}
        policy_loss: -0.002062950748950243
        total_loss: 0.006407255306839943
        vf_explained_var: 0.07275080680847168
        vf_loss: 89.53749084472656
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5356522798538208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017851502634584904
        model: {}
        policy_loss: -0.0019351928494870663
        total_loss: 0.006270409096032381
        vf_explained_var: 0.052803829312324524
        vf_loss: 91.48346710205078
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9218205213546753
        entropy_coeff: 0.0017600000137463212
        kl: 0.00188622553832829
        model: {}
        policy_loss: -0.0016934736631810665
        total_loss: 6.564287468791008e-05
        vf_explained_var: 0.028585880994796753
        vf_loss: 33.81520080566406
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2655128836631775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009718649089336395
        model: {}
        policy_loss: -0.0018874453380703926
        total_loss: 0.0008149412460625172
        vf_explained_var: 0.08825977146625519
        vf_loss: 31.696908950805664
    load_time_ms: 14383.919
    num_steps_sampled: 75264000
    num_steps_trained: 75264000
    sample_time_ms: 100682.901
    update_time_ms: 13.488
  iterations_since_restore: 134
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.95690607734807
    ram_util_percent: 15.196132596685082
  pid: 28385
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 433.0
    agent-3: 433.0
    agent-4: 255.5
    agent-5: 255.5
  policy_reward_mean:
    agent-0: 240.38
    agent-1: 240.38
    agent-2: 389.555
    agent-3: 389.555
    agent-4: 229.22
    agent-5: 229.22
  policy_reward_min:
    agent-0: 53.5
    agent-1: 53.5
    agent-2: 83.5
    agent-3: 83.5
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 26.602197184973015
    mean_inference_ms: 12.96376230643504
    mean_processing_ms: 59.689120461197525
  time_since_restore: 17125.2469022274
  time_this_iter_s: 126.84222269058228
  time_total_s: 99257.68927240372
  timestamp: 1637611100
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 75264000
  training_iteration: 784
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    784 |          99257.7 | 75264000 |  1718.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 5
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.33
    apples_agent-1_min: 14
    apples_agent-2_max: 408
    apples_agent-2_mean: 357.36
    apples_agent-2_min: 208
    apples_agent-3_max: 278
    apples_agent-3_mean: 225.56
    apples_agent-3_min: 140
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 382.39
    apples_agent-5_min: 248
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 393.29
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 19.49
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 438.55
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 4.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-00-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1844.0
  episode_reward_mean: 1747.38
  episode_reward_min: 1101.0
  episodes_this_iter: 96
  episodes_total: 75360
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11622.286
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34262317419052124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008568163029849529
        model: {}
        policy_loss: -0.0011525172740221024
        total_loss: 0.0015019206330180168
        vf_explained_var: 0.04223458468914032
        vf_loss: 32.57453918457031
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21769919991493225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011832233285531402
        model: {}
        policy_loss: -0.0018041208386421204
        total_loss: 0.0008263986092060804
        vf_explained_var: 0.11373040080070496
        vf_loss: 30.136695861816406
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27433910965919495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007809913950040936
        model: {}
        policy_loss: -0.0016143866814672947
        total_loss: 0.006532721221446991
        vf_explained_var: 0.04239015281200409
        vf_loss: 86.29944610595703
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5359988808631897
        entropy_coeff: 0.0017600000137463212
        kl: 0.001000391086563468
        model: {}
        policy_loss: -0.001643225084990263
        total_loss: 0.0059028263203799725
        vf_explained_var: 0.060401931405067444
        vf_loss: 84.89413452148438
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9145395755767822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014560926938429475
        model: {}
        policy_loss: -0.0016693477518856525
        total_loss: -0.00012135878205299377
        vf_explained_var: 0.010830312967300415
        vf_loss: 31.57582664489746
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26906439661979675
        entropy_coeff: 0.0017600000137463212
        kl: 0.000967273605056107
        model: {}
        policy_loss: -0.0019342396408319473
        total_loss: 0.0005988273769617081
        vf_explained_var: 0.05830173194408417
        vf_loss: 30.066200256347656
    load_time_ms: 14385.331
    num_steps_sampled: 75360000
    num_steps_trained: 75360000
    sample_time_ms: 100753.465
    update_time_ms: 13.462
  iterations_since_restore: 135
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 32.23480662983425
    ram_util_percent: 15.249723756906082
  pid: 28385
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 257.5
    agent-5: 257.5
  policy_reward_mean:
    agent-0: 243.935
    agent-1: 243.935
    agent-2: 396.475
    agent-3: 396.475
    agent-4: 233.28
    agent-5: 233.28
  policy_reward_min:
    agent-0: 153.5
    agent-1: 153.5
    agent-2: 246.0
    agent-3: 246.0
    agent-4: 151.0
    agent-5: 151.0
  sampler_perf:
    mean_env_wait_ms: 26.60111301856945
    mean_inference_ms: 12.96432219811024
    mean_processing_ms: 59.68826894516242
  time_since_restore: 17252.248816728592
  time_this_iter_s: 127.00191450119019
  time_total_s: 99384.6911869049
  timestamp: 1637611227
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 75360000
  training_iteration: 785
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    785 |          99384.7 | 75360000 |  1747.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 32.43
    apples_agent-1_min: 16
    apples_agent-2_max: 412
    apples_agent-2_mean: 360.11
    apples_agent-2_min: 300
    apples_agent-3_max: 274
    apples_agent-3_mean: 222.78
    apples_agent-3_min: 158
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 383.85
    apples_agent-5_min: 327
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 384.93
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.4
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 19.18
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 439.73
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-02-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1846.0
  episode_reward_mean: 1748.47
  episode_reward_min: 1512.0
  episodes_this_iter: 96
  episodes_total: 75456
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11611.917
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.334491491317749
        entropy_coeff: 0.0017600000137463212
        kl: 0.002118831966072321
        model: {}
        policy_loss: -0.0013677207753062248
        total_loss: 0.0012356366496533155
        vf_explained_var: 0.03148351609706879
        vf_loss: 31.92059326171875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21388553082942963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011966979363933206
        model: {}
        policy_loss: -0.0016268840990960598
        total_loss: 0.00094615388661623
        vf_explained_var: 0.10542288422584534
        vf_loss: 29.49476432800293
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.272180438041687
        entropy_coeff: 0.0017600000137463212
        kl: 0.001194440177641809
        model: {}
        policy_loss: -0.0015814932994544506
        total_loss: 0.006144118495285511
        vf_explained_var: 0.02683427929878235
        vf_loss: 82.04649353027344
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5407887697219849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012768497690558434
        model: {}
        policy_loss: -0.0016957382904365659
        total_loss: 0.005198570434004068
        vf_explained_var: 0.0719221979379654
        vf_loss: 78.46096801757812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9111766219139099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014742916682735085
        model: {}
        policy_loss: -0.0017758999019861221
        total_loss: -0.00035608280450105667
        vf_explained_var: 0.0075104087591171265
        vf_loss: 30.23486328125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2666594982147217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008614463731646538
        model: {}
        policy_loss: -0.0017431223532184958
        total_loss: 0.0005610406515188515
        vf_explained_var: 0.08881951868534088
        vf_loss: 27.73482322692871
    load_time_ms: 14366.029
    num_steps_sampled: 75456000
    num_steps_trained: 75456000
    sample_time_ms: 100805.006
    update_time_ms: 13.495
  iterations_since_restore: 136
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.94111111111111
    ram_util_percent: 15.263333333333334
  pid: 28385
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 433.5
    agent-3: 433.5
    agent-4: 252.0
    agent-5: 252.0
  policy_reward_mean:
    agent-0: 246.185
    agent-1: 246.185
    agent-2: 395.96
    agent-3: 395.96
    agent-4: 232.09
    agent-5: 232.09
  policy_reward_min:
    agent-0: 197.5
    agent-1: 197.5
    agent-2: 343.5
    agent-3: 343.5
    agent-4: 208.5
    agent-5: 208.5
  sampler_perf:
    mean_env_wait_ms: 26.599869292465964
    mean_inference_ms: 12.964272156695193
    mean_processing_ms: 59.68643429425124
  time_since_restore: 17378.829187631607
  time_this_iter_s: 126.58037090301514
  time_total_s: 99511.27155780792
  timestamp: 1637611354
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 75456000
  training_iteration: 786
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    786 |          99511.3 | 75456000 |  1748.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 30.82
    apples_agent-1_min: 17
    apples_agent-2_max: 406
    apples_agent-2_mean: 359.82
    apples_agent-2_min: 288
    apples_agent-3_max: 265
    apples_agent-3_mean: 224.77
    apples_agent-3_min: 154
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 429
    apples_agent-5_mean: 386.03
    apples_agent-5_min: 327
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 393.11
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.7
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 18.32
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 435.91
    cleaning_beam_agent-4_min: 382
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-04-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1865.0
  episode_reward_mean: 1757.59
  episode_reward_min: 1541.0
  episodes_this_iter: 96
  episodes_total: 75552
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11614.352
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33177924156188965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015420448035001755
        model: {}
        policy_loss: -0.001325999153777957
        total_loss: 0.0013821248430758715
        vf_explained_var: 0.016926437616348267
        vf_loss: 32.9205322265625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2161898910999298
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009943359764292836
        model: {}
        policy_loss: -0.0017206890042871237
        total_loss: 0.0009867518674582243
        vf_explained_var: 0.07745109498500824
        vf_loss: 30.879344940185547
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2726742625236511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007715545361861587
        model: {}
        policy_loss: -0.0016166763380169868
        total_loss: 0.0061293127946555614
        vf_explained_var: 0.031260907649993896
        vf_loss: 82.25897216796875
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5423144102096558
        entropy_coeff: 0.0017600000137463212
        kl: 0.001809888519346714
        model: {}
        policy_loss: -0.001663782517425716
        total_loss: 0.0052575175650417805
        vf_explained_var: 0.07461005449295044
        vf_loss: 78.75774383544922
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9144204258918762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019574472680687904
        model: {}
        policy_loss: -0.001816715463064611
        total_loss: -0.00024185405345633626
        vf_explained_var: 0.006181791424751282
        vf_loss: 31.84244155883789
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2540668547153473
        entropy_coeff: 0.0017600000137463212
        kl: 0.001502257538959384
        model: {}
        policy_loss: -0.0018940275767818093
        total_loss: 0.0005769997369498014
        vf_explained_var: 0.08575423061847687
        vf_loss: 29.181888580322266
    load_time_ms: 14400.718
    num_steps_sampled: 75552000
    num_steps_trained: 75552000
    sample_time_ms: 100672.11
    update_time_ms: 13.198
  iterations_since_restore: 137
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.711049723756904
    ram_util_percent: 15.238121546961327
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 437.0
    agent-3: 437.0
    agent-4: 267.0
    agent-5: 267.0
  policy_reward_mean:
    agent-0: 245.385
    agent-1: 245.385
    agent-2: 397.115
    agent-3: 397.115
    agent-4: 236.295
    agent-5: 236.295
  policy_reward_min:
    agent-0: 208.0
    agent-1: 208.0
    agent-2: 356.0
    agent-3: 356.0
    agent-4: 190.5
    agent-5: 190.5
  sampler_perf:
    mean_env_wait_ms: 26.598372979285553
    mean_inference_ms: 12.96423451779276
    mean_processing_ms: 59.68433435616731
  time_since_restore: 17505.892112016678
  time_this_iter_s: 127.0629243850708
  time_total_s: 99638.334482193
  timestamp: 1637611481
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 75552000
  training_iteration: 787
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    787 |          99638.3 | 75552000 |  1757.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.3
    apples_agent-1_min: 10
    apples_agent-2_max: 414
    apples_agent-2_mean: 357.76
    apples_agent-2_min: 106
    apples_agent-3_max: 266
    apples_agent-3_mean: 220.82
    apples_agent-3_min: 69
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 380.77
    apples_agent-5_min: 95
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 389.53
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 18.84
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 440.2
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-06-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1855.0
  episode_reward_mean: 1745.03
  episode_reward_min: 477.0
  episodes_this_iter: 96
  episodes_total: 75648
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11604.791
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33650097250938416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010210657492280006
        model: {}
        policy_loss: -0.0014680861495435238
        total_loss: 0.0014279000461101532
        vf_explained_var: 0.07075321674346924
        vf_loss: 34.882266998291016
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21800680458545685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006741398829035461
        model: {}
        policy_loss: -0.0018969159573316574
        total_loss: 0.0008699249010533094
        vf_explained_var: 0.16176773607730865
        vf_loss: 31.505332946777344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27290773391723633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011828952701762319
        model: {}
        policy_loss: -0.0018901461735367775
        total_loss: 0.00612260214984417
        vf_explained_var: 0.06980907917022705
        vf_loss: 84.93064880371094
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5611315369606018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021414884831756353
        model: {}
        policy_loss: -0.0019132938468828797
        total_loss: 0.005763415712863207
        vf_explained_var: 0.053227558732032776
        vf_loss: 86.64300537109375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9020630121231079
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017479750094935298
        model: {}
        policy_loss: -0.0018425027374178171
        total_loss: -0.00012911995872855186
        vf_explained_var: -0.005413919687271118
        vf_loss: 33.01012420654297
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2581050992012024
        entropy_coeff: 0.0017600000137463212
        kl: 0.001162853091955185
        model: {}
        policy_loss: -0.001983784604817629
        total_loss: 0.0005111617501825094
        vf_explained_var: 0.10074859857559204
        vf_loss: 29.49212646484375
    load_time_ms: 14389.676
    num_steps_sampled: 75648000
    num_steps_trained: 75648000
    sample_time_ms: 100747.026
    update_time_ms: 13.374
  iterations_since_restore: 138
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.738674033149174
    ram_util_percent: 15.193370165745856
  pid: 28385
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 424.5
    agent-3: 424.5
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 245.76
    agent-1: 245.76
    agent-2: 394.45
    agent-3: 394.45
    agent-4: 232.305
    agent-5: 232.305
  policy_reward_min:
    agent-0: 51.0
    agent-1: 51.0
    agent-2: 126.0
    agent-3: 126.0
    agent-4: 61.5
    agent-5: 61.5
  sampler_perf:
    mean_env_wait_ms: 26.597425597044722
    mean_inference_ms: 12.964121860672035
    mean_processing_ms: 59.68252954471735
  time_since_restore: 17633.1129758358
  time_this_iter_s: 127.22086381912231
  time_total_s: 99765.55534601212
  timestamp: 1637611608
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 75648000
  training_iteration: 788
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    788 |          99765.6 | 75648000 |  1745.03 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 32.11
    apples_agent-1_min: 18
    apples_agent-2_max: 422
    apples_agent-2_mean: 361.44
    apples_agent-2_min: 245
    apples_agent-3_max: 287
    apples_agent-3_mean: 225.05
    apples_agent-3_min: 115
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 382.46
    apples_agent-5_min: 264
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 388.36
    cleaning_beam_agent-0_min: 348
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.39
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 19.13
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 447.38
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 4.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-08-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1882.0
  episode_reward_mean: 1751.24
  episode_reward_min: 1187.0
  episodes_this_iter: 96
  episodes_total: 75744
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11606.746
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33060866594314575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016431771218776703
        model: {}
        policy_loss: -0.0013204584829509258
        total_loss: 0.001487232744693756
        vf_explained_var: 0.021363601088523865
        vf_loss: 33.89561462402344
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21703147888183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008869083831086755
        model: {}
        policy_loss: -0.0015575909055769444
        total_loss: 0.0010983040556311607
        vf_explained_var: 0.12577305734157562
        vf_loss: 30.378719329833984
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2711353003978729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009651436703279614
        model: {}
        policy_loss: -0.001818440854549408
        total_loss: 0.006258895620703697
        vf_explained_var: 0.03862610459327698
        vf_loss: 85.54536437988281
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5420714020729065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024205073714256287
        model: {}
        policy_loss: -0.0018231624271720648
        total_loss: 0.005649744998663664
        vf_explained_var: 0.05489903688430786
        vf_loss: 84.26953887939453
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9113529920578003
        entropy_coeff: 0.0017600000137463212
        kl: 0.003277804935351014
        model: {}
        policy_loss: -0.0020890035666525364
        total_loss: -0.0005220368038862944
        vf_explained_var: 0.03699275851249695
        vf_loss: 31.7094669342041
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25807657837867737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006984316860325634
        model: {}
        policy_loss: -0.0018341380637139082
        total_loss: 0.0007293475791811943
        vf_explained_var: 0.08169791102409363
        vf_loss: 30.177040100097656
    load_time_ms: 14394.464
    num_steps_sampled: 75744000
    num_steps_trained: 75744000
    sample_time_ms: 100818.922
    update_time_ms: 13.406
  iterations_since_restore: 139
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.310497237569066
    ram_util_percent: 15.251381215469616
  pid: 28385
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 433.5
    agent-3: 433.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 245.945
    agent-1: 245.945
    agent-2: 395.8
    agent-3: 395.8
    agent-4: 233.875
    agent-5: 233.875
  policy_reward_min:
    agent-0: 146.5
    agent-1: 146.5
    agent-2: 266.0
    agent-3: 266.0
    agent-4: 160.0
    agent-5: 160.0
  sampler_perf:
    mean_env_wait_ms: 26.595033049970787
    mean_inference_ms: 12.964067225620456
    mean_processing_ms: 59.68271128997497
  time_since_restore: 17759.874530553818
  time_this_iter_s: 126.76155471801758
  time_total_s: 99892.31690073013
  timestamp: 1637611735
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 75744000
  training_iteration: 789
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    789 |          99892.3 | 75744000 |  1751.24 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.58
    apples_agent-1_min: 16
    apples_agent-2_max: 403
    apples_agent-2_mean: 360.98
    apples_agent-2_min: 310
    apples_agent-3_max: 281
    apples_agent-3_mean: 225.47
    apples_agent-3_min: 160
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 388.08
    apples_agent-5_min: 340
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 391.68
    cleaning_beam_agent-0_min: 355
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.43
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.72
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 18.02
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 434.9
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 4.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-11-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1903.0
  episode_reward_mean: 1760.94
  episode_reward_min: 1617.0
  episodes_this_iter: 96
  episodes_total: 75840
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11610.944
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32178109884262085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018986831419169903
        model: {}
        policy_loss: -0.0008751231944188476
        total_loss: 0.0017256613355129957
        vf_explained_var: 0.008512571454048157
        vf_loss: 31.67121124267578
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21498921513557434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007774580735713243
        model: {}
        policy_loss: -0.0015615178272128105
        total_loss: 0.0009561171755194664
        vf_explained_var: 0.09423157572746277
        vf_loss: 28.960193634033203
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2689109742641449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009458028362132609
        model: {}
        policy_loss: -0.0018550741951912642
        total_loss: 0.006061364896595478
        vf_explained_var: 0.005709260702133179
        vf_loss: 83.89722442626953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5297204256057739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010660277912393212
        model: {}
        policy_loss: -0.0016989556606858969
        total_loss: 0.0054517765529453754
        vf_explained_var: 0.04689106345176697
        vf_loss: 80.83038330078125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.913345217704773
        entropy_coeff: 0.0017600000137463212
        kl: 0.002629640744999051
        model: {}
        policy_loss: -0.0024615717120468616
        total_loss: -0.0010344113688915968
        vf_explained_var: 0.0034582167863845825
        vf_loss: 30.34649085998535
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2528812885284424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007393399719148874
        model: {}
        policy_loss: -0.0015536543214693666
        total_loss: 0.0008108763722702861
        vf_explained_var: 0.07082386314868927
        vf_loss: 28.09603500366211
    load_time_ms: 14413.688
    num_steps_sampled: 75840000
    num_steps_trained: 75840000
    sample_time_ms: 100811.833
    update_time_ms: 13.493
  iterations_since_restore: 140
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.366298342541437
    ram_util_percent: 15.193370165745856
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 267.0
    agent-5: 267.0
  policy_reward_mean:
    agent-0: 245.94
    agent-1: 245.94
    agent-2: 398.04
    agent-3: 398.04
    agent-4: 236.49
    agent-5: 236.49
  policy_reward_min:
    agent-0: 206.5
    agent-1: 206.5
    agent-2: 348.5
    agent-3: 348.5
    agent-4: 208.5
    agent-5: 208.5
  sampler_perf:
    mean_env_wait_ms: 26.593965230524706
    mean_inference_ms: 12.964329515975951
    mean_processing_ms: 59.68330963880061
  time_since_restore: 17887.18457698822
  time_this_iter_s: 127.31004643440247
  time_total_s: 100019.62694716454
  timestamp: 1637611863
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 75840000
  training_iteration: 790
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    790 |           100020 | 75840000 |  1760.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.13
    apples_agent-1_min: 11
    apples_agent-2_max: 404
    apples_agent-2_mean: 356.91
    apples_agent-2_min: 244
    apples_agent-3_max: 285
    apples_agent-3_mean: 224.95
    apples_agent-3_min: 154
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 378.2
    apples_agent-5_min: 249
    cleaning_beam_agent-0_max: 414
    cleaning_beam_agent-0_mean: 377.06
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 19.24
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 426.47
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-13-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1852.0
  episode_reward_mean: 1736.68
  episode_reward_min: 1203.0
  episodes_this_iter: 96
  episodes_total: 75936
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11596.451
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3232976496219635
        entropy_coeff: 0.0017600000137463212
        kl: 0.001793046249076724
        model: {}
        policy_loss: -0.0015000593848526478
        total_loss: 0.0011955280788242817
        vf_explained_var: 0.03242260217666626
        vf_loss: 32.645957946777344
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21260660886764526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011858975049108267
        model: {}
        policy_loss: -0.001881847158074379
        total_loss: 0.0008154337992891669
        vf_explained_var: 0.08876453340053558
        vf_loss: 30.71468734741211
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2743111252784729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009949528612196445
        model: {}
        policy_loss: -0.0017599700950086117
        total_loss: 0.006058968137949705
        vf_explained_var: 0.056043773889541626
        vf_loss: 83.01728820800781
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5359888672828674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008596580591984093
        model: {}
        policy_loss: -0.001933269202709198
        total_loss: 0.0054035913199186325
        vf_explained_var: 0.05800780653953552
        vf_loss: 82.80200958251953
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9134801030158997
        entropy_coeff: 0.0017600000137463212
        kl: 0.002886998001486063
        model: {}
        policy_loss: -0.001968138851225376
        total_loss: -0.0005679756868630648
        vf_explained_var: 0.024893000721931458
        vf_loss: 30.078866958618164
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26252344250679016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008597082924097776
        model: {}
        policy_loss: -0.0017902799881994724
        total_loss: 0.0006421785801649094
        vf_explained_var: 0.06362581253051758
        vf_loss: 28.945003509521484
    load_time_ms: 14408.656
    num_steps_sampled: 75936000
    num_steps_trained: 75936000
    sample_time_ms: 100802.947
    update_time_ms: 13.744
  iterations_since_restore: 141
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.182513661202183
    ram_util_percent: 14.951366120218578
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 433.0
    agent-3: 433.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 243.37
    agent-1: 243.37
    agent-2: 393.655
    agent-3: 393.655
    agent-4: 231.315
    agent-5: 231.315
  policy_reward_min:
    agent-0: 169.0
    agent-1: 169.0
    agent-2: 285.5
    agent-3: 285.5
    agent-4: 147.0
    agent-5: 147.0
  sampler_perf:
    mean_env_wait_ms: 26.591563651800808
    mean_inference_ms: 12.964834851271682
    mean_processing_ms: 59.68186426980692
  time_since_restore: 18013.592451810837
  time_this_iter_s: 126.40787482261658
  time_total_s: 100146.03482198715
  timestamp: 1637611991
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 75936000
  training_iteration: 791
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    791 |           100146 | 75936000 |  1736.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 31.37
    apples_agent-1_min: 17
    apples_agent-2_max: 412
    apples_agent-2_mean: 358.66
    apples_agent-2_min: 263
    apples_agent-3_max: 270
    apples_agent-3_mean: 224.68
    apples_agent-3_min: 172
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 380.59
    apples_agent-5_min: 275
    cleaning_beam_agent-0_max: 400
    cleaning_beam_agent-0_mean: 369.19
    cleaning_beam_agent-0_min: 306
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.3
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 18.38
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 421.87
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-15-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1916.0
  episode_reward_mean: 1742.88
  episode_reward_min: 1264.0
  episodes_this_iter: 96
  episodes_total: 76032
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11588.57
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3275891840457916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009041496668942273
        model: {}
        policy_loss: -0.0009635654278099537
        total_loss: 0.0018325871787965298
        vf_explained_var: 0.03411012887954712
        vf_loss: 33.72707748413086
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2140059769153595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009040540317073464
        model: {}
        policy_loss: -0.0017230878584086895
        total_loss: 0.0010517267510294914
        vf_explained_var: 0.09811794757843018
        vf_loss: 31.514667510986328
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2700478434562683
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009339295793324709
        model: {}
        policy_loss: -0.0018531987443566322
        total_loss: 0.006229951977729797
        vf_explained_var: 0.050626009702682495
        vf_loss: 85.58436584472656
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5526489019393921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009299618541263044
        model: {}
        policy_loss: -0.001861083204858005
        total_loss: 0.00555725721642375
        vf_explained_var: 0.0695091038942337
        vf_loss: 83.91006469726562
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9105122685432434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013984071556478739
        model: {}
        policy_loss: -0.001815124647691846
        total_loss: -0.00027365441201254725
        vf_explained_var: 0.007733583450317383
        vf_loss: 31.43972396850586
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.265272319316864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010893689468502998
        model: {}
        policy_loss: -0.0015779766254127026
        total_loss: 0.0008425670675933361
        vf_explained_var: 0.08761279284954071
        vf_loss: 28.874225616455078
    load_time_ms: 14445.184
    num_steps_sampled: 76032000
    num_steps_trained: 76032000
    sample_time_ms: 100781.795
    update_time_ms: 13.798
  iterations_since_restore: 142
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.31988950276243
    ram_util_percent: 14.108287292817677
  pid: 28385
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 436.0
    agent-3: 436.0
    agent-4: 262.5
    agent-5: 262.5
  policy_reward_mean:
    agent-0: 242.69
    agent-1: 242.69
    agent-2: 395.42
    agent-3: 395.42
    agent-4: 233.33
    agent-5: 233.33
  policy_reward_min:
    agent-0: 173.5
    agent-1: 173.5
    agent-2: 286.0
    agent-3: 286.0
    agent-4: 172.5
    agent-5: 172.5
  sampler_perf:
    mean_env_wait_ms: 26.588536621837644
    mean_inference_ms: 12.964249641607863
    mean_processing_ms: 59.68136914222697
  time_since_restore: 18140.522741556168
  time_this_iter_s: 126.93028974533081
  time_total_s: 100272.96511173248
  timestamp: 1637612118
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 76032000
  training_iteration: 792
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    792 |           100273 | 76032000 |  1742.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 31.77
    apples_agent-1_min: 9
    apples_agent-2_max: 436
    apples_agent-2_mean: 360.23
    apples_agent-2_min: 83
    apples_agent-3_max: 268
    apples_agent-3_mean: 224.47
    apples_agent-3_min: 55
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 424
    apples_agent-5_mean: 380.69
    apples_agent-5_min: 113
    cleaning_beam_agent-0_max: 407
    cleaning_beam_agent-0_mean: 373.86
    cleaning_beam_agent-0_min: 268
    cleaning_beam_agent-1_max: 17
    cleaning_beam_agent-1_mean: 1.55
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 2.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 19.87
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 435.55
    cleaning_beam_agent-4_min: 371
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 4.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-17-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1872.0
  episode_reward_mean: 1739.83
  episode_reward_min: 452.0
  episodes_this_iter: 96
  episodes_total: 76128
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11577.919
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33078375458717346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013981242664158344
        model: {}
        policy_loss: -0.0015764444833621383
        total_loss: 0.0011848933063447475
        vf_explained_var: 0.07333928346633911
        vf_loss: 33.435203552246094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21652114391326904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009224690147675574
        model: {}
        policy_loss: -0.0017758305184543133
        total_loss: 0.0009966923389583826
        vf_explained_var: 0.12554627656936646
        vf_loss: 31.53598976135254
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27024853229522705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012011814396828413
        model: {}
        policy_loss: -0.0017556380480527878
        total_loss: 0.006412596441805363
        vf_explained_var: 0.09897121787071228
        vf_loss: 86.438720703125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5542886257171631
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007628733292222023
        model: {}
        policy_loss: -0.001596203539520502
        total_loss: 0.006532750092446804
        vf_explained_var: 0.05193972587585449
        vf_loss: 91.04500579833984
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8986979722976685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0043460954912006855
        model: {}
        policy_loss: -0.0024076728150248528
        total_loss: -0.0007486240356229246
        vf_explained_var: 0.00763268768787384
        vf_loss: 32.40755081176758
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26525402069091797
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009339613607153296
        model: {}
        policy_loss: -0.001852058689109981
        total_loss: 0.0006827769102528691
        vf_explained_var: 0.0814739465713501
        vf_loss: 30.016830444335938
    load_time_ms: 14460.39
    num_steps_sampled: 76128000
    num_steps_trained: 76128000
    sample_time_ms: 100839.053
    update_time_ms: 13.979
  iterations_since_restore: 143
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.589010989010994
    ram_util_percent: 14.060439560439558
  pid: 28385
  policy_reward_max:
    agent-0: 266.0
    agent-1: 266.0
    agent-2: 436.5
    agent-3: 436.5
    agent-4: 256.0
    agent-5: 256.0
  policy_reward_mean:
    agent-0: 241.19
    agent-1: 241.19
    agent-2: 396.805
    agent-3: 396.805
    agent-4: 231.92
    agent-5: 231.92
  policy_reward_min:
    agent-0: 62.0
    agent-1: 62.0
    agent-2: 97.0
    agent-3: 97.0
    agent-4: 67.0
    agent-5: 67.0
  sampler_perf:
    mean_env_wait_ms: 26.586818397933328
    mean_inference_ms: 12.963717855236814
    mean_processing_ms: 59.68399241016418
  time_since_restore: 18268.187762737274
  time_this_iter_s: 127.66502118110657
  time_total_s: 100400.63013291359
  timestamp: 1637612246
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 76128000
  training_iteration: 793
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    793 |           100401 | 76128000 |  1739.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 31.04
    apples_agent-1_min: 18
    apples_agent-2_max: 412
    apples_agent-2_mean: 361.56
    apples_agent-2_min: 252
    apples_agent-3_max: 268
    apples_agent-3_mean: 222.63
    apples_agent-3_min: 137
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 385.84
    apples_agent-5_min: 266
    cleaning_beam_agent-0_max: 396
    cleaning_beam_agent-0_mean: 367.94
    cleaning_beam_agent-0_min: 330
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.54
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 20.45
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 460.12
    cleaning_beam_agent-4_min: 414
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-19-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1861.0
  episode_reward_mean: 1755.56
  episode_reward_min: 1277.0
  episodes_this_iter: 96
  episodes_total: 76224
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11559.212
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3249084949493408
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016889270627871156
        model: {}
        policy_loss: -0.0013578543439507484
        total_loss: 0.0012501468881964684
        vf_explained_var: 0.03188765048980713
        vf_loss: 31.79837417602539
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2147839069366455
        entropy_coeff: 0.0017600000137463212
        kl: 0.001182478736154735
        model: {}
        policy_loss: -0.0016017495654523373
        total_loss: 0.0009842985309660435
        vf_explained_var: 0.10048060119152069
        vf_loss: 29.64069175720215
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2665516138076782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010851555271074176
        model: {}
        policy_loss: -0.0017570084892213345
        total_loss: 0.006311524659395218
        vf_explained_var: 0.032150134444236755
        vf_loss: 85.37664031982422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.560221791267395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010044798254966736
        model: {}
        policy_loss: -0.0016210037283599377
        total_loss: 0.005697683896869421
        vf_explained_var: 0.060628458857536316
        vf_loss: 83.04680633544922
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8976873159408569
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018851046916097403
        model: {}
        policy_loss: -0.0017210340593010187
        total_loss: -8.72565433382988e-05
        vf_explained_var: 0.012381210923194885
        vf_loss: 32.137081146240234
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25877076387405396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006881316076032817
        model: {}
        policy_loss: -0.0018823619466274977
        total_loss: 0.0006682472303509712
        vf_explained_var: 0.07673457264900208
        vf_loss: 30.06046485900879
    load_time_ms: 14446.3
    num_steps_sampled: 76224000
    num_steps_trained: 76224000
    sample_time_ms: 100807.214
    update_time_ms: 13.94
  iterations_since_restore: 144
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.411666666666665
    ram_util_percent: 14.094999999999999
  pid: 28385
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 269.5
    agent-5: 269.5
  policy_reward_mean:
    agent-0: 244.965
    agent-1: 244.965
    agent-2: 397.325
    agent-3: 397.325
    agent-4: 235.49
    agent-5: 235.49
  policy_reward_min:
    agent-0: 196.0
    agent-1: 196.0
    agent-2: 282.0
    agent-3: 282.0
    agent-4: 160.5
    agent-5: 160.5
  sampler_perf:
    mean_env_wait_ms: 26.58460743716042
    mean_inference_ms: 12.962427321876655
    mean_processing_ms: 59.680816532199614
  time_since_restore: 18394.340167999268
  time_this_iter_s: 126.15240526199341
  time_total_s: 100526.78253817558
  timestamp: 1637612372
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 76224000
  training_iteration: 794
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    794 |           100527 | 76224000 |  1755.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 66
    apples_agent-1_mean: 32.13
    apples_agent-1_min: 19
    apples_agent-2_max: 421
    apples_agent-2_mean: 366.81
    apples_agent-2_min: 313
    apples_agent-3_max: 277
    apples_agent-3_mean: 227.48
    apples_agent-3_min: 169
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 384.27
    apples_agent-5_min: 303
    cleaning_beam_agent-0_max: 409
    cleaning_beam_agent-0_mean: 375.81
    cleaning_beam_agent-0_min: 349
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.2
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 1.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 20.79
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 598
    cleaning_beam_agent-4_mean: 460.99
    cleaning_beam_agent-4_min: 386
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-21-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1920.0
  episode_reward_mean: 1765.61
  episode_reward_min: 1487.0
  episodes_this_iter: 96
  episodes_total: 76320
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11548.407
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3227303624153137
        entropy_coeff: 0.0017600000137463212
        kl: 0.001126969000324607
        model: {}
        policy_loss: -0.0012139729224145412
        total_loss: 0.001504505518823862
        vf_explained_var: 0.015322431921958923
        vf_loss: 32.8648567199707
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2140459269285202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008439728990197182
        model: {}
        policy_loss: -0.0016129408031702042
        total_loss: 0.0011115290690213442
        vf_explained_var: 0.0730835497379303
        vf_loss: 31.01186180114746
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26678580045700073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008248724625445902
        model: {}
        policy_loss: -0.0015719342045485973
        total_loss: 0.006539038382470608
        vf_explained_var: 0.01872788369655609
        vf_loss: 85.80516052246094
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5586638450622559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006105120410211384
        model: {}
        policy_loss: -0.0016265228623524308
        total_loss: 0.005615564063191414
        vf_explained_var: 0.06243954598903656
        vf_loss: 82.25338745117188
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8966774940490723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012739149387925863
        model: {}
        policy_loss: -0.0016500474885106087
        total_loss: -0.0002619614824652672
        vf_explained_var: 0.004951879382133484
        vf_loss: 29.662376403808594
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.257148802280426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008493574569001794
        model: {}
        policy_loss: -0.0015304358676075935
        total_loss: 0.000799485482275486
        vf_explained_var: 0.06474654376506805
        vf_loss: 27.825040817260742
    load_time_ms: 14433.636
    num_steps_sampled: 76320000
    num_steps_trained: 76320000
    sample_time_ms: 100773.982
    update_time_ms: 13.871
  iterations_since_restore: 145
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.19833333333333
    ram_util_percent: 14.037777777777777
  pid: 28385
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 442.0
    agent-3: 442.0
    agent-4: 269.5
    agent-5: 269.5
  policy_reward_mean:
    agent-0: 246.845
    agent-1: 246.845
    agent-2: 401.03
    agent-3: 401.03
    agent-4: 234.93
    agent-5: 234.93
  policy_reward_min:
    agent-0: 200.5
    agent-1: 200.5
    agent-2: 350.5
    agent-3: 350.5
    agent-4: 188.0
    agent-5: 188.0
  sampler_perf:
    mean_env_wait_ms: 26.5833666574402
    mean_inference_ms: 12.961574648170902
    mean_processing_ms: 59.67955659721556
  time_since_restore: 18520.82067322731
  time_this_iter_s: 126.4805052280426
  time_total_s: 100653.26304340363
  timestamp: 1637612499
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 76320000
  training_iteration: 795
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    795 |           100653 | 76320000 |  1765.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 31.79
    apples_agent-1_min: 15
    apples_agent-2_max: 423
    apples_agent-2_mean: 363.28
    apples_agent-2_min: 294
    apples_agent-3_max: 270
    apples_agent-3_mean: 225.56
    apples_agent-3_min: 162
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 386.24
    apples_agent-5_min: 281
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 373.28
    cleaning_beam_agent-0_min: 341
    cleaning_beam_agent-1_max: 16
    cleaning_beam_agent-1_mean: 1.47
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.93
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 20.62
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 462.08
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 4.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-23-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1853.0
  episode_reward_mean: 1756.74
  episode_reward_min: 1555.0
  episodes_this_iter: 96
  episodes_total: 76416
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11540.266
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3226467967033386
        entropy_coeff: 0.0017600000137463212
        kl: 0.001223719329573214
        model: {}
        policy_loss: -0.0010460594203323126
        total_loss: 0.0016153298784047365
        vf_explained_var: 0.013183549046516418
        vf_loss: 32.29246520996094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.211238294839859
        entropy_coeff: 0.0017600000137463212
        kl: 0.000915456679649651
        model: {}
        policy_loss: -0.0016189254820346832
        total_loss: 0.0010575354099273682
        vf_explained_var: 0.07041768729686737
        vf_loss: 30.482357025146484
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26993507146835327
        entropy_coeff: 0.0017600000137463212
        kl: 0.001237306511029601
        model: {}
        policy_loss: -0.0017027491703629494
        total_loss: 0.006171440705657005
        vf_explained_var: 0.012146949768066406
        vf_loss: 83.49276733398438
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.567802906036377
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013978885253891349
        model: {}
        policy_loss: -0.0015648303087800741
        total_loss: 0.005387605633586645
        vf_explained_var: 0.06208731234073639
        vf_loss: 79.51768493652344
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9068328738212585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012946058996021748
        model: {}
        policy_loss: -0.0017720088362693787
        total_loss: -0.00028349878266453743
        vf_explained_var: -0.000809982419013977
        vf_loss: 30.845321655273438
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25526538491249084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008433793555013835
        model: {}
        policy_loss: -0.0016872461419552565
        total_loss: 0.0007629157043993473
        vf_explained_var: 0.05640240013599396
        vf_loss: 28.9942626953125
    load_time_ms: 14457.147
    num_steps_sampled: 76416000
    num_steps_trained: 76416000
    sample_time_ms: 100741.77
    update_time_ms: 13.816
  iterations_since_restore: 146
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.575
    ram_util_percent: 14.078333333333333
  pid: 28385
  policy_reward_max:
    agent-0: 282.5
    agent-1: 282.5
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 244.995
    agent-1: 244.995
    agent-2: 397.635
    agent-3: 397.635
    agent-4: 235.74
    agent-5: 235.74
  policy_reward_min:
    agent-0: 209.0
    agent-1: 209.0
    agent-2: 350.5
    agent-3: 350.5
    agent-4: 174.5
    agent-5: 174.5
  sampler_perf:
    mean_env_wait_ms: 26.581804016451287
    mean_inference_ms: 12.960497504712523
    mean_processing_ms: 59.67790006878685
  time_since_restore: 18647.24549984932
  time_this_iter_s: 126.42482662200928
  time_total_s: 100779.68787002563
  timestamp: 1637612625
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 76416000
  training_iteration: 796
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    796 |           100780 | 76416000 |  1756.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 32.22
    apples_agent-1_min: 16
    apples_agent-2_max: 413
    apples_agent-2_mean: 360.23
    apples_agent-2_min: 271
    apples_agent-3_max: 270
    apples_agent-3_mean: 221.71
    apples_agent-3_min: 133
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 431
    apples_agent-5_mean: 384.63
    apples_agent-5_min: 289
    cleaning_beam_agent-0_max: 404
    cleaning_beam_agent-0_mean: 370.35
    cleaning_beam_agent-0_min: 340
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.38
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.19
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 23.37
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 448.89
    cleaning_beam_agent-4_min: 383
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 3.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-25-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1862.0
  episode_reward_mean: 1756.19
  episode_reward_min: 1324.0
  episodes_this_iter: 96
  episodes_total: 76512
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11529.373
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32242852449417114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017598065314814448
        model: {}
        policy_loss: -0.0014103505527600646
        total_loss: 0.0012135586002841592
        vf_explained_var: 0.007179871201515198
        vf_loss: 31.91383934020996
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21716231107711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014178059063851833
        model: {}
        policy_loss: -0.0015903571620583534
        total_loss: 0.0009373864158987999
        vf_explained_var: 0.09443014860153198
        vf_loss: 29.09950065612793
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27253448963165283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007340919109992683
        model: {}
        policy_loss: -0.0015623122453689575
        total_loss: 0.006339087150990963
        vf_explained_var: 0.03449772298336029
        vf_loss: 83.81060791015625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5796606540679932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008365699322894216
        model: {}
        policy_loss: -0.0016192798502743244
        total_loss: 0.005371600855141878
        vf_explained_var: 0.07666096091270447
        vf_loss: 80.11085510253906
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9209204912185669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016199212986975908
        model: {}
        policy_loss: -0.0018471021903678775
        total_loss: -0.0004198733950033784
        vf_explained_var: 0.015353038907051086
        vf_loss: 30.480504989624023
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2595806419849396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010366064961999655
        model: {}
        policy_loss: -0.001411961391568184
        total_loss: 0.001005317084491253
        vf_explained_var: 0.0703185647726059
        vf_loss: 28.74142837524414
    load_time_ms: 14453.246
    num_steps_sampled: 76512000
    num_steps_trained: 76512000
    sample_time_ms: 100709.513
    update_time_ms: 14.161
  iterations_since_restore: 147
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.638333333333332
    ram_util_percent: 14.006111111111109
  pid: 28385
  policy_reward_max:
    agent-0: 272.0
    agent-1: 272.0
    agent-2: 435.5
    agent-3: 435.5
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 246.255
    agent-1: 246.255
    agent-2: 396.11
    agent-3: 396.11
    agent-4: 235.73
    agent-5: 235.73
  policy_reward_min:
    agent-0: 192.5
    agent-1: 192.5
    agent-2: 290.5
    agent-3: 290.5
    agent-4: 179.0
    agent-5: 179.0
  sampler_perf:
    mean_env_wait_ms: 26.57913840594163
    mean_inference_ms: 12.95996441874435
    mean_processing_ms: 59.67688375050317
  time_since_restore: 18773.784448862076
  time_this_iter_s: 126.53894901275635
  time_total_s: 100906.22681903839
  timestamp: 1637612752
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 76512000
  training_iteration: 797
  trial_id: '00000'
  
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    797 |           100906 | 76512000 |  1756.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 31.61
    apples_agent-1_min: 16
    apples_agent-2_max: 400
    apples_agent-2_mean: 356.86
    apples_agent-2_min: 310
    apples_agent-3_max: 275
    apples_agent-3_mean: 222.28
    apples_agent-3_min: 168
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 385.87
    apples_agent-5_min: 337
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 379.64
    cleaning_beam_agent-0_min: 337
    cleaning_beam_agent-1_max: 20
    cleaning_beam_agent-1_mean: 2.08
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 24.32
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 444.88
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-27-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1892.0
  episode_reward_mean: 1760.18
  episode_reward_min: 1512.0
  episodes_this_iter: 96
  episodes_total: 76608
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11523.451
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3223874568939209
        entropy_coeff: 0.0017600000137463212
        kl: 0.001998048974201083
        model: {}
        policy_loss: -0.001199202612042427
        total_loss: 0.0015672012232244015
        vf_explained_var: 0.013626053929328918
        vf_loss: 33.338050842285156
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2164011150598526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011716993758454919
        model: {}
        policy_loss: -0.0017665466293692589
        total_loss: 0.0008923197165131569
        vf_explained_var: 0.09991805255413055
        vf_loss: 30.397323608398438
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2780930697917938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009505933849141002
        model: {}
        policy_loss: -0.0016695486847311258
        total_loss: 0.006001668982207775
        vf_explained_var: 0.028944283723831177
        vf_loss: 81.60661315917969
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5773892402648926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005568887572735548
        model: {}
        policy_loss: -0.0013867823872715235
        total_loss: 0.0053880829364061356
        vf_explained_var: 0.07338276505470276
        vf_loss: 77.91070556640625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9114264249801636
        entropy_coeff: 0.0017600000137463212
        kl: 0.002260832581669092
        model: {}
        policy_loss: -0.0018500587902963161
        total_loss: -0.00047772936522960663
        vf_explained_var: 0.004959091544151306
        vf_loss: 29.764421463012695
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2654198110103607
        entropy_coeff: 0.0017600000137463212
        kl: 0.001312903012149036
        model: {}
        policy_loss: -0.0018779246602207422
        total_loss: 0.0004918593913316727
        vf_explained_var: 0.05064551532268524
        vf_loss: 28.369224548339844
    load_time_ms: 14465.104
    num_steps_sampled: 76608000
    num_steps_trained: 76608000
    sample_time_ms: 100687.09
    update_time_ms: 13.979
  iterations_since_restore: 148
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.798895027624308
    ram_util_percent: 14.093370165745856
  pid: 28385
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 430.0
    agent-3: 430.0
    agent-4: 274.0
    agent-5: 274.0
  policy_reward_mean:
    agent-0: 248.25
    agent-1: 248.25
    agent-2: 395.98
    agent-3: 395.98
    agent-4: 235.86
    agent-5: 235.86
  policy_reward_min:
    agent-0: 210.5
    agent-1: 210.5
    agent-2: 338.0
    agent-3: 338.0
    agent-4: 207.5
    agent-5: 207.5
  sampler_perf:
    mean_env_wait_ms: 26.57824197527917
    mean_inference_ms: 12.959609870820946
    mean_processing_ms: 59.67714838576068
  time_since_restore: 18900.88728260994
  time_this_iter_s: 127.10283374786377
  time_total_s: 101033.32965278625
  timestamp: 1637612879
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 76608000
  training_iteration: 798
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    798 |           101033 | 76608000 |  1760.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.17
    apples_agent-1_min: 16
    apples_agent-2_max: 406
    apples_agent-2_mean: 356.36
    apples_agent-2_min: 226
    apples_agent-3_max: 272
    apples_agent-3_mean: 223.65
    apples_agent-3_min: 133
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 382.69
    apples_agent-5_min: 218
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 387.06
    cleaning_beam_agent-0_min: 345
    cleaning_beam_agent-1_max: 45
    cleaning_beam_agent-1_mean: 2.17
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 24.64
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 452.56
    cleaning_beam_agent-4_min: 401
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 3.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-30-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1878.0
  episode_reward_mean: 1744.18
  episode_reward_min: 1073.0
  episodes_this_iter: 96
  episodes_total: 76704
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11512.967
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32873114943504333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016723844455555081
        model: {}
        policy_loss: -0.0015056920237839222
        total_loss: 0.0012940543238073587
        vf_explained_var: 0.03969980776309967
        vf_loss: 33.78314971923828
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.217594712972641
        entropy_coeff: 0.0017600000137463212
        kl: 0.000749946222640574
        model: {}
        policy_loss: -0.0017964798025786877
        total_loss: 0.0009812992066144943
        vf_explained_var: 0.10289034247398376
        vf_loss: 31.607454299926758
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2772296071052551
        entropy_coeff: 0.0017600000137463212
        kl: 0.000953424721956253
        model: {}
        policy_loss: -0.0019072536379098892
        total_loss: 0.005918159149587154
        vf_explained_var: 0.04049953818321228
        vf_loss: 83.13339233398438
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.58463454246521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012177310418337584
        model: {}
        policy_loss: -0.0017182999290525913
        total_loss: 0.005472387187182903
        vf_explained_var: 0.050767555832862854
        vf_loss: 82.19647216796875
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9198227524757385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014533724170178175
        model: {}
        policy_loss: -0.0018397178500890732
        total_loss: -0.00023591890931129456
        vf_explained_var: 0.0071990638971328735
        vf_loss: 32.22687530517578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2705024480819702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010162928374484181
        model: {}
        policy_loss: -0.0019779058638960123
        total_loss: 0.0005743458168581128
        vf_explained_var: 0.06587575376033783
        vf_loss: 30.283355712890625
    load_time_ms: 14456.694
    num_steps_sampled: 76704000
    num_steps_trained: 76704000
    sample_time_ms: 100653.139
    update_time_ms: 13.98
  iterations_since_restore: 149
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.785
    ram_util_percent: 14.043888888888887
  pid: 28385
  policy_reward_max:
    agent-0: 266.5
    agent-1: 266.5
    agent-2: 430.5
    agent-3: 430.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 244.965
    agent-1: 244.965
    agent-2: 394.76
    agent-3: 394.76
    agent-4: 232.365
    agent-5: 232.365
  policy_reward_min:
    agent-0: 151.0
    agent-1: 151.0
    agent-2: 253.0
    agent-3: 253.0
    agent-4: 132.5
    agent-5: 132.5
  sampler_perf:
    mean_env_wait_ms: 26.57572841043645
    mean_inference_ms: 12.958245412103466
    mean_processing_ms: 59.67335453221774
  time_since_restore: 19027.119705438614
  time_this_iter_s: 126.23242282867432
  time_total_s: 101159.56207561493
  timestamp: 1637613005
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 76704000
  training_iteration: 799
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    799 |           101160 | 76704000 |  1744.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 30.7
    apples_agent-1_min: 16
    apples_agent-2_max: 417
    apples_agent-2_mean: 355.16
    apples_agent-2_min: 157
    apples_agent-3_max: 287
    apples_agent-3_mean: 225.77
    apples_agent-3_min: 116
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 381.59
    apples_agent-5_min: 293
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 392.2
    cleaning_beam_agent-0_min: 343
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.54
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.59
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 22.92
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 572
    cleaning_beam_agent-4_mean: 451.2
    cleaning_beam_agent-4_min: 403
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-32-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1905.0
  episode_reward_mean: 1743.46
  episode_reward_min: 1097.0
  episodes_this_iter: 96
  episodes_total: 76800
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11500.713
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32431477308273315
        entropy_coeff: 0.0017600000137463212
        kl: 0.001007942482829094
        model: {}
        policy_loss: -0.0012941807508468628
        total_loss: 0.0015191398561000824
        vf_explained_var: 0.024584516882896423
        vf_loss: 33.841163635253906
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21447770297527313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012093796394765377
        model: {}
        policy_loss: -0.0017635500989854336
        total_loss: 0.0009493189863860607
        vf_explained_var: 0.10871155560016632
        vf_loss: 30.903499603271484
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27890366315841675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010928935371339321
        model: {}
        policy_loss: -0.001987555529922247
        total_loss: 0.006088883150368929
        vf_explained_var: 0.058351367712020874
        vf_loss: 85.67308044433594
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5790035128593445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010424602078273892
        model: {}
        policy_loss: -0.0020220139995217323
        total_loss: 0.005408076103776693
        vf_explained_var: 0.07156446576118469
        vf_loss: 84.4913558959961
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.931828498840332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014046444557607174
        model: {}
        policy_loss: -0.0018314453773200512
        total_loss: -0.00037997704930603504
        vf_explained_var: 0.02167317271232605
        vf_loss: 30.914833068847656
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2759726643562317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006768762832507491
        model: {}
        policy_loss: -0.0015936114359647036
        total_loss: 0.0008694489952176809
        vf_explained_var: 0.06795282661914825
        vf_loss: 29.487720489501953
    load_time_ms: 14435.467
    num_steps_sampled: 76800000
    num_steps_trained: 76800000
    sample_time_ms: 100700.943
    update_time_ms: 13.981
  iterations_since_restore: 150
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.511538461538457
    ram_util_percent: 14.120329670329669
  pid: 28385
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 435.0
    agent-3: 435.0
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 246.715
    agent-1: 246.715
    agent-2: 392.735
    agent-3: 392.735
    agent-4: 232.28
    agent-5: 232.28
  policy_reward_min:
    agent-0: 178.5
    agent-1: 178.5
    agent-2: 201.0
    agent-3: 201.0
    agent-4: 169.0
    agent-5: 169.0
  sampler_perf:
    mean_env_wait_ms: 26.573680539445334
    mean_inference_ms: 12.957409859245745
    mean_processing_ms: 59.67189674471877
  time_since_restore: 19154.528210878372
  time_this_iter_s: 127.4085054397583
  time_total_s: 101286.97058105469
  timestamp: 1637613133
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 76800000
  training_iteration: 800
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    800 |           101287 | 76800000 |  1743.46 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 32.11
    apples_agent-1_min: 19
    apples_agent-2_max: 417
    apples_agent-2_mean: 361.1
    apples_agent-2_min: 298
    apples_agent-3_max: 262
    apples_agent-3_mean: 223.7
    apples_agent-3_min: 167
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 384.84
    apples_agent-5_min: 318
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 391.53
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 1.37
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 2.05
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 20.71
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 464.9
    cleaning_beam_agent-4_min: 395
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 4.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-34-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1865.0
  episode_reward_mean: 1754.55
  episode_reward_min: 1430.0
  episodes_this_iter: 96
  episodes_total: 76896
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11500.415
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3179381489753723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015571368858218193
        model: {}
        policy_loss: -0.0010882173664867878
        total_loss: 0.0015797284431755543
        vf_explained_var: 0.019676893949508667
        vf_loss: 32.27518081665039
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20973336696624756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009572413982823491
        model: {}
        policy_loss: -0.00159997318405658
        total_loss: 0.0009166100062429905
        vf_explained_var: 0.12146598100662231
        vf_loss: 28.857147216796875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2765690088272095
        entropy_coeff: 0.0017600000137463212
        kl: 0.001030132407322526
        model: {}
        policy_loss: -0.0017739944159984589
        total_loss: 0.006120370700955391
        vf_explained_var: 0.02812539041042328
        vf_loss: 83.81127166748047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5680478811264038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016314417589455843
        model: {}
        policy_loss: -0.0019750543870031834
        total_loss: 0.005149488337337971
        vf_explained_var: 0.05822880566120148
        vf_loss: 81.2430648803711
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9319143891334534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017872131429612637
        model: {}
        policy_loss: -0.0016682231798768044
        total_loss: -0.0002692500129342079
        vf_explained_var: 0.007498458027839661
        vf_loss: 30.391427993774414
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27433186769485474
        entropy_coeff: 0.0017600000137463212
        kl: 0.000901221064850688
        model: {}
        policy_loss: -0.001661165151745081
        total_loss: 0.000713270390406251
        vf_explained_var: 0.06504310667514801
        vf_loss: 28.572656631469727
    load_time_ms: 14446.791
    num_steps_sampled: 76896000
    num_steps_trained: 76896000
    sample_time_ms: 100724.135
    update_time_ms: 13.803
  iterations_since_restore: 151
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.891256830601094
    ram_util_percent: 13.971584699453548
  pid: 28385
  policy_reward_max:
    agent-0: 280.5
    agent-1: 280.5
    agent-2: 435.0
    agent-3: 435.0
    agent-4: 266.0
    agent-5: 266.0
  policy_reward_mean:
    agent-0: 247.52
    agent-1: 247.52
    agent-2: 395.45
    agent-3: 395.45
    agent-4: 234.305
    agent-5: 234.305
  policy_reward_min:
    agent-0: 203.5
    agent-1: 203.5
    agent-2: 321.0
    agent-3: 321.0
    agent-4: 186.5
    agent-5: 186.5
  sampler_perf:
    mean_env_wait_ms: 26.57208064011913
    mean_inference_ms: 12.956327423578706
    mean_processing_ms: 59.669181718951016
  time_since_restore: 19281.264002084732
  time_this_iter_s: 126.73579120635986
  time_total_s: 101413.70637226105
  timestamp: 1637613261
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 76896000
  training_iteration: 801
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    801 |           101414 | 76896000 |  1754.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 32.26
    apples_agent-1_min: 20
    apples_agent-2_max: 410
    apples_agent-2_mean: 359.54
    apples_agent-2_min: 248
    apples_agent-3_max: 286
    apples_agent-3_mean: 228.81
    apples_agent-3_min: 129
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 383.43
    apples_agent-5_min: 254
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 384.79
    cleaning_beam_agent-0_min: 342
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.68
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 2.19
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 18.97
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 443.96
    cleaning_beam_agent-4_min: 389
    cleaning_beam_agent-5_max: 20
    cleaning_beam_agent-5_mean: 4.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-36-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1863.0
  episode_reward_mean: 1746.36
  episode_reward_min: 1224.0
  episodes_this_iter: 96
  episodes_total: 76992
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11503.518
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3190992474555969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015670533757656813
        model: {}
        policy_loss: -0.0011817258782684803
        total_loss: 0.0016228659078478813
        vf_explained_var: 0.018160387873649597
        vf_loss: 33.66206359863281
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2168058454990387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010126256383955479
        model: {}
        policy_loss: -0.0015263562090694904
        total_loss: 0.0011728079989552498
        vf_explained_var: 0.10147552192211151
        vf_loss: 30.807418823242188
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2784159183502197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007029816042631865
        model: {}
        policy_loss: -0.001677071675658226
        total_loss: 0.006268722005188465
        vf_explained_var: 0.030181393027305603
        vf_loss: 84.35806274414062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5770216584205627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006472678505815566
        model: {}
        policy_loss: -0.0015837422106415033
        total_loss: 0.00559523468837142
        vf_explained_var: 0.057958319783210754
        vf_loss: 81.94532775878906
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9374759793281555
        entropy_coeff: 0.0017600000137463212
        kl: 0.00304116471670568
        model: {}
        policy_loss: -0.0019865031354129314
        total_loss: -0.0004993925103917718
        vf_explained_var: 0.0291607528924942
        vf_loss: 31.370662689208984
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27346083521842957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007408884121105075
        model: {}
        policy_loss: -0.0017948474269360304
        total_loss: 0.0007336819544434547
        vf_explained_var: 0.0677875429391861
        vf_loss: 30.09821891784668
    load_time_ms: 14466.188
    num_steps_sampled: 76992000
    num_steps_trained: 76992000
    sample_time_ms: 100678.013
    update_time_ms: 13.747
  iterations_since_restore: 152
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.230555555555554
    ram_util_percent: 14.092777777777776
  pid: 28385
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 435.0
    agent-3: 435.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 243.775
    agent-1: 243.775
    agent-2: 395.5
    agent-3: 395.5
    agent-4: 233.905
    agent-5: 233.905
  policy_reward_min:
    agent-0: 170.0
    agent-1: 170.0
    agent-2: 266.5
    agent-3: 266.5
    agent-4: 153.5
    agent-5: 153.5
  sampler_perf:
    mean_env_wait_ms: 26.569587268712645
    mean_inference_ms: 12.955027885412083
    mean_processing_ms: 59.6662715664582
  time_since_restore: 19407.9530274868
  time_this_iter_s: 126.68902540206909
  time_total_s: 101540.39539766312
  timestamp: 1637613388
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 76992000
  training_iteration: 802
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.3 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:28385 |    802 |           101540 | 76992000 |  1746.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=28385)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7faa6a35f5c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.7
    apples_agent-1_min: 15
    apples_agent-2_max: 416
    apples_agent-2_mean: 358.14
    apples_agent-2_min: 225
    apples_agent-3_max: 266
    apples_agent-3_mean: 227.42
    apples_agent-3_min: 142
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.09
    apples_agent-4_min: 0
    apples_agent-5_max: 438
    apples_agent-5_mean: 384.41
    apples_agent-5_min: 198
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 380.51
    cleaning_beam_agent-0_min: 309
    cleaning_beam_agent-1_max: 22
    cleaning_beam_agent-1_mean: 1.65
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.69
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 19.82
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 429.89
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_15-38-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1869.0
  episode_reward_mean: 1748.08
  episode_reward_min: 1020.0
  episodes_this_iter: 96
  episodes_total: 77088
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11505.829
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3209819495677948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011812645243480802
        model: {}
        policy_loss: -0.0014693629927933216
        total_loss: 0.0011653844267129898
        vf_explained_var: 0.04177510738372803
        vf_loss: 31.99676513671875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21646755933761597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012613451108336449
        model: {}
        policy_loss: -0.0018246802501380444
        total_loss: 0.0007438450120389462
        vf_explained_var: 0.11913219094276428
        vf_loss: 29.49509048461914
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2770111858844757
        entropy_coeff: 0.0017600000137463212
        kl: 0.000965750019531697
        model: {}
        policy_loss: -0.001985126407817006
        total_loss: 0.005971870385110378
        vf_explained_var: 0.03117324411869049
        vf_loss: 84.44537353515625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5859326720237732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012340046232566237
        model: {}
        policy_loss: -0.001508217304944992
        total_loss: 0.0057769534178078175
        vf_explained_var: 0.04762692749500275
        vf_loss: 83.16412353515625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9247081279754639
        entropy_coeff: 0.0017600000137463212
        kl: 0.002316912403330207
        model: {}
        policy_loss: -0.0019533128943294287
        total_loss: -0.0004781073657795787
        vf_explained_var: 0.016655296087265015
        vf_loss: 31.02688217163086
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2684161961078644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010683151194825768
        model: {}
        policy_loss: -0.0017362304497510195
        total_loss: 0.0006666459375992417
        vf_explained_var: 0.08803164958953857
        vf_loss: 28.75286865234375
    load_time_ms: 14465.554
    num_steps_sampled: 77088000
    num_steps_trained: 77088000
    sample_time_ms: 100589.429
    update_time_ms: 13.555
  iterations_since_restore: 153
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.773480662983424
    ram_util_percent: 14.085635359116022
  pid: 28385
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 429.5
    agent-3: 429.5
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 243.61
    agent-1: 243.61
    agent-2: 396.155
    agent-3: 396.155
    agent-4: 234.275
    agent-5: 234.275
  policy_reward_min:
    agent-0: 132.5
    agent-1: 132.5
    agent-2: 244.5
    agent-3: 244.5
    agent-4: 133.0
    agent-5: 133.0
  sampler_perf:
    mean_env_wait_ms: 26.56678046770392
    mean_inference_ms: 12.953648455068043
    mean_processing_ms: 59.664551684113015
  time_since_restore: 19534.744039535522
  time_this_iter_s: 126.79101204872131
  time_total_s: 101667.18640971184
  timestamp: 1637613515
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 77088000
  training_iteration: 803
  trial_id: '00000'
  >>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-21_10-26-43brn7k2yg/checkpoint_960
== Status ==
Memory usage on this node: 11.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    960 |           122481 | 92160000 |  1798.78 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m 2021-11-22 22:02:09,711	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=12895)[0m 2021-11-22 22:02:09,727	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=12895)[0m 2021-11-22 22:04:03,445	INFO trainable.py:180 -- _setup took 113.734 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=12895)[0m 2021-11-22 22:04:03,445	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=12895)[0m 2021-11-22 22:04:03,446	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=12895)[0m 2021-11-22 22:04:06,889	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=12895)[0m 2021-11-22 22:04:06,889	INFO trainable.py:423 -- Restored on 172.17.8.31 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-21_10-26-43brn7k2yg/tmpfdr45ytvrestore_from_object/checkpoint-960
[2m[36m(pid=12895)[0m 2021-11-22 22:04:06,889	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 960, '_timesteps_total': 92160000, '_time_total': 122481.02270841599, '_episodes_total': 92160}
== Status ==
Memory usage on this node: 16.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    960 |           122481 | 92160000 |  1798.78 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.010416666666666666
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.697916666666668
    apples_agent-1_min: 11
    apples_agent-2_max: 416
    apples_agent-2_mean: 362.5729166666667
    apples_agent-2_min: 323
    apples_agent-3_max: 298
    apples_agent-3_mean: 246.36458333333334
    apples_agent-3_min: 207
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 387.8645833333333
    apples_agent-5_min: 342
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 449.2395833333333
    cleaning_beam_agent-0_min: 410
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.0833333333333333
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.5416666666666667
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 16.520833333333332
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 422.3645833333333
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.6145833333333335
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.010416666666666666
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.010416666666666666
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-06-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1949.0
  episode_reward_mean: 1804.8229166666667
  episode_reward_min: 1680.0
  episodes_this_iter: 96
  episodes_total: 92256
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 21879.982
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3560272753238678
        entropy_coeff: 0.0017600000137463212
        kl: 0.001095527783036232
        model: {}
        policy_loss: -0.0015870914794504642
        total_loss: 0.0011955611407756805
        vf_explained_var: 0.04144582152366638
        vf_loss: 31.901546478271484
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2062286138534546
        entropy_coeff: 0.0017600000137463212
        kl: 0.000666527368593961
        model: {}
        policy_loss: -0.001998085994273424
        total_loss: 0.0009500824380666018
        vf_explained_var: 0.04386775195598602
        vf_loss: 31.778247833251953
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24942098557949066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008483679848723114
        model: {}
        policy_loss: -0.002199077047407627
        total_loss: 0.006290136836469173
        vf_explained_var: 0.03189021348953247
        vf_loss: 87.58523559570312
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4782957434654236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005075790686532855
        model: {}
        policy_loss: -0.0017161364667117596
        total_loss: 0.0061205122619867325
        vf_explained_var: 0.05389074981212616
        vf_loss: 85.76934051513672
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9468487501144409
        entropy_coeff: 0.0017600000137463212
        kl: 0.001179151120595634
        model: {}
        policy_loss: -0.0020510461181402206
        total_loss: -0.0005841075908392668
        vf_explained_var: 0.028399065136909485
        vf_loss: 28.975603103637695
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2376980483531952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007444834336638451
        model: {}
        policy_loss: -0.0020155878737568855
        total_loss: 0.00041869841516017914
        vf_explained_var: 0.09355667233467102
        vf_loss: 27.03739356994629
    load_time_ms: 28016.394
    num_steps_sampled: 92256000
    num_steps_trained: 92256000
    sample_time_ms: 108048.348
    update_time_ms: 3707.127
  iterations_since_restore: 1
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.893199999999997
    ram_util_percent: 10.571599999999998
  pid: 12895
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 452.0
    agent-3: 452.0
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 250.203125
    agent-1: 250.203125
    agent-2: 413.96875
    agent-3: 413.96875
    agent-4: 238.23958333333334
    agent-5: 238.23958333333334
  policy_reward_min:
    agent-0: 214.5
    agent-1: 214.5
    agent-2: 377.0
    agent-3: 377.0
    agent-4: 214.0
    agent-5: 214.0
  sampler_perf:
    mean_env_wait_ms: 26.47392486994004
    mean_inference_ms: 15.067391898923423
    mean_processing_ms: 60.567546518016506
  time_since_restore: 165.14282703399658
  time_this_iter_s: 165.14282703399658
  time_total_s: 122646.16553544998
  timestamp: 1637636818
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 92256000
  training_iteration: 961
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    961 |           122646 | 92256000 |  1804.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 0.23
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 30.58
    apples_agent-1_min: 1
    apples_agent-2_max: 437
    apples_agent-2_mean: 358.48
    apples_agent-2_min: 19
    apples_agent-3_max: 296
    apples_agent-3_mean: 243.33
    apples_agent-3_min: 5
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.07
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 382.57
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 450.8
    cleaning_beam_agent-0_min: 419
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.84
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 1.99
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 15.42
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 422.05
    cleaning_beam_agent-4_min: 321
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 3.96
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-09-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1895.0
  episode_reward_mean: 1779.69
  episode_reward_min: 37.0
  episodes_this_iter: 96
  episodes_total: 92352
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 16573.73
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3617541491985321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011878522345796227
        model: {}
        policy_loss: -0.0016687419265508652
        total_loss: 0.0011590616777539253
        vf_explained_var: 0.11973315477371216
        vf_loss: 33.45705032348633
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21172288060188293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010262365685775876
        model: {}
        policy_loss: -0.0019543510861694813
        total_loss: 0.0011781079228967428
        vf_explained_var: 0.10567453503608704
        vf_loss: 34.024662017822266
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26116445660591125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011165589094161987
        model: {}
        policy_loss: -0.002094708848744631
        total_loss: 0.006668462418019772
        vf_explained_var: 0.10903334617614746
        vf_loss: 91.11167907714844
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48262572288513184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007428925600834191
        model: {}
        policy_loss: -0.0014483941486105323
        total_loss: 0.007470856886357069
        vf_explained_var: 0.05024261772632599
        vf_loss: 96.94381713867188
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9488252997398376
        entropy_coeff: 0.0017600000137463212
        kl: 0.002182571217417717
        model: {}
        policy_loss: -0.0021753902547061443
        total_loss: -0.00026229117065668106
        vf_explained_var: 0.04600350558757782
        vf_loss: 33.64775085449219
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2500753700733185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007577462238259614
        model: {}
        policy_loss: -0.0019441349431872368
        total_loss: 0.0008003944531083107
        vf_explained_var: 0.1193876564502716
        vf_loss: 31.088878631591797
    load_time_ms: 22510.557
    num_steps_sampled: 92352000
    num_steps_trained: 92352000
    sample_time_ms: 105463.756
    update_time_ms: 1860.249
  iterations_since_restore: 2
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.714973262032085
    ram_util_percent: 11.856684491978609
  pid: 12895
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 444.5
    agent-3: 444.5
    agent-4: 267.5
    agent-5: 267.5
  policy_reward_mean:
    agent-0: 245.54
    agent-1: 245.54
    agent-2: 408.005
    agent-3: 408.005
    agent-4: 236.3
    agent-5: 236.3
  policy_reward_min:
    agent-0: 4.5
    agent-1: 4.5
    agent-2: 10.0
    agent-3: 10.0
    agent-4: 4.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 26.462146277119864
    mean_inference_ms: 14.378199248223602
    mean_processing_ms: 60.40001939726865
  time_since_restore: 296.38962841033936
  time_this_iter_s: 131.24680137634277
  time_total_s: 122777.41233682632
  timestamp: 1637636950
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 92352000
  training_iteration: 962
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 21.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    962 |           122777 | 92352000 |  1779.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 31.0
    apples_agent-1_min: 19
    apples_agent-2_max: 431
    apples_agent-2_mean: 361.09
    apples_agent-2_min: 189
    apples_agent-3_max: 290
    apples_agent-3_mean: 245.89
    apples_agent-3_min: 111
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.11
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 389.1
    apples_agent-5_min: 193
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 451.29
    cleaning_beam_agent-0_min: 422
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 0.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.54
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 420.49
    cleaning_beam_agent-4_min: 300
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-11-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1931.0
  episode_reward_mean: 1801.9
  episode_reward_min: 983.0
  episodes_this_iter: 96
  episodes_total: 92448
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 14804.608
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3509364724159241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015015305252745748
        model: {}
        policy_loss: -0.0014964567963033915
        total_loss: 0.0011566962348297238
        vf_explained_var: 0.03684788942337036
        vf_loss: 31.957275390625
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21199211478233337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006021703593432903
        model: {}
        policy_loss: -0.001754505094140768
        total_loss: 0.0010751988738775253
        vf_explained_var: 0.044061318039894104
        vf_loss: 31.727031707763672
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25482404232025146
        entropy_coeff: 0.0017600000137463212
        kl: 0.000610933406278491
        model: {}
        policy_loss: -0.0017472764011472464
        total_loss: 0.006619371473789215
        vf_explained_var: 0.039413973689079285
        vf_loss: 87.84591674804688
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4826231896877289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006413891678676009
        model: {}
        policy_loss: -0.001423753798007965
        total_loss: 0.006524395197629929
        vf_explained_var: 0.044473037123680115
        vf_loss: 87.65498352050781
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9466601610183716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014849122380837798
        model: {}
        policy_loss: -0.0017823479138314724
        total_loss: -0.00042853783816099167
        vf_explained_var: 0.0429251492023468
        vf_loss: 29.456876754760742
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2374115288257599
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009097105939872563
        model: {}
        policy_loss: -0.0016582203097641468
        total_loss: 0.0007749968208372593
        vf_explained_var: 0.08724074065685272
        vf_loss: 28.055747985839844
    load_time_ms: 20063.888
    num_steps_sampled: 92448000
    num_steps_trained: 92448000
    sample_time_ms: 104676.727
    update_time_ms: 1244.943
  iterations_since_restore: 3
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.86594594594595
    ram_util_percent: 11.882702702702701
  pid: 12895
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 448.5
    agent-3: 448.5
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 247.925
    agent-1: 247.925
    agent-2: 413.725
    agent-3: 413.725
    agent-4: 239.3
    agent-5: 239.3
  policy_reward_min:
    agent-0: 135.0
    agent-1: 135.0
    agent-2: 222.5
    agent-3: 222.5
    agent-4: 134.0
    agent-5: 134.0
  sampler_perf:
    mean_env_wait_ms: 26.47644178583916
    mean_inference_ms: 13.996653682544347
    mean_processing_ms: 60.43318581387868
  time_since_restore: 426.05402421951294
  time_this_iter_s: 129.66439580917358
  time_total_s: 122907.0767326355
  timestamp: 1637637080
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 92448000
  training_iteration: 963
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 21.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    963 |           122907 | 92448000 |   1801.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 29.8
    apples_agent-1_min: 14
    apples_agent-2_max: 411
    apples_agent-2_mean: 361.48
    apples_agent-2_min: 187
    apples_agent-3_max: 288
    apples_agent-3_mean: 241.97
    apples_agent-3_min: 113
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 385.82
    apples_agent-5_min: 189
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 451.91
    cleaning_beam_agent-0_min: 400
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.32
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 1.67
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 16.06
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 409.12
    cleaning_beam_agent-4_min: 318
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 3.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-13-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1882.0
  episode_reward_mean: 1789.31
  episode_reward_min: 892.0
  episodes_this_iter: 96
  episodes_total: 92544
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 13956.167
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3638750910758972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016789252404123545
        model: {}
        policy_loss: -0.0013663768768310547
        total_loss: 0.001227447297424078
        vf_explained_var: 0.06161805987358093
        vf_loss: 31.922718048095703
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21038813889026642
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005812379531562328
        model: {}
        policy_loss: -0.0015658657066524029
        total_loss: 0.0012911148369312286
        vf_explained_var: 0.055061161518096924
        vf_loss: 32.127349853515625
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25554579496383667
        entropy_coeff: 0.0017600000137463212
        kl: 0.000932147610001266
        model: {}
        policy_loss: -0.0017595125827938318
        total_loss: 0.006705009378492832
        vf_explained_var: 0.06432090699672699
        vf_loss: 88.90980529785156
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.489465594291687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007340150186792016
        model: {}
        policy_loss: -0.0017317761667072773
        total_loss: 0.0065226368606090546
        vf_explained_var: 0.042854756116867065
        vf_loss: 90.9751968383789
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9458581209182739
        entropy_coeff: 0.0017600000137463212
        kl: 0.002808830700814724
        model: {}
        policy_loss: -0.0020160700660198927
        total_loss: -0.0004955260083079338
        vf_explained_var: 0.02263297140598297
        vf_loss: 31.15031623840332
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23865923285484314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006177001632750034
        model: {}
        policy_loss: -0.0017239078879356384
        total_loss: 0.000712268054485321
        vf_explained_var: 0.10922221839427948
        vf_loss: 28.40777587890625
    load_time_ms: 18785.466
    num_steps_sampled: 92544000
    num_steps_trained: 92544000
    sample_time_ms: 104297.405
    update_time_ms: 937.39
  iterations_since_restore: 4
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.083783783783783
    ram_util_percent: 11.905945945945945
  pid: 12895
  policy_reward_max:
    agent-0: 270.5
    agent-1: 270.5
    agent-2: 452.5
    agent-3: 452.5
    agent-4: 262.0
    agent-5: 262.0
  policy_reward_mean:
    agent-0: 247.13
    agent-1: 247.13
    agent-2: 410.195
    agent-3: 410.195
    agent-4: 237.33
    agent-5: 237.33
  policy_reward_min:
    agent-0: 122.0
    agent-1: 122.0
    agent-2: 201.5
    agent-3: 201.5
    agent-4: 122.5
    agent-5: 122.5
  sampler_perf:
    mean_env_wait_ms: 26.50973999552917
    mean_inference_ms: 13.788215005013067
    mean_processing_ms: 60.4002043376065
  time_since_restore: 555.7201588153839
  time_this_iter_s: 129.66613459587097
  time_total_s: 123036.74286723137
  timestamp: 1637637209
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 92544000
  training_iteration: 964
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 21.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    964 |           123037 | 92544000 |  1789.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 31.14
    apples_agent-1_min: 15
    apples_agent-2_max: 415
    apples_agent-2_mean: 360.63
    apples_agent-2_min: 228
    apples_agent-3_max: 287
    apples_agent-3_mean: 242.02
    apples_agent-3_min: 113
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 386.21
    apples_agent-5_min: 229
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 458.76
    cleaning_beam_agent-0_min: 417
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.07
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 1.73
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 15.79
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 407.49
    cleaning_beam_agent-4_min: 306
    cleaning_beam_agent-5_max: 35
    cleaning_beam_agent-5_mean: 3.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-15-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1916.0
  episode_reward_mean: 1783.53
  episode_reward_min: 1087.0
  episodes_this_iter: 96
  episodes_total: 92640
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 13435.667
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35932791233062744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008977057877928019
        model: {}
        policy_loss: -0.0013733967207372189
        total_loss: 0.0013414777349680662
        vf_explained_var: 0.04596759378910065
        vf_loss: 33.3607177734375
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21256214380264282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011915592476725578
        model: {}
        policy_loss: -0.001987550873309374
        total_loss: 0.0009275684133172035
        vf_explained_var: 0.06282395124435425
        vf_loss: 32.743350982666016
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2585679292678833
        entropy_coeff: 0.0017600000137463212
        kl: 0.000914556032512337
        model: {}
        policy_loss: -0.0017234254628419876
        total_loss: 0.006874450482428074
        vf_explained_var: 0.06685802340507507
        vf_loss: 90.41521453857422
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4927660822868347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015196164604276419
        model: {}
        policy_loss: -0.0018437234684824944
        total_loss: 0.00634426437318325
        vf_explained_var: 0.06557333469390869
        vf_loss: 90.36257934570312
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9433744549751282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009563871426507831
        model: {}
        policy_loss: -0.0017160545103251934
        total_loss: -0.00017197616398334503
        vf_explained_var: 0.0461704283952713
        vf_loss: 31.924623489379883
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24280217289924622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006418471457436681
        model: {}
        policy_loss: -0.001752843614667654
        total_loss: 0.0007766690105199814
        vf_explained_var: 0.11918951570987701
        vf_loss: 29.488218307495117
    load_time_ms: 17982.633
    num_steps_sampled: 92640000
    num_steps_trained: 92640000
    sample_time_ms: 103774.815
    update_time_ms: 752.733
  iterations_since_restore: 5
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.79617486338797
    ram_util_percent: 11.872677595628412
  pid: 12895
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 451.5
    agent-3: 451.5
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 247.3
    agent-1: 247.3
    agent-2: 406.855
    agent-3: 406.855
    agent-4: 237.61
    agent-5: 237.61
  policy_reward_min:
    agent-0: 152.5
    agent-1: 152.5
    agent-2: 242.5
    agent-3: 242.5
    agent-4: 145.0
    agent-5: 145.0
  sampler_perf:
    mean_env_wait_ms: 26.468298952443877
    mean_inference_ms: 13.656673955424031
    mean_processing_ms: 60.250962289181544
  time_since_restore: 683.6153855323792
  time_this_iter_s: 127.89522671699524
  time_total_s: 123164.63809394836
  timestamp: 1637637337
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 92640000
  training_iteration: 965
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    965 |           123165 | 92640000 |  1783.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 30.41
    apples_agent-1_min: 15
    apples_agent-2_max: 416
    apples_agent-2_mean: 360.02
    apples_agent-2_min: 206
    apples_agent-3_max: 284
    apples_agent-3_mean: 246.61
    apples_agent-3_min: 202
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 424
    apples_agent-5_mean: 384.44
    apples_agent-5_min: 328
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 460.57
    cleaning_beam_agent-0_min: 392
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.07
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.39
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 14.05
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 469
    cleaning_beam_agent-4_mean: 409.58
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-17-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1900.0
  episode_reward_mean: 1791.54
  episode_reward_min: 1379.0
  episodes_this_iter: 96
  episodes_total: 92736
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 13099.413
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.364261269569397
        entropy_coeff: 0.0017600000137463212
        kl: 0.001513080787844956
        model: {}
        policy_loss: -0.0012918300926685333
        total_loss: 0.0011618495918810368
        vf_explained_var: 0.04665501415729523
        vf_loss: 30.85324478149414
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20709381997585297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008634886471554637
        model: {}
        policy_loss: -0.0016453973948955536
        total_loss: 0.0011284919455647469
        vf_explained_var: 0.033260226249694824
        vf_loss: 31.329784393310547
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2547903060913086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011362999211996794
        model: {}
        policy_loss: -0.0017817486077547073
        total_loss: 0.006640349980443716
        vf_explained_var: 0.05540764331817627
        vf_loss: 88.63427734375
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48520439863204956
        entropy_coeff: 0.0017600000137463212
        kl: 0.001302559976466
        model: {}
        policy_loss: -0.0017428123392164707
        total_loss: 0.006183064077049494
        vf_explained_var: 0.06463377177715302
        vf_loss: 87.71697998046875
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9338340759277344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012892655795440078
        model: {}
        policy_loss: -0.0018296083435416222
        total_loss: -0.0005582058802247047
        vf_explained_var: 0.047243744134902954
        vf_loss: 29.06892204284668
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23931936919689178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007843762869015336
        model: {}
        policy_loss: -0.0014984429581090808
        total_loss: 0.0008727940730750561
        vf_explained_var: 0.08427339792251587
        vf_loss: 27.8753662109375
    load_time_ms: 17460.462
    num_steps_sampled: 92736000
    num_steps_trained: 92736000
    sample_time_ms: 103534.696
    update_time_ms: 629.703
  iterations_since_restore: 6
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.04153005464481
    ram_util_percent: 11.880874316939888
  pid: 12895
  policy_reward_max:
    agent-0: 269.0
    agent-1: 269.0
    agent-2: 449.0
    agent-3: 449.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 248.355
    agent-1: 248.355
    agent-2: 411.345
    agent-3: 411.345
    agent-4: 236.07
    agent-5: 236.07
  policy_reward_min:
    agent-0: 227.5
    agent-1: 227.5
    agent-2: 253.0
    agent-3: 253.0
    agent-4: 197.5
    agent-5: 197.5
  sampler_perf:
    mean_env_wait_ms: 26.475650142182676
    mean_inference_ms: 13.562981765694154
    mean_processing_ms: 60.19198608758881
  time_since_restore: 812.3566613197327
  time_this_iter_s: 128.74127578735352
  time_total_s: 123293.37936973572
  timestamp: 1637637466
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 92736000
  training_iteration: 966
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    966 |           123293 | 92736000 |  1791.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 31.07
    apples_agent-1_min: 18
    apples_agent-2_max: 404
    apples_agent-2_mean: 359.88
    apples_agent-2_min: 293
    apples_agent-3_max: 292
    apples_agent-3_mean: 246.29
    apples_agent-3_min: 189
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 386.07
    apples_agent-5_min: 297
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 462.82
    cleaning_beam_agent-0_min: 428
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 1.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.82
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 485
    cleaning_beam_agent-4_mean: 420.49
    cleaning_beam_agent-4_min: 312
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-19-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1960.0
  episode_reward_mean: 1796.93
  episode_reward_min: 1544.0
  episodes_this_iter: 96
  episodes_total: 92832
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12836.322
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36733195185661316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009179070475511253
        model: {}
        policy_loss: -0.001049684826284647
        total_loss: 0.001522622536867857
        vf_explained_var: 0.04128740727901459
        vf_loss: 32.15947341918945
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2070254534482956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006007919437251985
        model: {}
        policy_loss: -0.0015970142558217049
        total_loss: 0.0011912309564650059
        vf_explained_var: 0.058716028928756714
        vf_loss: 31.507335662841797
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2558397948741913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009461708832532167
        model: {}
        policy_loss: -0.001898929476737976
        total_loss: 0.006153319962322712
        vf_explained_var: 0.048097044229507446
        vf_loss: 84.99571990966797
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4761860966682434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017030099406838417
        model: {}
        policy_loss: -0.0017193108797073364
        total_loss: 0.005865655839443207
        vf_explained_var: 0.057638853788375854
        vf_loss: 84.17733001708984
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9215027093887329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018718168139457703
        model: {}
        policy_loss: -0.0017618881538510323
        total_loss: -0.00041370137478224933
        vf_explained_var: 0.035518303513526917
        vf_loss: 29.641830444335938
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23993006348609924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009967910591512918
        model: {}
        policy_loss: -0.0014659997541457415
        total_loss: 0.0009083210024982691
        vf_explained_var: 0.08960923552513123
        vf_loss: 27.93484878540039
    load_time_ms: 17075.316
    num_steps_sampled: 92832000
    num_steps_trained: 92832000
    sample_time_ms: 103386.567
    update_time_ms: 541.802
  iterations_since_restore: 7
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.8016393442623
    ram_util_percent: 11.873224043715844
  pid: 12895
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 441.0
    agent-3: 441.0
    agent-4: 275.0
    agent-5: 275.0
  policy_reward_mean:
    agent-0: 251.035
    agent-1: 251.035
    agent-2: 410.925
    agent-3: 410.925
    agent-4: 236.505
    agent-5: 236.505
  policy_reward_min:
    agent-0: 205.0
    agent-1: 205.0
    agent-2: 352.5
    agent-3: 352.5
    agent-4: 199.5
    agent-5: 199.5
  sampler_perf:
    mean_env_wait_ms: 26.486820187859465
    mean_inference_ms: 13.491447134190064
    mean_processing_ms: 60.13765029981687
  time_since_restore: 940.9629707336426
  time_this_iter_s: 128.6063094139099
  time_total_s: 123421.98567914963
  timestamp: 1637637595
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 92832000
  training_iteration: 967
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 21.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    967 |           123422 | 92832000 |  1796.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.09
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 30.7
    apples_agent-1_min: 10
    apples_agent-2_max: 418
    apples_agent-2_mean: 356.42
    apples_agent-2_min: 129
    apples_agent-3_max: 299
    apples_agent-3_mean: 245.21
    apples_agent-3_min: 80
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 383.68
    apples_agent-5_min: 126
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 465.95
    cleaning_beam_agent-0_min: 324
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.19
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.36
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 14.88
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 427.98
    cleaning_beam_agent-4_min: 328
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 3.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-22-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1938.0
  episode_reward_mean: 1778.51
  episode_reward_min: 581.0
  episodes_this_iter: 96
  episodes_total: 92928
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12636.256
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3816797137260437
        entropy_coeff: 0.0017600000137463212
        kl: 0.001613664673641324
        model: {}
        policy_loss: -0.001855360809713602
        total_loss: 0.0008809443097561598
        vf_explained_var: 0.1090548187494278
        vf_loss: 34.0554084777832
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2166653424501419
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011274784337729216
        model: {}
        policy_loss: -0.001895102672278881
        total_loss: 0.00119585613720119
        vf_explained_var: 0.09248408675193787
        vf_loss: 34.705284118652344
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25934886932373047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011990395141765475
        model: {}
        policy_loss: -0.0018331739120185375
        total_loss: 0.006780054420232773
        vf_explained_var: 0.10439784824848175
        vf_loss: 90.67810821533203
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4733065664768219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010102939559146762
        model: {}
        policy_loss: -0.0016300855204463005
        total_loss: 0.0071648419834673405
        vf_explained_var: 0.049019038677215576
        vf_loss: 96.26373291015625
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9221333861351013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012198390904814005
        model: {}
        policy_loss: -0.001896311528980732
        total_loss: -0.0001582074910402298
        vf_explained_var: 0.036188989877700806
        vf_loss: 33.591514587402344
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25086522102355957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007425336516462266
        model: {}
        policy_loss: -0.002311362884938717
        total_loss: 0.0003135331207886338
        vf_explained_var: 0.12115925550460815
        vf_loss: 30.652605056762695
    load_time_ms: 16776.083
    num_steps_sampled: 92928000
    num_steps_trained: 92928000
    sample_time_ms: 103352.45
    update_time_ms: 475.979
  iterations_since_restore: 8
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.052972972972977
    ram_util_percent: 11.822162162162163
  pid: 12895
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 444.5
    agent-3: 444.5
    agent-4: 262.5
    agent-5: 262.5
  policy_reward_mean:
    agent-0: 245.37
    agent-1: 245.37
    agent-2: 407.24
    agent-3: 407.24
    agent-4: 236.645
    agent-5: 236.645
  policy_reward_min:
    agent-0: 76.5
    agent-1: 76.5
    agent-2: 140.0
    agent-3: 140.0
    agent-4: 74.0
    agent-5: 74.0
  sampler_perf:
    mean_env_wait_ms: 26.521579730025927
    mean_inference_ms: 13.446240084137724
    mean_processing_ms: 60.17040886666821
  time_since_restore: 1070.1109948158264
  time_this_iter_s: 129.14802408218384
  time_total_s: 123551.13370323181
  timestamp: 1637637724
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 92928000
  training_iteration: 968
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    968 |           123551 | 92928000 |  1778.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 30.24
    apples_agent-1_min: 16
    apples_agent-2_max: 410
    apples_agent-2_mean: 359.27
    apples_agent-2_min: 252
    apples_agent-3_max: 293
    apples_agent-3_mean: 246.67
    apples_agent-3_min: 172
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 383.39
    apples_agent-5_min: 316
    cleaning_beam_agent-0_max: 505
    cleaning_beam_agent-0_mean: 468.51
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.05
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 1.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 13.08
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 432.49
    cleaning_beam_agent-4_min: 339
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-24-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1909.0
  episode_reward_mean: 1783.0
  episode_reward_min: 1394.0
  episodes_this_iter: 96
  episodes_total: 93024
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12494.88
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3689616918563843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018478564452379942
        model: {}
        policy_loss: -0.0014697043225169182
        total_loss: 0.0011588921770453453
        vf_explained_var: 0.04697798192501068
        vf_loss: 32.76528549194336
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21021392941474915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010159368393942714
        model: {}
        policy_loss: -0.0016895427834242582
        total_loss: 0.0011216079583391547
        vf_explained_var: 0.07476828992366791
        vf_loss: 31.80335807800293
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25324884057044983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008727944805286825
        model: {}
        policy_loss: -0.0017086840234696865
        total_loss: 0.006596580613404512
        vf_explained_var: 0.060066938400268555
        vf_loss: 87.50303649902344
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48286980390548706
        entropy_coeff: 0.0017600000137463212
        kl: 0.000851738965138793
        model: {}
        policy_loss: -0.0015578405000269413
        total_loss: 0.0064298612996935844
        vf_explained_var: 0.051455795764923096
        vf_loss: 88.36890411376953
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9183124899864197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008768988773226738
        model: {}
        policy_loss: -0.00182759715244174
        total_loss: -0.000472137238830328
        vf_explained_var: 0.03677928447723389
        vf_loss: 29.71006202697754
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2506919503211975
        entropy_coeff: 0.0017600000137463212
        kl: 0.000998263363726437
        model: {}
        policy_loss: -0.0017498350935056806
        total_loss: 0.0004982237005606294
        vf_explained_var: 0.12597642838954926
        vf_loss: 26.884981155395508
    load_time_ms: 16551.825
    num_steps_sampled: 93024000
    num_steps_trained: 93024000
    sample_time_ms: 103423.277
    update_time_ms: 424.683
  iterations_since_restore: 9
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.40594594594595
    ram_util_percent: 11.865945945945946
  pid: 12895
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 442.0
    agent-3: 442.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 246.19
    agent-1: 246.19
    agent-2: 409.445
    agent-3: 409.445
    agent-4: 235.865
    agent-5: 235.865
  policy_reward_min:
    agent-0: 185.5
    agent-1: 185.5
    agent-2: 310.5
    agent-3: 310.5
    agent-4: 184.5
    agent-5: 184.5
  sampler_perf:
    mean_env_wait_ms: 26.558595394532794
    mean_inference_ms: 13.417325840043805
    mean_processing_ms: 60.229485721369045
  time_since_restore: 1200.3519067764282
  time_this_iter_s: 130.2409119606018
  time_total_s: 123681.37461519241
  timestamp: 1637637855
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 93024000
  training_iteration: 969
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    969 |           123681 | 93024000 |     1783 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 31.04
    apples_agent-1_min: 11
    apples_agent-2_max: 414
    apples_agent-2_mean: 361.08
    apples_agent-2_min: 292
    apples_agent-3_max: 298
    apples_agent-3_mean: 246.76
    apples_agent-3_min: 186
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 425
    apples_agent-5_mean: 381.06
    apples_agent-5_min: 279
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 472.32
    cleaning_beam_agent-0_min: 422
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.98
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 12.94
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 516
    cleaning_beam_agent-4_mean: 435.27
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.67
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-26-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1890.0
  episode_reward_mean: 1795.63
  episode_reward_min: 1429.0
  episodes_this_iter: 96
  episodes_total: 93120
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12382.278
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3679042458534241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014636735431849957
        model: {}
        policy_loss: -0.001212284667417407
        total_loss: 0.0012825040612369776
        vf_explained_var: 0.03772808611392975
        vf_loss: 31.417322158813477
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20844818651676178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006505170604214072
        model: {}
        policy_loss: -0.00156519771553576
        total_loss: 0.0012003970332443714
        vf_explained_var: 0.03957556188106537
        vf_loss: 31.32210922241211
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2526984214782715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009981755865737796
        model: {}
        policy_loss: -0.0016755179967731237
        total_loss: 0.006589518394321203
        vf_explained_var: 0.0378088504076004
        vf_loss: 87.09397888183594
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4847312867641449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011650689411908388
        model: {}
        policy_loss: -0.0014787281397730112
        total_loss: 0.0062292213551700115
        vf_explained_var: 0.056635454297065735
        vf_loss: 85.60621643066406
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9269921183586121
        entropy_coeff: 0.0017600000137463212
        kl: 0.002748725935816765
        model: {}
        policy_loss: -0.0022444049827754498
        total_loss: -0.0008725384250283241
        vf_explained_var: 0.044883742928504944
        vf_loss: 30.022960662841797
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24718229472637177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009726083371788263
        model: {}
        policy_loss: -0.0017302746418863535
        total_loss: 0.0006507029756903648
        vf_explained_var: 0.10422827303409576
        vf_loss: 28.156415939331055
    load_time_ms: 17167.078
    num_steps_sampled: 93120000
    num_steps_trained: 93120000
    sample_time_ms: 103547.882
    update_time_ms: 383.639
  iterations_since_restore: 10
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.661111111111108
    ram_util_percent: 13.479797979797977
  pid: 12895
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 446.5
    agent-3: 446.5
    agent-4: 259.0
    agent-5: 259.0
  policy_reward_mean:
    agent-0: 248.645
    agent-1: 248.645
    agent-2: 413.735
    agent-3: 413.735
    agent-4: 235.435
    agent-5: 235.435
  policy_reward_min:
    agent-0: 184.5
    agent-1: 184.5
    agent-2: 345.5
    agent-3: 345.5
    agent-4: 184.5
    agent-5: 184.5
  sampler_perf:
    mean_env_wait_ms: 26.593301615895516
    mean_inference_ms: 13.39267747663278
    mean_processing_ms: 60.24891733625104
  time_since_restore: 1339.2414424419403
  time_this_iter_s: 138.88953566551208
  time_total_s: 123820.26415085793
  timestamp: 1637637994
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 93120000
  training_iteration: 970
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 26.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    970 |           123820 | 93120000 |  1795.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 30.47
    apples_agent-1_min: 8
    apples_agent-2_max: 409
    apples_agent-2_mean: 362.48
    apples_agent-2_min: 92
    apples_agent-3_max: 306
    apples_agent-3_mean: 244.91
    apples_agent-3_min: 62
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 386.31
    apples_agent-5_min: 108
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 464.33
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 14.58
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 424.96
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 3.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-28-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1941.0
  episode_reward_mean: 1796.74
  episode_reward_min: 487.0
  episodes_this_iter: 96
  episodes_total: 93216
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11318.26
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36925509572029114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014550166670233011
        model: {}
        policy_loss: -0.0014568949118256569
        total_loss: 0.0012672820594161749
        vf_explained_var: 0.07130372524261475
        vf_loss: 33.73781204223633
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2088710367679596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010011177510023117
        model: {}
        policy_loss: -0.0017633200623095036
        total_loss: 0.0012705978006124496
        vf_explained_var: 0.06545624136924744
        vf_loss: 34.01338195800781
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25586235523223877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016734434757381678
        model: {}
        policy_loss: -0.0020579076372087
        total_loss: 0.0064590140245854855
        vf_explained_var: 0.07734714448451996
        vf_loss: 89.66911315917969
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49596214294433594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022781069856137037
        model: {}
        policy_loss: -0.0017017987556755543
        total_loss: 0.006848171353340149
        vf_explained_var: 0.031073525547981262
        vf_loss: 94.2242202758789
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9139614105224609
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015077909920364618
        model: {}
        policy_loss: -0.0018767742440104485
        total_loss: -0.00024021416902542114
        vf_explained_var: 0.027519792318344116
        vf_loss: 32.44841003417969
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24418792128562927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008635458070784807
        model: {}
        policy_loss: -0.0018392729107290506
        total_loss: 0.0006932765245437622
        vf_explained_var: 0.1119508296251297
        vf_loss: 29.621551513671875
    load_time_ms: 16139.095
    num_steps_sampled: 93216000
    num_steps_trained: 93216000
    sample_time_ms: 103423.639
    update_time_ms: 14.292
  iterations_since_restore: 11
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.392893401015225
    ram_util_percent: 14.629441624365482
  pid: 12895
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 457.5
    agent-3: 457.5
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 248.835
    agent-1: 248.835
    agent-2: 411.705
    agent-3: 411.705
    agent-4: 237.83
    agent-5: 237.83
  policy_reward_min:
    agent-0: 73.0
    agent-1: 73.0
    agent-2: 104.0
    agent-3: 104.0
    agent-4: 66.5
    agent-5: 66.5
  sampler_perf:
    mean_env_wait_ms: 26.584786410267633
    mean_inference_ms: 13.36628436385623
    mean_processing_ms: 60.33027143356966
  time_since_restore: 1475.1200222969055
  time_this_iter_s: 135.8785798549652
  time_total_s: 123956.14273071289
  timestamp: 1637638132
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 93216000
  training_iteration: 971
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    971 |           123956 | 93216000 |  1796.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.68
    apples_agent-1_min: 16
    apples_agent-2_max: 424
    apples_agent-2_mean: 361.79
    apples_agent-2_min: 275
    apples_agent-3_max: 291
    apples_agent-3_mean: 242.47
    apples_agent-3_min: 155
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 449
    apples_agent-5_mean: 384.0
    apples_agent-5_min: 330
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 460.46
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 15.56
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 429.46
    cleaning_beam_agent-4_min: 319
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-30-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1917.0
  episode_reward_mean: 1797.93
  episode_reward_min: 1444.0
  episodes_this_iter: 96
  episodes_total: 93312
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11327.463
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37297701835632324
        entropy_coeff: 0.0017600000137463212
        kl: 0.001385984243825078
        model: {}
        policy_loss: -0.001251844223588705
        total_loss: 0.0012951683020219207
        vf_explained_var: 0.04993721842765808
        vf_loss: 32.033233642578125
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2086065709590912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007720760768279433
        model: {}
        policy_loss: -0.0016079891938716173
        total_loss: 0.0012473971582949162
        vf_explained_var: 0.04584847390651703
        vf_loss: 32.224632263183594
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25859305262565613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012473788810893893
        model: {}
        policy_loss: -0.001922718482092023
        total_loss: 0.0063773454166948795
        vf_explained_var: 0.05749623477458954
        vf_loss: 87.55067443847656
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48896464705467224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011824634857475758
        model: {}
        policy_loss: -0.001635854598134756
        total_loss: 0.006120511330664158
        vf_explained_var: 0.07235565781593323
        vf_loss: 86.16835021972656
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.927308976650238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033128177747130394
        model: {}
        policy_loss: -0.002308294177055359
        total_loss: -0.0008445540443062782
        vf_explained_var: 0.04463270306587219
        vf_loss: 30.954811096191406
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24746903777122498
        entropy_coeff: 0.0017600000137463212
        kl: 0.001169243361800909
        model: {}
        policy_loss: -0.0016789869405329227
        total_loss: 0.0008552605286240578
        vf_explained_var: 0.08658765256404877
        vf_loss: 29.696781158447266
    load_time_ms: 16023.901
    num_steps_sampled: 93312000
    num_steps_trained: 93312000
    sample_time_ms: 103047.878
    update_time_ms: 14.261
  iterations_since_restore: 12
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.615555555555556
    ram_util_percent: 14.744444444444444
  pid: 12895
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 457.0
    agent-3: 457.0
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 249.365
    agent-1: 249.365
    agent-2: 411.715
    agent-3: 411.715
    agent-4: 237.885
    agent-5: 237.885
  policy_reward_min:
    agent-0: 212.5
    agent-1: 212.5
    agent-2: 325.0
    agent-3: 325.0
    agent-4: 178.5
    agent-5: 178.5
  sampler_perf:
    mean_env_wait_ms: 26.500109203769046
    mean_inference_ms: 13.338358031770918
    mean_processing_ms: 60.13934067459
  time_since_restore: 1601.588121175766
  time_this_iter_s: 126.46809887886047
  time_total_s: 124082.61082959175
  timestamp: 1637638258
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 93312000
  training_iteration: 972
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    972 |           124083 | 93312000 |  1797.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 31.17
    apples_agent-1_min: 16
    apples_agent-2_max: 424
    apples_agent-2_mean: 357.18
    apples_agent-2_min: 241
    apples_agent-3_max: 287
    apples_agent-3_mean: 240.33
    apples_agent-3_min: 165
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 385.94
    apples_agent-5_min: 256
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 455.8
    cleaning_beam_agent-0_min: 370
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.91
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 1.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 15.18
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 475
    cleaning_beam_agent-4_mean: 402.21
    cleaning_beam_agent-4_min: 286
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-33-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1917.0
  episode_reward_mean: 1795.54
  episode_reward_min: 1171.0
  episodes_this_iter: 96
  episodes_total: 93408
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11336.317
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37514156103134155
        entropy_coeff: 0.0017600000137463212
        kl: 0.001439959043636918
        model: {}
        policy_loss: -0.0012977453880012035
        total_loss: 0.0013470400590449572
        vf_explained_var: 0.05483978986740112
        vf_loss: 33.04966354370117
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20867414772510529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006941379979252815
        model: {}
        policy_loss: -0.001576056471094489
        total_loss: 0.001336659537628293
        vf_explained_var: 0.06128823757171631
        vf_loss: 32.79951095581055
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2596985995769501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005891186883673072
        model: {}
        policy_loss: -0.001788873691111803
        total_loss: 0.006630764342844486
        vf_explained_var: 0.04819047451019287
        vf_loss: 88.76680755615234
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5211042165756226
        entropy_coeff: 0.0017600000137463212
        kl: 0.000862074492033571
        model: {}
        policy_loss: -0.0015739286318421364
        total_loss: 0.006245995406061411
        vf_explained_var: 0.06452347338199615
        vf_loss: 87.37025451660156
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9379693269729614
        entropy_coeff: 0.0017600000137463212
        kl: 0.001705348608084023
        model: {}
        policy_loss: -0.001966657117009163
        total_loss: -0.000546648632735014
        vf_explained_var: 0.06208758056163788
        vf_loss: 30.707534790039062
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2493913173675537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007555445190519094
        model: {}
        policy_loss: -0.001673004124313593
        total_loss: 0.0008463705889880657
        vf_explained_var: 0.09577657282352448
        vf_loss: 29.582691192626953
    load_time_ms: 15982.553
    num_steps_sampled: 93408000
    num_steps_trained: 93408000
    sample_time_ms: 102761.473
    update_time_ms: 14.381
  iterations_since_restore: 13
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.386111111111113
    ram_util_percent: 14.716111111111111
  pid: 12895
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 445.0
    agent-3: 445.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 248.965
    agent-1: 248.965
    agent-2: 410.495
    agent-3: 410.495
    agent-4: 238.31
    agent-5: 238.31
  policy_reward_min:
    agent-0: 165.0
    agent-1: 165.0
    agent-2: 267.0
    agent-3: 267.0
    agent-4: 153.5
    agent-5: 153.5
  sampler_perf:
    mean_env_wait_ms: 26.41523387574359
    mean_inference_ms: 13.300715834738543
    mean_processing_ms: 59.955763043861886
  time_since_restore: 1728.0217757225037
  time_this_iter_s: 126.43365454673767
  time_total_s: 124209.04448413849
  timestamp: 1637638385
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 93408000
  training_iteration: 973
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    973 |           124209 | 93408000 |  1795.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.02
    apples_agent-1_min: 18
    apples_agent-2_max: 399
    apples_agent-2_mean: 361.7
    apples_agent-2_min: 308
    apples_agent-3_max: 287
    apples_agent-3_mean: 241.72
    apples_agent-3_min: 152
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 388.15
    apples_agent-5_min: 340
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 455.56
    cleaning_beam_agent-0_min: 407
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.83
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 17.78
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 462
    cleaning_beam_agent-4_mean: 398.73
    cleaning_beam_agent-4_min: 312
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-35-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1934.0
  episode_reward_mean: 1808.88
  episode_reward_min: 1554.0
  episodes_this_iter: 96
  episodes_total: 93504
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11343.821
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37510496377944946
        entropy_coeff: 0.0017600000137463212
        kl: 0.001990901306271553
        model: {}
        policy_loss: -0.0012658205814659595
        total_loss: 0.0013103950768709183
        vf_explained_var: 0.0359921008348465
        vf_loss: 32.36354446411133
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20856517553329468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006843017181381583
        model: {}
        policy_loss: -0.0016145824920386076
        total_loss: 0.0012207010295242071
        vf_explained_var: 0.04387131333351135
        vf_loss: 32.02346420288086
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25586047768592834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009056062554009259
        model: {}
        policy_loss: -0.001702482346445322
        total_loss: 0.006405860651284456
        vf_explained_var: 0.03514723479747772
        vf_loss: 85.58639526367188
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5177234411239624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018874075030907989
        model: {}
        policy_loss: -0.0016573462635278702
        total_loss: 0.005817940924316645
        vf_explained_var: 0.05607910454273224
        vf_loss: 83.86434936523438
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9496772289276123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034909986425191164
        model: {}
        policy_loss: -0.002100376645103097
        total_loss: -0.0008250719401985407
        vf_explained_var: 0.040568798780441284
        vf_loss: 29.466556549072266
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2475726157426834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001069898484274745
        model: {}
        policy_loss: -0.0017992483917623758
        total_loss: 0.0005322848446667194
        vf_explained_var: 0.09849464893341064
        vf_loss: 27.672361373901367
    load_time_ms: 15984.203
    num_steps_sampled: 93504000
    num_steps_trained: 93504000
    sample_time_ms: 102673.033
    update_time_ms: 14.46
  iterations_since_restore: 14
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.176086956521736
    ram_util_percent: 14.768478260869568
  pid: 12895
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 453.5
    agent-3: 453.5
    agent-4: 273.0
    agent-5: 273.0
  policy_reward_mean:
    agent-0: 250.435
    agent-1: 250.435
    agent-2: 413.41
    agent-3: 413.41
    agent-4: 240.595
    agent-5: 240.595
  policy_reward_min:
    agent-0: 217.0
    agent-1: 217.0
    agent-2: 325.0
    agent-3: 325.0
    agent-4: 205.5
    agent-5: 205.5
  sampler_perf:
    mean_env_wait_ms: 26.345118022784522
    mean_inference_ms: 13.274406819873981
    mean_processing_ms: 59.819144842879005
  time_since_restore: 1856.8783876895905
  time_this_iter_s: 128.8566119670868
  time_total_s: 124337.90109610558
  timestamp: 1637638514
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 93504000
  training_iteration: 974
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    974 |           124338 | 93504000 |  1808.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.09
    apples_agent-1_min: 12
    apples_agent-2_max: 400
    apples_agent-2_mean: 353.98
    apples_agent-2_min: 87
    apples_agent-3_max: 295
    apples_agent-3_mean: 236.41
    apples_agent-3_min: 55
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 379.77
    apples_agent-5_min: 89
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 443.21
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.83
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 1.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.65
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 375.6
    cleaning_beam_agent-4_min: 289
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 2.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-37-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1911.0
  episode_reward_mean: 1768.09
  episode_reward_min: 419.0
  episodes_this_iter: 96
  episodes_total: 93600
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11344.884
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3713562786579132
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012701736995950341
        model: {}
        policy_loss: -0.0014749658294022083
        total_loss: 0.0012781918048858643
        vf_explained_var: 0.11973929405212402
        vf_loss: 34.06732940673828
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21612675487995148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010334048420190811
        model: {}
        policy_loss: -0.001941184513270855
        total_loss: 0.0011280833277851343
        vf_explained_var: 0.10932996869087219
        vf_loss: 34.496368408203125
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2657117545604706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010996107012033463
        model: {}
        policy_loss: -0.002172643318772316
        total_loss: 0.006586515344679356
        vf_explained_var: 0.09761254489421844
        vf_loss: 92.26799011230469
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5006919503211975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012610615231096745
        model: {}
        policy_loss: -0.0015802429988980293
        total_loss: 0.007167660631239414
        vf_explained_var: 0.056325703859329224
        vf_loss: 96.29108428955078
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9429729580879211
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018988584633916616
        model: {}
        policy_loss: -0.0018324735574424267
        total_loss: -0.000175421591848135
        vf_explained_var: 0.048939675092697144
        vf_loss: 33.16663360595703
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2572866380214691
        entropy_coeff: 0.0017600000137463212
        kl: 0.00147333147469908
        model: {}
        policy_loss: -0.0020197262056171894
        total_loss: 0.0006316774524748325
        vf_explained_var: 0.1098988801240921
        vf_loss: 31.042142868041992
    load_time_ms: 15994.279
    num_steps_sampled: 93600000
    num_steps_trained: 93600000
    sample_time_ms: 103082.491
    update_time_ms: 14.582
  iterations_since_restore: 15
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.872340425531913
    ram_util_percent: 14.697872340425532
  pid: 12895
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 447.5
    agent-3: 447.5
    agent-4: 273.0
    agent-5: 273.0
  policy_reward_mean:
    agent-0: 245.03
    agent-1: 245.03
    agent-2: 405.15
    agent-3: 405.15
    agent-4: 233.865
    agent-5: 233.865
  policy_reward_min:
    agent-0: 61.0
    agent-1: 61.0
    agent-2: 90.0
    agent-3: 90.0
    agent-4: 58.5
    agent-5: 58.5
  sampler_perf:
    mean_env_wait_ms: 26.307639922780535
    mean_inference_ms: 13.256078716039596
    mean_processing_ms: 59.76926502346685
  time_since_restore: 1988.9764895439148
  time_this_iter_s: 132.09810185432434
  time_total_s: 124469.9991979599
  timestamp: 1637638646
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 93600000
  training_iteration: 975
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    975 |           124470 | 93600000 |  1768.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 44
    apples_agent-1_mean: 28.82
    apples_agent-1_min: 17
    apples_agent-2_max: 416
    apples_agent-2_mean: 354.57
    apples_agent-2_min: 102
    apples_agent-3_max: 293
    apples_agent-3_mean: 235.66
    apples_agent-3_min: 88
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 383.02
    apples_agent-5_min: 134
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 443.26
    cleaning_beam_agent-0_min: 396
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.65
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 2.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.64
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 441
    cleaning_beam_agent-4_mean: 388.3
    cleaning_beam_agent-4_min: 291
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 2.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-39-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1939.0
  episode_reward_mean: 1784.11
  episode_reward_min: 599.0
  episodes_this_iter: 96
  episodes_total: 93696
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11339.387
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36910495162010193
        entropy_coeff: 0.0017600000137463212
        kl: 0.00202940939925611
        model: {}
        policy_loss: -0.0014406968839466572
        total_loss: 0.0013216715306043625
        vf_explained_var: 0.07383176684379578
        vf_loss: 34.11979675292969
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21181339025497437
        entropy_coeff: 0.0017600000137463212
        kl: 0.000676298514008522
        model: {}
        policy_loss: -0.001673565711826086
        total_loss: 0.0013863000785931945
        vf_explained_var: 0.06783506274223328
        vf_loss: 34.326541900634766
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2609383761882782
        entropy_coeff: 0.0017600000137463212
        kl: 0.00164708960801363
        model: {}
        policy_loss: -0.002050677314400673
        total_loss: 0.006495480891317129
        vf_explained_var: 0.06306996941566467
        vf_loss: 90.05401611328125
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5192539095878601
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014632639940828085
        model: {}
        policy_loss: -0.001725032925605774
        total_loss: 0.006614683195948601
        vf_explained_var: 0.034981369972229004
        vf_loss: 92.53596496582031
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9393067955970764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016835980350151658
        model: {}
        policy_loss: -0.0019436536822468042
        total_loss: -0.00035306462086737156
        vf_explained_var: 0.0423109233379364
        vf_loss: 32.4376220703125
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2514396607875824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006751441396772861
        model: {}
        policy_loss: -0.0017577828839421272
        total_loss: 0.0008195331320166588
        vf_explained_var: 0.10835927724838257
        vf_loss: 30.198461532592773
    load_time_ms: 15983.154
    num_steps_sampled: 93696000
    num_steps_trained: 93696000
    sample_time_ms: 102678.791
    update_time_ms: 14.373
  iterations_since_restore: 16
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.583146067415736
    ram_util_percent: 14.74438202247191
  pid: 12895
  policy_reward_max:
    agent-0: 282.0
    agent-1: 282.0
    agent-2: 444.0
    agent-3: 444.0
    agent-4: 269.0
    agent-5: 269.0
  policy_reward_mean:
    agent-0: 248.125
    agent-1: 248.125
    agent-2: 407.575
    agent-3: 407.575
    agent-4: 236.355
    agent-5: 236.355
  policy_reward_min:
    agent-0: 89.0
    agent-1: 89.0
    agent-2: 128.5
    agent-3: 128.5
    agent-4: 82.0
    agent-5: 82.0
  sampler_perf:
    mean_env_wait_ms: 26.241544115404178
    mean_inference_ms: 13.232544081503413
    mean_processing_ms: 59.64552544208148
  time_since_restore: 2113.454246044159
  time_this_iter_s: 124.47775650024414
  time_total_s: 124594.47695446014
  timestamp: 1637638771
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 93696000
  training_iteration: 976
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    976 |           124594 | 93696000 |  1784.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 30.87
    apples_agent-1_min: 17
    apples_agent-2_max: 406
    apples_agent-2_mean: 360.5
    apples_agent-2_min: 295
    apples_agent-3_max: 277
    apples_agent-3_mean: 238.46
    apples_agent-3_min: 157
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 387.88
    apples_agent-5_min: 338
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 430.46
    cleaning_beam_agent-0_min: 387
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.98
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 18.14
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 451
    cleaning_beam_agent-4_mean: 387.94
    cleaning_beam_agent-4_min: 300
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-41-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1939.0
  episode_reward_mean: 1801.67
  episode_reward_min: 1547.0
  episodes_this_iter: 96
  episodes_total: 93792
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11343.589
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35633668303489685
        entropy_coeff: 0.0017600000137463212
        kl: 0.001571486471220851
        model: {}
        policy_loss: -0.0014658875297755003
        total_loss: 0.00098512414842844
        vf_explained_var: 0.04957324266433716
        vf_loss: 30.78163719177246
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2090582549571991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010527705308049917
        model: {}
        policy_loss: -0.0016855662688612938
        total_loss: 0.0010423660278320312
        vf_explained_var: 0.04422132670879364
        vf_loss: 30.95873260498047
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2571778893470764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008494944777339697
        model: {}
        policy_loss: -0.0015878560952842236
        total_loss: 0.006523186340928078
        vf_explained_var: 0.04584924876689911
        vf_loss: 85.63672637939453
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5031224489212036
        entropy_coeff: 0.0017600000137463212
        kl: 0.00229657837189734
        model: {}
        policy_loss: -0.0017309054965153337
        total_loss: 0.005872383713722229
        vf_explained_var: 0.0539683997631073
        vf_loss: 84.8878173828125
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9389220476150513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027580633759498596
        model: {}
        policy_loss: -0.002051535528153181
        total_loss: -0.0008758017793297768
        vf_explained_var: 0.04916711151599884
        vf_loss: 28.282304763793945
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24785053730010986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006193633307702839
        model: {}
        policy_loss: -0.0015300551895052195
        total_loss: 0.0008601966546848416
        vf_explained_var: 0.05034415423870087
        vf_loss: 28.26466941833496
    load_time_ms: 15985.221
    num_steps_sampled: 93792000
    num_steps_trained: 93792000
    sample_time_ms: 102795.261
    update_time_ms: 14.205
  iterations_since_restore: 17
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.082702702702697
    ram_util_percent: 14.715675675675676
  pid: 12895
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 440.0
    agent-3: 440.0
    agent-4: 269.0
    agent-5: 269.0
  policy_reward_mean:
    agent-0: 250.27
    agent-1: 250.27
    agent-2: 412.29
    agent-3: 412.29
    agent-4: 238.275
    agent-5: 238.275
  policy_reward_min:
    agent-0: 222.5
    agent-1: 222.5
    agent-2: 323.0
    agent-3: 323.0
    agent-4: 205.5
    agent-5: 205.5
  sampler_perf:
    mean_env_wait_ms: 26.198041661142774
    mean_inference_ms: 13.21278245834184
    mean_processing_ms: 59.56384399330631
  time_since_restore: 2243.2866082191467
  time_this_iter_s: 129.8323621749878
  time_total_s: 124724.30931663513
  timestamp: 1637638900
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 93792000
  training_iteration: 977
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    977 |           124724 | 93792000 |  1801.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.21
    apples_agent-1_min: 11
    apples_agent-2_max: 418
    apples_agent-2_mean: 361.43
    apples_agent-2_min: 168
    apples_agent-3_max: 296
    apples_agent-3_mean: 245.08
    apples_agent-3_min: 102
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 460
    apples_agent-5_mean: 387.08
    apples_agent-5_min: 193
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 429.79
    cleaning_beam_agent-0_min: 400
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.16
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 1.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 16.51
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 402.4
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-43-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1908.0
  episode_reward_mean: 1796.62
  episode_reward_min: 830.0
  episodes_this_iter: 96
  episodes_total: 93888
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11354.753
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3530515432357788
        entropy_coeff: 0.0017600000137463212
        kl: 0.001161366468295455
        model: {}
        policy_loss: -0.0010738200508058071
        total_loss: 0.0014958978863433003
        vf_explained_var: 0.0889337807893753
        vf_loss: 31.910911560058594
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21167342364788055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005070889019407332
        model: {}
        policy_loss: -0.0015304218977689743
        total_loss: 0.0013797921128571033
        vf_explained_var: 0.06256972253322601
        vf_loss: 32.827579498291016
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25956010818481445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009739322122186422
        model: {}
        policy_loss: -0.0017375396564602852
        total_loss: 0.006735912524163723
        vf_explained_var: 0.056941062211990356
        vf_loss: 89.30276489257812
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5098844170570374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007161482935771346
        model: {}
        policy_loss: -0.001405432354658842
        total_loss: 0.006876086816191673
        vf_explained_var: 0.032780423760414124
        vf_loss: 91.7891845703125
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9343811273574829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013033407740294933
        model: {}
        policy_loss: -0.0018299100920557976
        total_loss: -0.00034470815444365144
        vf_explained_var: 0.044774577021598816
        vf_loss: 31.2971248626709
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24997228384017944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007373676635324955
        model: {}
        policy_loss: -0.0018097818829119205
        total_loss: 0.0007463716901838779
        vf_explained_var: 0.08414089679718018
        vf_loss: 29.96106719970703
    load_time_ms: 15987.702
    num_steps_sampled: 93888000
    num_steps_trained: 93888000
    sample_time_ms: 103180.186
    update_time_ms: 14.082
  iterations_since_restore: 18
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.80631578947369
    ram_util_percent: 14.787894736842103
  pid: 12895
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 454.5
    agent-3: 454.5
    agent-4: 262.5
    agent-5: 262.5
  policy_reward_mean:
    agent-0: 248.185
    agent-1: 248.185
    agent-2: 412.675
    agent-3: 412.675
    agent-4: 237.45
    agent-5: 237.45
  policy_reward_min:
    agent-0: 116.5
    agent-1: 116.5
    agent-2: 185.0
    agent-3: 185.0
    agent-4: 113.5
    agent-5: 113.5
  sampler_perf:
    mean_env_wait_ms: 26.18191886883145
    mean_inference_ms: 13.199151095393907
    mean_processing_ms: 59.54170386105667
  time_since_restore: 2376.426381587982
  time_this_iter_s: 133.13977336883545
  time_total_s: 124857.44909000397
  timestamp: 1637639034
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 93888000
  training_iteration: 978
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    978 |           124857 | 93888000 |  1796.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.54
    apples_agent-1_min: 13
    apples_agent-2_max: 418
    apples_agent-2_mean: 360.41
    apples_agent-2_min: 296
    apples_agent-3_max: 299
    apples_agent-3_mean: 238.39
    apples_agent-3_min: 166
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.16
    apples_agent-4_min: 0
    apples_agent-5_max: 451
    apples_agent-5_mean: 388.95
    apples_agent-5_min: 326
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 420.98
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.22
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 19.98
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 454
    cleaning_beam_agent-4_mean: 393.58
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 2.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-46-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1914.0
  episode_reward_mean: 1794.09
  episode_reward_min: 1462.0
  episodes_this_iter: 96
  episodes_total: 93984
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11345.187
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3503730893135071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013280771672725677
        model: {}
        policy_loss: -0.001266984734684229
        total_loss: 0.0014020380331203341
        vf_explained_var: 0.046779051423072815
        vf_loss: 32.85676956176758
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20798608660697937
        entropy_coeff: 0.0017600000137463212
        kl: 0.000982811558060348
        model: {}
        policy_loss: -0.001424779649823904
        total_loss: 0.0014171097427606583
        vf_explained_var: 0.06899505853652954
        vf_loss: 32.07948303222656
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25759291648864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.000851317192427814
        model: {}
        policy_loss: -0.00162358651868999
        total_loss: 0.006534083280712366
        vf_explained_var: 0.06265988945960999
        vf_loss: 86.1103286743164
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49989116191864014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012499159201979637
        model: {}
        policy_loss: -0.0015318390214815736
        total_loss: 0.006372477859258652
        vf_explained_var: 0.044003427028656006
        vf_loss: 87.84126281738281
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9396722316741943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011487278388813138
        model: {}
        policy_loss: -0.0018176203593611717
        total_loss: -0.0004491969011723995
        vf_explained_var: 0.054225459694862366
        vf_loss: 30.22244644165039
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24340501427650452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012959737796336412
        model: {}
        policy_loss: -0.0017107287421822548
        total_loss: 0.0008185090264305472
        vf_explained_var: 0.06967160105705261
        vf_loss: 29.576324462890625
    load_time_ms: 15999.26
    num_steps_sampled: 93984000
    num_steps_trained: 93984000
    sample_time_ms: 103708.98
    update_time_ms: 14.058
  iterations_since_restore: 19
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.389637305699484
    ram_util_percent: 14.795854922279794
  pid: 12895
  policy_reward_max:
    agent-0: 284.0
    agent-1: 284.0
    agent-2: 446.5
    agent-3: 446.5
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 249.77
    agent-1: 249.77
    agent-2: 408.2
    agent-3: 408.2
    agent-4: 239.075
    agent-5: 239.075
  policy_reward_min:
    agent-0: 209.0
    agent-1: 209.0
    agent-2: 327.0
    agent-3: 327.0
    agent-4: 195.0
    agent-5: 195.0
  sampler_perf:
    mean_env_wait_ms: 26.161497932985053
    mean_inference_ms: 13.186399207006357
    mean_processing_ms: 59.557511450012925
  time_since_restore: 2511.92729306221
  time_this_iter_s: 135.5009114742279
  time_total_s: 124992.9500014782
  timestamp: 1637639169
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 93984000
  training_iteration: 979
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 26.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    979 |           124993 | 93984000 |  1794.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 30.23
    apples_agent-1_min: 10
    apples_agent-2_max: 418
    apples_agent-2_mean: 363.24
    apples_agent-2_min: 126
    apples_agent-3_max: 295
    apples_agent-3_mean: 242.52
    apples_agent-3_min: 92
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 379.5
    apples_agent-5_min: 145
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 420.16
    cleaning_beam_agent-0_min: 369
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.95
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 16.88
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 470
    cleaning_beam_agent-4_mean: 403.91
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-48-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1891.0
  episode_reward_mean: 1783.0
  episode_reward_min: 696.0
  episodes_this_iter: 96
  episodes_total: 94080
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11342.641
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35263025760650635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018083311151713133
        model: {}
        policy_loss: -0.001677664928138256
        total_loss: 0.001096014166250825
        vf_explained_var: 0.08718241751194
        vf_loss: 33.94305419921875
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.206377774477005
        entropy_coeff: 0.0017600000137463212
        kl: 0.001007485087029636
        model: {}
        policy_loss: -0.0016643534181639552
        total_loss: 0.001364760915748775
        vf_explained_var: 0.08844690024852753
        vf_loss: 33.923377990722656
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2606430649757385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016384038608521223
        model: {}
        policy_loss: -0.0023491131141781807
        total_loss: 0.006525125354528427
        vf_explained_var: 0.07898476719856262
        vf_loss: 93.3296890258789
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5155049562454224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015268167480826378
        model: {}
        policy_loss: -0.0017673508264124393
        total_loss: 0.007218032609671354
        vf_explained_var: 0.025655150413513184
        vf_loss: 98.92674255371094
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9290346503257751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019812965765595436
        model: {}
        policy_loss: -0.0016877055168151855
        total_loss: -0.00014279200695455074
        vf_explained_var: 0.04191140830516815
        vf_loss: 31.800134658813477
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2552154064178467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010304258903488517
        model: {}
        policy_loss: -0.0019686035811901093
        total_loss: 0.0005288748070597649
        vf_explained_var: 0.11083914339542389
        vf_loss: 29.466567993164062
    load_time_ms: 15206.925
    num_steps_sampled: 94080000
    num_steps_trained: 94080000
    sample_time_ms: 104488.628
    update_time_ms: 13.919
  iterations_since_restore: 20
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.853030303030305
    ram_util_percent: 14.621212121212121
  pid: 12895
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 445.0
    agent-3: 445.0
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 248.095
    agent-1: 248.095
    agent-2: 409.175
    agent-3: 409.175
    agent-4: 234.23
    agent-5: 234.23
  policy_reward_min:
    agent-0: 90.5
    agent-1: 90.5
    agent-2: 163.5
    agent-3: 163.5
    agent-4: 94.0
    agent-5: 94.0
  sampler_perf:
    mean_env_wait_ms: 26.162339357136922
    mean_inference_ms: 13.17308976595465
    mean_processing_ms: 59.57789037962946
  time_since_restore: 2650.601588487625
  time_this_iter_s: 138.67429542541504
  time_total_s: 125131.62429690361
  timestamp: 1637639308
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 94080000
  training_iteration: 980
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    980 |           125132 | 94080000 |     1783 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.42
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 363.22
    apples_agent-2_min: 271
    apples_agent-3_max: 289
    apples_agent-3_mean: 241.82
    apples_agent-3_min: 173
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 386.58
    apples_agent-5_min: 299
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 412.99
    cleaning_beam_agent-0_min: 350
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.93
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.45
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 17.19
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 412.43
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-50-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1934.0
  episode_reward_mean: 1795.23
  episode_reward_min: 1382.0
  episodes_this_iter: 96
  episodes_total: 94176
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11366.142
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3490973114967346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014073812635615468
        model: {}
        policy_loss: -0.0009852582588791847
        total_loss: 0.0016570528969168663
        vf_explained_var: 0.05139508843421936
        vf_loss: 32.56723403930664
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21031521260738373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011881975224241614
        model: {}
        policy_loss: -0.0016005303477868438
        total_loss: 0.0012730450835078955
        vf_explained_var: 0.055365338921546936
        vf_loss: 32.437313079833984
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25715410709381104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007230285555124283
        model: {}
        policy_loss: -0.0017430249135941267
        total_loss: 0.006742596626281738
        vf_explained_var: 0.06781734526157379
        vf_loss: 89.38211822509766
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.512427568435669
        entropy_coeff: 0.0017600000137463212
        kl: 0.001466067973524332
        model: {}
        policy_loss: -0.001613500528037548
        total_loss: 0.006643255241215229
        vf_explained_var: 0.048915013670921326
        vf_loss: 91.5862808227539
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9310526251792908
        entropy_coeff: 0.0017600000137463212
        kl: 0.001739472965709865
        model: {}
        policy_loss: -0.0019343779422342777
        total_loss: -0.0006322325207293034
        vf_explained_var: 0.042314738035202026
        vf_loss: 29.4079532623291
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2582172453403473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010497912298887968
        model: {}
        policy_loss: -0.0017978765536099672
        total_loss: 0.0005794025491923094
        vf_explained_var: 0.07803654670715332
        vf_loss: 28.317401885986328
    load_time_ms: 14923.036
    num_steps_sampled: 94176000
    num_steps_trained: 94176000
    sample_time_ms: 104432.276
    update_time_ms: 13.777
  iterations_since_restore: 21
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.777083333333337
    ram_util_percent: 14.115104166666669
  pid: 12895
  policy_reward_max:
    agent-0: 277.0
    agent-1: 277.0
    agent-2: 445.5
    agent-3: 445.5
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 248.605
    agent-1: 248.605
    agent-2: 410.835
    agent-3: 410.835
    agent-4: 238.175
    agent-5: 238.175
  policy_reward_min:
    agent-0: 179.5
    agent-1: 179.5
    agent-2: 319.0
    agent-3: 319.0
    agent-4: 192.5
    agent-5: 192.5
  sampler_perf:
    mean_env_wait_ms: 26.124427508528253
    mean_inference_ms: 13.17918719135066
    mean_processing_ms: 59.52094306471922
  time_since_restore: 2783.341281414032
  time_this_iter_s: 132.73969292640686
  time_total_s: 125264.36398983002
  timestamp: 1637639443
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 94176000
  training_iteration: 981
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    981 |           125264 | 94176000 |  1795.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.14
    apples_agent-1_min: 18
    apples_agent-2_max: 415
    apples_agent-2_mean: 361.62
    apples_agent-2_min: 289
    apples_agent-3_max: 289
    apples_agent-3_mean: 243.95
    apples_agent-3_min: 191
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 389.14
    apples_agent-5_min: 345
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 405.86
    cleaning_beam_agent-0_min: 356
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 15.45
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 415.74
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-53-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1936.0
  episode_reward_mean: 1809.76
  episode_reward_min: 1523.0
  episodes_this_iter: 96
  episodes_total: 94272
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11468.594
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3436201810836792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012945827329531312
        model: {}
        policy_loss: -0.0011380305513739586
        total_loss: 0.0014529854524880648
        vf_explained_var: 0.03289280831813812
        vf_loss: 31.957901000976562
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20644550025463104
        entropy_coeff: 0.0017600000137463212
        kl: 0.000701267272233963
        model: {}
        policy_loss: -0.0015866511967033148
        total_loss: 0.0012240349315106869
        vf_explained_var: 0.039376214146614075
        vf_loss: 31.74030113220215
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25406205654144287
        entropy_coeff: 0.0017600000137463212
        kl: 0.001279839314520359
        model: {}
        policy_loss: -0.001660983543843031
        total_loss: 0.00637076236307621
        vf_explained_var: 0.061796724796295166
        vf_loss: 84.78897857666016
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5024721622467041
        entropy_coeff: 0.0017600000137463212
        kl: 0.001358098816126585
        model: {}
        policy_loss: -0.0014823167584836483
        total_loss: 0.006263004150241613
        vf_explained_var: 0.05072076618671417
        vf_loss: 86.29672241210938
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9351114630699158
        entropy_coeff: 0.0017600000137463212
        kl: 0.001979608554393053
        model: {}
        policy_loss: -0.0017868238501250744
        total_loss: -0.0004437323659658432
        vf_explained_var: 0.04490232467651367
        vf_loss: 29.888883590698242
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2553342580795288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009189979173243046
        model: {}
        policy_loss: -0.0014801167417317629
        total_loss: 0.0009242922533303499
        vf_explained_var: 0.08808019757270813
        vf_loss: 28.53800392150879
    load_time_ms: 14818.204
    num_steps_sampled: 94272000
    num_steps_trained: 94272000
    sample_time_ms: 107058.25
    update_time_ms: 13.845
  iterations_since_restore: 22
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.772935779816514
    ram_util_percent: 13.639449541284405
  pid: 12895
  policy_reward_max:
    agent-0: 286.5
    agent-1: 286.5
    agent-2: 460.0
    agent-3: 460.0
    agent-4: 258.5
    agent-5: 258.5
  policy_reward_mean:
    agent-0: 250.085
    agent-1: 250.085
    agent-2: 414.8
    agent-3: 414.8
    agent-4: 239.995
    agent-5: 239.995
  policy_reward_min:
    agent-0: 211.0
    agent-1: 211.0
    agent-2: 348.5
    agent-3: 348.5
    agent-4: 202.0
    agent-5: 202.0
  sampler_perf:
    mean_env_wait_ms: 26.160733903516046
    mean_inference_ms: 13.195150092371883
    mean_processing_ms: 59.58558520643487
  time_since_restore: 2935.9926176071167
  time_this_iter_s: 152.65133619308472
  time_total_s: 125417.0153260231
  timestamp: 1637639596
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 94272000
  training_iteration: 982
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    982 |           125417 | 94272000 |  1809.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 31.75
    apples_agent-1_min: 13
    apples_agent-2_max: 420
    apples_agent-2_mean: 358.53
    apples_agent-2_min: 148
    apples_agent-3_max: 294
    apples_agent-3_mean: 246.69
    apples_agent-3_min: 91
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 385.69
    apples_agent-5_min: 168
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 404.51
    cleaning_beam_agent-0_min: 203
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.27
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 14.7
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 410.69
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-55-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1895.0
  episode_reward_mean: 1789.96
  episode_reward_min: 762.0
  episodes_this_iter: 96
  episodes_total: 94368
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11544.386
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3505159616470337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014872897882014513
        model: {}
        policy_loss: -0.00138913094997406
        total_loss: 0.0012983642518520355
        vf_explained_var: 0.07056301832199097
        vf_loss: 33.04405975341797
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21363918483257294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008991601644083858
        model: {}
        policy_loss: -0.0018259179778397083
        total_loss: 0.0010332115925848484
        vf_explained_var: 0.08981777727603912
        vf_loss: 32.35138702392578
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2575715184211731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007205605506896973
        model: {}
        policy_loss: -0.0018259808421134949
        total_loss: 0.006639928091317415
        vf_explained_var: 0.0865851640701294
        vf_loss: 89.19233703613281
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4953731894493103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009865687461569905
        model: {}
        policy_loss: -0.0015300088562071323
        total_loss: 0.006868606433272362
        vf_explained_var: 0.05065849423408508
        vf_loss: 92.70478820800781
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9375068545341492
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017893996555358171
        model: {}
        policy_loss: -0.0019008051604032516
        total_loss: -0.00041318126022815704
        vf_explained_var: 0.0697990208864212
        vf_loss: 31.376317977905273
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26172810792922974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012513650581240654
        model: {}
        policy_loss: -0.0018930938094854355
        total_loss: 0.0007317885756492615
        vf_explained_var: 0.08691234886646271
        vf_loss: 30.855224609375
    load_time_ms: 14825.787
    num_steps_sampled: 94368000
    num_steps_trained: 94368000
    sample_time_ms: 109735.264
    update_time_ms: 13.653
  iterations_since_restore: 23
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.807762557077627
    ram_util_percent: 13.575342465753424
  pid: 12895
  policy_reward_max:
    agent-0: 284.0
    agent-1: 284.0
    agent-2: 447.0
    agent-3: 447.0
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 247.23
    agent-1: 247.23
    agent-2: 410.83
    agent-3: 410.83
    agent-4: 236.92
    agent-5: 236.92
  policy_reward_min:
    agent-0: 106.0
    agent-1: 106.0
    agent-2: 171.0
    agent-3: 171.0
    agent-4: 104.0
    agent-5: 104.0
  sampler_perf:
    mean_env_wait_ms: 26.23064301278516
    mean_inference_ms: 13.19593417807277
    mean_processing_ms: 59.73918938971698
  time_since_restore: 3090.030544757843
  time_this_iter_s: 154.03792715072632
  time_total_s: 125571.05325317383
  timestamp: 1637639750
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 94368000
  training_iteration: 983
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    983 |           125571 | 94368000 |  1789.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.63
    apples_agent-1_min: 18
    apples_agent-2_max: 415
    apples_agent-2_mean: 362.54
    apples_agent-2_min: 319
    apples_agent-3_max: 296
    apples_agent-3_mean: 249.53
    apples_agent-3_min: 180
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 431
    apples_agent-5_mean: 386.44
    apples_agent-5_min: 334
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 406.81
    cleaning_beam_agent-0_min: 380
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.08
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 1.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 14.68
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 422.29
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_22-58-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1931.0
  episode_reward_mean: 1804.87
  episode_reward_min: 1644.0
  episodes_this_iter: 96
  episodes_total: 94464
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11629.455
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34749096632003784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013988778227940202
        model: {}
        policy_loss: -0.001084609772078693
        total_loss: 0.001480874721892178
        vf_explained_var: 0.04341897368431091
        vf_loss: 31.77070426940918
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20752087235450745
        entropy_coeff: 0.0017600000137463212
        kl: 0.000603828695602715
        model: {}
        policy_loss: -0.001476784935221076
        total_loss: 0.0012464604806154966
        vf_explained_var: 0.0702952891588211
        vf_loss: 30.88481903076172
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25792762637138367
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012193704023957253
        model: {}
        policy_loss: -0.0017190029611811042
        total_loss: 0.00643315352499485
        vf_explained_var: 0.03670242428779602
        vf_loss: 86.06108093261719
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5036923289299011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009628609986975789
        model: {}
        policy_loss: -0.001374214654788375
        total_loss: 0.006246958859264851
        vf_explained_var: 0.049123615026474
        vf_loss: 85.07672119140625
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9434447884559631
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010997205972671509
        model: {}
        policy_loss: -0.001887020654976368
        total_loss: -0.0004977677017450333
        vf_explained_var: 0.028386220335960388
        vf_loss: 30.497182846069336
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.255202054977417
        entropy_coeff: 0.0017600000137463212
        kl: 0.001301613636314869
        model: {}
        policy_loss: -0.0015083863399922848
        total_loss: 0.0009189480915665627
        vf_explained_var: 0.08180698752403259
        vf_loss: 28.764907836914062
    load_time_ms: 14830.016
    num_steps_sampled: 94464000
    num_steps_trained: 94464000
    sample_time_ms: 112266.698
    update_time_ms: 14.313
  iterations_since_restore: 24
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.682805429864253
    ram_util_percent: 13.553846153846152
  pid: 12895
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 453.0
    agent-3: 453.0
    agent-4: 261.5
    agent-5: 261.5
  policy_reward_mean:
    agent-0: 249.665
    agent-1: 249.665
    agent-2: 414.41
    agent-3: 414.41
    agent-4: 238.36
    agent-5: 238.36
  policy_reward_min:
    agent-0: 215.0
    agent-1: 215.0
    agent-2: 354.0
    agent-3: 354.0
    agent-4: 204.0
    agent-5: 204.0
  sampler_perf:
    mean_env_wait_ms: 26.287192683381498
    mean_inference_ms: 13.212555823314533
    mean_processing_ms: 59.8713594302664
  time_since_restore: 3245.061134815216
  time_this_iter_s: 155.03059005737305
  time_total_s: 125726.0838432312
  timestamp: 1637639905
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 94464000
  training_iteration: 984
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    984 |           125726 | 94464000 |  1804.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 30.78
    apples_agent-1_min: 17
    apples_agent-2_max: 419
    apples_agent-2_mean: 364.46
    apples_agent-2_min: 300
    apples_agent-3_max: 304
    apples_agent-3_mean: 246.52
    apples_agent-3_min: 186
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 389.82
    apples_agent-5_min: 313
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 403.64
    cleaning_beam_agent-0_min: 361
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 16.12
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 420.79
    cleaning_beam_agent-4_min: 345
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-01-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1915.0
  episode_reward_mean: 1808.2
  episode_reward_min: 1529.0
  episodes_this_iter: 96
  episodes_total: 94560
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11752.43
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3466615676879883
        entropy_coeff: 0.0017600000137463212
        kl: 0.00148497661575675
        model: {}
        policy_loss: -0.0013394271954894066
        total_loss: 0.0013123429380357265
        vf_explained_var: 0.03391990065574646
        vf_loss: 32.618953704833984
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20595873892307281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009370686602778733
        model: {}
        policy_loss: -0.0016835166607052088
        total_loss: 0.0011782583314925432
        vf_explained_var: 0.043814197182655334
        vf_loss: 32.24262619018555
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25734978914260864
        entropy_coeff: 0.0017600000137463212
        kl: 0.001231272704899311
        model: {}
        policy_loss: -0.0016383249312639236
        total_loss: 0.006701994687318802
        vf_explained_var: 0.04313063621520996
        vf_loss: 87.93253326416016
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49849826097488403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018974894192069769
        model: {}
        policy_loss: -0.0015887562185525894
        total_loss: 0.006162448786199093
        vf_explained_var: 0.06339196860790253
        vf_loss: 86.28561401367188
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9348958730697632
        entropy_coeff: 0.0017600000137463212
        kl: 0.002226973418146372
        model: {}
        policy_loss: -0.0019748001359403133
        total_loss: -0.0006385187152773142
        vf_explained_var: 0.038842275738716125
        vf_loss: 29.816991806030273
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2503700256347656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013180200476199389
        model: {}
        policy_loss: -0.0017842603847384453
        total_loss: 0.0005950136110186577
        vf_explained_var: 0.08821333944797516
        vf_loss: 28.199256896972656
    load_time_ms: 15189.596
    num_steps_sampled: 94560000
    num_steps_trained: 94560000
    sample_time_ms: 114067.756
    update_time_ms: 14.564
  iterations_since_restore: 25
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.974208144796382
    ram_util_percent: 13.642533936651583
  pid: 12895
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 456.5
    agent-3: 456.5
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 248.655
    agent-1: 248.655
    agent-2: 416.155
    agent-3: 416.155
    agent-4: 239.29
    agent-5: 239.29
  policy_reward_min:
    agent-0: 214.5
    agent-1: 214.5
    agent-2: 354.0
    agent-3: 354.0
    agent-4: 193.0
    agent-5: 193.0
  sampler_perf:
    mean_env_wait_ms: 26.310101665583176
    mean_inference_ms: 13.22881035894094
    mean_processing_ms: 59.91246189776585
  time_since_restore: 3400.0551426410675
  time_this_iter_s: 154.99400782585144
  time_total_s: 125881.07785105705
  timestamp: 1637640060
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 94560000
  training_iteration: 985
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    985 |           125881 | 94560000 |   1808.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 29.96
    apples_agent-1_min: 1
    apples_agent-2_max: 406
    apples_agent-2_mean: 358.43
    apples_agent-2_min: 2
    apples_agent-3_max: 294
    apples_agent-3_mean: 241.35
    apples_agent-3_min: 6
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 381.99
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 401.0
    cleaning_beam_agent-0_min: 333
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 1.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 15.89
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 424.94
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 2.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-03-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1920.0
  episode_reward_mean: 1775.93
  episode_reward_min: -18.0
  episodes_this_iter: 96
  episodes_total: 94656
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11839.073
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35739266872406006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015302333049476147
        model: {}
        policy_loss: -0.0016935830935835838
        total_loss: 0.0012094809208065271
        vf_explained_var: 0.17205318808555603
        vf_loss: 35.32077407836914
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.212550550699234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006326306611299515
        model: {}
        policy_loss: -0.0020393389277160168
        total_loss: 0.0012627700343728065
        vf_explained_var: 0.13645482063293457
        vf_loss: 36.76197052001953
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2672503590583801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009363871067762375
        model: {}
        policy_loss: -0.002271336503326893
        total_loss: 0.00723527604714036
        vf_explained_var: 0.15201056003570557
        vf_loss: 99.76973724365234
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4954148530960083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019473962020128965
        model: {}
        policy_loss: -0.0019003741908818483
        total_loss: 0.00854115653783083
        vf_explained_var: 0.03956781327724457
        vf_loss: 113.13460540771484
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.924323320388794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028747664764523506
        model: {}
        policy_loss: -0.0019481368362903595
        total_loss: 0.0005753482691943645
        vf_explained_var: 0.01460084319114685
        vf_loss: 41.50292205810547
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25798648595809937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013618620578199625
        model: {}
        policy_loss: -0.0023582270368933678
        total_loss: 0.0007996440399438143
        vf_explained_var: 0.14225344359874725
        vf_loss: 36.11928939819336
    load_time_ms: 15234.03
    num_steps_sampled: 94656000
    num_steps_trained: 94656000
    sample_time_ms: 116451.656
    update_time_ms: 14.934
  iterations_since_restore: 26
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.00280373831776
    ram_util_percent: 13.580373831775699
  pid: 12895
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 449.5
    agent-3: 449.5
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 246.06
    agent-1: 246.06
    agent-2: 407.4
    agent-3: 407.4
    agent-4: 234.505
    agent-5: 234.505
  policy_reward_min:
    agent-0: 4.0
    agent-1: 4.0
    agent-2: 7.5
    agent-3: 7.5
    agent-4: -21.0
    agent-5: -21.0
  sampler_perf:
    mean_env_wait_ms: 26.313707789002265
    mean_inference_ms: 13.248723487015315
    mean_processing_ms: 59.93590991310322
  time_since_restore: 3549.6914551258087
  time_this_iter_s: 149.6363124847412
  time_total_s: 126030.7141635418
  timestamp: 1637640210
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 94656000
  training_iteration: 986
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    986 |           126031 | 94656000 |  1775.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.65
    apples_agent-1_min: 18
    apples_agent-2_max: 415
    apples_agent-2_mean: 365.25
    apples_agent-2_min: 307
    apples_agent-3_max: 304
    apples_agent-3_mean: 252.31
    apples_agent-3_min: 206
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 390.25
    apples_agent-5_min: 335
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 409.61
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.24
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 14.81
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 440.16
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-06-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1947.0
  episode_reward_mean: 1817.42
  episode_reward_min: 1551.0
  episodes_this_iter: 96
  episodes_total: 94752
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11939.099
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3444406986236572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012775559443980455
        model: {}
        policy_loss: -0.0011685355566442013
        total_loss: 0.0015395833179354668
        vf_explained_var: 0.02344280481338501
        vf_loss: 33.14335632324219
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20337750017642975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011026808060705662
        model: {}
        policy_loss: -0.0015563089400529861
        total_loss: 0.001212839037179947
        vf_explained_var: 0.07833258807659149
        vf_loss: 31.27090072631836
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2571958303451538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010226243175566196
        model: {}
        policy_loss: -0.00177859328687191
        total_loss: 0.006633505690842867
        vf_explained_var: 0.028981804847717285
        vf_loss: 88.64762878417969
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49176648259162903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011907422449439764
        model: {}
        policy_loss: -0.001549574313685298
        total_loss: 0.00636581564322114
        vf_explained_var: 0.04323582351207733
        vf_loss: 87.80899810791016
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9221647381782532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016889937687665224
        model: {}
        policy_loss: -0.0017352905124425888
        total_loss: -0.0004255371168255806
        vf_explained_var: 0.05595311522483826
        vf_loss: 29.327632904052734
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24224025011062622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009975674329325557
        model: {}
        policy_loss: -0.0016959747299551964
        total_loss: 0.0007357548456639051
        vf_explained_var: 0.08128264546394348
        vf_loss: 28.580726623535156
    load_time_ms: 15261.335
    num_steps_sampled: 94752000
    num_steps_trained: 94752000
    sample_time_ms: 118491.092
    update_time_ms: 15.133
  iterations_since_restore: 27
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.946296296296296
    ram_util_percent: 13.585185185185184
  pid: 12895
  policy_reward_max:
    agent-0: 283.0
    agent-1: 283.0
    agent-2: 464.0
    agent-3: 464.0
    agent-4: 258.0
    agent-5: 258.0
  policy_reward_mean:
    agent-0: 252.06
    agent-1: 252.06
    agent-2: 417.375
    agent-3: 417.375
    agent-4: 239.275
    agent-5: 239.275
  policy_reward_min:
    agent-0: 177.5
    agent-1: 177.5
    agent-2: 371.0
    agent-3: 371.0
    agent-4: 210.0
    agent-5: 210.0
  sampler_perf:
    mean_env_wait_ms: 26.348237419736478
    mean_inference_ms: 13.254871915512782
    mean_processing_ms: 59.99169863248689
  time_since_restore: 3701.2335307598114
  time_this_iter_s: 151.54207563400269
  time_total_s: 126182.2562391758
  timestamp: 1637640362
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 94752000
  training_iteration: 987
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    987 |           126182 | 94752000 |  1817.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 44
    apples_agent-1_mean: 30.31
    apples_agent-1_min: 14
    apples_agent-2_max: 418
    apples_agent-2_mean: 368.93
    apples_agent-2_min: 301
    apples_agent-3_max: 293
    apples_agent-3_mean: 248.96
    apples_agent-3_min: 191
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 392.82
    apples_agent-5_min: 309
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 403.8
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.92
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 1.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 15.49
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 452.73
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-08-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1920.0
  episode_reward_mean: 1812.54
  episode_reward_min: 1640.0
  episodes_this_iter: 96
  episodes_total: 94848
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12037.906
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3410550057888031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014032574836164713
        model: {}
        policy_loss: -0.0013698344118893147
        total_loss: 0.0014436445198953152
        vf_explained_var: 0.018612593412399292
        vf_loss: 34.137367248535156
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20711728930473328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005943677970208228
        model: {}
        policy_loss: -0.0015845021698623896
        total_loss: 0.0012156429002061486
        vf_explained_var: 0.08979709446430206
        vf_loss: 31.646747589111328
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25319045782089233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013014054857194424
        model: {}
        policy_loss: -0.0018546548672020435
        total_loss: 0.006581053137779236
        vf_explained_var: 0.049778878688812256
        vf_loss: 88.81320190429688
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.486846923828125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007341962773352861
        model: {}
        policy_loss: -0.0013078022748231888
        total_loss: 0.006728068459779024
        vf_explained_var: 0.04943211376667023
        vf_loss: 88.92724609375
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9208471179008484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016529301647096872
        model: {}
        policy_loss: -0.002155821770429611
        total_loss: -0.0008116307435557246
        vf_explained_var: 0.040294989943504333
        vf_loss: 29.64883041381836
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2356032133102417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007872488349676132
        model: {}
        policy_loss: -0.0014753099530935287
        total_loss: 0.0009469324722886086
        vf_explained_var: 0.08196592330932617
        vf_loss: 28.369083404541016
    load_time_ms: 15602.312
    num_steps_sampled: 94848000
    num_steps_trained: 94848000
    sample_time_ms: 120273.766
    update_time_ms: 15.289
  iterations_since_restore: 28
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.618552036199095
    ram_util_percent: 13.617194570135746
  pid: 12895
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 449.5
    agent-3: 449.5
    agent-4: 271.0
    agent-5: 271.0
  policy_reward_mean:
    agent-0: 250.395
    agent-1: 250.395
    agent-2: 415.645
    agent-3: 415.645
    agent-4: 240.23
    agent-5: 240.23
  policy_reward_min:
    agent-0: 142.0
    agent-1: 142.0
    agent-2: 372.0
    agent-3: 372.0
    agent-4: 187.5
    agent-5: 187.5
  sampler_perf:
    mean_env_wait_ms: 26.369633350815835
    mean_inference_ms: 13.263228368984546
    mean_processing_ms: 60.02993571935122
  time_since_restore: 3856.7024099826813
  time_this_iter_s: 155.46887922286987
  time_total_s: 126337.72511839867
  timestamp: 1637640517
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 94848000
  training_iteration: 988
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    988 |           126338 | 94848000 |  1812.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 30.68
    apples_agent-1_min: 16
    apples_agent-2_max: 401
    apples_agent-2_mean: 364.25
    apples_agent-2_min: 191
    apples_agent-3_max: 289
    apples_agent-3_mean: 248.89
    apples_agent-3_min: 133
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.15
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 390.43
    apples_agent-5_min: 199
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 398.61
    cleaning_beam_agent-0_min: 368
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 0.81
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 1.59
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 15.4
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 449.74
    cleaning_beam_agent-4_min: 217
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-11-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1928.0
  episode_reward_mean: 1805.31
  episode_reward_min: 889.0
  episodes_this_iter: 96
  episodes_total: 94944
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12163.94
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.342180997133255
        entropy_coeff: 0.0017600000137463212
        kl: 0.001420905813574791
        model: {}
        policy_loss: -0.0014313329011201859
        total_loss: 0.0015166476368904114
        vf_explained_var: 0.01936870813369751
        vf_loss: 35.50218200683594
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2063782513141632
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009800486732274294
        model: {}
        policy_loss: -0.0017435518093407154
        total_loss: 0.0012754248455166817
        vf_explained_var: 0.064537912607193
        vf_loss: 33.8220100402832
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2553752064704895
        entropy_coeff: 0.0017600000137463212
        kl: 0.001478791469708085
        model: {}
        policy_loss: -0.001932334154844284
        total_loss: 0.006511347368359566
        vf_explained_var: 0.06573712825775146
        vf_loss: 88.93145751953125
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48309072852134705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008140355930663645
        model: {}
        policy_loss: -0.0016353833489120007
        total_loss: 0.006465056911110878
        vf_explained_var: 0.05964180827140808
        vf_loss: 89.50682067871094
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9141516089439392
        entropy_coeff: 0.0017600000137463212
        kl: 0.003190986579284072
        model: {}
        policy_loss: -0.002130890730768442
        total_loss: -0.000684153288602829
        vf_explained_var: 0.08280667662620544
        vf_loss: 30.556459426879883
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2359931468963623
        entropy_coeff: 0.0017600000137463212
        kl: 0.000566045637242496
        model: {}
        policy_loss: -0.0016116169281303883
        total_loss: 0.000986053142696619
        vf_explained_var: 0.09501735866069794
        vf_loss: 30.130168914794922
    load_time_ms: 15613.01
    num_steps_sampled: 94944000
    num_steps_trained: 94944000
    sample_time_ms: 121977.256
    update_time_ms: 15.411
  iterations_since_restore: 29
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.77409090909091
    ram_util_percent: 13.595454545454546
  pid: 12895
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 460.0
    agent-3: 460.0
    agent-4: 267.5
    agent-5: 267.5
  policy_reward_mean:
    agent-0: 249.545
    agent-1: 249.545
    agent-2: 412.89
    agent-3: 412.89
    agent-4: 240.22
    agent-5: 240.22
  policy_reward_min:
    agent-0: 122.5
    agent-1: 122.5
    agent-2: 211.0
    agent-3: 211.0
    agent-4: 111.0
    agent-5: 111.0
  sampler_perf:
    mean_env_wait_ms: 26.395024372817954
    mean_inference_ms: 13.27500339064195
    mean_processing_ms: 60.082065616434456
  time_since_restore: 4010.6548080444336
  time_this_iter_s: 153.95239806175232
  time_total_s: 126491.67751646042
  timestamp: 1637640671
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 94944000
  training_iteration: 989
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    989 |           126492 | 94944000 |  1805.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.93
    apples_agent-1_min: 16
    apples_agent-2_max: 427
    apples_agent-2_mean: 365.88
    apples_agent-2_min: 237
    apples_agent-3_max: 310
    apples_agent-3_mean: 246.12
    apples_agent-3_min: 153
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.1
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 392.8
    apples_agent-5_min: 274
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 397.1
    cleaning_beam_agent-0_min: 331
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 15.7
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 506
    cleaning_beam_agent-4_mean: 441.35
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.24
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-13-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1942.0
  episode_reward_mean: 1802.68
  episode_reward_min: 1206.0
  episodes_this_iter: 96
  episodes_total: 95040
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12263.252
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34622621536254883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011436083586886525
        model: {}
        policy_loss: -0.001356614287942648
        total_loss: 0.0012518499279394746
        vf_explained_var: 0.04099321365356445
        vf_loss: 32.17823028564453
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20640002191066742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009422901785001159
        model: {}
        policy_loss: -0.0017629344947636127
        total_loss: 0.0009532696567475796
        vf_explained_var: 0.0825890302658081
        vf_loss: 30.7946720123291
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25594550371170044
        entropy_coeff: 0.0017600000137463212
        kl: 0.000982558587566018
        model: {}
        policy_loss: -0.0018499474972486496
        total_loss: 0.006465233862400055
        vf_explained_var: 0.05641546845436096
        vf_loss: 87.65644836425781
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4892318546772003
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009340338292531669
        model: {}
        policy_loss: -0.0017410998698323965
        total_loss: 0.006113721989095211
        vf_explained_var: 0.06339555978775024
        vf_loss: 87.15870666503906
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9269669651985168
        entropy_coeff: 0.0017600000137463212
        kl: 0.002183009637519717
        model: {}
        policy_loss: -0.001881162403151393
        total_loss: -0.0003991307457908988
        vf_explained_var: 0.044112905859947205
        vf_loss: 31.134939193725586
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24286219477653503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014181550359353423
        model: {}
        policy_loss: -0.0018369369208812714
        total_loss: 0.0006663231179118156
        vf_explained_var: 0.10143579542636871
        vf_loss: 29.306982040405273
    load_time_ms: 15646.65
    num_steps_sampled: 95040000
    num_steps_trained: 95040000
    sample_time_ms: 123157.607
    update_time_ms: 15.608
  iterations_since_restore: 30
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.90138888888889
    ram_util_percent: 13.665740740740743
  pid: 12895
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 458.5
    agent-3: 458.5
    agent-4: 266.0
    agent-5: 266.0
  policy_reward_mean:
    agent-0: 249.63
    agent-1: 249.63
    agent-2: 412.215
    agent-3: 412.215
    agent-4: 239.495
    agent-5: 239.495
  policy_reward_min:
    agent-0: 166.0
    agent-1: 166.0
    agent-2: 264.5
    agent-3: 264.5
    agent-4: 164.5
    agent-5: 164.5
  sampler_perf:
    mean_env_wait_ms: 26.411001065123273
    mean_inference_ms: 13.279190231674658
    mean_processing_ms: 60.12046844699894
  time_since_restore: 4162.465459346771
  time_this_iter_s: 151.81065130233765
  time_total_s: 126643.48816776276
  timestamp: 1637640823
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 95040000
  training_iteration: 990
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    990 |           126643 | 95040000 |  1802.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.51
    apples_agent-1_min: 17
    apples_agent-2_max: 419
    apples_agent-2_mean: 366.87
    apples_agent-2_min: 287
    apples_agent-3_max: 289
    apples_agent-3_mean: 251.01
    apples_agent-3_min: 202
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 443
    apples_agent-5_mean: 393.08
    apples_agent-5_min: 342
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 398.73
    cleaning_beam_agent-0_min: 371
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.17
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 15.19
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 432.46
    cleaning_beam_agent-4_min: 331
    cleaning_beam_agent-5_max: 6
    cleaning_beam_agent-5_mean: 1.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-16-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1931.0
  episode_reward_mean: 1816.97
  episode_reward_min: 1473.0
  episodes_this_iter: 96
  episodes_total: 95136
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12240.713
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3390764594078064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018216869793832302
        model: {}
        policy_loss: -0.0015942440368235111
        total_loss: 0.0010211423505097628
        vf_explained_var: 0.029301270842552185
        vf_loss: 32.12160110473633
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2044706642627716
        entropy_coeff: 0.0017600000137463212
        kl: 0.001232791692018509
        model: {}
        policy_loss: -0.0015690627042204142
        total_loss: 0.0011745248921215534
        vf_explained_var: 0.06296603381633759
        vf_loss: 31.0345458984375
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2556939721107483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010195757495239377
        model: {}
        policy_loss: -0.001572563429363072
        total_loss: 0.006915600039064884
        vf_explained_var: 0.032648101449012756
        vf_loss: 89.38185119628906
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4763016402721405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016735214740037918
        model: {}
        policy_loss: -0.0017285938374698162
        total_loss: 0.006079558283090591
        vf_explained_var: 0.06699728965759277
        vf_loss: 86.46441650390625
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9392991662025452
        entropy_coeff: 0.0017600000137463212
        kl: 0.001344016520306468
        model: {}
        policy_loss: -0.0018861969001591206
        total_loss: -0.0004574516788125038
        vf_explained_var: 0.04646046459674835
        vf_loss: 30.819124221801758
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23965470492839813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008212293032556772
        model: {}
        policy_loss: -0.0015277289785444736
        total_loss: 0.0009757713414728642
        vf_explained_var: 0.09971725940704346
        vf_loss: 29.25292205810547
    load_time_ms: 15658.816
    num_steps_sampled: 95136000
    num_steps_trained: 95136000
    sample_time_ms: 125486.226
    update_time_ms: 15.802
  iterations_since_restore: 31
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.470666666666666
    ram_util_percent: 13.630222222222224
  pid: 12895
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 452.5
    agent-3: 452.5
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 249.815
    agent-1: 249.815
    agent-2: 417.065
    agent-3: 417.065
    agent-4: 241.605
    agent-5: 241.605
  policy_reward_min:
    agent-0: 191.0
    agent-1: 191.0
    agent-2: 332.0
    agent-3: 332.0
    agent-4: 212.0
    agent-5: 212.0
  sampler_perf:
    mean_env_wait_ms: 26.446892430025265
    mean_inference_ms: 13.282869849905131
    mean_processing_ms: 60.197295212245145
  time_since_restore: 4318.490860462189
  time_this_iter_s: 156.02540111541748
  time_total_s: 126799.51356887817
  timestamp: 1637640981
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 95136000
  training_iteration: 991
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    991 |           126800 | 95136000 |  1816.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.4
    apples_agent-1_min: 18
    apples_agent-2_max: 417
    apples_agent-2_mean: 364.86
    apples_agent-2_min: 302
    apples_agent-3_max: 303
    apples_agent-3_mean: 246.46
    apples_agent-3_min: 202
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 450
    apples_agent-5_mean: 389.91
    apples_agent-5_min: 279
    cleaning_beam_agent-0_max: 414
    cleaning_beam_agent-0_mean: 389.53
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.97
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 15.83
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 429.52
    cleaning_beam_agent-4_min: 357
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 1.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-18-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1938.0
  episode_reward_mean: 1808.22
  episode_reward_min: 1463.0
  episodes_this_iter: 96
  episodes_total: 95232
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12256.293
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33957281708717346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014981217682361603
        model: {}
        policy_loss: -0.0011014589108526707
        total_loss: 0.001632419414818287
        vf_explained_var: 0.02900928258895874
        vf_loss: 33.31529235839844
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2024308741092682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010375448036938906
        model: {}
        policy_loss: -0.0016733692027628422
        total_loss: 0.0012146474327892065
        vf_explained_var: 0.054145753383636475
        vf_loss: 32.44291687011719
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2558116316795349
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007860143668949604
        model: {}
        policy_loss: -0.0016179485246539116
        total_loss: 0.00629407586529851
        vf_explained_var: 0.04975810647010803
        vf_loss: 83.62249755859375
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.482820987701416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007755015976727009
        model: {}
        policy_loss: -0.0014570695348083973
        total_loss: 0.005847436375916004
        vf_explained_var: 0.07309794425964355
        vf_loss: 81.54273223876953
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.945541262626648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021745590493083
        model: {}
        policy_loss: -0.001818392425775528
        total_loss: -0.0004060291685163975
        vf_explained_var: 0.04177749156951904
        vf_loss: 30.765167236328125
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24400269985198975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012372479541227221
        model: {}
        policy_loss: -0.0016715910751372576
        total_loss: 0.0008288644021376967
        vf_explained_var: 0.08888153731822968
        vf_loss: 29.29901123046875
    load_time_ms: 15667.069
    num_steps_sampled: 95232000
    num_steps_trained: 95232000
    sample_time_ms: 125763.836
    update_time_ms: 15.918
  iterations_since_restore: 32
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.9463963963964
    ram_util_percent: 13.655405405405409
  pid: 12895
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 445.5
    agent-3: 445.5
    agent-4: 266.0
    agent-5: 266.0
  policy_reward_mean:
    agent-0: 251.03
    agent-1: 251.03
    agent-2: 412.885
    agent-3: 412.885
    agent-4: 240.195
    agent-5: 240.195
  policy_reward_min:
    agent-0: 214.5
    agent-1: 214.5
    agent-2: 344.5
    agent-3: 344.5
    agent-4: 172.5
    agent-5: 172.5
  sampler_perf:
    mean_env_wait_ms: 26.488732576122192
    mean_inference_ms: 13.283904416347395
    mean_processing_ms: 60.29048969134925
  time_since_restore: 4474.176102876663
  time_this_iter_s: 155.6852424144745
  time_total_s: 126955.19881129265
  timestamp: 1637641137
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 95232000
  training_iteration: 992
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    992 |           126955 | 95232000 |  1808.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.79
    apples_agent-1_min: 12
    apples_agent-2_max: 417
    apples_agent-2_mean: 362.21
    apples_agent-2_min: 127
    apples_agent-3_max: 293
    apples_agent-3_mean: 245.4
    apples_agent-3_min: 132
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 390.67
    apples_agent-5_min: 212
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 390.51
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 1.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 15.88
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 423.3
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-21-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1930.0
  episode_reward_mean: 1810.92
  episode_reward_min: 925.0
  episodes_this_iter: 96
  episodes_total: 95328
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12286.832
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33836352825164795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016655789222568274
        model: {}
        policy_loss: -0.0012287835124880075
        total_loss: 0.0015326347202062607
        vf_explained_var: 0.03360745310783386
        vf_loss: 33.56939697265625
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2025020271539688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010884434450417757
        model: {}
        policy_loss: -0.0017449758015573025
        total_loss: 0.0010453271679580212
        vf_explained_var: 0.09356766939163208
        vf_loss: 31.467060089111328
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2567487955093384
        entropy_coeff: 0.0017600000137463212
        kl: 0.001037765759974718
        model: {}
        policy_loss: -0.0017856033518910408
        total_loss: 0.006725371815264225
        vf_explained_var: 0.06491069495677948
        vf_loss: 89.62855529785156
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4917822778224945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007702153525315225
        model: {}
        policy_loss: -0.0014117038808763027
        total_loss: 0.0068380217999219894
        vf_explained_var: 0.05074070394039154
        vf_loss: 91.15261840820312
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9376753568649292
        entropy_coeff: 0.0017600000137463212
        kl: 0.002367691369727254
        model: {}
        policy_loss: -0.002097793621942401
        total_loss: -0.0006263977847993374
        vf_explained_var: 0.04526941478252411
        vf_loss: 31.217044830322266
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24669814109802246
        entropy_coeff: 0.0017600000137463212
        kl: 0.000586033274885267
        model: {}
        policy_loss: -0.0014769737608730793
        total_loss: 0.0011088440660387278
        vf_explained_var: 0.07861511409282684
        vf_loss: 30.20005226135254
    load_time_ms: 15688.039
    num_steps_sampled: 95328000
    num_steps_trained: 95328000
    sample_time_ms: 125708.238
    update_time_ms: 15.952
  iterations_since_restore: 33
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.802272727272726
    ram_util_percent: 13.628636363636362
  pid: 12895
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 456.5
    agent-3: 456.5
    agent-4: 267.5
    agent-5: 267.5
  policy_reward_mean:
    agent-0: 251.31
    agent-1: 251.31
    agent-2: 413.425
    agent-3: 413.425
    agent-4: 240.725
    agent-5: 240.725
  policy_reward_min:
    agent-0: 137.5
    agent-1: 137.5
    agent-2: 194.5
    agent-3: 194.5
    agent-4: 130.5
    agent-5: 130.5
  sampler_perf:
    mean_env_wait_ms: 26.513096065240024
    mean_inference_ms: 13.289889026261758
    mean_processing_ms: 60.344362543019486
  time_since_restore: 4628.1902685165405
  time_this_iter_s: 154.01416563987732
  time_total_s: 127109.21297693253
  timestamp: 1637641291
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 95328000
  training_iteration: 993
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    993 |           127109 | 95328000 |  1810.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.45
    apples_agent-1_min: 7
    apples_agent-2_max: 411
    apples_agent-2_mean: 364.69
    apples_agent-2_min: 104
    apples_agent-3_max: 295
    apples_agent-3_mean: 241.41
    apples_agent-3_min: 66
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 384.81
    apples_agent-5_min: 117
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 382.87
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.02
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 16.65
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 421.7
    cleaning_beam_agent-4_min: 349
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 2.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-23-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1930.0
  episode_reward_mean: 1795.11
  episode_reward_min: 539.0
  episodes_this_iter: 96
  episodes_total: 95424
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12178.577
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33543431758880615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011651546228677034
        model: {}
        policy_loss: -0.001421760767698288
        total_loss: 0.0014917436055839062
        vf_explained_var: 0.06810355186462402
        vf_loss: 35.0386962890625
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20696675777435303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007457411848008633
        model: {}
        policy_loss: -0.0016022990457713604
        total_loss: 0.0014164508320391178
        vf_explained_var: 0.09987060725688934
        vf_loss: 33.830101013183594
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2615929841995239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005591496592387557
        model: {}
        policy_loss: -0.0018533901311457157
        total_loss: 0.006807771045714617
        vf_explained_var: 0.08575773239135742
        vf_loss: 91.21566772460938
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4776042699813843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015256083570420742
        model: {}
        policy_loss: -0.0015987236984074116
        total_loss: 0.007049113046377897
        vf_explained_var: 0.048786550760269165
        vf_loss: 94.88420104980469
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9434463977813721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011006343411281705
        model: {}
        policy_loss: -0.0017320355400443077
        total_loss: -9.81893390417099e-05
        vf_explained_var: 0.04065118730068207
        vf_loss: 32.94309997558594
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2527817189693451
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009437783737666905
        model: {}
        policy_loss: -0.0019279294647276402
        total_loss: 0.0006515500135719776
        vf_explained_var: 0.11979570984840393
        vf_loss: 30.24372673034668
    load_time_ms: 15702.971
    num_steps_sampled: 95424000
    num_steps_trained: 95424000
    sample_time_ms: 123237.676
    update_time_ms: 15.43
  iterations_since_restore: 34
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.483695652173914
    ram_util_percent: 13.07554347826087
  pid: 12895
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 465.0
    agent-3: 465.0
    agent-4: 276.0
    agent-5: 276.0
  policy_reward_mean:
    agent-0: 249.715
    agent-1: 249.715
    agent-2: 410.03
    agent-3: 410.03
    agent-4: 237.81
    agent-5: 237.81
  policy_reward_min:
    agent-0: 76.0
    agent-1: 76.0
    agent-2: 112.5
    agent-3: 112.5
    agent-4: 81.0
    agent-5: 81.0
  sampler_perf:
    mean_env_wait_ms: 26.473037232919214
    mean_inference_ms: 13.284815408554177
    mean_processing_ms: 60.26290177213095
  time_since_restore: 4757.6199605464935
  time_this_iter_s: 129.429692029953
  time_total_s: 127238.64266896248
  timestamp: 1637641421
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 95424000
  training_iteration: 994
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    994 |           127239 | 95424000 |  1795.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.07
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.21
    apples_agent-1_min: 7
    apples_agent-2_max: 415
    apples_agent-2_mean: 360.06
    apples_agent-2_min: 104
    apples_agent-3_max: 296
    apples_agent-3_mean: 245.39
    apples_agent-3_min: 66
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 383.26
    apples_agent-5_min: 117
    cleaning_beam_agent-0_max: 416
    cleaning_beam_agent-0_mean: 380.81
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 14.69
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 432.44
    cleaning_beam_agent-4_min: 361
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 2.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-25-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1942.0
  episode_reward_mean: 1791.29
  episode_reward_min: 539.0
  episodes_this_iter: 96
  episodes_total: 95520
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 12038.771
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3281724452972412
        entropy_coeff: 0.0017600000137463212
        kl: 0.002627534093335271
        model: {}
        policy_loss: -0.0015100901946425438
        total_loss: 0.0012358096428215504
        vf_explained_var: 0.028224557638168335
        vf_loss: 33.23481750488281
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20028962194919586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011314291041344404
        model: {}
        policy_loss: -0.0017762957140803337
        total_loss: 0.0010587070137262344
        vf_explained_var: 0.068667471408844
        vf_loss: 31.875099182128906
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26004311442375183
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008357813348993659
        model: {}
        policy_loss: -0.0015511938836425543
        total_loss: 0.006850939244031906
        vf_explained_var: 0.03713151812553406
        vf_loss: 88.59808349609375
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49032700061798096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016966379480436444
        model: {}
        policy_loss: -0.001847336534410715
        total_loss: 0.006017032079398632
        vf_explained_var: 0.053213104605674744
        vf_loss: 87.2734603881836
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9476370811462402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016233959468081594
        model: {}
        policy_loss: -0.0017795637249946594
        total_loss: -0.000481518916785717
        vf_explained_var: 0.04273223876953125
        vf_loss: 29.658855438232422
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24897781014442444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007022955687716603
        model: {}
        policy_loss: -0.0016774809919297695
        total_loss: 0.0007186359725892544
        vf_explained_var: 0.08390021324157715
        vf_loss: 28.34320831298828
    load_time_ms: 15354.436
    num_steps_sampled: 95520000
    num_steps_trained: 95520000
    sample_time_ms: 120405.828
    update_time_ms: 15.048
  iterations_since_restore: 35
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.11264367816092
    ram_util_percent: 12.79827586206897
  pid: 12895
  policy_reward_max:
    agent-0: 277.0
    agent-1: 277.0
    agent-2: 449.0
    agent-3: 449.0
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 249.37
    agent-1: 249.37
    agent-2: 410.14
    agent-3: 410.14
    agent-4: 236.135
    agent-5: 236.135
  policy_reward_min:
    agent-0: 76.0
    agent-1: 76.0
    agent-2: 112.5
    agent-3: 112.5
    agent-4: 81.0
    agent-5: 81.0
  sampler_perf:
    mean_env_wait_ms: 26.41659812879764
    mean_inference_ms: 13.279122863623734
    mean_processing_ms: 60.14488855525569
  time_since_restore: 4879.354461431503
  time_this_iter_s: 121.73450088500977
  time_total_s: 127360.37716984749
  timestamp: 1637641543
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 95520000
  training_iteration: 995
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    995 |           127360 | 95520000 |  1791.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.48
    apples_agent-1_min: 14
    apples_agent-2_max: 410
    apples_agent-2_mean: 364.91
    apples_agent-2_min: 310
    apples_agent-3_max: 296
    apples_agent-3_mean: 249.77
    apples_agent-3_min: 191
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 386.77
    apples_agent-5_min: 345
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 390.31
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.66
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 14.61
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 426.06
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-27-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1907.0
  episode_reward_mean: 1807.42
  episode_reward_min: 1669.0
  episodes_this_iter: 96
  episodes_total: 95616
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11933.362
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3240423798561096
        entropy_coeff: 0.0017600000137463212
        kl: 0.001976258121430874
        model: {}
        policy_loss: -0.001357194734737277
        total_loss: 0.0012821851996704936
        vf_explained_var: 0.01780042052268982
        vf_loss: 32.09699249267578
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19934600591659546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011836339253932238
        model: {}
        policy_loss: -0.0015380543190985918
        total_loss: 0.0011576374527066946
        vf_explained_var: 0.06776434183120728
        vf_loss: 30.465423583984375
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2568732798099518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009077205322682858
        model: {}
        policy_loss: -0.0015736725181341171
        total_loss: 0.00680152652785182
        vf_explained_var: 0.010929301381111145
        vf_loss: 88.27294921875
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4683001637458801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013004534412175417
        model: {}
        policy_loss: -0.0012246499536558986
        total_loss: 0.006472138687968254
        vf_explained_var: 0.0460074245929718
        vf_loss: 85.20997619628906
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.956903874874115
        entropy_coeff: 0.0017600000137463212
        kl: 0.002938332036137581
        model: {}
        policy_loss: -0.0020953575149178505
        total_loss: -0.0007855799049139023
        vf_explained_var: 0.03185071051120758
        vf_loss: 29.939308166503906
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24731580913066864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008664023480378091
        model: {}
        policy_loss: -0.0015098091680556536
        total_loss: 0.0008770609274506569
        vf_explained_var: 0.08816108107566833
        vf_loss: 28.221467971801758
    load_time_ms: 15329.346
    num_steps_sampled: 95616000
    num_steps_trained: 95616000
    sample_time_ms: 117962.806
    update_time_ms: 14.904
  iterations_since_restore: 36
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.113636363636363
    ram_util_percent: 12.716477272727273
  pid: 12895
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 443.5
    agent-3: 443.5
    agent-4: 271.5
    agent-5: 271.5
  policy_reward_mean:
    agent-0: 250.64
    agent-1: 250.64
    agent-2: 413.855
    agent-3: 413.855
    agent-4: 239.215
    agent-5: 239.215
  policy_reward_min:
    agent-0: 218.5
    agent-1: 218.5
    agent-2: 379.0
    agent-3: 379.0
    agent-4: 215.5
    agent-5: 215.5
  sampler_perf:
    mean_env_wait_ms: 26.361001380667517
    mean_inference_ms: 13.27101474208719
    mean_processing_ms: 60.046963297960275
  time_since_restore: 5003.293539047241
  time_this_iter_s: 123.93907761573792
  time_total_s: 127484.31624746323
  timestamp: 1637641667
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 95616000
  training_iteration: 996
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    996 |           127484 | 95616000 |  1807.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 29.82
    apples_agent-1_min: 16
    apples_agent-2_max: 406
    apples_agent-2_mean: 361.42
    apples_agent-2_min: 305
    apples_agent-3_max: 310
    apples_agent-3_mean: 250.7
    apples_agent-3_min: 195
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 387.97
    apples_agent-5_min: 329
    cleaning_beam_agent-0_max: 408
    cleaning_beam_agent-0_mean: 382.36
    cleaning_beam_agent-0_min: 352
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.77
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 16.18
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 437.2
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-29-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1928.0
  episode_reward_mean: 1813.29
  episode_reward_min: 1640.0
  episodes_this_iter: 96
  episodes_total: 95712
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11820.773
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3204610049724579
        entropy_coeff: 0.0017600000137463212
        kl: 0.001194109907373786
        model: {}
        policy_loss: -0.0012338646920397878
        total_loss: 0.0014556075911968946
        vf_explained_var: 0.016046062111854553
        vf_loss: 32.53483581542969
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19924244284629822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005171074299141765
        model: {}
        policy_loss: -0.0013970760628581047
        total_loss: 0.0013732649385929108
        vf_explained_var: 0.05618320405483246
        vf_loss: 31.210094451904297
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2549561858177185
        entropy_coeff: 0.0017600000137463212
        kl: 0.000996289891190827
        model: {}
        policy_loss: -0.001516267191618681
        total_loss: 0.006597882602363825
        vf_explained_var: 0.02553771436214447
        vf_loss: 85.62872314453125
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4746207594871521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009037225972861052
        model: {}
        policy_loss: -0.0010881740599870682
        total_loss: 0.006397399585694075
        vf_explained_var: 0.05456024408340454
        vf_loss: 83.20905303955078
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.947052001953125
        entropy_coeff: 0.0017600000137463212
        kl: 0.001782083883881569
        model: {}
        policy_loss: -0.002013993915170431
        total_loss: -0.0007677038665860891
        vf_explained_var: 0.03399351239204407
        vf_loss: 29.13105010986328
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24865414202213287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005456152139231563
        model: {}
        policy_loss: -0.0014387276023626328
        total_loss: 0.0008897818624973297
        vf_explained_var: 0.08240161836147308
        vf_loss: 27.66141128540039
    load_time_ms: 15320.023
    num_steps_sampled: 95712000
    num_steps_trained: 95712000
    sample_time_ms: 115697.569
    update_time_ms: 14.728
  iterations_since_restore: 37
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.557692307692307
    ram_util_percent: 12.715384615384615
  pid: 12895
  policy_reward_max:
    agent-0: 289.5
    agent-1: 289.5
    agent-2: 454.5
    agent-3: 454.5
    agent-4: 263.5
    agent-5: 263.5
  policy_reward_mean:
    agent-0: 251.75
    agent-1: 251.75
    agent-2: 415.155
    agent-3: 415.155
    agent-4: 239.74
    agent-5: 239.74
  policy_reward_min:
    agent-0: 213.0
    agent-1: 213.0
    agent-2: 372.0
    agent-3: 372.0
    agent-4: 209.0
    agent-5: 209.0
  sampler_perf:
    mean_env_wait_ms: 26.312862249314485
    mean_inference_ms: 13.261878792743834
    mean_processing_ms: 59.99173417561659
  time_since_restore: 5130.918241262436
  time_this_iter_s: 127.6247022151947
  time_total_s: 127611.94094967842
  timestamp: 1637641795
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 95712000
  training_iteration: 997
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    997 |           127612 | 95712000 |  1813.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 31.36
    apples_agent-1_min: 11
    apples_agent-2_max: 453
    apples_agent-2_mean: 366.66
    apples_agent-2_min: 235
    apples_agent-3_max: 292
    apples_agent-3_mean: 246.03
    apples_agent-3_min: 165
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 388.46
    apples_agent-5_min: 295
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 386.04
    cleaning_beam_agent-0_min: 360
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 1.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 16.63
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 444.17
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-32-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1922.0
  episode_reward_mean: 1811.29
  episode_reward_min: 1257.0
  episodes_this_iter: 96
  episodes_total: 95808
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11751.943
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32156917452812195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008787520928308368
        model: {}
        policy_loss: -0.0010729138739407063
        total_loss: 0.0020657896529883146
        vf_explained_var: 0.02584947645664215
        vf_loss: 37.04665756225586
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2039322853088379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008760780328884721
        model: {}
        policy_loss: -0.0018947707721963525
        total_loss: 0.001221653656102717
        vf_explained_var: 0.085723876953125
        vf_loss: 34.75347900390625
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25037911534309387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012071329401805997
        model: {}
        policy_loss: -0.0018669192213565111
        total_loss: 0.006813354324549437
        vf_explained_var: 0.024601340293884277
        vf_loss: 91.20943450927734
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4745393991470337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010846259538084269
        model: {}
        policy_loss: -0.0018325598211959004
        total_loss: 0.006213723216205835
        vf_explained_var: 0.052348583936691284
        vf_loss: 88.8147201538086
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9451011419296265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009041459998115897
        model: {}
        policy_loss: -0.0014500231482088566
        total_loss: -9.62080666795373e-05
        vf_explained_var: 0.039929747581481934
        vf_loss: 30.17196273803711
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24647866189479828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009738681837916374
        model: {}
        policy_loss: -0.001653924584388733
        total_loss: 0.0006910320371389389
        vf_explained_var: 0.11072960495948792
        vf_loss: 27.78760528564453
    load_time_ms: 15075.545
    num_steps_sampled: 95808000
    num_steps_trained: 95808000
    sample_time_ms: 114097.634
    update_time_ms: 14.654
  iterations_since_restore: 38
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.43846153846154
    ram_util_percent: 12.762051282051283
  pid: 12895
  policy_reward_max:
    agent-0: 277.5
    agent-1: 277.5
    agent-2: 470.0
    agent-3: 470.0
    agent-4: 271.5
    agent-5: 271.5
  policy_reward_mean:
    agent-0: 251.66
    agent-1: 251.66
    agent-2: 413.455
    agent-3: 413.455
    agent-4: 240.53
    agent-5: 240.53
  policy_reward_min:
    agent-0: 89.5
    agent-1: 89.5
    agent-2: 277.0
    agent-3: 277.0
    agent-4: 180.0
    agent-5: 180.0
  sampler_perf:
    mean_env_wait_ms: 26.304163995422332
    mean_inference_ms: 13.257725674225373
    mean_processing_ms: 59.96773159511671
  time_since_restore: 5267.1951212883
  time_this_iter_s: 136.27688002586365
  time_total_s: 127748.21782970428
  timestamp: 1637641931
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 95808000
  training_iteration: 998
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    998 |           127748 | 95808000 |  1811.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 29.9
    apples_agent-1_min: 11
    apples_agent-2_max: 453
    apples_agent-2_mean: 365.86
    apples_agent-2_min: 313
    apples_agent-3_max: 287
    apples_agent-3_mean: 248.53
    apples_agent-3_min: 179
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 385.42
    apples_agent-5_min: 343
    cleaning_beam_agent-0_max: 419
    cleaning_beam_agent-0_mean: 387.58
    cleaning_beam_agent-0_min: 343
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.93
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 1.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.11
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 443.17
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-34-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1897.0
  episode_reward_mean: 1806.22
  episode_reward_min: 1645.0
  episodes_this_iter: 96
  episodes_total: 95904
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11617.515
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3255350887775421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015521347522735596
        model: {}
        policy_loss: -0.001164260320365429
        total_loss: 0.001443992368876934
        vf_explained_var: 0.03178638219833374
        vf_loss: 31.81192398071289
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2048867642879486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010710908100008965
        model: {}
        policy_loss: -0.0015549017116427422
        total_loss: 0.0011360365897417068
        vf_explained_var: 0.06974998116493225
        vf_loss: 30.5153751373291
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2558533549308777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009007549379020929
        model: {}
        policy_loss: -0.0017045439453795552
        total_loss: 0.0062332781963050365
        vf_explained_var: 0.028498440980911255
        vf_loss: 83.88125610351562
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49067962169647217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014010919257998466
        model: {}
        policy_loss: -0.0014724195934832096
        total_loss: 0.006017978768795729
        vf_explained_var: 0.036839768290519714
        vf_loss: 83.53995513916016
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9461656212806702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024360432289540768
        model: {}
        policy_loss: -0.002023527631536126
        total_loss: -0.0006742407567799091
        vf_explained_var: 0.032448604702949524
        vf_loss: 30.145387649536133
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25376540422439575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013272857759147882
        model: {}
        policy_loss: -0.001810617744922638
        total_loss: 0.0005719074979424477
        vf_explained_var: 0.08898189663887024
        vf_loss: 28.291534423828125
    load_time_ms: 15067.178
    num_steps_sampled: 95904000
    num_steps_trained: 95904000
    sample_time_ms: 111095.72
    update_time_ms: 14.766
  iterations_since_restore: 39
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.23735632183908
    ram_util_percent: 12.735632183908047
  pid: 12895
  policy_reward_max:
    agent-0: 284.0
    agent-1: 284.0
    agent-2: 470.0
    agent-3: 470.0
    agent-4: 271.5
    agent-5: 271.5
  policy_reward_mean:
    agent-0: 250.35
    agent-1: 250.35
    agent-2: 413.275
    agent-3: 413.275
    agent-4: 239.485
    agent-5: 239.485
  policy_reward_min:
    agent-0: 89.5
    agent-1: 89.5
    agent-2: 358.5
    agent-3: 358.5
    agent-4: 217.5
    agent-5: 217.5
  sampler_perf:
    mean_env_wait_ms: 26.264047904129328
    mean_inference_ms: 13.25017074595854
    mean_processing_ms: 59.87664087830662
  time_since_restore: 5389.656407594681
  time_this_iter_s: 122.46128630638123
  time_total_s: 127870.67911601067
  timestamp: 1637642054
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 95904000
  training_iteration: 999
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |    999 |           127871 | 95904000 |  1806.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 31.68
    apples_agent-1_min: 14
    apples_agent-2_max: 412
    apples_agent-2_mean: 362.75
    apples_agent-2_min: 262
    apples_agent-3_max: 290
    apples_agent-3_mean: 246.34
    apples_agent-3_min: 190
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 387.97
    apples_agent-5_min: 301
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 396.04
    cleaning_beam_agent-0_min: 311
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.88
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.78
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 15.63
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 443.05
    cleaning_beam_agent-4_min: 379
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 2.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-36-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1909.0
  episode_reward_mean: 1802.55
  episode_reward_min: 1378.0
  episodes_this_iter: 96
  episodes_total: 96000
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11498.157
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33123746514320374
        entropy_coeff: 0.0017600000137463212
        kl: 0.00133818993344903
        model: {}
        policy_loss: -0.001395847531966865
        total_loss: 0.0013417813461273909
        vf_explained_var: 0.039150699973106384
        vf_loss: 33.20609664916992
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20577304065227509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012425822205841541
        model: {}
        policy_loss: -0.0016486593522131443
        total_loss: 0.00109093205537647
        vf_explained_var: 0.10280945897102356
        vf_loss: 31.017501831054688
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2588617205619812
        entropy_coeff: 0.0017600000137463212
        kl: 0.001529515371657908
        model: {}
        policy_loss: -0.0019160732626914978
        total_loss: 0.006472242530435324
        vf_explained_var: 0.03559949994087219
        vf_loss: 88.43914031982422
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49075740575790405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019502276554703712
        model: {}
        policy_loss: -0.0016432609409093857
        total_loss: 0.00621233182027936
        vf_explained_var: 0.049568772315979004
        vf_loss: 87.19326782226562
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9528303146362305
        entropy_coeff: 0.0017600000137463212
        kl: 0.00314910477027297
        model: {}
        policy_loss: -0.0019311192445456982
        total_loss: -0.0005173089448362589
        vf_explained_var: 0.030915558338165283
        vf_loss: 30.907922744750977
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25866660475730896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008928942261263728
        model: {}
        policy_loss: -0.001657861634157598
        total_loss: 0.0008473641937598586
        vf_explained_var: 0.07036249339580536
        vf_loss: 29.604751586914062
    load_time_ms: 15039.397
    num_steps_sampled: 96000000
    num_steps_trained: 96000000
    sample_time_ms: 108291.252
    update_time_ms: 15.05
  iterations_since_restore: 40
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.161714285714286
    ram_util_percent: 12.728571428571428
  pid: 12895
  policy_reward_max:
    agent-0: 284.0
    agent-1: 284.0
    agent-2: 448.0
    agent-3: 448.0
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 250.955
    agent-1: 250.955
    agent-2: 411.215
    agent-3: 411.215
    agent-4: 239.105
    agent-5: 239.105
  policy_reward_min:
    agent-0: 195.5
    agent-1: 195.5
    agent-2: 305.0
    agent-3: 305.0
    agent-4: 184.5
    agent-5: 184.5
  sampler_perf:
    mean_env_wait_ms: 26.225561926570677
    mean_inference_ms: 13.244227670251997
    mean_processing_ms: 59.78862472659129
  time_since_restore: 5511.9911959171295
  time_this_iter_s: 122.33478832244873
  time_total_s: 127993.01390433311
  timestamp: 1637642176
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 96000000
  training_iteration: 1000
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1000 |           127993 | 96000000 |  1802.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 30.77
    apples_agent-1_min: 18
    apples_agent-2_max: 411
    apples_agent-2_mean: 364.76
    apples_agent-2_min: 308
    apples_agent-3_max: 286
    apples_agent-3_mean: 246.89
    apples_agent-3_min: 172
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 444
    apples_agent-5_mean: 391.56
    apples_agent-5_min: 340
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 400.36
    cleaning_beam_agent-0_min: 363
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.91
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 16.52
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 445.99
    cleaning_beam_agent-4_min: 363
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-38-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1944.0
  episode_reward_mean: 1810.59
  episode_reward_min: 1600.0
  episodes_this_iter: 96
  episodes_total: 96096
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11489.181
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3328312635421753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010358720319345593
        model: {}
        policy_loss: -0.001273821690119803
        total_loss: 0.0012890768703073263
        vf_explained_var: 0.04174782335758209
        vf_loss: 31.486801147460938
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20489320158958435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010858757887035608
        model: {}
        policy_loss: -0.0017550429329276085
        total_loss: 0.0009244815446436405
        vf_explained_var: 0.07455486059188843
        vf_loss: 30.401363372802734
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2550252676010132
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006984697538428009
        model: {}
        policy_loss: -0.001598326489329338
        total_loss: 0.0068722437135875225
        vf_explained_var: 0.023081332445144653
        vf_loss: 89.19413757324219
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47913554310798645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008225373458117247
        model: {}
        policy_loss: -0.001502912025898695
        total_loss: 0.006282922346144915
        vf_explained_var: 0.056610047817230225
        vf_loss: 86.29112243652344
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9567277431488037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023975451476871967
        model: {}
        policy_loss: -0.002078014425933361
        total_loss: -0.0007167094154283404
        vf_explained_var: 0.037133023142814636
        vf_loss: 30.45144271850586
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26134181022644043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006653702002950013
        model: {}
        policy_loss: -0.0015396981034427881
        total_loss: 0.0008346056565642357
        vf_explained_var: 0.1066475510597229
        vf_loss: 28.342670440673828
    load_time_ms: 15010.801
    num_steps_sampled: 96096000
    num_steps_trained: 96096000
    sample_time_ms: 105269.784
    update_time_ms: 14.98
  iterations_since_restore: 41
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.677348066298343
    ram_util_percent: 12.794475138121548
  pid: 12895
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 446.0
    agent-3: 446.0
    agent-4: 272.5
    agent-5: 272.5
  policy_reward_mean:
    agent-0: 251.205
    agent-1: 251.205
    agent-2: 413.08
    agent-3: 413.08
    agent-4: 241.01
    agent-5: 241.01
  policy_reward_min:
    agent-0: 223.5
    agent-1: 223.5
    agent-2: 354.0
    agent-3: 354.0
    agent-4: 209.0
    agent-5: 209.0
  sampler_perf:
    mean_env_wait_ms: 26.18374097800394
    mean_inference_ms: 13.234725943581028
    mean_processing_ms: 59.73230468949932
  time_since_restore: 5637.284502506256
  time_this_iter_s: 125.29330658912659
  time_total_s: 128118.30721092224
  timestamp: 1637642303
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 96096000
  training_iteration: 1001
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1001 |           128118 | 96096000 |  1810.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 30.81
    apples_agent-1_min: 12
    apples_agent-2_max: 416
    apples_agent-2_mean: 360.13
    apples_agent-2_min: 146
    apples_agent-3_max: 290
    apples_agent-3_mean: 243.31
    apples_agent-3_min: 84
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 441
    apples_agent-5_mean: 383.67
    apples_agent-5_min: 144
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 401.26
    cleaning_beam_agent-0_min: 357
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 0.96
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 18.13
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 494
    cleaning_beam_agent-4_mean: 433.84
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 2.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-40-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1930.0
  episode_reward_mean: 1794.22
  episode_reward_min: 744.0
  episodes_this_iter: 96
  episodes_total: 96192
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11378.473
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.336137592792511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013705504825338721
        model: {}
        policy_loss: -0.0015458748675882816
        total_loss: 0.0013552719028666615
        vf_explained_var: 0.06768707931041718
        vf_loss: 34.9275016784668
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2091655433177948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012304929550737143
        model: {}
        policy_loss: -0.0021551470272243023
        total_loss: 0.0008213622495532036
        vf_explained_var: 0.10758499801158905
        vf_loss: 33.44639205932617
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25394901633262634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011222027242183685
        model: {}
        policy_loss: -0.0018218248151242733
        total_loss: 0.006803570780903101
        vf_explained_var: 0.07072946429252625
        vf_loss: 90.72345733642578
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48924195766448975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008208421058952808
        model: {}
        policy_loss: -0.0014717977028340101
        total_loss: 0.0070062242448329926
        vf_explained_var: 0.04294419288635254
        vf_loss: 93.39088439941406
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9542593955993652
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022344719618558884
        model: {}
        policy_loss: -0.0017020124942064285
        total_loss: -0.00020590797066688538
        vf_explained_var: 0.031432971358299255
        vf_loss: 31.756038665771484
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2678629457950592
        entropy_coeff: 0.0017600000137463212
        kl: 0.000847398885525763
        model: {}
        policy_loss: -0.001859178300946951
        total_loss: 0.0006338868988677859
        vf_explained_var: 0.0946774035692215
        vf_loss: 29.645038604736328
    load_time_ms: 15026.364
    num_steps_sampled: 96192000
    num_steps_trained: 96192000
    sample_time_ms: 102526.158
    update_time_ms: 14.86
  iterations_since_restore: 42
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.75934065934066
    ram_util_percent: 12.735714285714286
  pid: 12895
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 444.5
    agent-3: 444.5
    agent-4: 262.5
    agent-5: 262.5
  policy_reward_mean:
    agent-0: 249.755
    agent-1: 249.755
    agent-2: 410.03
    agent-3: 410.03
    agent-4: 237.325
    agent-5: 237.325
  policy_reward_min:
    agent-0: 102.5
    agent-1: 102.5
    agent-2: 170.0
    agent-3: 170.0
    agent-4: 99.5
    agent-5: 99.5
  sampler_perf:
    mean_env_wait_ms: 26.144360638286635
    mean_inference_ms: 13.227206906170442
    mean_processing_ms: 59.68073642030573
  time_since_restore: 5764.607700109482
  time_this_iter_s: 127.32319760322571
  time_total_s: 128245.63040852547
  timestamp: 1637642431
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 96192000
  training_iteration: 1002
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1002 |           128246 | 96192000 |  1794.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.37
    apples_agent-1_min: 19
    apples_agent-2_max: 409
    apples_agent-2_mean: 362.3
    apples_agent-2_min: 241
    apples_agent-3_max: 294
    apples_agent-3_mean: 244.43
    apples_agent-3_min: 190
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.04
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 385.28
    apples_agent-5_min: 281
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 401.82
    cleaning_beam_agent-0_min: 362
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 0.92
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.11
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 17.09
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 516
    cleaning_beam_agent-4_mean: 450.66
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-42-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1913.0
  episode_reward_mean: 1806.66
  episode_reward_min: 1381.0
  episodes_this_iter: 96
  episodes_total: 96288
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11253.238
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3300439715385437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017233267426490784
        model: {}
        policy_loss: -0.0013201062101870775
        total_loss: 0.0013599873054772615
        vf_explained_var: 0.06053173542022705
        vf_loss: 32.60972213745117
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20621076226234436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006051021628081799
        model: {}
        policy_loss: -0.001713282777927816
        total_loss: 0.0009708581492304802
        vf_explained_var: 0.11922551691532135
        vf_loss: 30.470722198486328
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2554199695587158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010234086075797677
        model: {}
        policy_loss: -0.0018156571313738823
        total_loss: 0.006548767443746328
        vf_explained_var: 0.05019417405128479
        vf_loss: 88.13961791992188
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4862385392189026
        entropy_coeff: 0.0017600000137463212
        kl: 0.000619741971604526
        model: {}
        policy_loss: -0.0015451046638190746
        total_loss: 0.006427675019949675
        vf_explained_var: 0.04909783601760864
        vf_loss: 88.28557586669922
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9552388787269592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018144992645829916
        model: {}
        policy_loss: -0.0017996556125581264
        total_loss: -0.00035810889676213264
        vf_explained_var: 0.03989909589290619
        vf_loss: 31.227676391601562
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2622566819190979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009875301038846374
        model: {}
        policy_loss: -0.0015170653350651264
        total_loss: 0.000956993259023875
        vf_explained_var: 0.09761656820774078
        vf_loss: 29.356338500976562
    load_time_ms: 15010.965
    num_steps_sampled: 96288000
    num_steps_trained: 96288000
    sample_time_ms: 99738.96
    update_time_ms: 14.853
  iterations_since_restore: 43
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.948022598870057
    ram_util_percent: 12.811864406779662
  pid: 12895
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 446.0
    agent-3: 446.0
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 252.025
    agent-1: 252.025
    agent-2: 412.385
    agent-3: 412.385
    agent-4: 238.92
    agent-5: 238.92
  policy_reward_min:
    agent-0: 181.5
    agent-1: 181.5
    agent-2: 304.0
    agent-3: 304.0
    agent-4: 181.0
    agent-5: 181.0
  sampler_perf:
    mean_env_wait_ms: 26.114268693012328
    mean_inference_ms: 13.219413217850013
    mean_processing_ms: 59.621246514003204
  time_since_restore: 5889.3240694999695
  time_this_iter_s: 124.71636939048767
  time_total_s: 128370.34677791595
  timestamp: 1637642556
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 96288000
  training_iteration: 1003
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1003 |           128370 | 96288000 |  1806.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.22
    apples_agent-1_min: 17
    apples_agent-2_max: 418
    apples_agent-2_mean: 366.73
    apples_agent-2_min: 255
    apples_agent-3_max: 309
    apples_agent-3_mean: 246.92
    apples_agent-3_min: 160
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.08
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 388.63
    apples_agent-5_min: 267
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 407.98
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 10
    cleaning_beam_agent-1_mean: 1.09
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 17.53
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 450.16
    cleaning_beam_agent-4_min: 364
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-44-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1932.0
  episode_reward_mean: 1816.56
  episode_reward_min: 1215.0
  episodes_this_iter: 96
  episodes_total: 96384
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11247.104
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3281956911087036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012015951797366142
        model: {}
        policy_loss: -0.0011285023065283895
        total_loss: 0.0016992536839097738
        vf_explained_var: 0.037553563714027405
        vf_loss: 34.053829193115234
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2037585824728012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010739907156676054
        model: {}
        policy_loss: -0.0015728524886071682
        total_loss: 0.0012589944526553154
        vf_explained_var: 0.09721533954143524
        vf_loss: 31.90465545654297
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2505773901939392
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013009855756536126
        model: {}
        policy_loss: -0.0017776775639504194
        total_loss: 0.006813845597207546
        vf_explained_var: 0.01627601683139801
        vf_loss: 90.32539367675781
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48112720251083374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006063847104087472
        model: {}
        policy_loss: -0.0015642782673239708
        total_loss: 0.006391571834683418
        vf_explained_var: 0.04180513322353363
        vf_loss: 88.02633666992188
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9589718580245972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013084354577586055
        model: {}
        policy_loss: -0.0016732197254896164
        total_loss: -0.00044690724462270737
        vf_explained_var: 0.03278389573097229
        vf_loss: 29.141002655029297
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25618812441825867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008844967233017087
        model: {}
        policy_loss: -0.0019215214997529984
        total_loss: 0.0005252780392765999
        vf_explained_var: 0.03955836594104767
        vf_loss: 28.976905822753906
    load_time_ms: 14995.296
    num_steps_sampled: 96384000
    num_steps_trained: 96384000
    sample_time_ms: 99071.271
    update_time_ms: 14.413
  iterations_since_restore: 44
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.221714285714285
    ram_util_percent: 12.826285714285717
  pid: 12895
  policy_reward_max:
    agent-0: 287.0
    agent-1: 287.0
    agent-2: 459.0
    agent-3: 459.0
    agent-4: 265.5
    agent-5: 265.5
  policy_reward_mean:
    agent-0: 252.9
    agent-1: 252.9
    agent-2: 415.805
    agent-3: 415.805
    agent-4: 239.575
    agent-5: 239.575
  policy_reward_min:
    agent-0: 149.5
    agent-1: 149.5
    agent-2: 284.0
    agent-3: 284.0
    agent-4: 174.0
    agent-5: 174.0
  sampler_perf:
    mean_env_wait_ms: 26.086942391494116
    mean_inference_ms: 13.212364867937895
    mean_processing_ms: 59.546972712969094
  time_since_restore: 6011.811226606369
  time_this_iter_s: 122.48715710639954
  time_total_s: 128492.83393502235
  timestamp: 1637642678
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 96384000
  training_iteration: 1004
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1004 |           128493 | 96384000 |  1816.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 57
    apples_agent-1_mean: 30.98
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 366.83
    apples_agent-2_min: 309
    apples_agent-3_max: 289
    apples_agent-3_mean: 243.74
    apples_agent-3_min: 196
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 456
    apples_agent-5_mean: 389.57
    apples_agent-5_min: 338
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 408.51
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.13
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 17.72
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 441.04
    cleaning_beam_agent-4_min: 385
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-46-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1924.0
  episode_reward_mean: 1807.81
  episode_reward_min: 1550.0
  episodes_this_iter: 96
  episodes_total: 96480
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11246.705
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.331582248210907
        entropy_coeff: 0.0017600000137463212
        kl: 0.001263285637833178
        model: {}
        policy_loss: -0.0011495300568640232
        total_loss: 0.0014253845438361168
        vf_explained_var: 0.04549121856689453
        vf_loss: 31.584976196289062
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2053125649690628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007542898529209197
        model: {}
        policy_loss: -0.0015960093587636948
        total_loss: 0.0011141905561089516
        vf_explained_var: 0.07129207253456116
        vf_loss: 30.715469360351562
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2561756372451782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010518995113670826
        model: {}
        policy_loss: -0.002093162154778838
        total_loss: 0.006353973411023617
        vf_explained_var: 0.032469555735588074
        vf_loss: 88.98004150390625
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48217153549194336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014516710070893168
        model: {}
        policy_loss: -0.001577501418069005
        total_loss: 0.006186304148286581
        vf_explained_var: 0.0624125599861145
        vf_loss: 86.124267578125
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9736486077308655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019903986249119043
        model: {}
        policy_loss: -0.0019448017701506615
        total_loss: -0.0006243349052965641
        vf_explained_var: 0.02294313907623291
        vf_loss: 30.34090232849121
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.255919873714447
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008057507802732289
        model: {}
        policy_loss: -0.0016206796281039715
        total_loss: 0.0007984667317941785
        vf_explained_var: 0.07307252287864685
        vf_loss: 28.695667266845703
    load_time_ms: 14987.906
    num_steps_sampled: 96480000
    num_steps_trained: 96480000
    sample_time_ms: 99074.635
    update_time_ms: 14.795
  iterations_since_restore: 45
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.30578034682081
    ram_util_percent: 12.836994219653178
  pid: 12895
  policy_reward_max:
    agent-0: 271.5
    agent-1: 271.5
    agent-2: 451.0
    agent-3: 451.0
    agent-4: 271.5
    agent-5: 271.5
  policy_reward_mean:
    agent-0: 249.96
    agent-1: 249.96
    agent-2: 414.055
    agent-3: 414.055
    agent-4: 239.89
    agent-5: 239.89
  policy_reward_min:
    agent-0: 216.0
    agent-1: 216.0
    agent-2: 352.5
    agent-3: 352.5
    agent-4: 205.5
    agent-5: 205.5
  sampler_perf:
    mean_env_wait_ms: 26.05428602016064
    mean_inference_ms: 13.206752377006449
    mean_processing_ms: 59.46635488960581
  time_since_restore: 6133.57043838501
  time_this_iter_s: 121.75921177864075
  time_total_s: 128614.593146801
  timestamp: 1637642800
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 96480000
  training_iteration: 1005
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1005 |           128615 | 96480000 |  1807.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 30.78
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 365.06
    apples_agent-2_min: 285
    apples_agent-3_max: 296
    apples_agent-3_mean: 246.03
    apples_agent-3_min: 176
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.06
    apples_agent-4_min: 0
    apples_agent-5_max: 435
    apples_agent-5_mean: 384.01
    apples_agent-5_min: 294
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 412.57
    cleaning_beam_agent-0_min: 349
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.8
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 15.97
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 433.12
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-48-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1932.0
  episode_reward_mean: 1798.13
  episode_reward_min: 1358.0
  episodes_this_iter: 96
  episodes_total: 96576
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11250.591
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3348790109157562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007756899576634169
        model: {}
        policy_loss: -0.0010712267830967903
        total_loss: 0.0017198207788169384
        vf_explained_var: 0.0531114786863327
        vf_loss: 33.80434799194336
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21286143362522125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009599935729056597
        model: {}
        policy_loss: -0.0018339669331908226
        total_loss: 0.0010786186903715134
        vf_explained_var: 0.07974414527416229
        vf_loss: 32.87223815917969
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2570546865463257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014194308314472437
        model: {}
        policy_loss: -0.002028646180406213
        total_loss: 0.006546496879309416
        vf_explained_var: 0.044841468334198
        vf_loss: 90.27557373046875
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4710824489593506
        entropy_coeff: 0.0017600000137463212
        kl: 0.00141001062002033
        model: {}
        policy_loss: -0.0016350976657122374
        total_loss: 0.0065406132489442825
        vf_explained_var: 0.04916156828403473
        vf_loss: 90.04815673828125
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9761034250259399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030587005894631147
        model: {}
        policy_loss: -0.0019609383307397366
        total_loss: -0.0006538692396134138
        vf_explained_var: 0.030571937561035156
        vf_loss: 30.2501277923584
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2623558044433594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007765358313918114
        model: {}
        policy_loss: -0.0016482500359416008
        total_loss: 0.0007540327496826649
        vf_explained_var: 0.08656102418899536
        vf_loss: 28.64029312133789
    load_time_ms: 14981.694
    num_steps_sampled: 96576000
    num_steps_trained: 96576000
    sample_time_ms: 99067.792
    update_time_ms: 14.582
  iterations_since_restore: 46
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.08700564971751
    ram_util_percent: 12.843502824858758
  pid: 12895
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 449.0
    agent-3: 449.0
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 247.785
    agent-1: 247.785
    agent-2: 415.05
    agent-3: 415.05
    agent-4: 236.23
    agent-5: 236.23
  policy_reward_min:
    agent-0: 161.5
    agent-1: 161.5
    agent-2: 330.0
    agent-3: 330.0
    agent-4: 181.5
    agent-5: 181.5
  sampler_perf:
    mean_env_wait_ms: 26.024760903531433
    mean_inference_ms: 13.201194837192043
    mean_processing_ms: 59.410062958939186
  time_since_restore: 6257.37530875206
  time_this_iter_s: 123.80487036705017
  time_total_s: 128738.39801716805
  timestamp: 1637642924
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 96576000
  training_iteration: 1006
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1006 |           128738 | 96576000 |  1798.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 29.75
    apples_agent-1_min: 18
    apples_agent-2_max: 410
    apples_agent-2_mean: 364.4
    apples_agent-2_min: 306
    apples_agent-3_max: 332
    apples_agent-3_mean: 249.71
    apples_agent-3_min: 183
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 448
    apples_agent-5_mean: 385.47
    apples_agent-5_min: 340
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 411.68
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.86
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 14.68
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 470
    cleaning_beam_agent-4_mean: 411.73
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-50-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1899.0
  episode_reward_mean: 1806.97
  episode_reward_min: 1669.0
  episodes_this_iter: 96
  episodes_total: 96672
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11250.075
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33011558651924133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019990229047834873
        model: {}
        policy_loss: -0.001324274460785091
        total_loss: 0.0012097483268007636
        vf_explained_var: 0.040922611951828
        vf_loss: 31.150270462036133
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20805053412914276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005660627502948046
        model: {}
        policy_loss: -0.0013587624998763204
        total_loss: 0.001216998789459467
        vf_explained_var: 0.09368583559989929
        vf_loss: 29.419321060180664
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25397878885269165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012131375260651112
        model: {}
        policy_loss: -0.0016637155786156654
        total_loss: 0.006594745442271233
        vf_explained_var: 0.025182798504829407
        vf_loss: 87.0546646118164
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46189603209495544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009860065765678883
        model: {}
        policy_loss: -0.001487143337726593
        total_loss: 0.0062302639707922935
        vf_explained_var: 0.04711070656776428
        vf_loss: 85.303466796875
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9920669794082642
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017481166869401932
        model: {}
        policy_loss: -0.0018091003876179457
        total_loss: -0.0007121292874217033
        vf_explained_var: 0.02744150161743164
        vf_loss: 28.430091857910156
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2541809678077698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010332632809877396
        model: {}
        policy_loss: -0.0016579246148467064
        total_loss: 0.0006101233884692192
        vf_explained_var: 0.07353098690509796
        vf_loss: 27.154064178466797
    load_time_ms: 14987.052
    num_steps_sampled: 96672000
    num_steps_trained: 96672000
    sample_time_ms: 98720.227
    update_time_ms: 14.738
  iterations_since_restore: 47
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.260451977401132
    ram_util_percent: 12.762146892655366
  pid: 12895
  policy_reward_max:
    agent-0: 274.0
    agent-1: 274.0
    agent-2: 444.5
    agent-3: 444.5
    agent-4: 270.0
    agent-5: 270.0
  policy_reward_mean:
    agent-0: 250.295
    agent-1: 250.295
    agent-2: 415.335
    agent-3: 415.335
    agent-4: 237.855
    agent-5: 237.855
  policy_reward_min:
    agent-0: 218.0
    agent-1: 218.0
    agent-2: 373.5
    agent-3: 373.5
    agent-4: 217.0
    agent-5: 217.0
  sampler_perf:
    mean_env_wait_ms: 25.995963484950224
    mean_inference_ms: 13.194778489137379
    mean_processing_ms: 59.36192292994788
  time_since_restore: 6381.618601799011
  time_this_iter_s: 124.2432930469513
  time_total_s: 128862.641310215
  timestamp: 1637643049
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 96672000
  training_iteration: 1007
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1007 |           128863 | 96672000 |  1806.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.63
    apples_agent-1_min: 18
    apples_agent-2_max: 423
    apples_agent-2_mean: 361.72
    apples_agent-2_min: 178
    apples_agent-3_max: 281
    apples_agent-3_mean: 244.87
    apples_agent-3_min: 124
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 442
    apples_agent-5_mean: 385.59
    apples_agent-5_min: 206
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 420.3
    cleaning_beam_agent-0_min: 377
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.21
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 14.75
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 463
    cleaning_beam_agent-4_mean: 414.7
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-52-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1929.0
  episode_reward_mean: 1795.41
  episode_reward_min: 993.0
  episodes_this_iter: 96
  episodes_total: 96768
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11225.877
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3385721445083618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014510555192828178
        model: {}
        policy_loss: -0.0013607449363917112
        total_loss: 0.0012570604449138045
        vf_explained_var: 0.0523560494184494
        vf_loss: 32.13692855834961
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20946435630321503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005724151851609349
        model: {}
        policy_loss: -0.0015903450548648834
        total_loss: 0.0011254874989390373
        vf_explained_var: 0.08964641392230988
        vf_loss: 30.844924926757812
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2581571340560913
        entropy_coeff: 0.0017600000137463212
        kl: 0.000654777220916003
        model: {}
        policy_loss: -0.0015442720614373684
        total_loss: 0.006733812391757965
        vf_explained_var: 0.052552491426467896
        vf_loss: 87.32442474365234
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46821364760398865
        entropy_coeff: 0.0017600000137463212
        kl: 0.002253962680697441
        model: {}
        policy_loss: -0.0016932799480855465
        total_loss: 0.0061548370867967606
        vf_explained_var: 0.05761885643005371
        vf_loss: 86.72174072265625
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9913468360900879
        entropy_coeff: 0.0017600000137463212
        kl: 0.001330578001216054
        model: {}
        policy_loss: -0.0018496294505894184
        total_loss: -0.00047527579590678215
        vf_explained_var: 0.017142340540885925
        vf_loss: 31.191246032714844
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26045891642570496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010604159906506538
        model: {}
        policy_loss: -0.0019080317579209805
        total_loss: 0.0006085764616727829
        vf_explained_var: 0.06198948621749878
        vf_loss: 29.75015640258789
    load_time_ms: 14893.872
    num_steps_sampled: 96768000
    num_steps_trained: 96768000
    sample_time_ms: 97543.339
    update_time_ms: 14.689
  iterations_since_restore: 48
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.166477272727274
    ram_util_percent: 12.829545454545457
  pid: 12895
  policy_reward_max:
    agent-0: 283.5
    agent-1: 283.5
    agent-2: 445.5
    agent-3: 445.5
    agent-4: 269.0
    agent-5: 269.0
  policy_reward_mean:
    agent-0: 248.7
    agent-1: 248.7
    agent-2: 410.545
    agent-3: 410.545
    agent-4: 238.46
    agent-5: 238.46
  policy_reward_min:
    agent-0: 142.0
    agent-1: 142.0
    agent-2: 227.0
    agent-3: 227.0
    agent-4: 127.5
    agent-5: 127.5
  sampler_perf:
    mean_env_wait_ms: 25.971473573835546
    mean_inference_ms: 13.191785769290775
    mean_processing_ms: 59.30277240366111
  time_since_restore: 6504.871905088425
  time_this_iter_s: 123.25330328941345
  time_total_s: 128985.89461350441
  timestamp: 1637643172
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 96768000
  training_iteration: 1008
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1008 |           128986 | 96768000 |  1795.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.16
    apples_agent-1_min: 16
    apples_agent-2_max: 410
    apples_agent-2_mean: 364.21
    apples_agent-2_min: 305
    apples_agent-3_max: 309
    apples_agent-3_mean: 249.5
    apples_agent-3_min: 195
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 385.12
    apples_agent-5_min: 339
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 425.59
    cleaning_beam_agent-0_min: 391
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.3
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 14.62
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 415.07
    cleaning_beam_agent-4_min: 356
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-54-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1929.0
  episode_reward_mean: 1809.72
  episode_reward_min: 1673.0
  episodes_this_iter: 96
  episodes_total: 96864
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11223.743
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33479106426239014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011507937451824546
        model: {}
        policy_loss: -0.0013336208648979664
        total_loss: 0.001162782427854836
        vf_explained_var: 0.03196156024932861
        vf_loss: 30.85637855529785
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20713523030281067
        entropy_coeff: 0.0017600000137463212
        kl: 0.000737507245503366
        model: {}
        policy_loss: -0.00159431342035532
        total_loss: 0.0009531457908451557
        vf_explained_var: 0.08616958558559418
        vf_loss: 29.120162963867188
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25904226303100586
        entropy_coeff: 0.0017600000137463212
        kl: 0.001003574812784791
        model: {}
        policy_loss: -0.0018071113154292107
        total_loss: 0.006265494506806135
        vf_explained_var: 0.017234861850738525
        vf_loss: 85.28521728515625
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4647287130355835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006696106283925474
        model: {}
        policy_loss: -0.0012996741570532322
        total_loss: 0.006233096122741699
        vf_explained_var: 0.03896477818489075
        vf_loss: 83.50690460205078
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9758496880531311
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024786624126136303
        model: {}
        policy_loss: -0.0018836462404578924
        total_loss: -0.0007167917210608721
        vf_explained_var: 0.032621338963508606
        vf_loss: 28.843515396118164
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2585833668708801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009663928649388254
        model: {}
        policy_loss: -0.0016782667953521013
        total_loss: 0.0006243406096473336
        vf_explained_var: 0.07404333353042603
        vf_loss: 27.5771427154541
    load_time_ms: 14895.61
    num_steps_sampled: 96864000
    num_steps_trained: 96864000
    sample_time_ms: 97639.364
    update_time_ms: 14.71
  iterations_since_restore: 49
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.13806818181818
    ram_util_percent: 12.90227272727273
  pid: 12895
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 450.5
    agent-3: 450.5
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 251.54
    agent-1: 251.54
    agent-2: 415.13
    agent-3: 415.13
    agent-4: 238.19
    agent-5: 238.19
  policy_reward_min:
    agent-0: 227.0
    agent-1: 227.0
    agent-2: 377.0
    agent-3: 377.0
    agent-4: 213.5
    agent-5: 213.5
  sampler_perf:
    mean_env_wait_ms: 25.94754936167848
    mean_inference_ms: 13.188403449172892
    mean_processing_ms: 59.24753543890799
  time_since_restore: 6628.331199169159
  time_this_iter_s: 123.45929408073425
  time_total_s: 129109.35390758514
  timestamp: 1637643295
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 96864000
  training_iteration: 1009
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1009 |           129109 | 96864000 |  1809.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 30.48
    apples_agent-1_min: 17
    apples_agent-2_max: 429
    apples_agent-2_mean: 364.64
    apples_agent-2_min: 280
    apples_agent-3_max: 313
    apples_agent-3_mean: 246.61
    apples_agent-3_min: 197
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 384.83
    apples_agent-5_min: 323
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 426.46
    cleaning_beam_agent-0_min: 366
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.0
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 1.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 14.23
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 427.13
    cleaning_beam_agent-4_min: 366
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-57-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1907.0
  episode_reward_mean: 1808.48
  episode_reward_min: 1583.0
  episodes_this_iter: 96
  episodes_total: 96960
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11227.75
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3393919765949249
        entropy_coeff: 0.0017600000137463212
        kl: 0.001460942323319614
        model: {}
        policy_loss: -0.001257278025150299
        total_loss: 0.0012544780038297176
        vf_explained_var: 0.04235681891441345
        vf_loss: 31.090843200683594
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2064957320690155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010380884632468224
        model: {}
        policy_loss: -0.0016028806567192078
        total_loss: 0.0010267416946589947
        vf_explained_var: 0.08175113797187805
        vf_loss: 29.93055534362793
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25990545749664307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007630714680999517
        model: {}
        policy_loss: -0.0017192498780786991
        total_loss: 0.006534946616739035
        vf_explained_var: 0.031231999397277832
        vf_loss: 87.11629486083984
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48051196336746216
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014961319975554943
        model: {}
        policy_loss: -0.0016877753660082817
        total_loss: 0.005861478857696056
        vf_explained_var: 0.06733672320842743
        vf_loss: 83.94957733154297
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9829909801483154
        entropy_coeff: 0.0017600000137463212
        kl: 0.002047402085736394
        model: {}
        policy_loss: -0.0016899695619940758
        total_loss: -0.0004949681460857391
        vf_explained_var: 0.03985610604286194
        vf_loss: 29.250661849975586
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25660890340805054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007822648040018976
        model: {}
        policy_loss: -0.0015533582773059607
        total_loss: 0.0007917939219623804
        vf_explained_var: 0.08166192471981049
        vf_loss: 27.967815399169922
    load_time_ms: 14899.173
    num_steps_sampled: 96960000
    num_steps_trained: 96960000
    sample_time_ms: 98072.465
    update_time_ms: 14.344
  iterations_since_restore: 50
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.82
    ram_util_percent: 12.832222222222223
  pid: 12895
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 459.0
    agent-3: 459.0
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 252.22
    agent-1: 252.22
    agent-2: 413.855
    agent-3: 413.855
    agent-4: 238.165
    agent-5: 238.165
  policy_reward_min:
    agent-0: 222.0
    agent-1: 222.0
    agent-2: 354.5
    agent-3: 354.5
    agent-4: 208.0
    agent-5: 208.0
  sampler_perf:
    mean_env_wait_ms: 25.91937350436242
    mean_inference_ms: 13.182225480330876
    mean_processing_ms: 59.21527785037733
  time_since_restore: 6755.030754804611
  time_this_iter_s: 126.69955563545227
  time_total_s: 129236.0534632206
  timestamp: 1637643422
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 96960000
  training_iteration: 1010
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1010 |           129236 | 96960000 |  1808.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 31.06
    apples_agent-1_min: 14
    apples_agent-2_max: 415
    apples_agent-2_mean: 359.51
    apples_agent-2_min: 300
    apples_agent-3_max: 306
    apples_agent-3_mean: 246.44
    apples_agent-3_min: 176
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 384.74
    apples_agent-5_min: 308
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 422.89
    cleaning_beam_agent-0_min: 394
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 13.95
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 457
    cleaning_beam_agent-4_mean: 417.88
    cleaning_beam_agent-4_min: 372
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-22_23-59-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1894.0
  episode_reward_mean: 1791.34
  episode_reward_min: 1448.0
  episodes_this_iter: 96
  episodes_total: 97056
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11235.592
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34126031398773193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011562403524294496
        model: {}
        policy_loss: -0.0012281271629035473
        total_loss: 0.0013269460760056973
        vf_explained_var: 0.040903136134147644
        vf_loss: 31.556900024414062
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21265491843223572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009375410154461861
        model: {}
        policy_loss: -0.0016944846138358116
        total_loss: 0.0009765150025486946
        vf_explained_var: 0.07458490133285522
        vf_loss: 30.452743530273438
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2632845342159271
        entropy_coeff: 0.0017600000137463212
        kl: 0.000737930356990546
        model: {}
        policy_loss: -0.0017381818033754826
        total_loss: 0.006295308470726013
        vf_explained_var: 0.03577466309070587
        vf_loss: 84.96870422363281
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48707789182662964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008552602957934141
        model: {}
        policy_loss: -0.0014669797383248806
        total_loss: 0.00593054573982954
        vf_explained_var: 0.0635882019996643
        vf_loss: 82.54783630371094
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9901531934738159
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006982068880461156
        model: {}
        policy_loss: -0.001657665241509676
        total_loss: -0.00046265817945823073
        vf_explained_var: 0.054447516798973083
        vf_loss: 29.37676239013672
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2523888647556305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014529386535286903
        model: {}
        policy_loss: -0.0018971444806084037
        total_loss: 0.0005050674080848694
        vf_explained_var: 0.08342370390892029
        vf_loss: 28.464153289794922
    load_time_ms: 14927.712
    num_steps_sampled: 97056000
    num_steps_trained: 97056000
    sample_time_ms: 98615.951
    update_time_ms: 14.268
  iterations_since_restore: 51
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.21684210526316
    ram_util_percent: 12.883157894736838
  pid: 12895
  policy_reward_max:
    agent-0: 275.5
    agent-1: 275.5
    agent-2: 446.0
    agent-3: 446.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 247.375
    agent-1: 247.375
    agent-2: 410.95
    agent-3: 410.95
    agent-4: 237.345
    agent-5: 237.345
  policy_reward_min:
    agent-0: 198.0
    agent-1: 198.0
    agent-2: 335.0
    agent-3: 335.0
    agent-4: 190.5
    agent-5: 190.5
  sampler_perf:
    mean_env_wait_ms: 25.91191462383088
    mean_inference_ms: 13.18000642798139
    mean_processing_ms: 59.18998174540516
  time_since_restore: 6886.160143136978
  time_this_iter_s: 131.12938833236694
  time_total_s: 129367.18285155296
  timestamp: 1637643556
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 97056000
  training_iteration: 1011
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1011 |           129367 | 97056000 |  1791.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 31.25
    apples_agent-1_min: 16
    apples_agent-2_max: 402
    apples_agent-2_mean: 360.98
    apples_agent-2_min: 307
    apples_agent-3_max: 287
    apples_agent-3_mean: 244.17
    apples_agent-3_min: 176
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 384.87
    apples_agent-5_min: 332
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 425.05
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 14.41
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 421.82
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 2.96
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-01-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1916.0
  episode_reward_mean: 1800.09
  episode_reward_min: 1621.0
  episodes_this_iter: 96
  episodes_total: 97152
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11209.889
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3339168429374695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015362650156021118
        model: {}
        policy_loss: -0.0012264149263501167
        total_loss: 0.0013598259538412094
        vf_explained_var: 0.04160183668136597
        vf_loss: 31.73932647705078
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21172724664211273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008903132984414697
        model: {}
        policy_loss: -0.0015950766392052174
        total_loss: 0.0010941778309643269
        vf_explained_var: 0.0739307850599289
        vf_loss: 30.618972778320312
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25664716958999634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014647359494119883
        model: {}
        policy_loss: -0.0017963862046599388
        total_loss: 0.006424569524824619
        vf_explained_var: 0.02274206280708313
        vf_loss: 86.72654724121094
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4759187698364258
        entropy_coeff: 0.0017600000137463212
        kl: 0.001909154001623392
        model: {}
        policy_loss: -0.0018369434401392937
        total_loss: 0.005663228686898947
        vf_explained_var: 0.06118758022785187
        vf_loss: 83.37791442871094
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9765094518661499
        entropy_coeff: 0.0017600000137463212
        kl: 0.001901002018712461
        model: {}
        policy_loss: -0.001634796615689993
        total_loss: -0.0004031853750348091
        vf_explained_var: 0.039910390973091125
        vf_loss: 29.502683639526367
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2483862042427063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006218389607965946
        model: {}
        policy_loss: -0.0017039291560649872
        total_loss: 0.000674813985824585
        vf_explained_var: 0.08447366952896118
        vf_loss: 28.159021377563477
    load_time_ms: 14906.558
    num_steps_sampled: 97152000
    num_steps_trained: 97152000
    sample_time_ms: 98484.163
    update_time_ms: 14.261
  iterations_since_restore: 52
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.415083798882684
    ram_util_percent: 12.945251396648043
  pid: 12895
  policy_reward_max:
    agent-0: 281.5
    agent-1: 281.5
    agent-2: 449.0
    agent-3: 449.0
    agent-4: 268.0
    agent-5: 268.0
  policy_reward_mean:
    agent-0: 249.96
    agent-1: 249.96
    agent-2: 411.735
    agent-3: 411.735
    agent-4: 238.35
    agent-5: 238.35
  policy_reward_min:
    agent-0: 221.0
    agent-1: 221.0
    agent-2: 371.0
    agent-3: 371.0
    agent-4: 206.0
    agent-5: 206.0
  sampler_perf:
    mean_env_wait_ms: 25.891134564127583
    mean_inference_ms: 13.17781635174956
    mean_processing_ms: 59.15371730300662
  time_since_restore: 7011.650440692902
  time_this_iter_s: 125.49029755592346
  time_total_s: 129492.67314910889
  timestamp: 1637643681
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 97152000
  training_iteration: 1012
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1012 |           129493 | 97152000 |  1800.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 29.41
    apples_agent-1_min: 1
    apples_agent-2_max: 416
    apples_agent-2_mean: 352.03
    apples_agent-2_min: 3
    apples_agent-3_max: 290
    apples_agent-3_mean: 239.8
    apples_agent-3_min: 4
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.14
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 371.02
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 425.24
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 1.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 1.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 15.71
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 422.46
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 3.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-03-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1929.0
  episode_reward_mean: 1749.48
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 97248
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11218.122
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3547731041908264
        entropy_coeff: 0.0017600000137463212
        kl: 0.001445968635380268
        model: {}
        policy_loss: -0.002143086865544319
        total_loss: 0.001208210363984108
        vf_explained_var: 0.17466330528259277
        vf_loss: 39.75696563720703
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22380313277244568
        entropy_coeff: 0.0017600000137463212
        kl: 0.001102927839383483
        model: {}
        policy_loss: -0.002289470285177231
        total_loss: 0.001265185885131359
        vf_explained_var: 0.18248775601387024
        vf_loss: 39.485504150390625
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26987212896347046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009019173448905349
        model: {}
        policy_loss: -0.0024794181808829308
        total_loss: 0.007993603125214577
        vf_explained_var: 0.17807316780090332
        vf_loss: 109.47994232177734
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.471393346786499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009565545478835702
        model: {}
        policy_loss: -0.0018385772127658129
        total_loss: 0.01013267319649458
        vf_explained_var: 0.040990978479385376
        vf_loss: 128.009033203125
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9563905596733093
        entropy_coeff: 0.0017600000137463212
        kl: 0.002666679210960865
        model: {}
        policy_loss: -0.002165889833122492
        total_loss: 0.0003364591393619776
        vf_explained_var: 0.023346826434135437
        vf_loss: 41.85597229003906
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26937296986579895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006287300493568182
        model: {}
        policy_loss: -0.002765239682048559
        total_loss: 0.00026665895711630583
        vf_explained_var: 0.17776137590408325
        vf_loss: 35.05994415283203
    load_time_ms: 14897.278
    num_steps_sampled: 97248000
    num_steps_trained: 97248000
    sample_time_ms: 98659.073
    update_time_ms: 14.247
  iterations_since_restore: 53
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.86
    ram_util_percent: 12.899999999999997
  pid: 12895
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 449.5
    agent-3: 449.5
    agent-4: 266.0
    agent-5: 266.0
  policy_reward_mean:
    agent-0: 243.19
    agent-1: 243.19
    agent-2: 402.01
    agent-3: 402.01
    agent-4: 229.54
    agent-5: 229.54
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: 4.0
    agent-3: 4.0
    agent-4: 2.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 25.873828150833745
    mean_inference_ms: 13.173833275646237
    mean_processing_ms: 59.12080238710878
  time_since_restore: 7138.105371236801
  time_this_iter_s: 126.45493054389954
  time_total_s: 129619.12807965279
  timestamp: 1637643808
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 97248000
  training_iteration: 1013
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1013 |           129619 | 97248000 |  1749.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 30.14
    apples_agent-1_min: 18
    apples_agent-2_max: 413
    apples_agent-2_mean: 364.5
    apples_agent-2_min: 318
    apples_agent-3_max: 299
    apples_agent-3_mean: 248.11
    apples_agent-3_min: 202
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 449
    apples_agent-5_mean: 388.24
    apples_agent-5_min: 309
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 428.82
    cleaning_beam_agent-0_min: 399
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 14.52
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 437.91
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-05-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1923.0
  episode_reward_mean: 1810.43
  episode_reward_min: 1656.0
  episodes_this_iter: 96
  episodes_total: 97344
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11218.333
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3394758999347687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013419205788522959
        model: {}
        policy_loss: -0.001296354690566659
        total_loss: 0.001343709183856845
        vf_explained_var: 0.025693044066429138
        vf_loss: 32.375423431396484
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20865722000598907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009170402772724628
        model: {}
        policy_loss: -0.0015518362633883953
        total_loss: 0.0011558872647583485
        vf_explained_var: 0.07568444311618805
        vf_loss: 30.749610900878906
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2567213773727417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006615811144001782
        model: {}
        policy_loss: -0.0016975370235741138
        total_loss: 0.006501500029116869
        vf_explained_var: 0.028300881385803223
        vf_loss: 86.50867462158203
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.457580029964447
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014434256590902805
        model: {}
        policy_loss: -0.0014661746099591255
        total_loss: 0.0060514723882079124
        vf_explained_var: 0.06317141652107239
        vf_loss: 83.2298812866211
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9405731558799744
        entropy_coeff: 0.0017600000137463212
        kl: 0.002785275923088193
        model: {}
        policy_loss: -0.0022080536000430584
        total_loss: -0.000888790818862617
        vf_explained_var: 0.04719357192516327
        vf_loss: 29.746719360351562
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25130170583724976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010318504646420479
        model: {}
        policy_loss: -0.0016244372818619013
        total_loss: 0.0007868148386478424
        vf_explained_var: 0.08926264941692352
        vf_loss: 28.535438537597656
    load_time_ms: 14887.056
    num_steps_sampled: 97344000
    num_steps_trained: 97344000
    sample_time_ms: 98753.81
    update_time_ms: 14.29
  iterations_since_restore: 54
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.460227272727273
    ram_util_percent: 12.915340909090908
  pid: 12895
  policy_reward_max:
    agent-0: 274.5
    agent-1: 274.5
    agent-2: 451.5
    agent-3: 451.5
    agent-4: 268.0
    agent-5: 268.0
  policy_reward_mean:
    agent-0: 250.485
    agent-1: 250.485
    agent-2: 415.28
    agent-3: 415.28
    agent-4: 239.45
    agent-5: 239.45
  policy_reward_min:
    agent-0: 224.5
    agent-1: 224.5
    agent-2: 376.5
    agent-3: 376.5
    agent-4: 204.0
    agent-5: 204.0
  sampler_perf:
    mean_env_wait_ms: 25.85734807891764
    mean_inference_ms: 13.172691995527607
    mean_processing_ms: 59.074218902201956
  time_since_restore: 7261.436448812485
  time_this_iter_s: 123.3310775756836
  time_total_s: 129742.45915722847
  timestamp: 1637643931
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 97344000
  training_iteration: 1014
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1014 |           129742 | 97344000 |  1810.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 52
    apples_agent-1_mean: 29.58
    apples_agent-1_min: 9
    apples_agent-2_max: 421
    apples_agent-2_mean: 362.56
    apples_agent-2_min: 123
    apples_agent-3_max: 301
    apples_agent-3_mean: 251.5
    apples_agent-3_min: 99
    apples_agent-4_max: 1
    apples_agent-4_mean: 0.01
    apples_agent-4_min: 0
    apples_agent-5_max: 440
    apples_agent-5_mean: 382.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 426.39
    cleaning_beam_agent-0_min: 230
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 1.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 13.17
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 452.22
    cleaning_beam_agent-4_min: 381
    cleaning_beam_agent-5_max: 23
    cleaning_beam_agent-5_mean: 3.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-07-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1918.0
  episode_reward_mean: 1804.3
  episode_reward_min: 398.0
  episodes_this_iter: 96
  episodes_total: 97440
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11246.854
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33768293261528015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012310747988522053
        model: {}
        policy_loss: -0.0014094393700361252
        total_loss: 0.0013733012601733208
        vf_explained_var: 0.0830221176147461
        vf_loss: 33.770626068115234
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20851629972457886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008546337485313416
        model: {}
        policy_loss: -0.0017905144486576319
        total_loss: 0.0011627762578427792
        vf_explained_var: 0.09848847985267639
        vf_loss: 33.20280456542969
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25711697340011597
        entropy_coeff: 0.0017600000137463212
        kl: 0.00098443403840065
        model: {}
        policy_loss: -0.0017703548073768616
        total_loss: 0.007018642034381628
        vf_explained_var: 0.049973130226135254
        vf_loss: 92.41521453857422
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4692975878715515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017676461720839143
        model: {}
        policy_loss: -0.0015230071730911732
        total_loss: 0.006987260654568672
        vf_explained_var: 0.041208818554878235
        vf_loss: 93.3623046875
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9381197094917297
        entropy_coeff: 0.0017600000137463212
        kl: 0.003334375564008951
        model: {}
        policy_loss: -0.002030099742114544
        total_loss: -0.00034497492015361786
        vf_explained_var: 0.05219268798828125
        vf_loss: 33.362144470214844
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2562984228134155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007947720587253571
        model: {}
        policy_loss: -0.0019323471933603287
        total_loss: 0.0009089484810829163
        vf_explained_var: 0.06998153030872345
        vf_loss: 32.92378616333008
    load_time_ms: 14879.213
    num_steps_sampled: 97440000
    num_steps_trained: 97440000
    sample_time_ms: 98765.187
    update_time_ms: 13.893
  iterations_since_restore: 55
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.10689655172414
    ram_util_percent: 12.928735632183905
  pid: 12895
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 460.5
    agent-3: 460.5
    agent-4: 271.0
    agent-5: 271.0
  policy_reward_mean:
    agent-0: 249.57
    agent-1: 249.57
    agent-2: 415.53
    agent-3: 415.53
    agent-4: 237.05
    agent-5: 237.05
  policy_reward_min:
    agent-0: 69.0
    agent-1: 69.0
    agent-2: 130.0
    agent-3: 130.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.837114249530117
    mean_inference_ms: 13.167543916267814
    mean_processing_ms: 59.01862474251894
  time_since_restore: 7383.443867921829
  time_this_iter_s: 122.00741910934448
  time_total_s: 129864.46657633781
  timestamp: 1637644053
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 97440000
  training_iteration: 1015
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1015 |           129864 | 97440000 |   1804.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 31.07
    apples_agent-1_min: 14
    apples_agent-2_max: 406
    apples_agent-2_mean: 362.05
    apples_agent-2_min: 319
    apples_agent-3_max: 300
    apples_agent-3_mean: 251.48
    apples_agent-3_min: 193
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 431
    apples_agent-5_mean: 384.72
    apples_agent-5_min: 322
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 426.41
    cleaning_beam_agent-0_min: 383
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 1.08
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 1.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 13.66
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 459.82
    cleaning_beam_agent-4_min: 407
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-09-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1936.0
  episode_reward_mean: 1811.7
  episode_reward_min: 1649.0
  episodes_this_iter: 96
  episodes_total: 97536
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11254.609
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33757245540618896
        entropy_coeff: 0.0017600000137463212
        kl: 0.001099194516427815
        model: {}
        policy_loss: -0.0012336240615695715
        total_loss: 0.001324179582297802
        vf_explained_var: 0.03794638812541962
        vf_loss: 31.519306182861328
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.209035724401474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006440578144975007
        model: {}
        policy_loss: -0.0013813492842018604
        total_loss: 0.001245478168129921
        vf_explained_var: 0.08588093519210815
        vf_loss: 29.94729995727539
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2571207284927368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008611053344793618
        model: {}
        policy_loss: -0.0017762931529432535
        total_loss: 0.006538153626024723
        vf_explained_var: 0.014540553092956543
        vf_loss: 87.6697769165039
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4659322202205658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010464872466400266
        model: {}
        policy_loss: -0.0015441523864865303
        total_loss: 0.006059838458895683
        vf_explained_var: 0.05609689652919769
        vf_loss: 84.24032592773438
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9407712817192078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011729627149179578
        model: {}
        policy_loss: -0.001750228926539421
        total_loss: -0.0005168479401618242
        vf_explained_var: 0.03919073939323425
        vf_loss: 28.891395568847656
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2546480596065521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007303679012693465
        model: {}
        policy_loss: -0.001654510386288166
        total_loss: 0.0006959461607038975
        vf_explained_var: 0.07337898015975952
        vf_loss: 27.986351013183594
    load_time_ms: 14881.156
    num_steps_sampled: 97536000
    num_steps_trained: 97536000
    sample_time_ms: 98898.582
    update_time_ms: 13.76
  iterations_since_restore: 56
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.059550561797753
    ram_util_percent: 12.912921348314606
  pid: 12895
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 446.0
    agent-3: 446.0
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 250.875
    agent-1: 250.875
    agent-2: 416.505
    agent-3: 416.505
    agent-4: 238.47
    agent-5: 238.47
  policy_reward_min:
    agent-0: 222.5
    agent-1: 222.5
    agent-2: 366.0
    agent-3: 366.0
    agent-4: 200.0
    agent-5: 200.0
  sampler_perf:
    mean_env_wait_ms: 25.81852434838178
    mean_inference_ms: 13.163419947245934
    mean_processing_ms: 58.98301892889558
  time_since_restore: 7508.716132879257
  time_this_iter_s: 125.27226495742798
  time_total_s: 129989.73884129524
  timestamp: 1637644179
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 97536000
  training_iteration: 1016
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1016 |           129990 | 97536000 |   1811.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.04
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 30.67
    apples_agent-1_min: 17
    apples_agent-2_max: 411
    apples_agent-2_mean: 367.82
    apples_agent-2_min: 312
    apples_agent-3_max: 296
    apples_agent-3_mean: 254.96
    apples_agent-3_min: 212
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 452
    apples_agent-5_mean: 389.75
    apples_agent-5_min: 335
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 426.32
    cleaning_beam_agent-0_min: 367
    cleaning_beam_agent-1_max: 11
    cleaning_beam_agent-1_mean: 0.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 1.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 12.53
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 474.02
    cleaning_beam_agent-4_min: 380
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-11-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1933.0
  episode_reward_mean: 1829.9
  episode_reward_min: 1606.0
  episodes_this_iter: 96
  episodes_total: 97632
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11292.249
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.34322476387023926
        entropy_coeff: 0.0017600000137463212
        kl: 0.001203443855047226
        model: {}
        policy_loss: -0.001123211346566677
        total_loss: 0.0015178760513663292
        vf_explained_var: 0.01916201412677765
        vf_loss: 32.451622009277344
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20949429273605347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009908652864396572
        model: {}
        policy_loss: -0.0016892969142645597
        total_loss: 0.0010907631367444992
        vf_explained_var: 0.05209438502788544
        vf_loss: 31.487707138061523
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2521038055419922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011549085611477494
        model: {}
        policy_loss: -0.0017582885921001434
        total_loss: 0.006812563166022301
        vf_explained_var: 0.022359028458595276
        vf_loss: 90.14555358886719
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47138291597366333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015684296377003193
        model: {}
        policy_loss: -0.001326665049418807
        total_loss: 0.006642189808189869
        vf_explained_var: 0.05210953950881958
        vf_loss: 87.9848861694336
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9230369329452515
        entropy_coeff: 0.0017600000137463212
        kl: 0.002204343443736434
        model: {}
        policy_loss: -0.0019509405829012394
        total_loss: -0.0005704555660486221
        vf_explained_var: 0.03331176936626434
        vf_loss: 30.050312042236328
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.258041113615036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006283456459641457
        model: {}
        policy_loss: -0.001479591242969036
        total_loss: 0.0009311484172940254
        vf_explained_var: 0.08617046475410461
        vf_loss: 28.64887237548828
    load_time_ms: 14863.215
    num_steps_sampled: 97632000
    num_steps_trained: 97632000
    sample_time_ms: 99104.224
    update_time_ms: 13.667
  iterations_since_restore: 57
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.69889502762431
    ram_util_percent: 12.921546961325962
  pid: 12895
  policy_reward_max:
    agent-0: 279.5
    agent-1: 279.5
    agent-2: 465.0
    agent-3: 465.0
    agent-4: 270.0
    agent-5: 270.0
  policy_reward_mean:
    agent-0: 252.855
    agent-1: 252.855
    agent-2: 421.465
    agent-3: 421.465
    agent-4: 240.63
    agent-5: 240.63
  policy_reward_min:
    agent-0: 210.0
    agent-1: 210.0
    agent-2: 376.0
    agent-3: 376.0
    agent-4: 207.0
    agent-5: 207.0
  sampler_perf:
    mean_env_wait_ms: 25.802961997141168
    mean_inference_ms: 13.158944558785286
    mean_processing_ms: 58.95028452928713
  time_since_restore: 7635.169089317322
  time_this_iter_s: 126.45295643806458
  time_total_s: 130116.1917977333
  timestamp: 1637644305
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 97632000
  training_iteration: 1017
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1017 |           130116 | 97632000 |   1829.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 30.71
    apples_agent-1_min: 13
    apples_agent-2_max: 410
    apples_agent-2_mean: 362.89
    apples_agent-2_min: 226
    apples_agent-3_max: 293
    apples_agent-3_mean: 248.61
    apples_agent-3_min: 192
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 433
    apples_agent-5_mean: 384.49
    apples_agent-5_min: 304
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 428.78
    cleaning_beam_agent-0_min: 388
    cleaning_beam_agent-1_max: 4
    cleaning_beam_agent-1_mean: 1.0
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 14.91
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 489.74
    cleaning_beam_agent-4_min: 437
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-13-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1915.0
  episode_reward_mean: 1808.31
  episode_reward_min: 1534.0
  episodes_this_iter: 96
  episodes_total: 97728
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11268.722
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3445499539375305
        entropy_coeff: 0.0017600000137463212
        kl: 0.001085743890143931
        model: {}
        policy_loss: -0.0011081821285188198
        total_loss: 0.001583941513672471
        vf_explained_var: 0.04114094376564026
        vf_loss: 32.98530578613281
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20976221561431885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004965952830389142
        model: {}
        policy_loss: -0.001538767828606069
        total_loss: 0.0012407235335558653
        vf_explained_var: 0.08535447716712952
        vf_loss: 31.48672103881836
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.252652108669281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007290202775038779
        model: {}
        policy_loss: -0.001557900570333004
        total_loss: 0.006784234195947647
        vf_explained_var: 0.04727952182292938
        vf_loss: 87.86798858642578
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4797070026397705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024010108318179846
        model: {}
        policy_loss: -0.0019276048988103867
        total_loss: 0.005845659412443638
        vf_explained_var: 0.0662381649017334
        vf_loss: 86.175537109375
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9208821654319763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016874537104740739
        model: {}
        policy_loss: -0.002020082203671336
        total_loss: -0.0006296733627095819
        vf_explained_var: 0.05143330991268158
        vf_loss: 30.11158561706543
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.262595534324646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008876558276824653
        model: {}
        policy_loss: -0.0015346985310316086
        total_loss: 0.0009371251799166203
        vf_explained_var: 0.07603652775287628
        vf_loss: 29.339920043945312
    load_time_ms: 14869.976
    num_steps_sampled: 97728000
    num_steps_trained: 97728000
    sample_time_ms: 99406.154
    update_time_ms: 13.546
  iterations_since_restore: 58
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.932222222222222
    ram_util_percent: 12.914444444444447
  pid: 12895
  policy_reward_max:
    agent-0: 281.0
    agent-1: 281.0
    agent-2: 446.0
    agent-3: 446.0
    agent-4: 267.0
    agent-5: 267.0
  policy_reward_mean:
    agent-0: 252.75
    agent-1: 252.75
    agent-2: 413.58
    agent-3: 413.58
    agent-4: 237.825
    agent-5: 237.825
  policy_reward_min:
    agent-0: 218.5
    agent-1: 218.5
    agent-2: 318.0
    agent-3: 318.0
    agent-4: 199.0
    agent-5: 199.0
  sampler_perf:
    mean_env_wait_ms: 25.79005889857473
    mean_inference_ms: 13.154663492205424
    mean_processing_ms: 58.925446669721644
  time_since_restore: 7761.2663650512695
  time_this_iter_s: 126.09727573394775
  time_total_s: 130242.28907346725
  timestamp: 1637644432
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 97728000
  training_iteration: 1018
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1018 |           130242 | 97728000 |  1808.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 29.95
    apples_agent-1_min: 7
    apples_agent-2_max: 421
    apples_agent-2_mean: 364.26
    apples_agent-2_min: 209
    apples_agent-3_max: 293
    apples_agent-3_mean: 245.5
    apples_agent-3_min: 135
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 386.54
    apples_agent-5_min: 208
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 430.58
    cleaning_beam_agent-0_min: 376
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.16
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 1.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.04
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 486.27
    cleaning_beam_agent-4_min: 440
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 3.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-15-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1916.0
  episode_reward_mean: 1801.67
  episode_reward_min: 966.0
  episodes_this_iter: 96
  episodes_total: 97824
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11269.851
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3501059114933014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015094074187800288
        model: {}
        policy_loss: -0.001222667284309864
        total_loss: 0.0015160776674747467
        vf_explained_var: 0.06337723135948181
        vf_loss: 33.54931640625
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21316075325012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008021785761229694
        model: {}
        policy_loss: -0.0016965661197900772
        total_loss: 0.0011504441499710083
        vf_explained_var: 0.10005335509777069
        vf_loss: 32.221763610839844
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2483624517917633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008040867396630347
        model: {}
        policy_loss: -0.0017786803655326366
        total_loss: 0.006690515670925379
        vf_explained_var: 0.058245182037353516
        vf_loss: 89.06311798095703
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48136401176452637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013333030510693789
        model: {}
        policy_loss: -0.0014365389943122864
        total_loss: 0.00647316686809063
        vf_explained_var: 0.07509489357471466
        vf_loss: 87.56906127929688
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9023571014404297
        entropy_coeff: 0.0017600000137463212
        kl: 0.00240947213023901
        model: {}
        policy_loss: -0.001816134317778051
        total_loss: -0.00021525321062654257
        vf_explained_var: 0.045135319232940674
        vf_loss: 31.89031219482422
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2624913454055786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008054558420553803
        model: {}
        policy_loss: -0.0017764093354344368
        total_loss: 0.0007999707013368607
        vf_explained_var: 0.08975441753864288
        vf_loss: 30.383684158325195
    load_time_ms: 14859.918
    num_steps_sampled: 97824000
    num_steps_trained: 97824000
    sample_time_ms: 99324.154
    update_time_ms: 13.286
  iterations_since_restore: 59
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.23218390804598
    ram_util_percent: 12.92471264367816
  pid: 12895
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 452.0
    agent-3: 452.0
    agent-4: 261.0
    agent-5: 261.0
  policy_reward_mean:
    agent-0: 250.08
    agent-1: 250.08
    agent-2: 412.575
    agent-3: 412.575
    agent-4: 238.18
    agent-5: 238.18
  policy_reward_min:
    agent-0: 134.0
    agent-1: 134.0
    agent-2: 226.5
    agent-3: 226.5
    agent-4: 122.5
    agent-5: 122.5
  sampler_perf:
    mean_env_wait_ms: 25.775752085590693
    mean_inference_ms: 13.150939269638686
    mean_processing_ms: 58.8779539291871
  time_since_restore: 7883.778886079788
  time_this_iter_s: 122.51252102851868
  time_total_s: 130364.80159449577
  timestamp: 1637644554
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 97824000
  training_iteration: 1019
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1019 |           130365 | 97824000 |  1801.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 29.42
    apples_agent-1_min: 1
    apples_agent-2_max: 412
    apples_agent-2_mean: 355.53
    apples_agent-2_min: 9
    apples_agent-3_max: 291
    apples_agent-3_mean: 246.04
    apples_agent-3_min: 0
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 381.86
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 431.28
    cleaning_beam_agent-0_min: 364
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.15
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 1.48
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 17.45
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 501.31
    cleaning_beam_agent-4_min: 447
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 3.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-18-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1897.0
  episode_reward_mean: 1782.82
  episode_reward_min: 41.0
  episodes_this_iter: 96
  episodes_total: 97920
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11271.842
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3598392605781555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008944945875555277
        model: {}
        policy_loss: -0.0014638504944741726
        total_loss: 0.0013586352579295635
        vf_explained_var: 0.12185169756412506
        vf_loss: 34.55807113647461
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21666866540908813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011092869099229574
        model: {}
        policy_loss: -0.002329394454136491
        total_loss: 0.0007145334966480732
        vf_explained_var: 0.12963534891605377
        vf_loss: 34.252628326416016
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25407716631889343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010014232248067856
        model: {}
        policy_loss: -0.0016833972185850143
        total_loss: 0.007211897522211075
        vf_explained_var: 0.11960253119468689
        vf_loss: 93.4246826171875
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.488118052482605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009537844453006983
        model: {}
        policy_loss: -0.001407700590789318
        total_loss: 0.007869835942983627
        vf_explained_var: 0.04699349403381348
        vf_loss: 101.36625671386719
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9153611660003662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020108099561184645
        model: {}
        policy_loss: -0.0020270156674087048
        total_loss: -0.00012273341417312622
        vf_explained_var: 0.05690106749534607
        vf_loss: 35.1531982421875
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2693271040916443
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009643803350627422
        model: {}
        policy_loss: -0.002308460185304284
        total_loss: 0.000497864093631506
        vf_explained_var: 0.11886972188949585
        vf_loss: 32.80340576171875
    load_time_ms: 14858.46
    num_steps_sampled: 97920000
    num_steps_trained: 97920000
    sample_time_ms: 99208.085
    update_time_ms: 13.141
  iterations_since_restore: 60
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.94413407821229
    ram_util_percent: 12.854189944134081
  pid: 12895
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 453.0
    agent-3: 453.0
    agent-4: 270.0
    agent-5: 270.0
  policy_reward_mean:
    agent-0: 245.97
    agent-1: 245.97
    agent-2: 409.075
    agent-3: 409.075
    agent-4: 236.365
    agent-5: 236.365
  policy_reward_min:
    agent-0: 7.5
    agent-1: 7.5
    agent-2: 8.5
    agent-3: 8.5
    agent-4: 4.5
    agent-5: 4.5
  sampler_perf:
    mean_env_wait_ms: 25.76780320396228
    mean_inference_ms: 13.147891907949754
    mean_processing_ms: 58.84507671139311
  time_since_restore: 8009.3592410087585
  time_this_iter_s: 125.58035492897034
  time_total_s: 130490.38194942474
  timestamp: 1637644680
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 97920000
  training_iteration: 1020
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1020 |           130490 | 97920000 |  1782.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 4
    apples_agent-0_mean: 0.05
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 29.94
    apples_agent-1_min: 5
    apples_agent-2_max: 410
    apples_agent-2_mean: 361.8
    apples_agent-2_min: 15
    apples_agent-3_max: 290
    apples_agent-3_mean: 245.32
    apples_agent-3_min: 14
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 382.68
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 433.43
    cleaning_beam_agent-0_min: 401
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 1.62
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 16.11
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 490.52
    cleaning_beam_agent-4_min: 436
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 3.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-20-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1936.0
  episode_reward_mean: 1790.53
  episode_reward_min: 91.0
  episodes_this_iter: 96
  episodes_total: 98016
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11267.118
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3648461401462555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014333026483654976
        model: {}
        policy_loss: -0.0017289677634835243
        total_loss: 0.0011828197166323662
        vf_explained_var: 0.13334481418132782
        vf_loss: 35.53916549682617
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21019938588142395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009170890552923083
        model: {}
        policy_loss: -0.0021750153973698616
        total_loss: 0.0009721312671899796
        vf_explained_var: 0.14064626395702362
        vf_loss: 35.17097473144531
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2560858726501465
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012272337917238474
        model: {}
        policy_loss: -0.002065229695290327
        total_loss: 0.007437517400830984
        vf_explained_var: 0.12083694338798523
        vf_loss: 99.53458404541016
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47371774911880493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015354829374700785
        model: {}
        policy_loss: -0.001802519429475069
        total_loss: 0.007947521284222603
        vf_explained_var: 0.06475841999053955
        vf_loss: 105.83784484863281
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9249289631843567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015394093934446573
        model: {}
        policy_loss: -0.0017916960641741753
        total_loss: 0.0001869318075478077
        vf_explained_var: 0.027300238609313965
        vf_loss: 36.06504440307617
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26374250650405884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011639650911092758
        model: {}
        policy_loss: -0.002220684429630637
        total_loss: 0.0006159415934234858
        vf_explained_var: 0.10923050343990326
        vf_loss: 33.00813293457031
    load_time_ms: 14854.596
    num_steps_sampled: 98016000
    num_steps_trained: 98016000
    sample_time_ms: 98339.883
    update_time_ms: 13.116
  iterations_since_restore: 61
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.955617977528092
    ram_util_percent: 13.002247191011234
  pid: 12895
  policy_reward_max:
    agent-0: 280.5
    agent-1: 280.5
    agent-2: 456.5
    agent-3: 456.5
    agent-4: 262.5
    agent-5: 262.5
  policy_reward_mean:
    agent-0: 247.835
    agent-1: 247.835
    agent-2: 411.375
    agent-3: 411.375
    agent-4: 236.055
    agent-5: 236.055
  policy_reward_min:
    agent-0: 16.0
    agent-1: 16.0
    agent-2: 21.5
    agent-3: 21.5
    agent-4: 8.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 25.754642730395886
    mean_inference_ms: 13.144141138554485
    mean_processing_ms: 58.803015672964754
  time_since_restore: 8131.682114839554
  time_this_iter_s: 122.32287383079529
  time_total_s: 130612.70482325554
  timestamp: 1637644804
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 98016000
  training_iteration: 1021
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1021 |           130613 | 98016000 |  1790.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.68
    apples_agent-1_min: 14
    apples_agent-2_max: 409
    apples_agent-2_mean: 363.85
    apples_agent-2_min: 314
    apples_agent-3_max: 289
    apples_agent-3_mean: 248.48
    apples_agent-3_min: 190
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 464
    apples_agent-5_mean: 386.96
    apples_agent-5_min: 298
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 434.73
    cleaning_beam_agent-0_min: 391
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.1
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 15.92
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 474.17
    cleaning_beam_agent-4_min: 422
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-22-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1930.0
  episode_reward_mean: 1813.95
  episode_reward_min: 1561.0
  episodes_this_iter: 96
  episodes_total: 98112
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11268.802
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3635818362236023
        entropy_coeff: 0.0017600000137463212
        kl: 0.002133780624717474
        model: {}
        policy_loss: -0.0015667148400098085
        total_loss: 0.0010886989766731858
        vf_explained_var: 0.04238824546337128
        vf_loss: 32.953147888183594
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20671291649341583
        entropy_coeff: 0.0017600000137463212
        kl: 0.00085399707313627
        model: {}
        policy_loss: -0.0016393454279750586
        total_loss: 0.001128209289163351
        vf_explained_var: 0.08766672015190125
        vf_loss: 31.31371307373047
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24548745155334473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010639658430591226
        model: {}
        policy_loss: -0.0016915765590965748
        total_loss: 0.006704858038574457
        vf_explained_var: 0.03758123517036438
        vf_loss: 88.284912109375
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4637034833431244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016672563506290317
        model: {}
        policy_loss: -0.001504135550931096
        total_loss: 0.0060321372002363205
        vf_explained_var: 0.08903943002223969
        vf_loss: 83.52389526367188
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9207353591918945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0039039389230310917
        model: {}
        policy_loss: -0.002389266388490796
        total_loss: -0.001046899240463972
        vf_explained_var: 0.04986207187175751
        vf_loss: 29.628644943237305
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2534407079219818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009455500985495746
        model: {}
        policy_loss: -0.0016784099861979485
        total_loss: 0.0007374761626124382
        vf_explained_var: 0.0766376405954361
        vf_loss: 28.619403839111328
    load_time_ms: 14851.661
    num_steps_sampled: 98112000
    num_steps_trained: 98112000
    sample_time_ms: 98252.152
    update_time_ms: 13.155
  iterations_since_restore: 62
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.282485875706215
    ram_util_percent: 12.934463276836158
  pid: 12895
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 450.0
    agent-3: 450.0
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 252.2
    agent-1: 252.2
    agent-2: 415.37
    agent-3: 415.37
    agent-4: 239.405
    agent-5: 239.405
  policy_reward_min:
    agent-0: 216.5
    agent-1: 216.5
    agent-2: 358.0
    agent-3: 358.0
    agent-4: 192.0
    agent-5: 192.0
  sampler_perf:
    mean_env_wait_ms: 25.74502883122136
    mean_inference_ms: 13.142156942707134
    mean_processing_ms: 58.768541912569624
  time_since_restore: 8256.28592467308
  time_this_iter_s: 124.60380983352661
  time_total_s: 130737.30863308907
  timestamp: 1637644929
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 98112000
  training_iteration: 1022
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1022 |           130737 | 98112000 |  1813.95 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 31.43
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 363.94
    apples_agent-2_min: 303
    apples_agent-3_max: 293
    apples_agent-3_mean: 244.71
    apples_agent-3_min: 186
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 386.08
    apples_agent-5_min: 338
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 443.08
    cleaning_beam_agent-0_min: 402
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.76
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 1.45
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 16.93
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 561
    cleaning_beam_agent-4_mean: 490.92
    cleaning_beam_agent-4_min: 424
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-24-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1963.0
  episode_reward_mean: 1810.97
  episode_reward_min: 1664.0
  episodes_this_iter: 96
  episodes_total: 98208
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11301.677
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3628169298171997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010572252795100212
        model: {}
        policy_loss: -0.0011044619604945183
        total_loss: 0.0013971291482448578
        vf_explained_var: 0.0484476238489151
        vf_loss: 31.401477813720703
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2085569053888321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011808566050603986
        model: {}
        policy_loss: -0.0016455149743705988
        total_loss: 0.001016565365716815
        vf_explained_var: 0.08190931379795074
        vf_loss: 30.291425704956055
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24965603649616241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008488550665788352
        model: {}
        policy_loss: -0.0017769993282854557
        total_loss: 0.006604855880141258
        vf_explained_var: 0.035697221755981445
        vf_loss: 88.21250915527344
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4591861963272095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009596154559403658
        model: {}
        policy_loss: -0.0015507452189922333
        total_loss: 0.006111524999141693
        vf_explained_var: 0.07279525697231293
        vf_loss: 84.70437622070312
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9310834407806396
        entropy_coeff: 0.0017600000137463212
        kl: 0.001976277679204941
        model: {}
        policy_loss: -0.002072576666250825
        total_loss: -0.000848875381052494
        vf_explained_var: 0.04308405518531799
        vf_loss: 28.624107360839844
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2527179718017578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006887272465974092
        model: {}
        policy_loss: -0.0014270511455833912
        total_loss: 0.0009066951461136341
        vf_explained_var: 0.06347484886646271
        vf_loss: 27.785295486450195
    load_time_ms: 14862.776
    num_steps_sampled: 98208000
    num_steps_trained: 98208000
    sample_time_ms: 97867.887
    update_time_ms: 13.151
  iterations_since_restore: 63
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.231818181818184
    ram_util_percent: 12.995454545454544
  pid: 12895
  policy_reward_max:
    agent-0: 271.0
    agent-1: 271.0
    agent-2: 461.0
    agent-3: 461.0
    agent-4: 271.0
    agent-5: 271.0
  policy_reward_mean:
    agent-0: 250.515
    agent-1: 250.515
    agent-2: 415.455
    agent-3: 415.455
    agent-4: 239.515
    agent-5: 239.515
  policy_reward_min:
    agent-0: 224.0
    agent-1: 224.0
    agent-2: 354.0
    agent-3: 354.0
    agent-4: 212.0
    agent-5: 212.0
  sampler_perf:
    mean_env_wait_ms: 25.737028631590317
    mean_inference_ms: 13.14018448785136
    mean_processing_ms: 58.72942226844151
  time_since_restore: 8379.381103277206
  time_this_iter_s: 123.09517860412598
  time_total_s: 130860.40381169319
  timestamp: 1637645052
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 98208000
  training_iteration: 1023
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1023 |           130860 | 98208000 |  1810.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 45
    apples_agent-1_mean: 30.03
    apples_agent-1_min: 18
    apples_agent-2_max: 415
    apples_agent-2_mean: 365.23
    apples_agent-2_min: 246
    apples_agent-3_max: 289
    apples_agent-3_mean: 247.45
    apples_agent-3_min: 181
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 454
    apples_agent-5_mean: 385.07
    apples_agent-5_min: 260
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 441.03
    cleaning_beam_agent-0_min: 377
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.88
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 1.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 16.74
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 486.77
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-26-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1939.0
  episode_reward_mean: 1816.94
  episode_reward_min: 1255.0
  episodes_this_iter: 96
  episodes_total: 98304
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11302.146
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35370051860809326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016647384036332369
        model: {}
        policy_loss: -0.001258794334717095
        total_loss: 0.0013570006703957915
        vf_explained_var: 0.043634384870529175
        vf_loss: 32.383094787597656
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20895439386367798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005630212835967541
        model: {}
        policy_loss: -0.001566876657307148
        total_loss: 0.0010929058771580458
        vf_explained_var: 0.1056830883026123
        vf_loss: 30.275428771972656
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24650534987449646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008693587733432651
        model: {}
        policy_loss: -0.0017346902750432491
        total_loss: 0.00705152191221714
        vf_explained_var: 0.02655714750289917
        vf_loss: 92.20063781738281
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45460253953933716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016733035445213318
        model: {}
        policy_loss: -0.001710185781121254
        total_loss: 0.006339054089039564
        vf_explained_var: 0.06279279291629791
        vf_loss: 88.49340057373047
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9253786206245422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031758295372128487
        model: {}
        policy_loss: -0.002289780182763934
        total_loss: -0.0009394933003932238
        vf_explained_var: 0.06163597106933594
        vf_loss: 29.78955078125
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2620270848274231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012808006722480059
        model: {}
        policy_loss: -0.001676281914114952
        total_loss: 0.0007674619555473328
        vf_explained_var: 0.08170339465141296
        vf_loss: 29.049089431762695
    load_time_ms: 14841.462
    num_steps_sampled: 98304000
    num_steps_trained: 98304000
    sample_time_ms: 97698.154
    update_time_ms: 13.464
  iterations_since_restore: 64
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.3635838150289
    ram_util_percent: 12.960693641618496
  pid: 12895
  policy_reward_max:
    agent-0: 283.0
    agent-1: 283.0
    agent-2: 456.5
    agent-3: 456.5
    agent-4: 270.0
    agent-5: 270.0
  policy_reward_mean:
    agent-0: 250.455
    agent-1: 250.455
    agent-2: 418.78
    agent-3: 418.78
    agent-4: 239.235
    agent-5: 239.235
  policy_reward_min:
    agent-0: 169.5
    agent-1: 169.5
    agent-2: 300.0
    agent-3: 300.0
    agent-4: 158.0
    agent-5: 158.0
  sampler_perf:
    mean_env_wait_ms: 25.72477636075349
    mean_inference_ms: 13.137241491882342
    mean_processing_ms: 58.685178026527325
  time_since_restore: 8500.809900283813
  time_this_iter_s: 121.42879700660706
  time_total_s: 130981.8326086998
  timestamp: 1637645174
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 98304000
  training_iteration: 1024
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1024 |           130982 | 98304000 |  1816.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 30.62
    apples_agent-1_min: 17
    apples_agent-2_max: 430
    apples_agent-2_mean: 365.01
    apples_agent-2_min: 330
    apples_agent-3_max: 285
    apples_agent-3_mean: 248.79
    apples_agent-3_min: 173
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 429
    apples_agent-5_mean: 385.76
    apples_agent-5_min: 321
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 447.68
    cleaning_beam_agent-0_min: 405
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 16.14
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 491.04
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-28-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1922.0
  episode_reward_mean: 1816.14
  episode_reward_min: 1723.0
  episodes_this_iter: 96
  episodes_total: 98400
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11285.409
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3588462769985199
        entropy_coeff: 0.0017600000137463212
        kl: 0.000946307263802737
        model: {}
        policy_loss: -0.001186908921226859
        total_loss: 0.0014691045507788658
        vf_explained_var: 0.040220677852630615
        vf_loss: 32.875858306884766
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20996077358722687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009001549915410578
        model: {}
        policy_loss: -0.001997410086914897
        total_loss: 0.000636193435639143
        vf_explained_var: 0.12344574928283691
        vf_loss: 30.031356811523438
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2454768717288971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008211724343709648
        model: {}
        policy_loss: -0.0016859201714396477
        total_loss: 0.006591978017240763
        vf_explained_var: 0.020488440990447998
        vf_loss: 87.09937286376953
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4507763087749481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011321010533720255
        model: {}
        policy_loss: -0.0015044646570459008
        total_loss: 0.005853048525750637
        vf_explained_var: 0.08257453143596649
        vf_loss: 81.50875854492188
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9351758360862732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023988052271306515
        model: {}
        policy_loss: -0.001992722973227501
        total_loss: -0.0006477546412497759
        vf_explained_var: 0.05659162998199463
        vf_loss: 29.908756256103516
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25387343764305115
        entropy_coeff: 0.0017600000137463212
        kl: 0.000634720316156745
        model: {}
        policy_loss: -0.0015984105411916971
        total_loss: 0.0008211089298129082
        vf_explained_var: 0.09000125527381897
        vf_loss: 28.66337013244629
    load_time_ms: 14836.703
    num_steps_sampled: 98400000
    num_steps_trained: 98400000
    sample_time_ms: 97749.896
    update_time_ms: 13.371
  iterations_since_restore: 65
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.308620689655175
    ram_util_percent: 13.02126436781609
  pid: 12895
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 461.0
    agent-3: 461.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 251.085
    agent-1: 251.085
    agent-2: 417.535
    agent-3: 417.535
    agent-4: 239.45
    agent-5: 239.45
  policy_reward_min:
    agent-0: 164.0
    agent-1: 164.0
    agent-2: 377.0
    agent-3: 377.0
    agent-4: 213.5
    agent-5: 213.5
  sampler_perf:
    mean_env_wait_ms: 25.715490567632983
    mean_inference_ms: 13.134756934617021
    mean_processing_ms: 58.64726389260896
  time_since_restore: 8623.161859989166
  time_this_iter_s: 122.35195970535278
  time_total_s: 131104.18456840515
  timestamp: 1637645296
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 98400000
  training_iteration: 1025
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1025 |           131104 | 98400000 |  1816.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 31.35
    apples_agent-1_min: 16
    apples_agent-2_max: 409
    apples_agent-2_mean: 362.55
    apples_agent-2_min: 269
    apples_agent-3_max: 301
    apples_agent-3_mean: 241.97
    apples_agent-3_min: 175
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 378.46
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 445.69
    cleaning_beam_agent-0_min: 391
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.89
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 17.65
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 494.63
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-30-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1909.0
  episode_reward_mean: 1798.81
  episode_reward_min: 1108.0
  episodes_this_iter: 96
  episodes_total: 98496
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11278.107
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.359578013420105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014595761895179749
        model: {}
        policy_loss: -0.0012038776185363531
        total_loss: 0.0013938855845481157
        vf_explained_var: 0.04487164318561554
        vf_loss: 32.306190490722656
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2058611810207367
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011411277810111642
        model: {}
        policy_loss: -0.0016338364221155643
        total_loss: 0.0010849094251170754
        vf_explained_var: 0.08764089643955231
        vf_loss: 30.81061363220215
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24689437448978424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010278202826157212
        model: {}
        policy_loss: -0.0015533408150076866
        total_loss: 0.007046050857752562
        vf_explained_var: 0.017724722623825073
        vf_loss: 90.33924865722656
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4678139388561249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009591005509719253
        model: {}
        policy_loss: -0.0014302851632237434
        total_loss: 0.006213185843080282
        vf_explained_var: 0.07970604300498962
        vf_loss: 84.6682357788086
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9313976764678955
        entropy_coeff: 0.0017600000137463212
        kl: 0.001551336608827114
        model: {}
        policy_loss: -0.0018475670367479324
        total_loss: -0.0002708500251173973
        vf_explained_var: 0.05407814681529999
        vf_loss: 32.159759521484375
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25054967403411865
        entropy_coeff: 0.0017600000137463212
        kl: 0.001639322261326015
        model: {}
        policy_loss: -0.0018113006372004747
        total_loss: 0.0007319867145270109
        vf_explained_var: 0.12625698745250702
        vf_loss: 29.84254264831543
    load_time_ms: 14846.353
    num_steps_sampled: 98496000
    num_steps_trained: 98496000
    sample_time_ms: 97839.822
    update_time_ms: 13.619
  iterations_since_restore: 66
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.377777777777776
    ram_util_percent: 12.986666666666666
  pid: 12895
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 456.0
    agent-3: 456.0
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 251.095
    agent-1: 251.095
    agent-2: 413.45
    agent-3: 413.45
    agent-4: 234.86
    agent-5: 234.86
  policy_reward_min:
    agent-0: 203.0
    agent-1: 203.0
    agent-2: 321.0
    agent-3: 321.0
    agent-4: 30.0
    agent-5: 30.0
  sampler_perf:
    mean_env_wait_ms: 25.704465677013076
    mean_inference_ms: 13.133331866081344
    mean_processing_ms: 58.62376214380314
  time_since_restore: 8749.321283340454
  time_this_iter_s: 126.15942335128784
  time_total_s: 131230.34399175644
  timestamp: 1637645423
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 98496000
  training_iteration: 1026
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1026 |           131230 | 98496000 |  1798.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 30.59
    apples_agent-1_min: 13
    apples_agent-2_max: 402
    apples_agent-2_mean: 363.4
    apples_agent-2_min: 304
    apples_agent-3_max: 281
    apples_agent-3_mean: 246.12
    apples_agent-3_min: 197
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 424
    apples_agent-5_mean: 381.84
    apples_agent-5_min: 294
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 445.6
    cleaning_beam_agent-0_min: 400
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 0.9
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 16.07
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 506.91
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-32-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1933.0
  episode_reward_mean: 1814.05
  episode_reward_min: 1505.0
  episodes_this_iter: 96
  episodes_total: 98592
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11240.048
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36016708612442017
        entropy_coeff: 0.0017600000137463212
        kl: 0.001316678710281849
        model: {}
        policy_loss: -0.0013705692254006863
        total_loss: 0.001211351715028286
        vf_explained_var: 0.039733171463012695
        vf_loss: 32.15815734863281
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20766732096672058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009682795498520136
        model: {}
        policy_loss: -0.0015212181024253368
        total_loss: 0.0011197011917829514
        vf_explained_var: 0.10251319408416748
        vf_loss: 30.064117431640625
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24444475769996643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009795579826459289
        model: {}
        policy_loss: -0.001688447780907154
        total_loss: 0.006639048457145691
        vf_explained_var: 0.02475142478942871
        vf_loss: 87.57720947265625
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4625537395477295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008478971431031823
        model: {}
        policy_loss: -0.0014917325461283326
        total_loss: 0.006052947603166103
        vf_explained_var: 0.06910255551338196
        vf_loss: 83.58775329589844
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.946672260761261
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033872597850859165
        model: {}
        policy_loss: -0.002452813321724534
        total_loss: -0.0013103692326694727
        vf_explained_var: 0.05824217200279236
        vf_loss: 28.085920333862305
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2553163766860962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007954097236506641
        model: {}
        policy_loss: -0.0016985294641926885
        total_loss: 0.0005465708673000336
        vf_explained_var: 0.0943114310503006
        vf_loss: 26.94455337524414
    load_time_ms: 14847.214
    num_steps_sampled: 98592000
    num_steps_trained: 98592000
    sample_time_ms: 97453.237
    update_time_ms: 13.528
  iterations_since_restore: 67
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.04228571428572
    ram_util_percent: 13.012571428571427
  pid: 12895
  policy_reward_max:
    agent-0: 276.5
    agent-1: 276.5
    agent-2: 450.0
    agent-3: 450.0
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 251.13
    agent-1: 251.13
    agent-2: 417.6
    agent-3: 417.6
    agent-4: 238.295
    agent-5: 238.295
  policy_reward_min:
    agent-0: 206.0
    agent-1: 206.0
    agent-2: 350.5
    agent-3: 350.5
    agent-4: 196.0
    agent-5: 196.0
  sampler_perf:
    mean_env_wait_ms: 25.69478653283224
    mean_inference_ms: 13.129894098170318
    mean_processing_ms: 58.58532659636289
  time_since_restore: 8871.536724567413
  time_this_iter_s: 122.21544122695923
  time_total_s: 131352.5594329834
  timestamp: 1637645545
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 98592000
  training_iteration: 1027
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1027 |           131353 | 98592000 |  1814.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 2
    apples_agent-0_mean: 0.03
    apples_agent-0_min: 0
    apples_agent-1_max: 61
    apples_agent-1_mean: 30.27
    apples_agent-1_min: 17
    apples_agent-2_max: 427
    apples_agent-2_mean: 359.77
    apples_agent-2_min: 250
    apples_agent-3_max: 288
    apples_agent-3_mean: 245.87
    apples_agent-3_min: 140
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 428
    apples_agent-5_mean: 380.5
    apples_agent-5_min: 240
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 443.82
    cleaning_beam_agent-0_min: 411
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.87
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.2
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 15.4
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 490.83
    cleaning_beam_agent-4_min: 443
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 2.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-34-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1919.0
  episode_reward_mean: 1797.86
  episode_reward_min: 1159.0
  episodes_this_iter: 96
  episodes_total: 98688
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11238.709
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36403924226760864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012908438220620155
        model: {}
        policy_loss: -0.0011550190392881632
        total_loss: 0.0013575684279203415
        vf_explained_var: 0.05715407431125641
        vf_loss: 31.5329647064209
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2114424854516983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005682621267624199
        model: {}
        policy_loss: -0.001761449035257101
        total_loss: 0.0008763894438743591
        vf_explained_var: 0.10482245683670044
        vf_loss: 30.099773406982422
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24436697363853455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012781561817973852
        model: {}
        policy_loss: -0.0019788285717368126
        total_loss: 0.0065837702713906765
        vf_explained_var: 0.04608629643917084
        vf_loss: 89.92688751220703
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46467915177345276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010729720816016197
        model: {}
        policy_loss: -0.0013411669060587883
        total_loss: 0.006553788669407368
        vf_explained_var: 0.07446689903736115
        vf_loss: 87.12789916992188
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9431915283203125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017840577056631446
        model: {}
        policy_loss: -0.002009815536439419
        total_loss: -0.000602448359131813
        vf_explained_var: 0.05068229138851166
        vf_loss: 30.673843383789062
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2527655363082886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008790905121713877
        model: {}
        policy_loss: -0.001733461394906044
        total_loss: 0.0006902528693899512
        vf_explained_var: 0.11049085855484009
        vf_loss: 28.68579864501953
    load_time_ms: 14855.281
    num_steps_sampled: 98688000
    num_steps_trained: 98688000
    sample_time_ms: 97007.861
    update_time_ms: 13.539
  iterations_since_restore: 68
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.36242774566474
    ram_util_percent: 13.036416184971094
  pid: 12895
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 454.0
    agent-3: 454.0
    agent-4: 266.5
    agent-5: 266.5
  policy_reward_mean:
    agent-0: 248.66
    agent-1: 248.66
    agent-2: 413.565
    agent-3: 413.565
    agent-4: 236.705
    agent-5: 236.705
  policy_reward_min:
    agent-0: 156.0
    agent-1: 156.0
    agent-2: 264.5
    agent-3: 264.5
    agent-4: 159.0
    agent-5: 159.0
  sampler_perf:
    mean_env_wait_ms: 25.683707518574565
    mean_inference_ms: 13.127252812102103
    mean_processing_ms: 58.54589652700646
  time_since_restore: 8993.292536497116
  time_this_iter_s: 121.75581192970276
  time_total_s: 131474.3152449131
  timestamp: 1637645667
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 98688000
  training_iteration: 1028
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1028 |           131474 | 98688000 |  1797.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 29.39
    apples_agent-1_min: 7
    apples_agent-2_max: 415
    apples_agent-2_mean: 362.05
    apples_agent-2_min: 91
    apples_agent-3_max: 289
    apples_agent-3_mean: 245.27
    apples_agent-3_min: 67
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 447
    apples_agent-5_mean: 383.21
    apples_agent-5_min: 86
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 442.22
    cleaning_beam_agent-0_min: 409
    cleaning_beam_agent-1_max: 13
    cleaning_beam_agent-1_mean: 1.03
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 1.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 15.01
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 496.52
    cleaning_beam_agent-4_min: 434
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-36-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1923.0
  episode_reward_mean: 1804.6
  episode_reward_min: 496.0
  episodes_this_iter: 96
  episodes_total: 98784
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11238.651
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35973185300827026
        entropy_coeff: 0.0017600000137463212
        kl: 0.001032258733175695
        model: {}
        policy_loss: -0.0012793457135558128
        total_loss: 0.0014973777579143643
        vf_explained_var: 0.07540397346019745
        vf_loss: 34.09849548339844
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21038958430290222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006346003501676023
        model: {}
        policy_loss: -0.0016159536316990852
        total_loss: 0.0012585753574967384
        vf_explained_var: 0.12176156044006348
        vf_loss: 32.448158264160156
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24451413750648499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011255122954025865
        model: {}
        policy_loss: -0.0019283904694020748
        total_loss: 0.006693060044199228
        vf_explained_var: 0.07702994346618652
        vf_loss: 90.51795959472656
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4682835042476654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013910349225625396
        model: {}
        policy_loss: -0.0017259488813579082
        total_loss: 0.006658969912678003
        vf_explained_var: 0.06000819802284241
        vf_loss: 92.09098052978516
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9414748549461365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012731862952932715
        model: {}
        policy_loss: -0.001910218270495534
        total_loss: -0.00033574667759239674
        vf_explained_var: 0.054248347878456116
        vf_loss: 32.314666748046875
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2511613368988037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008737718453630805
        model: {}
        policy_loss: -0.0018320796079933643
        total_loss: 0.000762405339628458
        vf_explained_var: 0.11243630945682526
        vf_loss: 30.365270614624023
    load_time_ms: 14855.415
    num_steps_sampled: 98784000
    num_steps_trained: 98784000
    sample_time_ms: 97283.713
    update_time_ms: 13.398
  iterations_since_restore: 69
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.02513966480447
    ram_util_percent: 13.027374301675975
  pid: 12895
  policy_reward_max:
    agent-0: 270.0
    agent-1: 270.0
    agent-2: 464.0
    agent-3: 464.0
    agent-4: 273.5
    agent-5: 273.5
  policy_reward_mean:
    agent-0: 249.36
    agent-1: 249.36
    agent-2: 415.075
    agent-3: 415.075
    agent-4: 237.865
    agent-5: 237.865
  policy_reward_min:
    agent-0: 72.0
    agent-1: 72.0
    agent-2: 113.5
    agent-3: 113.5
    agent-4: 62.5
    agent-5: 62.5
  sampler_perf:
    mean_env_wait_ms: 25.67729434587976
    mean_inference_ms: 13.125710025138568
    mean_processing_ms: 58.5223331250366
  time_since_restore: 9118.553341150284
  time_this_iter_s: 125.26080465316772
  time_total_s: 131599.57604956627
  timestamp: 1637645792
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 98784000
  training_iteration: 1029
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1029 |           131600 | 98784000 |   1804.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 29.84
    apples_agent-1_min: 14
    apples_agent-2_max: 408
    apples_agent-2_mean: 363.42
    apples_agent-2_min: 311
    apples_agent-3_max: 287
    apples_agent-3_mean: 247.3
    apples_agent-3_min: 184
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 436
    apples_agent-5_mean: 381.96
    apples_agent-5_min: 338
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 439.24
    cleaning_beam_agent-0_min: 407
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.85
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 1.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 16.32
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 501.7
    cleaning_beam_agent-4_min: 438
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-38-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1930.0
  episode_reward_mean: 1810.6
  episode_reward_min: 1576.0
  episodes_this_iter: 96
  episodes_total: 98880
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11236.419
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3533424437046051
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122652028221637
        model: {}
        policy_loss: -0.0009431401267647743
        total_loss: 0.001586864236742258
        vf_explained_var: 0.03568162024021149
        vf_loss: 31.518856048583984
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20868128538131714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013927504187449813
        model: {}
        policy_loss: -0.0017517975065857172
        total_loss: 0.0009087291546165943
        vf_explained_var: 0.07627300918102264
        vf_loss: 30.27804946899414
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24177028238773346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008374913013540208
        model: {}
        policy_loss: -0.0015703924000263214
        total_loss: 0.006675627548247576
        vf_explained_var: 0.027225494384765625
        vf_loss: 86.71537017822266
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4704870581626892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018602123018354177
        model: {}
        policy_loss: -0.001535047311335802
        total_loss: 0.0060569182969629765
        vf_explained_var: 0.054672807455062866
        vf_loss: 84.20022583007812
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9289309978485107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013738243142142892
        model: {}
        policy_loss: -0.002051662653684616
        total_loss: -0.0008955984376370907
        vf_explained_var: 0.05979049205780029
        vf_loss: 27.90983009338379
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24893836677074432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005095714586786926
        model: {}
        policy_loss: -0.0014458169462159276
        total_loss: 0.000867421505972743
        vf_explained_var: 0.07265777885913849
        vf_loss: 27.513715744018555
    load_time_ms: 14865.008
    num_steps_sampled: 98880000
    num_steps_trained: 98880000
    sample_time_ms: 97307.025
    update_time_ms: 13.416
  iterations_since_restore: 70
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.999441340782127
    ram_util_percent: 13.031284916201114
  pid: 12895
  policy_reward_max:
    agent-0: 280.0
    agent-1: 280.0
    agent-2: 455.5
    agent-3: 455.5
    agent-4: 264.5
    agent-5: 264.5
  policy_reward_mean:
    agent-0: 252.585
    agent-1: 252.585
    agent-2: 416.34
    agent-3: 416.34
    agent-4: 236.375
    agent-5: 236.375
  policy_reward_min:
    agent-0: 215.5
    agent-1: 215.5
    agent-2: 345.0
    agent-3: 345.0
    agent-4: 214.0
    agent-5: 214.0
  sampler_perf:
    mean_env_wait_ms: 25.66945205985366
    mean_inference_ms: 13.122479906104568
    mean_processing_ms: 58.50860977070112
  time_since_restore: 9244.40553689003
  time_this_iter_s: 125.8521957397461
  time_total_s: 131725.42824530602
  timestamp: 1637645918
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 98880000
  training_iteration: 1030
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1030 |           131725 | 98880000 |   1810.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 50
    apples_agent-1_mean: 29.81
    apples_agent-1_min: 12
    apples_agent-2_max: 420
    apples_agent-2_mean: 364.03
    apples_agent-2_min: 304
    apples_agent-3_max: 292
    apples_agent-3_mean: 243.8
    apples_agent-3_min: 199
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 439
    apples_agent-5_mean: 385.84
    apples_agent-5_min: 325
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 433.16
    cleaning_beam_agent-0_min: 402
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 0.94
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 1.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 14.3
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 510.88
    cleaning_beam_agent-4_min: 447
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-40-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1904.0
  episode_reward_mean: 1808.23
  episode_reward_min: 1566.0
  episodes_this_iter: 96
  episodes_total: 98976
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11240.506
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3581351935863495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019515957683324814
        model: {}
        policy_loss: -0.0012319025117903948
        total_loss: 0.0012892980594187975
        vf_explained_var: 0.04578369855880737
        vf_loss: 31.51520538330078
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20985591411590576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010801809839904308
        model: {}
        policy_loss: -0.0017224838957190514
        total_loss: 0.000899268314242363
        vf_explained_var: 0.09554114937782288
        vf_loss: 29.910985946655273
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2449215054512024
        entropy_coeff: 0.0017600000137463212
        kl: 0.000496478343848139
        model: {}
        policy_loss: -0.0017367787659168243
        total_loss: 0.006871900521218777
        vf_explained_var: 0.02885572612285614
        vf_loss: 90.39741516113281
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46860289573669434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018816892988979816
        model: {}
        policy_loss: -0.0017877789214253426
        total_loss: 0.006184122525155544
        vf_explained_var: 0.05464912950992584
        vf_loss: 87.9664306640625
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9228979349136353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013066137908026576
        model: {}
        policy_loss: -0.0017681431490927935
        total_loss: -0.00048651290126144886
        vf_explained_var: 0.06388850510120392
        vf_loss: 29.059276580810547
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24550892412662506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007750146323814988
        model: {}
        policy_loss: -0.0014064358547329903
        total_loss: 0.0009347507730126381
        vf_explained_var: 0.10296222567558289
        vf_loss: 27.732847213745117
    load_time_ms: 14841.583
    num_steps_sampled: 98976000
    num_steps_trained: 98976000
    sample_time_ms: 97691.338
    update_time_ms: 13.411
  iterations_since_restore: 71
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.596174863387976
    ram_util_percent: 13.010928961748629
  pid: 12895
  policy_reward_max:
    agent-0: 276.0
    agent-1: 276.0
    agent-2: 446.5
    agent-3: 446.5
    agent-4: 263.0
    agent-5: 263.0
  policy_reward_mean:
    agent-0: 250.47
    agent-1: 250.47
    agent-2: 414.905
    agent-3: 414.905
    agent-4: 238.74
    agent-5: 238.74
  policy_reward_min:
    agent-0: 216.5
    agent-1: 216.5
    agent-2: 350.0
    agent-3: 350.0
    agent-4: 211.5
    agent-5: 211.5
  sampler_perf:
    mean_env_wait_ms: 25.662577185858275
    mean_inference_ms: 13.119537399638235
    mean_processing_ms: 58.489722664395394
  time_since_restore: 9370.422370672226
  time_this_iter_s: 126.01683378219604
  time_total_s: 131851.4450790882
  timestamp: 1637646046
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 98976000
  training_iteration: 1031
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1031 |           131851 | 98976000 |  1808.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 59
    apples_agent-1_mean: 30.81
    apples_agent-1_min: 18
    apples_agent-2_max: 424
    apples_agent-2_mean: 366.53
    apples_agent-2_min: 309
    apples_agent-3_max: 290
    apples_agent-3_mean: 246.63
    apples_agent-3_min: 195
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 386.2
    apples_agent-5_min: 307
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 435.34
    cleaning_beam_agent-0_min: 392
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 0.98
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 1.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 14.94
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 516.18
    cleaning_beam_agent-4_min: 468
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.07
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-42-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1921.0
  episode_reward_mean: 1813.36
  episode_reward_min: 1623.0
  episodes_this_iter: 96
  episodes_total: 99072
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11238.242
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3604295253753662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016916384920477867
        model: {}
        policy_loss: -0.0010812673717737198
        total_loss: 0.0014188841450959444
        vf_explained_var: 0.048345908522605896
        vf_loss: 31.3450927734375
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20611688494682312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009604383958503604
        model: {}
        policy_loss: -0.001657223328948021
        total_loss: 0.0010295845568180084
        vf_explained_var: 0.07617372274398804
        vf_loss: 30.495723724365234
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2412588745355606
        entropy_coeff: 0.0017600000137463212
        kl: 0.001263800309970975
        model: {}
        policy_loss: -0.0017172959633171558
        total_loss: 0.006438693962991238
        vf_explained_var: 0.032088860869407654
        vf_loss: 85.80607604980469
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45479682087898254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011870708549395204
        model: {}
        policy_loss: -0.0015963654732331634
        total_loss: 0.005714990198612213
        vf_explained_var: 0.08291952311992645
        vf_loss: 81.11798095703125
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9069739580154419
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024513143580406904
        model: {}
        policy_loss: -0.002193479798734188
        total_loss: -0.0009176833555102348
        vf_explained_var: 0.05356501042842865
        vf_loss: 28.72073745727539
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24948228895664215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005429554730653763
        model: {}
        policy_loss: -0.0015264530666172504
        total_loss: 0.0007807137444615364
        vf_explained_var: 0.0919712632894516
        vf_loss: 27.462539672851562
    load_time_ms: 14836.018
    num_steps_sampled: 99072000
    num_steps_trained: 99072000
    sample_time_ms: 97993.383
    update_time_ms: 13.387
  iterations_since_restore: 72
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.7967032967033
    ram_util_percent: 13.025824175824175
  pid: 12895
  policy_reward_max:
    agent-0: 275.0
    agent-1: 275.0
    agent-2: 452.0
    agent-3: 452.0
    agent-4: 265.0
    agent-5: 265.0
  policy_reward_mean:
    agent-0: 250.94
    agent-1: 250.94
    agent-2: 417.735
    agent-3: 417.735
    agent-4: 238.005
    agent-5: 238.005
  policy_reward_min:
    agent-0: 222.5
    agent-1: 222.5
    agent-2: 372.0
    agent-3: 372.0
    agent-4: 192.5
    agent-5: 192.5
  sampler_perf:
    mean_env_wait_ms: 25.658168222420034
    mean_inference_ms: 13.117119171921884
    mean_processing_ms: 58.47942703045408
  time_since_restore: 9497.968615293503
  time_this_iter_s: 127.54624462127686
  time_total_s: 131978.9913237095
  timestamp: 1637646174
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 99072000
  training_iteration: 1032
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 24.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1032 |           131979 | 99072000 |  1813.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 30.35
    apples_agent-1_min: 1
    apples_agent-2_max: 410
    apples_agent-2_mean: 360.79
    apples_agent-2_min: 4
    apples_agent-3_max: 295
    apples_agent-3_mean: 245.13
    apples_agent-3_min: 4
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 379.73
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 425.55
    cleaning_beam_agent-0_min: 367
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 1.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 14.65
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 632
    cleaning_beam_agent-4_mean: 527.9
    cleaning_beam_agent-4_min: 467
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 3.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-44-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1941.0
  episode_reward_mean: 1795.0
  episode_reward_min: 44.0
  episodes_this_iter: 96
  episodes_total: 99168
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11214.405
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36700186133384705
        entropy_coeff: 0.0017600000137463212
        kl: 0.001259499927982688
        model: {}
        policy_loss: -0.0015235021710395813
        total_loss: 0.0012045446783304214
        vf_explained_var: 0.13064263761043549
        vf_loss: 33.739707946777344
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21221581101417542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010455813026055694
        model: {}
        policy_loss: -0.0019151289016008377
        total_loss: 0.001023104414343834
        vf_explained_var: 0.14676514267921448
        vf_loss: 33.11729431152344
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25189724564552307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008232875843532383
        model: {}
        policy_loss: -0.0020726218353956938
        total_loss: 0.00723896874114871
        vf_explained_var: 0.11751462519168854
        vf_loss: 97.54931640625
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4584534764289856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006945687928237021
        model: {}
        policy_loss: -0.001205946085974574
        total_loss: 0.00827057845890522
        vf_explained_var: 0.06844323873519897
        vf_loss: 102.83401489257812
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9164583086967468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012073456309735775
        model: {}
        policy_loss: -0.0016717277467250824
        total_loss: 0.00028439518064260483
        vf_explained_var: 0.005120784044265747
        vf_loss: 35.69091033935547
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26279646158218384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012574163265526295
        model: {}
        policy_loss: -0.0022657287772744894
        total_loss: 0.0003174446464981884
        vf_explained_var: 0.15043403208255768
        vf_loss: 30.456941604614258
    load_time_ms: 14819.995
    num_steps_sampled: 99168000
    num_steps_trained: 99168000
    sample_time_ms: 97984.213
    update_time_ms: 13.472
  iterations_since_restore: 73
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.221264367816094
    ram_util_percent: 13.110919540229883
  pid: 12895
  policy_reward_max:
    agent-0: 279.0
    agent-1: 279.0
    agent-2: 459.5
    agent-3: 459.5
    agent-4: 260.0
    agent-5: 260.0
  policy_reward_mean:
    agent-0: 249.645
    agent-1: 249.645
    agent-2: 413.075
    agent-3: 413.075
    agent-4: 234.78
    agent-5: 234.78
  policy_reward_min:
    agent-0: 6.5
    agent-1: 6.5
    agent-2: 9.5
    agent-3: 9.5
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 25.650432945884294
    mean_inference_ms: 13.114799319166405
    mean_processing_ms: 58.447238623218354
  time_since_restore: 9620.530853509903
  time_this_iter_s: 122.56223821640015
  time_total_s: 132101.5535619259
  timestamp: 1637646297
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 99168000
  training_iteration: 1033
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1033 |           132102 | 99168000 |     1795 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 28.57
    apples_agent-1_min: 16
    apples_agent-2_max: 434
    apples_agent-2_mean: 368.83
    apples_agent-2_min: 297
    apples_agent-3_max: 293
    apples_agent-3_mean: 245.18
    apples_agent-3_min: 193
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 432
    apples_agent-5_mean: 383.9
    apples_agent-5_min: 272
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 428.68
    cleaning_beam_agent-0_min: 392
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 0.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 1.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 15.08
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 517.57
    cleaning_beam_agent-4_min: 452
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-47-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1914.0
  episode_reward_mean: 1803.03
  episode_reward_min: 1492.0
  episodes_this_iter: 96
  episodes_total: 99264
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11211.738
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3624277412891388
        entropy_coeff: 0.0017600000137463212
        kl: 0.001959102228283882
        model: {}
        policy_loss: -0.001184353488497436
        total_loss: 0.0013916692696511745
        vf_explained_var: 0.05127173662185669
        vf_loss: 32.13894271850586
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21166130900382996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006258061621338129
        model: {}
        policy_loss: -0.001678186352364719
        total_loss: 0.001007875776849687
        vf_explained_var: 0.09792622923851013
        vf_loss: 30.58584976196289
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24256455898284912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006961402832530439
        model: {}
        policy_loss: -0.0017234706319868565
        total_loss: 0.006980095058679581
        vf_explained_var: 0.040816932916641235
        vf_loss: 91.3048095703125
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46283695101737976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013186613796278834
        model: {}
        policy_loss: -0.0015751291066408157
        total_loss: 0.00653822161257267
        vf_explained_var: 0.06121569871902466
        vf_loss: 89.27944946289062
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.93111652135849
        entropy_coeff: 0.0017600000137463212
        kl: 0.002755753928795457
        model: {}
        policy_loss: -0.0021955580450594425
        total_loss: -0.0009278184152208269
        vf_explained_var: 0.07028615474700928
        vf_loss: 29.06505012512207
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25088566541671753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008354709716513753
        model: {}
        policy_loss: -0.0016293162479996681
        total_loss: 0.0006940499879419804
        vf_explained_var: 0.11553269624710083
        vf_loss: 27.649263381958008
    load_time_ms: 14829.174
    num_steps_sampled: 99264000
    num_steps_trained: 99264000
    sample_time_ms: 98351.703
    update_time_ms: 13.221
  iterations_since_restore: 74
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.941340782122904
    ram_util_percent: 13.02290502793296
  pid: 12895
  policy_reward_max:
    agent-0: 286.0
    agent-1: 286.0
    agent-2: 455.5
    agent-3: 455.5
    agent-4: 267.0
    agent-5: 267.0
  policy_reward_mean:
    agent-0: 248.105
    agent-1: 248.105
    agent-2: 416.495
    agent-3: 416.495
    agent-4: 236.915
    agent-5: 236.915
  policy_reward_min:
    agent-0: 200.5
    agent-1: 200.5
    agent-2: 353.0
    agent-3: 353.0
    agent-4: 182.0
    agent-5: 182.0
  sampler_perf:
    mean_env_wait_ms: 25.64170684802298
    mean_inference_ms: 13.11218745639743
    mean_processing_ms: 58.430262739750845
  time_since_restore: 9745.696775197983
  time_this_iter_s: 125.16592168807983
  time_total_s: 132226.71948361397
  timestamp: 1637646422
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 99264000
  training_iteration: 1034
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1034 |           132227 | 99264000 |  1803.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 29.96
    apples_agent-1_min: 18
    apples_agent-2_max: 417
    apples_agent-2_mean: 362.61
    apples_agent-2_min: 275
    apples_agent-3_max: 284
    apples_agent-3_mean: 247.43
    apples_agent-3_min: 175
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 437
    apples_agent-5_mean: 385.39
    apples_agent-5_min: 322
    cleaning_beam_agent-0_max: 462
    cleaning_beam_agent-0_mean: 436.2
    cleaning_beam_agent-0_min: 391
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 1.23
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 0.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 16.46
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 565
    cleaning_beam_agent-4_mean: 505.75
    cleaning_beam_agent-4_min: 455
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-49-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1942.0
  episode_reward_mean: 1811.32
  episode_reward_min: 1454.0
  episodes_this_iter: 96
  episodes_total: 99360
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11204.103
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36605191230773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008921943372115493
        model: {}
        policy_loss: -0.0010632108896970749
        total_loss: 0.0014445912092924118
        vf_explained_var: 0.058987587690353394
        vf_loss: 31.520511627197266
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20858556032180786
        entropy_coeff: 0.0017600000137463212
        kl: 0.001274456619285047
        model: {}
        policy_loss: -0.0018252874724566936
        total_loss: 0.000820223125629127
        vf_explained_var: 0.1025342047214508
        vf_loss: 30.1262149810791
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24447602033615112
        entropy_coeff: 0.0017600000137463212
        kl: 0.001051400788128376
        model: {}
        policy_loss: -0.0015924761537462473
        total_loss: 0.00658033974468708
        vf_explained_var: 0.03685668110847473
        vf_loss: 86.03096008300781
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4486638903617859
        entropy_coeff: 0.0017600000137463212
        kl: 0.00184411252848804
        model: {}
        policy_loss: -0.0014229279477149248
        total_loss: 0.005981553345918655
        vf_explained_var: 0.08285479247570038
        vf_loss: 81.94129180908203
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.929572582244873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018176671583205462
        model: {}
        policy_loss: -0.0018893997184932232
        total_loss: -0.0006145655643194914
        vf_explained_var: 0.05308283865451813
        vf_loss: 29.108787536621094
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2485903948545456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006570045370608568
        model: {}
        policy_loss: -0.0016154879704117775
        total_loss: 0.0007546023698523641
        vf_explained_var: 0.08605353534221649
        vf_loss: 28.07609748840332
    load_time_ms: 14825.32
    num_steps_sampled: 99360000
    num_steps_trained: 99360000
    sample_time_ms: 98601.685
    update_time_ms: 13.248
  iterations_since_restore: 75
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.098870056497177
    ram_util_percent: 13.112429378531074
  pid: 12895
  policy_reward_max:
    agent-0: 273.0
    agent-1: 273.0
    agent-2: 455.5
    agent-3: 455.5
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 250.075
    agent-1: 250.075
    agent-2: 415.91
    agent-3: 415.91
    agent-4: 239.675
    agent-5: 239.675
  policy_reward_min:
    agent-0: 201.0
    agent-1: 201.0
    agent-2: 325.5
    agent-3: 325.5
    agent-4: 200.5
    agent-5: 200.5
  sampler_perf:
    mean_env_wait_ms: 25.635053303628133
    mean_inference_ms: 13.109410089261791
    mean_processing_ms: 58.41397326205284
  time_since_restore: 9870.440294504166
  time_this_iter_s: 124.74351930618286
  time_total_s: 132351.46300292015
  timestamp: 1637646547
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 99360000
  training_iteration: 1035
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1035 |           132351 | 99360000 |  1811.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 30.36
    apples_agent-1_min: 19
    apples_agent-2_max: 399
    apples_agent-2_mean: 362.64
    apples_agent-2_min: 312
    apples_agent-3_max: 293
    apples_agent-3_mean: 245.72
    apples_agent-3_min: 184
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.02
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 385.48
    apples_agent-5_min: 333
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 436.19
    cleaning_beam_agent-0_min: 410
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 0.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 1.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 14.9
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 489.37
    cleaning_beam_agent-4_min: 440
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-51-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1934.0
  episode_reward_mean: 1816.93
  episode_reward_min: 1643.0
  episodes_this_iter: 96
  episodes_total: 99456
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11199.611
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36216628551483154
        entropy_coeff: 0.0017600000137463212
        kl: 0.001715178950689733
        model: {}
        policy_loss: -0.0014555334346368909
        total_loss: 0.0011653541587293148
        vf_explained_var: 0.036219522356987
        vf_loss: 32.582984924316406
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20552265644073486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010133297182619572
        model: {}
        policy_loss: -0.001444672467187047
        total_loss: 0.0012753265909850597
        vf_explained_var: 0.08840326964855194
        vf_loss: 30.817184448242188
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24447596073150635
        entropy_coeff: 0.0017600000137463212
        kl: 0.001141829532571137
        model: {}
        policy_loss: -0.001807989552617073
        total_loss: 0.006526274606585503
        vf_explained_var: 0.035264551639556885
        vf_loss: 87.64546203613281
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46056944131851196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013968624407425523
        model: {}
        policy_loss: -0.001810738816857338
        total_loss: 0.0059626162983477116
        vf_explained_var: 0.054412633180618286
        vf_loss: 85.83953857421875
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.932671070098877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016921269707381725
        model: {}
        policy_loss: -0.0019912535790354013
        total_loss: -0.0007102913223206997
        vf_explained_var: 0.06037965416908264
        vf_loss: 29.224655151367188
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24091380834579468
        entropy_coeff: 0.0017600000137463212
        kl: 0.001167484326288104
        model: {}
        policy_loss: -0.0017252760007977486
        total_loss: 0.000705227255821228
        vf_explained_var: 0.08045923709869385
        vf_loss: 28.54509925842285
    load_time_ms: 14804.406
    num_steps_sampled: 99456000
    num_steps_trained: 99456000
    sample_time_ms: 98223.393
    update_time_ms: 13.668
  iterations_since_restore: 76
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.3632183908046
    ram_util_percent: 13.114942528735632
  pid: 12895
  policy_reward_max:
    agent-0: 280.5
    agent-1: 280.5
    agent-2: 462.5
    agent-3: 462.5
    agent-4: 274.0
    agent-5: 274.0
  policy_reward_mean:
    agent-0: 251.655
    agent-1: 251.655
    agent-2: 418.295
    agent-3: 418.295
    agent-4: 238.515
    agent-5: 238.515
  policy_reward_min:
    agent-0: 224.0
    agent-1: 224.0
    agent-2: 370.0
    agent-3: 370.0
    agent-4: 202.5
    agent-5: 202.5
  sampler_perf:
    mean_env_wait_ms: 25.627312194903507
    mean_inference_ms: 13.107694945587
    mean_processing_ms: 58.38539364374179
  time_since_restore: 9992.563343524933
  time_this_iter_s: 122.12304902076721
  time_total_s: 132473.58605194092
  timestamp: 1637646669
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 99456000
  training_iteration: 1036
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1036 |           132474 | 99456000 |  1816.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 62
    apples_agent-1_mean: 31.29
    apples_agent-1_min: 16
    apples_agent-2_max: 411
    apples_agent-2_mean: 365.07
    apples_agent-2_min: 303
    apples_agent-3_max: 308
    apples_agent-3_mean: 248.71
    apples_agent-3_min: 194
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 457
    apples_agent-5_mean: 382.86
    apples_agent-5_min: 332
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 425.81
    cleaning_beam_agent-0_min: 393
    cleaning_beam_agent-1_max: 7
    cleaning_beam_agent-1_mean: 1.12
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 1.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 16.99
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 496.0
    cleaning_beam_agent-4_min: 432
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-53-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1948.0
  episode_reward_mean: 1803.52
  episode_reward_min: 1588.0
  episodes_this_iter: 96
  episodes_total: 99552
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11200.164
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35775989294052124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016481855418533087
        model: {}
        policy_loss: -0.0013666339218616486
        total_loss: 0.001320498064160347
        vf_explained_var: 0.05296257138252258
        vf_loss: 33.16790771484375
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20841895043849945
        entropy_coeff: 0.0017600000137463212
        kl: 0.000602421467192471
        model: {}
        policy_loss: -0.001557131763547659
        total_loss: 0.0012100094463676214
        vf_explained_var: 0.10523369908332825
        vf_loss: 31.339622497558594
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24550163745880127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009502974571660161
        model: {}
        policy_loss: -0.0017128237523138523
        total_loss: 0.006279333028942347
        vf_explained_var: 0.041974738240242004
        vf_loss: 84.24238586425781
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4641784727573395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011884276755154133
        model: {}
        policy_loss: -0.0013817492872476578
        total_loss: 0.005961287301033735
        vf_explained_var: 0.07171304523944855
        vf_loss: 81.59992980957031
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9306343197822571
        entropy_coeff: 0.0017600000137463212
        kl: 0.002028091810643673
        model: {}
        policy_loss: -0.0022771451622247696
        total_loss: -0.0010101504158228636
        vf_explained_var: 0.0512976348400116
        vf_loss: 29.04913330078125
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24265602231025696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007671128259971738
        model: {}
        policy_loss: -0.001530070905573666
        total_loss: 0.0008074246579781175
        vf_explained_var: 0.09557603299617767
        vf_loss: 27.645675659179688
    load_time_ms: 14795.369
    num_steps_sampled: 99552000
    num_steps_trained: 99552000
    sample_time_ms: 98321.001
    update_time_ms: 13.59
  iterations_since_restore: 77
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.12215909090909
    ram_util_percent: 13.107386363636364
  pid: 12895
  policy_reward_max:
    agent-0: 273.5
    agent-1: 273.5
    agent-2: 461.5
    agent-3: 461.5
    agent-4: 273.5
    agent-5: 273.5
  policy_reward_mean:
    agent-0: 248.67
    agent-1: 248.67
    agent-2: 415.275
    agent-3: 415.275
    agent-4: 237.815
    agent-5: 237.815
  policy_reward_min:
    agent-0: 196.0
    agent-1: 196.0
    agent-2: 365.0
    agent-3: 365.0
    agent-4: 203.0
    agent-5: 203.0
  sampler_perf:
    mean_env_wait_ms: 25.61967049016617
    mean_inference_ms: 13.105785070981824
    mean_processing_ms: 58.35964672514123
  time_since_restore: 10115.665955781937
  time_this_iter_s: 123.10261225700378
  time_total_s: 132596.68866419792
  timestamp: 1637646792
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 99552000
  training_iteration: 1037
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 23.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1037 |           132597 | 99552000 |  1803.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 0
    apples_agent-0_mean: 0.0
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 30.4
    apples_agent-1_min: 18
    apples_agent-2_max: 409
    apples_agent-2_mean: 364.56
    apples_agent-2_min: 279
    apples_agent-3_max: 302
    apples_agent-3_mean: 248.39
    apples_agent-3_min: 197
    apples_agent-4_max: 3
    apples_agent-4_mean: 0.05
    apples_agent-4_min: 0
    apples_agent-5_max: 446
    apples_agent-5_mean: 385.95
    apples_agent-5_min: 314
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 419.89
    cleaning_beam_agent-0_min: 396
    cleaning_beam_agent-1_max: 6
    cleaning_beam_agent-1_mean: 1.0
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 4
    cleaning_beam_agent-2_mean: 1.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 15.26
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 493.62
    cleaning_beam_agent-4_min: 405
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 2.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-55-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1922.0
  episode_reward_mean: 1814.84
  episode_reward_min: 1421.0
  episodes_this_iter: 96
  episodes_total: 99648
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11217.376
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3477531373500824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012611892307177186
        model: {}
        policy_loss: -0.0011220658197999
        total_loss: 0.0015204977244138718
        vf_explained_var: 0.037516385316848755
        vf_loss: 32.54612731933594
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20621357858181
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009260779479518533
        model: {}
        policy_loss: -0.0016454856377094984
        total_loss: 0.0010188466403633356
        vf_explained_var: 0.1058940440416336
        vf_loss: 30.272674560546875
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2451736032962799
        entropy_coeff: 0.0017600000137463212
        kl: 0.001052609528414905
        model: {}
        policy_loss: -0.001838812604546547
        total_loss: 0.0067151980474591255
        vf_explained_var: 0.03623214364051819
        vf_loss: 89.85517120361328
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46886783838272095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013599760131910443
        model: {}
        policy_loss: -0.0015381746925413609
        total_loss: 0.00621713837608695
        vf_explained_var: 0.08000804483890533
        vf_loss: 85.8051986694336
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9093465805053711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021613165736198425
        model: {}
        policy_loss: -0.0021868953481316566
        total_loss: -0.0009088236838579178
        vf_explained_var: 0.057596489787101746
        vf_loss: 28.785249710083008
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24481230974197388
        entropy_coeff: 0.0017600000137463212
        kl: 0.000983396079391241
        model: {}
        policy_loss: -0.001617704750970006
        total_loss: 0.0007214758079499006
        vf_explained_var: 0.09203286468982697
        vf_loss: 27.70049285888672
    load_time_ms: 14824.938
    num_steps_sampled: 99648000
    num_steps_trained: 99648000
    sample_time_ms: 98935.263
    update_time_ms: 13.561
  iterations_since_restore: 78
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.8672131147541
    ram_util_percent: 13.124590163934432
  pid: 12895
  policy_reward_max:
    agent-0: 278.0
    agent-1: 278.0
    agent-2: 450.0
    agent-3: 450.0
    agent-4: 267.5
    agent-5: 267.5
  policy_reward_mean:
    agent-0: 251.535
    agent-1: 251.535
    agent-2: 417.15
    agent-3: 417.15
    agent-4: 238.735
    agent-5: 238.735
  policy_reward_min:
    agent-0: 182.5
    agent-1: 182.5
    agent-2: 329.0
    agent-3: 329.0
    agent-4: 199.0
    agent-5: 199.0
  sampler_perf:
    mean_env_wait_ms: 25.618883884919242
    mean_inference_ms: 13.102862176467934
    mean_processing_ms: 58.34751193592452
  time_since_restore: 10244.04000878334
  time_this_iter_s: 128.3740530014038
  time_total_s: 132725.06271719933
  timestamp: 1637646921
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 99648000
  training_iteration: 1038
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 24.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1038 |           132725 | 99648000 |  1814.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 60
    apples_agent-1_mean: 31.56
    apples_agent-1_min: 18
    apples_agent-2_max: 407
    apples_agent-2_mean: 365.27
    apples_agent-2_min: 297
    apples_agent-3_max: 288
    apples_agent-3_mean: 249.58
    apples_agent-3_min: 164
    apples_agent-4_max: 2
    apples_agent-4_mean: 0.03
    apples_agent-4_min: 0
    apples_agent-5_max: 445
    apples_agent-5_mean: 382.68
    apples_agent-5_min: 326
    cleaning_beam_agent-0_max: 448
    cleaning_beam_agent-0_mean: 420.6
    cleaning_beam_agent-0_min: 377
    cleaning_beam_agent-1_max: 9
    cleaning_beam_agent-1_mean: 0.99
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 7
    cleaning_beam_agent-2_mean: 1.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 17.68
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 502.74
    cleaning_beam_agent-4_min: 435
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-57-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1945.0
  episode_reward_mean: 1813.54
  episode_reward_min: 1513.0
  episodes_this_iter: 96
  episodes_total: 99744
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11217.945
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3536308705806732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015476990956813097
        model: {}
        policy_loss: -0.0010591736063361168
        total_loss: 0.0016290538478642702
        vf_explained_var: 0.028039410710334778
        vf_loss: 33.10615539550781
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20416609942913055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010253202635794878
        model: {}
        policy_loss: -0.0016597914509475231
        total_loss: 0.0010631098411977291
        vf_explained_var: 0.09917221963405609
        vf_loss: 30.82233428955078
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24762728810310364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011020975653082132
        model: {}
        policy_loss: -0.0015075134579092264
        total_loss: 0.006769041530787945
        vf_explained_var: 0.03339222073554993
        vf_loss: 87.12379455566406
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4677029550075531
        entropy_coeff: 0.0017600000137463212
        kl: 0.001284841913729906
        model: {}
        policy_loss: -0.001605328405275941
        total_loss: 0.005726741626858711
        vf_explained_var: 0.09530745446681976
        vf_loss: 81.5522689819336
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8999307155609131
        entropy_coeff: 0.0017600000137463212
        kl: 0.0029994850046932697
        model: {}
        policy_loss: -0.0020723193883895874
        total_loss: -0.0007380391471087933
        vf_explained_var: 0.030036330223083496
        vf_loss: 29.181594848632812
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24917761981487274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008848445140756667
        model: {}
        policy_loss: -0.0016019279137253761
        total_loss: 0.0007052216678857803
        vf_explained_var: 0.08544540405273438
        vf_loss: 27.457000732421875
    load_time_ms: 14826.026
    num_steps_sampled: 99744000
    num_steps_trained: 99744000
    sample_time_ms: 98680.026
    update_time_ms: 13.783
  iterations_since_restore: 79
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.181714285714282
    ram_util_percent: 13.18971428571429
  pid: 12895
  policy_reward_max:
    agent-0: 278.5
    agent-1: 278.5
    agent-2: 455.0
    agent-3: 455.0
    agent-4: 259.5
    agent-5: 259.5
  policy_reward_mean:
    agent-0: 252.945
    agent-1: 252.945
    agent-2: 417.05
    agent-3: 417.05
    agent-4: 236.775
    agent-5: 236.775
  policy_reward_min:
    agent-0: 213.0
    agent-1: 213.0
    agent-2: 331.0
    agent-3: 331.0
    agent-4: 204.0
    agent-5: 204.0
  sampler_perf:
    mean_env_wait_ms: 25.60998141785904
    mean_inference_ms: 13.101173599378853
    mean_processing_ms: 58.31879930360779
  time_since_restore: 10366.766764640808
  time_this_iter_s: 122.72675585746765
  time_total_s: 132847.7894730568
  timestamp: 1637647044
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 99744000
  training_iteration: 1039
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1039 |           132848 | 99744000 |  1813.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 3
    apples_agent-0_mean: 0.06
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 31.45
    apples_agent-1_min: 2
    apples_agent-2_max: 415
    apples_agent-2_mean: 362.18
    apples_agent-2_min: 9
    apples_agent-3_max: 313
    apples_agent-3_mean: 249.83
    apples_agent-3_min: 6
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 434
    apples_agent-5_mean: 376.48
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 428.99
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 12
    cleaning_beam_agent-1_mean: 1.25
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 6
    cleaning_beam_agent-2_mean: 1.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 17.08
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 607
    cleaning_beam_agent-4_mean: 521.98
    cleaning_beam_agent-4_min: 424
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 2.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_00-59-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1923.0
  episode_reward_mean: 1796.42
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 99840
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11219.413
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3649566173553467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010365016059949994
        model: {}
        policy_loss: -0.001547160092741251
        total_loss: 0.0012255823239684105
        vf_explained_var: 0.1222812682390213
        vf_loss: 34.150665283203125
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21048223972320557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009991320548579097
        model: {}
        policy_loss: -0.0019207207951694727
        total_loss: 0.0009497981518507004
        vf_explained_var: 0.16703557968139648
        vf_loss: 32.409690856933594
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2542225122451782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008259927853941917
        model: {}
        policy_loss: -0.0020210761576890945
        total_loss: 0.006875778082758188
        vf_explained_var: 0.1159832775592804
        vf_loss: 93.44288635253906
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46352067589759827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007353870896622539
        model: {}
        policy_loss: -0.0016271239146590233
        total_loss: 0.0072716036811470985
        vf_explained_var: 0.08156664669513702
        vf_loss: 97.14523315429688
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8939879536628723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018793074414134026
        model: {}
        policy_loss: -0.0019973781891167164
        total_loss: -0.00018990918761119246
        vf_explained_var: 0.04206804931163788
        vf_loss: 33.80887222290039
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2686985731124878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014760324265807867
        model: {}
        policy_loss: -0.0025014912243932486
        total_loss: 0.0001417091116309166
        vf_explained_var: 0.11720827221870422
        vf_loss: 31.161096572875977
    load_time_ms: 14817.226
    num_steps_sampled: 99840000
    num_steps_trained: 99840000
    sample_time_ms: 98901.216
    update_time_ms: 13.7
  iterations_since_restore: 80
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.62185792349727
    ram_util_percent: 13.128415300546449
  pid: 12895
  policy_reward_max:
    agent-0: 282.5
    agent-1: 282.5
    agent-2: 452.0
    agent-3: 452.0
    agent-4: 268.5
    agent-5: 268.5
  policy_reward_mean:
    agent-0: 250.4
    agent-1: 250.4
    agent-2: 413.795
    agent-3: 413.795
    agent-4: 234.015
    agent-5: 234.015
  policy_reward_min:
    agent-0: 3.0
    agent-1: 3.0
    agent-2: 12.5
    agent-3: 12.5
    agent-4: 5.5
    agent-5: 5.5
  sampler_perf:
    mean_env_wait_ms: 25.602354573975173
    mean_inference_ms: 13.099151542230148
    mean_processing_ms: 58.311354521780224
  time_since_restore: 10494.753157138824
  time_this_iter_s: 127.98639249801636
  time_total_s: 132975.7758655548
  timestamp: 1637647172
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 99840000
  training_iteration: 1040
  trial_id: '00000'
  
[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
== Status ==
Memory usage on this node: 24.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1040 |           132976 | 99840000 |  1796.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.02
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 30.35
    apples_agent-1_min: 2
    apples_agent-2_max: 420
    apples_agent-2_mean: 363.93
    apples_agent-2_min: 9
    apples_agent-3_max: 293
    apples_agent-3_mean: 246.39
    apples_agent-3_min: 6
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 430
    apples_agent-5_mean: 380.73
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 431.62
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 8
    cleaning_beam_agent-1_mean: 1.11
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 1.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 19.9
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 515.83
    cleaning_beam_agent-4_min: 443
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 3.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_01-01-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1934.0
  episode_reward_mean: 1795.96
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 99936
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11217.743
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36481213569641113
        entropy_coeff: 0.0017600000137463212
        kl: 0.002549160737544298
        model: {}
        policy_loss: -0.0017272908007726073
        total_loss: 0.0010074670426547527
        vf_explained_var: 0.051294803619384766
        vf_loss: 33.768280029296875
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20576688647270203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009934266563504934
        model: {}
        policy_loss: -0.0019899755716323853
        total_loss: 0.0007798410952091217
        vf_explained_var: 0.11857098340988159
        vf_loss: 31.31971549987793
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2504834532737732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009477179264649749
        model: {}
        policy_loss: -0.0019574719481170177
        total_loss: 0.0072592138312757015
        vf_explained_var: 0.06517413258552551
        vf_loss: 96.57537078857422
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4686179757118225
        entropy_coeff: 0.0017600000137463212
        kl: 0.00072594138327986
        model: {}
        policy_loss: -0.0015067141503095627
        total_loss: 0.007369417231529951
        vf_explained_var: 0.06042391061782837
        vf_loss: 97.00901794433594
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8673322796821594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024390453472733498
        model: {}
        policy_loss: -0.0023240111768245697
        total_loss: -0.0006548515520989895
        vf_explained_var: 0.03819480538368225
        vf_loss: 31.956634521484375
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2547908127307892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009804593864828348
        model: {}
        policy_loss: -0.001779305748641491
        total_loss: 0.0007161186076700687
        vf_explained_var: 0.11049187183380127
        vf_loss: 29.43852996826172
    load_time_ms: 14836.201
    num_steps_sampled: 99936000
    num_steps_trained: 99936000
    sample_time_ms: 98713.009
    update_time_ms: 13.665
  iterations_since_restore: 81
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.241111111111113
    ram_util_percent: 13.166111111111114
  pid: 12895
  policy_reward_max:
    agent-0: 277.0
    agent-1: 277.0
    agent-2: 448.5
    agent-3: 448.5
    agent-4: 264.0
    agent-5: 264.0
  policy_reward_mean:
    agent-0: 249.94
    agent-1: 249.94
    agent-2: 412.405
    agent-3: 412.405
    agent-4: 235.635
    agent-5: 235.635
  policy_reward_min:
    agent-0: 3.0
    agent-1: 3.0
    agent-2: 12.5
    agent-3: 12.5
    agent-4: 5.5
    agent-5: 5.5
  sampler_perf:
    mean_env_wait_ms: 25.59595298904562
    mean_inference_ms: 13.099078861569343
    mean_processing_ms: 58.291278363541295
  time_since_restore: 10619.054864168167
  time_this_iter_s: 124.30170702934265
  time_total_s: 133100.07757258415
  timestamp: 1637647298
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 99936000
  training_iteration: 1041
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.31:12895 |   1041 |           133100 | 99936000 |  1795.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=12895)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa5db4a85c0> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 1
    apples_agent-0_mean: 0.01
    apples_agent-0_min: 0
    apples_agent-1_max: 48
    apples_agent-1_mean: 29.46
    apples_agent-1_min: 1
    apples_agent-2_max: 416
    apples_agent-2_mean: 360.3
    apples_agent-2_min: 15
    apples_agent-3_max: 292
    apples_agent-3_mean: 247.64
    apples_agent-3_min: 8
    apples_agent-4_max: 0
    apples_agent-4_mean: 0.0
    apples_agent-4_min: 0
    apples_agent-5_max: 427
    apples_agent-5_mean: 381.53
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 436.69
    cleaning_beam_agent-0_min: 380
    cleaning_beam_agent-1_max: 5
    cleaning_beam_agent-1_mean: 1.06
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 1.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 19.34
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 507.46
    cleaning_beam_agent-4_min: 429
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 3.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-23_01-03-44
  done: true
  episode_len_mean: 1000.0
  episode_reward_max: 1944.0
  episode_reward_mean: 1792.28
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 100032
  experiment_id: 41bd7d73f15d4d6483a8249e04d52da9
  experiment_tag: '0'
  hostname: gpu031
  info:
    grad_time_ms: 11216.212
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3736470937728882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013731117360293865
        model: {}
        policy_loss: -0.001320144161581993
        total_loss: 0.0014308781828731298
        vf_explained_var: 0.0877227634191513
        vf_loss: 34.08642578125
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20679250359535217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007660817936994135
        model: {}
        policy_loss: -0.0019575385376811028
        total_loss: 0.0009521464817225933
        vf_explained_var: 0.12589776515960693
        vf_loss: 32.736385345458984
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2524312138557434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008272118284367025
        model: {}
        policy_loss: -0.0019367709755897522
        total_loss: 0.007224997505545616
        vf_explained_var: 0.10438728332519531
        vf_loss: 96.06047821044922
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46180063486099243
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006205638055689633
        model: {}
        policy_loss: -0.0014521840494126081
        total_loss: 0.007838696241378784
        vf_explained_var: 0.0589304119348526
        vf_loss: 101.03646850585938
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8911091685295105
        entropy_coeff: 0.0017600000137463212
        kl: 0.003418863285332918
        model: {}
        policy_loss: -0.0020442139357328415
        total_loss: -0.00024796929210424423
        vf_explained_var: 0.046147435903549194
        vf_loss: 33.645965576171875
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2583913207054138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005239479942247272
        model: {}
        policy_loss: -0.0018534823320806026
        total_loss: 0.0007469021948054433
        vf_explained_var: 0.1321617215871811
        vf_loss: 30.551542282104492
    load_time_ms: 14842.485
    num_steps_sampled: 100032000
    num_steps_trained: 100032000
    sample_time_ms: 98454.257
    update_time_ms: 13.656
  iterations_since_restore: 82
  node_ip: 172.17.8.31
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.908988764044945
    ram_util_percent: 13.205056179775282
  pid: 12895
  policy_reward_max:
    agent-0: 286.0
    agent-1: 286.0
    agent-2: 453.5
    agent-3: 453.5
    agent-4: 260.5
    agent-5: 260.5
  policy_reward_mean:
    agent-0: 247.49
    agent-1: 247.49
    agent-2: 412.695
    agent-3: 412.695
    agent-4: 235.955
    agent-5: 235.955
  policy_reward_min:
    agent-0: 12.5
    agent-1: 12.5
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 18.5
    agent-5: 18.5
  sampler_perf:
    mean_env_wait_ms: 25.587125687457117
    mean_inference_ms: 13.09743117271415
    mean_processing_ms: 58.27073961447308
  time_since_restore: 10744.061188697815
  time_this_iter_s: 125.00632452964783
  time_total_s: 133225.0838971138
  timestamp: 1637647424
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 100032000
  training_iteration: 1042
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/32 CPUs, 0.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 TERMINATED)
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+
| Trial name                           | status     | loc   |   iter |   total time (s) |        ts |   reward |
|--------------------------------------+------------+-------+--------+------------------+-----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | TERMINATED |       |   1042 |           133225 | 100032000 |  1792.28 |
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+


== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/32 CPUs, 0.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.4 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_3teams_6agents_custom_metrics
Number of trials: 1 (1 TERMINATED)
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+
| Trial name                           | status     | loc   |   iter |   total time (s) |        ts |   reward |
|--------------------------------------+------------+-------+--------+------------------+-----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | TERMINATED |       |   1042 |           133225 | 100032000 |  1792.28 |
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+


