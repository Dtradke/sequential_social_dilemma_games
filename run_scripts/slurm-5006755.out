>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-38-56m07ou1p0/checkpoint_20
== Status ==
Memory usage on this node: 11.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m 2021-11-18 02:20:11,634	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=6435)[0m 2021-11-18 02:20:11,651	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=6435)[0m 2021-11-18 02:22:05,154	INFO trainable.py:180 -- _setup took 113.520 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=6435)[0m 2021-11-18 02:22:05,155	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=6435)[0m 2021-11-18 02:22:05,155	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=6435)[0m 2021-11-18 02:22:08,535	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=6435)[0m 2021-11-18 02:22:08,536	INFO trainable.py:423 -- Restored on 172.17.8.3 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-39-29lhwwiwf8/tmpiu9vkfrprestore_from_object/checkpoint-160
[2m[36m(pid=6435)[0m 2021-11-18 02:22:08,536	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': 15360000, '_time_total': 22510.283242464066, '_episodes_total': 15360}
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.7395833333333333
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.78125
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.7291666666666665
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.2916666666666665
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 2.8541666666666665
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 210
    cleaning_beam_agent-0_mean: 104.19791666666667
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 563
    cleaning_beam_agent-1_mean: 277.875
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 26.0
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 267
    cleaning_beam_agent-3_mean: 93.32291666666667
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 247
    cleaning_beam_agent-4_mean: 107.22916666666667
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 17.96875
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.010416666666666666
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.010416666666666666
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.020833333333333332
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-25-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 15.895833333333334
  episode_reward_min: -48.0
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 21890.1
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6398497223854065
        entropy_coeff: 0.0017600000137463212
        kl: 0.008817464113235474
        model: {}
        policy_loss: -0.02120022289454937
        total_loss: -0.020540807396173477
        vf_explained_var: 0.009221449494361877
        vf_loss: 0.2205187976360321
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5627270340919495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064523289911448956
        model: {}
        policy_loss: -0.01418265514075756
        total_loss: -0.013861916959285736
        vf_explained_var: 0.012798964977264404
        vf_loss: 0.2066805362701416
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5562811493873596
        entropy_coeff: 0.0017600000137463212
        kl: 0.008089936338365078
        model: {}
        policy_loss: -0.02127278409898281
        total_loss: -0.020600004121661186
        vf_explained_var: 0.01829148828983307
        vf_loss: 0.3384733200073242
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.680600106716156
        entropy_coeff: 0.0017600000137463212
        kl: 0.007074333261698484
        model: {}
        policy_loss: -0.018213564530014992
        total_loss: -0.017971836030483246
        vf_explained_var: 0.010705873370170593
        vf_loss: 0.24720293283462524
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7348576784133911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0065221963450312614
        model: {}
        policy_loss: -0.013177555054426193
        total_loss: -0.01302550733089447
        vf_explained_var: 0.012260332703590393
        vf_loss: 1.409578800201416
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.8281062841415405
        entropy_coeff: 0.0017600000137463212
        kl: 0.005803133361041546
        model: {}
        policy_loss: -0.009810876101255417
        total_loss: -0.009962139651179314
        vf_explained_var: 0.0012428313493728638
        vf_loss: 1.455764651298523
    load_time_ms: 49382.666
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 112560.019
    update_time_ms: 3740.561
  iterations_since_restore: 1
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.227622377622378
    ram_util_percent: 10.724825174825176
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 13.0
    agent-3: 10.0
    agent-4: 14.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.6354166666666665
    agent-1: 1.1979166666666667
    agent-2: 4.09375
    agent-3: 3.40625
    agent-4: 3.3125
    agent-5: 1.25
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 26.38877295590304
    mean_inference_ms: 15.244826093896641
    mean_processing_ms: 65.66298682809551
  time_since_restore: 190.8083198070526
  time_this_iter_s: 190.8083198070526
  time_total_s: 22701.091562271118
  timestamp: 1637220325
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    161 |          22701.1 | 15456000 |  15.8958 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.62
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 179
    cleaning_beam_agent-0_mean: 103.17
    cleaning_beam_agent-0_min: 47
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 255.67
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 95
    cleaning_beam_agent-2_mean: 24.93
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 217
    cleaning_beam_agent-3_mean: 93.27
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 337
    cleaning_beam_agent-4_mean: 109.62
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 18.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-28-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 15.3
  episode_reward_min: -94.0
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 16952.764
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6377288103103638
        entropy_coeff: 0.0017600000137463212
        kl: 0.005652789492160082
        model: {}
        policy_loss: -0.012696320191025734
        total_loss: -0.012536367401480675
        vf_explained_var: 0.008838921785354614
        vf_loss: 1.5179712772369385
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5783585906028748
        entropy_coeff: 0.0017600000137463212
        kl: 0.004925138782709837
        model: {}
        policy_loss: -0.008941879495978355
        total_loss: -0.00882781483232975
        vf_explained_var: 0.011287093162536621
        vf_loss: 1.4694883823394775
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5593662858009338
        entropy_coeff: 0.0017600000137463212
        kl: 0.008004635572433472
        model: {}
        policy_loss: -0.018445167690515518
        total_loss: -0.017771732062101364
        vf_explained_var: 0.006463482975959778
        vf_loss: 0.569943368434906
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6800255179405212
        entropy_coeff: 0.0017600000137463212
        kl: 0.006511343643069267
        model: {}
        policy_loss: -0.016041353344917297
        total_loss: -0.015893571078777313
        vf_explained_var: 0.022210493683815002
        vf_loss: 0.42360925674438477
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.728895902633667
        entropy_coeff: 0.0017600000137463212
        kl: 0.008619125932455063
        model: {}
        policy_loss: -0.021433737128973007
        total_loss: -0.02095339074730873
        vf_explained_var: 0.023898616433143616
        vf_loss: 0.393758088350296
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.803053617477417
        entropy_coeff: 0.0017600000137463212
        kl: 0.007770882919430733
        model: {}
        policy_loss: -0.01463853195309639
        total_loss: -0.014481446705758572
        vf_explained_var: 0.002863466739654541
        vf_loss: 0.1627960503101349
    load_time_ms: 45352.598
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 111746.043
    update_time_ms: 1878.206
  iterations_since_restore: 2
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.64468085106383
    ram_util_percent: 12.019574468085104
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 13.0
    agent-2: 26.0
    agent-3: 20.0
    agent-4: 18.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.12
    agent-1: 1.41
    agent-2: 4.31
    agent-3: 3.22
    agent-4: 3.08
    agent-5: 1.16
  policy_reward_min:
    agent-0: -48.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -48.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 26.312487258871787
    mean_inference_ms: 14.539702591620772
    mean_processing_ms: 65.77397505178766
  time_since_restore: 355.2615280151367
  time_this_iter_s: 164.4532082080841
  time_total_s: 22865.544770479202
  timestamp: 1637220490
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    162 |          22865.5 | 15552000 |     15.3 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.71
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.19
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.29
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 230
    cleaning_beam_agent-0_mean: 107.2
    cleaning_beam_agent-0_min: 38
    cleaning_beam_agent-1_max: 473
    cleaning_beam_agent-1_mean: 264.05
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 151
    cleaning_beam_agent-2_mean: 24.51
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 90.28
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 264
    cleaning_beam_agent-4_mean: 97.34
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 16.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-30-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 17.72
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 15270.012
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.6552780270576477
        entropy_coeff: 0.0017600000137463212
        kl: 0.00878037977963686
        model: {}
        policy_loss: -0.02165050059556961
        total_loss: -0.021025650203227997
        vf_explained_var: -0.00013008713722229004
        vf_loss: 0.22060894966125488
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00028955520247109234
        entropy: 0.557767391204834
        entropy_coeff: 0.0017600000137463212
        kl: 0.00891035981476307
        model: {}
        policy_loss: -0.01643911749124527
        total_loss: -0.016514841467142105
        vf_explained_var: 0.011269256472587585
        vf_loss: 0.1490953266620636
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.5438135266304016
        entropy_coeff: 0.0017600000137463212
        kl: 0.007224199827760458
        model: {}
        policy_loss: -0.01958390697836876
        total_loss: -0.01905735768377781
        vf_explained_var: 0.011302918195724487
        vf_loss: 0.3882151246070862
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.6913692951202393
        entropy_coeff: 0.0017600000137463212
        kl: 0.004970680922269821
        model: {}
        policy_loss: -0.010034865699708462
        total_loss: -0.010098909959197044
        vf_explained_var: 0.013864338397979736
        vf_loss: 1.5862765312194824
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7258847951889038
        entropy_coeff: 0.0017600000137463212
        kl: 0.008744167163968086
        model: {}
        policy_loss: -0.022024914622306824
        total_loss: -0.02151206135749817
        vf_explained_var: 0.02467276155948639
        vf_loss: 0.41584882140159607
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.8208786249160767
        entropy_coeff: 0.0017600000137463212
        kl: 0.007184059824794531
        model: {}
        policy_loss: -0.014345631003379822
        total_loss: -0.01434102188795805
        vf_explained_var: -0.00523000955581665
        vf_loss: 0.12544794380664825
    load_time_ms: 36883.383
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 111606.409
    update_time_ms: 1257.444
  iterations_since_restore: 3
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.956862745098032
    ram_util_percent: 11.794117647058824
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 13.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.65
    agent-1: 1.97
    agent-2: 4.26
    agent-3: 3.05
    agent-4: 4.19
    agent-5: 1.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 26.35419743350521
    mean_inference_ms: 14.30220404088024
    mean_processing_ms: 65.85151218162187
  time_since_restore: 498.6047022342682
  time_this_iter_s: 143.34317421913147
  time_total_s: 23008.887944698334
  timestamp: 1637220633
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    163 |          23008.9 | 15648000 |    17.72 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.09
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 1.65
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 199
    cleaning_beam_agent-0_mean: 95.58
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 239.18
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 151
    cleaning_beam_agent-2_mean: 22.48
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 253
    cleaning_beam_agent-3_mean: 95.6
    cleaning_beam_agent-3_min: 35
    cleaning_beam_agent-4_max: 273
    cleaning_beam_agent-4_mean: 96.6
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 16.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-33-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 14.4
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 14364.245
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.640120267868042
        entropy_coeff: 0.0017600000137463212
        kl: 0.005596335511654615
        model: {}
        policy_loss: -0.012400862760841846
        total_loss: -0.012259868904948235
        vf_explained_var: 0.017454713582992554
        vf_loss: 1.483363151550293
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.5327547788619995
        entropy_coeff: 0.0017600000137463212
        kl: 0.007505045738071203
        model: {}
        policy_loss: -0.01478198729455471
        total_loss: -0.014950252138078213
        vf_explained_var: 0.018969818949699402
        vf_loss: 0.18879815936088562
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.5481197237968445
        entropy_coeff: 0.0017600000137463212
        kl: 0.005377484019845724
        model: {}
        policy_loss: -0.011168603785336018
        total_loss: -0.010879864916205406
        vf_explained_var: 0.005242839455604553
        vf_loss: 1.779331088066101
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002835647901520133
        entropy: 0.6875714659690857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076745240949094296
        model: {}
        policy_loss: -0.01096310093998909
        total_loss: -0.011254366487264633
        vf_explained_var: 0.004134669899940491
        vf_loss: 1.5140578746795654
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.7207819223403931
        entropy_coeff: 0.0017600000137463212
        kl: 0.00552847795188427
        model: {}
        policy_loss: -0.011673924513161182
        total_loss: -0.011574208736419678
        vf_explained_var: 0.012338131666183472
        vf_loss: 2.625943422317505
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.828012228012085
        entropy_coeff: 0.0017600000137463212
        kl: 0.005766298156231642
        model: {}
        policy_loss: -0.00942978821694851
        total_loss: -0.009587449952960014
        vf_explained_var: 0.010447338223457336
        vf_loss: 1.463815450668335
    load_time_ms: 34601.067
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 111088.396
    update_time_ms: 947.209
  iterations_since_restore: 4
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.97887323943662
    ram_util_percent: 11.900469483568076
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 9.0
    agent-2: 27.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.36
    agent-1: 1.9
    agent-2: 3.85
    agent-3: 2.68
    agent-4: 2.61
    agent-5: 1.0
  policy_reward_min:
    agent-0: -51.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: -46.0
    agent-4: -48.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 26.275509249945536
    mean_inference_ms: 14.164992583329461
    mean_processing_ms: 65.73683838090454
  time_since_restore: 647.7016069889069
  time_this_iter_s: 149.09690475463867
  time_total_s: 23157.984849452972
  timestamp: 1637220783
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    164 |            23158 | 15744000 |     14.4 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.69
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.0
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.78
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 192
    cleaning_beam_agent-0_mean: 89.12
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 415
    cleaning_beam_agent-1_mean: 235.11
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 21.57
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 84.91
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 197
    cleaning_beam_agent-4_mean: 96.91
    cleaning_beam_agent-4_min: 27
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 14.96
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-35-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 15.03
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13842.748
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.6275448799133301
        entropy_coeff: 0.0017600000137463212
        kl: 0.008282655850052834
        model: {}
        policy_loss: -0.020594032481312752
        total_loss: -0.02002152055501938
        vf_explained_var: -0.014412552118301392
        vf_loss: 0.20457838475704193
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.5442385673522949
        entropy_coeff: 0.0017600000137463212
        kl: 0.007876676507294178
        model: {}
        policy_loss: -0.01520322635769844
        total_loss: -0.015358800068497658
        vf_explained_var: 0.03790725767612457
        vf_loss: 0.1462106853723526
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.5582069754600525
        entropy_coeff: 0.0017600000137463212
        kl: 0.007834432646632195
        model: {}
        policy_loss: -0.019929813221096992
        total_loss: -0.019312415271997452
        vf_explained_var: 0.007906049489974976
        vf_loss: 0.32957392930984497
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.6682102084159851
        entropy_coeff: 0.0017600000137463212
        kl: 0.008838639594614506
        model: {}
        policy_loss: -0.016766836866736412
        total_loss: -0.017033101990818977
        vf_explained_var: 4.0903687477111816e-05
        vf_loss: 0.25918278098106384
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.7268170714378357
        entropy_coeff: 0.0017600000137463212
        kl: 0.008348692208528519
        model: {}
        policy_loss: -0.02056928351521492
        total_loss: -0.02014380320906639
        vf_explained_var: 0.00963623821735382
        vf_loss: 0.3494062125682831
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.8196496963500977
        entropy_coeff: 0.0017600000137463212
        kl: 0.007466798182576895
        model: {}
        policy_loss: -0.014935162849724293
        total_loss: -0.014873359352350235
        vf_explained_var: -0.004498139023780823
        vf_loss: 0.11026947945356369
    load_time_ms: 31607.586
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 110710.01
    update_time_ms: 760.928
  iterations_since_restore: 5
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.077114427860696
    ram_util_percent: 11.764179104477615
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 12.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.5
    agent-1: 1.82
    agent-2: 3.56
    agent-3: 2.9
    agent-4: 2.78
    agent-5: 1.47
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -42.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 26.243668095647603
    mean_inference_ms: 14.091828508700555
    mean_processing_ms: 65.74620360884957
  time_since_restore: 788.4530646800995
  time_this_iter_s: 140.75145769119263
  time_total_s: 23298.736307144165
  timestamp: 1637220923
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    165 |          23298.7 | 15840000 |    15.03 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 1.69
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 1.84
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.27
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.51
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 171
    cleaning_beam_agent-0_mean: 93.68
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 235.07
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 24.06
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 80.33
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 243
    cleaning_beam_agent-4_mean: 105.54
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 14.4
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-37-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 14.98
  episode_reward_min: -44.0
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13483.285
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.6318206787109375
        entropy_coeff: 0.0017600000137463212
        kl: 0.005169708747416735
        model: {}
        policy_loss: -0.009730703197419643
        total_loss: -0.009662359952926636
        vf_explained_var: 0.006816044449806213
        vf_loss: 1.464053988456726
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.5547911524772644
        entropy_coeff: 0.0017600000137463212
        kl: 0.005682337563484907
        model: {}
        policy_loss: -0.008354011923074722
        total_loss: -0.008620871230959892
        vf_explained_var: 0.02153792977333069
        vf_loss: 1.4133967161178589
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.5422863960266113
        entropy_coeff: 0.0017600000137463212
        kl: 0.007448985707014799
        model: {}
        policy_loss: -0.018326029181480408
        total_loss: -0.017750049009919167
        vf_explained_var: 0.010396212339401245
        vf_loss: 0.4060612916946411
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.669360876083374
        entropy_coeff: 0.0017600000137463212
        kl: 0.008191028609871864
        model: {}
        policy_loss: -0.014489719644188881
        total_loss: -0.01481300126761198
        vf_explained_var: 0.0022667348384857178
        vf_loss: 0.35692116618156433
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.7284634113311768
        entropy_coeff: 0.0017600000137463212
        kl: 0.008615012280642986
        model: {}
        policy_loss: -0.021076736971735954
        total_loss: -0.020605800673365593
        vf_explained_var: 0.012937381863594055
        vf_loss: 0.30027687549591064
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.797981321811676
        entropy_coeff: 0.0017600000137463212
        kl: 0.006734570022672415
        model: {}
        policy_loss: -0.014706185087561607
        total_loss: -0.014753108844161034
        vf_explained_var: 0.009655341506004333
        vf_loss: 0.10613540560007095
    load_time_ms: 31397.309
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 110540.953
    update_time_ms: 637.101
  iterations_since_restore: 6
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.791666666666668
    ram_util_percent: 11.925
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.15
    agent-1: 1.0
    agent-2: 3.79
    agent-3: 3.17
    agent-4: 3.39
    agent-5: 1.48
  policy_reward_min:
    agent-0: -45.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 26.215964240046834
    mean_inference_ms: 14.046417807900768
    mean_processing_ms: 65.75092862836877
  time_since_restore: 940.3023915290833
  time_this_iter_s: 151.84932684898376
  time_total_s: 23450.58563399315
  timestamp: 1637221075
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    166 |          23450.6 | 15936000 |    14.98 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.13
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.88
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 2.5
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 194
    cleaning_beam_agent-0_mean: 91.55
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 239.0
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 98
    cleaning_beam_agent-2_mean: 23.78
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 75.47
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 258
    cleaning_beam_agent-4_mean: 99.22
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 15.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-40-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 14.49
  episode_reward_min: -183.0
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13213.808
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.6418490409851074
        entropy_coeff: 0.0017600000137463212
        kl: 0.007884448394179344
        model: {}
        policy_loss: -0.019243497401475906
        total_loss: -0.018767360597848892
        vf_explained_var: -0.002232566475868225
        vf_loss: 0.289015531539917
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.5532482862472534
        entropy_coeff: 0.0017600000137463212
        kl: 0.008052974939346313
        model: {}
        policy_loss: -0.016349859535694122
        total_loss: -0.016503937542438507
        vf_explained_var: 0.03530806303024292
        vf_loss: 0.14343653619289398
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.5537307858467102
        entropy_coeff: 0.0017600000137463212
        kl: 0.004175850190222263
        model: {}
        policy_loss: -0.006729867309331894
        total_loss: -0.004786367528140545
        vf_explained_var: 0.0009348094463348389
        vf_loss: 20.828964233398438
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.6587762832641602
        entropy_coeff: 0.0017600000137463212
        kl: 0.006299270782619715
        model: {}
        policy_loss: -0.009048492647707462
        total_loss: -0.009427091106772423
        vf_explained_var: 0.006973236799240112
        vf_loss: 1.5091782808303833
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.7243319153785706
        entropy_coeff: 0.0017600000137463212
        kl: 0.008702380582690239
        model: {}
        policy_loss: -0.022916220128536224
        total_loss: -0.022421376779675484
        vf_explained_var: 0.025836274027824402
        vf_loss: 0.2919275164604187
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.7908788323402405
        entropy_coeff: 0.0017600000137463212
        kl: 0.005471617449074984
        model: {}
        policy_loss: -0.00928693637251854
        total_loss: -0.009449601173400879
        vf_explained_var: 0.019724175333976746
        vf_loss: 1.349570870399475
    load_time_ms: 31241.395
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 110358.374
    update_time_ms: 548.456
  iterations_since_restore: 7
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.94583333333333
    ram_util_percent: 11.957407407407409
  pid: 6435
  policy_reward_max:
    agent-0: 20.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 14.0
    agent-4: 11.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.82
    agent-1: 2.05
    agent-2: 2.06
    agent-3: 2.62
    agent-4: 3.76
    agent-5: 1.18
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -195.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 26.20404332906013
    mean_inference_ms: 14.019722885217705
    mean_processing_ms: 65.82167463143199
  time_since_restore: 1091.647177696228
  time_this_iter_s: 151.34478616714478
  time_total_s: 23601.930420160294
  timestamp: 1637221227
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    167 |          23601.9 | 16032000 |    14.49 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.3
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 208
    cleaning_beam_agent-0_mean: 94.48
    cleaning_beam_agent-0_min: 40
    cleaning_beam_agent-1_max: 408
    cleaning_beam_agent-1_mean: 241.96
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 23.08
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 75.56
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 235
    cleaning_beam_agent-4_mean: 97.54
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 14.74
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-42-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 11.87
  episode_reward_min: -442.0
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 13014.058
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.6231886744499207
        entropy_coeff: 0.0017600000137463212
        kl: 0.007587514352053404
        model: {}
        policy_loss: -0.01777515560388565
        total_loss: -0.017318911850452423
        vf_explained_var: 0.009004533290863037
        vf_loss: 0.3554958403110504
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.5579277276992798
        entropy_coeff: 0.0017600000137463212
        kl: 0.007191774435341358
        model: {}
        policy_loss: -0.00832290854305029
        total_loss: -0.007324542850255966
        vf_explained_var: 0.008698597550392151
        vf_loss: 12.611405372619629
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.5451291799545288
        entropy_coeff: 0.0017600000137463212
        kl: 0.005063268821686506
        model: {}
        policy_loss: -0.007412316277623177
        total_loss: -0.0066507551819086075
        vf_explained_var: 0.006584912538528442
        vf_loss: 12.146629333496094
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.6798009872436523
        entropy_coeff: 0.0017600000137463212
        kl: 0.006014387123286724
        model: {}
        policy_loss: -0.006610502954572439
        total_loss: -0.0065849171951413155
        vf_explained_var: 0.007225647568702698
        vf_loss: 6.20599365234375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.7216125130653381
        entropy_coeff: 0.0017600000137463212
        kl: 0.006302039138972759
        model: {}
        policy_loss: -0.013261392712593079
        total_loss: -0.013102620840072632
        vf_explained_var: 0.0193624347448349
        vf_loss: 1.6840671300888062
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.7764912247657776
        entropy_coeff: 0.0017600000137463212
        kl: 0.005047968588769436
        model: {}
        policy_loss: -0.00843010563403368
        total_loss: -0.008643283508718014
        vf_explained_var: 0.005524039268493652
        vf_loss: 1.4385299682617188
    load_time_ms: 30309.543
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 110354.926
    update_time_ms: 481.991
  iterations_since_restore: 8
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.92980769230769
    ram_util_percent: 11.829807692307694
  pid: 6435
  policy_reward_max:
    agent-0: 17.0
    agent-1: 10.0
    agent-2: 26.0
    agent-3: 11.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.21
    agent-1: -0.05
    agent-2: 2.71
    agent-3: 1.73
    agent-4: 3.34
    agent-5: 0.93
  policy_reward_min:
    agent-0: -1.0
    agent-1: -200.0
    agent-2: -149.0
    agent-3: -95.0
    agent-4: -48.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 26.191980384033595
    mean_inference_ms: 13.995140622958175
    mean_processing_ms: 65.834192572426
  time_since_restore: 1237.492600440979
  time_this_iter_s: 145.84542274475098
  time_total_s: 23747.775842905045
  timestamp: 1637221373
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    168 |          23747.8 | 16128000 |    11.87 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.64
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.38
    apples_agent-2_min: 0
    apples_agent-3_max: 7
    apples_agent-3_mean: 2.12
    apples_agent-3_min: 0
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 86.64
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 232.59
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 115
    cleaning_beam_agent-2_mean: 24.11
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 77.74
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 212
    cleaning_beam_agent-4_mean: 92.53
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 17.23
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-45-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 16.44
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 12876.214
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.6233606338500977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0077597154304385185
        model: {}
        policy_loss: -0.02084052935242653
        total_loss: -0.020365316420793533
        vf_explained_var: -0.0004552006721496582
        vf_loss: 0.2038196623325348
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.5545636415481567
        entropy_coeff: 0.0017600000137463212
        kl: 0.008191103115677834
        model: {}
        policy_loss: -0.017268763855099678
        total_loss: -0.017413707450032234
        vf_explained_var: -0.10182580351829529
        vf_loss: 0.11975525319576263
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.5485068559646606
        entropy_coeff: 0.0017600000137463212
        kl: 0.008643927983939648
        model: {}
        policy_loss: -0.020275849848985672
        total_loss: -0.02034972980618477
        vf_explained_var: -0.046191275119781494
        vf_loss: 0.27098527550697327
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.6806839108467102
        entropy_coeff: 0.0017600000137463212
        kl: 0.007494156714528799
        model: {}
        policy_loss: -0.015889247879385948
        total_loss: -0.016310621052980423
        vf_explained_var: -0.017352163791656494
        vf_loss: 0.2721896469593048
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.7193250060081482
        entropy_coeff: 0.0017600000137463212
        kl: 0.008668207563459873
        model: {}
        policy_loss: -0.02188359946012497
        total_loss: -0.02138999104499817
        vf_explained_var: 0.0055136531591415405
        vf_loss: 0.2597942352294922
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.8245083689689636
        entropy_coeff: 0.0017600000137463212
        kl: 0.006632464937865734
        model: {}
        policy_loss: -0.014775671064853668
        total_loss: -0.014890194870531559
        vf_explained_var: 0.00401034951210022
        vf_loss: 0.1012023463845253
    load_time_ms: 29015.403
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 110201.488
    update_time_ms: 430.012
  iterations_since_restore: 9
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.310050251256285
    ram_util_percent: 11.74170854271357
  pid: 6435
  policy_reward_max:
    agent-0: 17.0
    agent-1: 8.0
    agent-2: 26.0
    agent-3: 11.0
    agent-4: 10.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.05
    agent-1: 1.66
    agent-2: 3.73
    agent-3: 3.28
    agent-4: 3.32
    agent-5: 1.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 26.184060549397373
    mean_inference_ms: 13.978692742295689
    mean_processing_ms: 65.86370220071544
  time_since_restore: 1376.9942064285278
  time_this_iter_s: 139.50160598754883
  time_total_s: 23887.277448892593
  timestamp: 1637221513
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 21.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    169 |          23887.3 | 16224000 |    16.44 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.13
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.79
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 95.06
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 231.91
    cleaning_beam_agent-1_min: 45
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 21.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 206
    cleaning_beam_agent-3_mean: 85.91
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 201
    cleaning_beam_agent-4_mean: 91.89
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 15.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-47-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 14.1
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 12757.706
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.6068323850631714
        entropy_coeff: 0.0017600000137463212
        kl: 0.007431798614561558
        model: {}
        policy_loss: -0.019260989502072334
        total_loss: -0.018825091421604156
        vf_explained_var: -0.0018009990453720093
        vf_loss: 0.17562176287174225
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5509474277496338
        entropy_coeff: 0.0017600000137463212
        kl: 0.007018567994236946
        model: {}
        policy_loss: -0.0132442656904459
        total_loss: -0.013489359058439732
        vf_explained_var: 0.004607602953910828
        vf_loss: 0.2271713763475418
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5458655953407288
        entropy_coeff: 0.0017600000137463212
        kl: 0.007955878973007202
        model: {}
        policy_loss: -0.018048040568828583
        total_loss: -0.018172400072216988
        vf_explained_var: -0.005530267953872681
        vf_loss: 0.4077899158000946
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.7009842395782471
        entropy_coeff: 0.0017600000137463212
        kl: 0.007925273850560188
        model: {}
        policy_loss: -0.01609499379992485
        total_loss: -0.016507795080542564
        vf_explained_var: 0.0020380914211273193
        vf_loss: 0.2840207517147064
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.7337507009506226
        entropy_coeff: 0.0017600000137463212
        kl: 0.004945180378854275
        model: {}
        policy_loss: -0.010039887391030788
        total_loss: -0.010168828070163727
        vf_explained_var: 0.015850737690925598
        vf_loss: 1.734255075454712
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.7759467363357544
        entropy_coeff: 0.0017600000137463212
        kl: 0.006285983137786388
        model: {}
        policy_loss: -0.01329972967505455
        total_loss: -0.01339888758957386
        vf_explained_var: -0.001429736614227295
        vf_loss: 0.09310578554868698
    load_time_ms: 28072.69
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 110135.693
    update_time_ms: 388.554
  iterations_since_restore: 10
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.107960199004978
    ram_util_percent: 11.376616915422884
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 18.0
    agent-2: 23.0
    agent-3: 14.0
    agent-4: 10.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.22
    agent-1: 1.94
    agent-2: 3.66
    agent-3: 2.84
    agent-4: 2.16
    agent-5: 1.28
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 26.175925346313903
    mean_inference_ms: 13.966012278631467
    mean_processing_ms: 65.89743173735783
  time_since_restore: 1517.950719833374
  time_this_iter_s: 140.9565134048462
  time_total_s: 24028.23396229744
  timestamp: 1637221654
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    170 |          24028.2 | 16320000 |     14.1 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 2.59
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 1.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 219
    cleaning_beam_agent-0_mean: 97.07
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 241.33
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 113
    cleaning_beam_agent-2_mean: 26.34
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 197
    cleaning_beam_agent-3_mean: 84.4
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 194
    cleaning_beam_agent-4_mean: 95.46
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 16.29
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-49-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 15.02
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11727.153
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.6231565475463867
        entropy_coeff: 0.0017600000137463212
        kl: 0.007656925357878208
        model: {}
        policy_loss: -0.018972285091876984
        total_loss: -0.018515313044190407
        vf_explained_var: 0.008259475231170654
        vf_loss: 0.22341153025627136
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5526444911956787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072634960524737835
        model: {}
        policy_loss: -0.01562079880386591
        total_loss: -0.01585490256547928
        vf_explained_var: 0.00479888916015625
        vf_loss: 0.12200877070426941
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5453246831893921
        entropy_coeff: 0.0017600000137463212
        kl: 0.00867236778140068
        model: {}
        policy_loss: -0.019185790792107582
        total_loss: -0.01924091763794422
        vf_explained_var: -0.0003255009651184082
        vf_loss: 0.374087393283844
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.6833832859992981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0065817865543067455
        model: {}
        policy_loss: -0.0075378092005848885
        total_loss: -0.007791435346007347
        vf_explained_var: 0.008975625038146973
        vf_loss: 2.909480094909668
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.7400489449501038
        entropy_coeff: 0.0017600000137463212
        kl: 0.008819369599223137
        model: {}
        policy_loss: -0.012033304199576378
        total_loss: -0.012304453179240227
        vf_explained_var: 0.014828428626060486
        vf_loss: 1.4940024614334106
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.7920143008232117
        entropy_coeff: 0.0017600000137463212
        kl: 0.005513597745448351
        model: {}
        policy_loss: -0.008517025038599968
        total_loss: -0.008549701422452927
        vf_explained_var: 0.005970105528831482
        vf_loss: 2.5854859352111816
    load_time_ms: 24995.882
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 109734.633
    update_time_ms: 15.928
  iterations_since_restore: 11
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.843434343434343
    ram_util_percent: 9.248989898989898
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.9
    agent-1: 1.79
    agent-2: 4.06
    agent-3: 2.51
    agent-4: 3.78
    agent-5: -0.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: -43.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 26.15977481426083
    mean_inference_ms: 13.942344726834229
    mean_processing_ms: 65.83205687946129
  time_since_restore: 1656.7880318164825
  time_this_iter_s: 138.83731198310852
  time_total_s: 24167.071274280548
  timestamp: 1637221793
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    171 |          24167.1 | 16416000 |    15.02 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 2.16
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.99
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 185
    cleaning_beam_agent-0_mean: 89.62
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 228.03
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 103
    cleaning_beam_agent-2_mean: 25.4
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 231
    cleaning_beam_agent-3_mean: 100.46
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 208
    cleaning_beam_agent-4_mean: 86.98
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 19.48
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-52-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 14.75
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11701.961
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.6069574356079102
        entropy_coeff: 0.0017600000137463212
        kl: 0.007147232536226511
        model: {}
        policy_loss: -0.017624273896217346
        total_loss: -0.017230186611413956
        vf_explained_var: 0.00597575306892395
        vf_loss: 0.3288295865058899
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5478512048721313
        entropy_coeff: 0.0017600000137463212
        kl: 0.004828223492950201
        model: {}
        policy_loss: -0.005788166541606188
        total_loss: -0.005742635112255812
        vf_explained_var: 0.012670457363128662
        vf_loss: 5.269275188446045
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5414972305297852
        entropy_coeff: 0.0017600000137463212
        kl: 0.008394493721425533
        model: {}
        policy_loss: -0.020139822736382484
        total_loss: -0.020224858075380325
        vf_explained_var: -0.022522270679473877
        vf_loss: 0.285466730594635
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.6853268146514893
        entropy_coeff: 0.0017600000137463212
        kl: 0.008743440732359886
        model: {}
        policy_loss: -0.0166646596044302
        total_loss: -0.016975603997707367
        vf_explained_var: -0.028332144021987915
        vf_loss: 0.20884212851524353
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.7086992859840393
        entropy_coeff: 0.0017600000137463212
        kl: 0.009742406196892262
        model: {}
        policy_loss: -0.021749624982476234
        total_loss: -0.02199406921863556
        vf_explained_var: 0.006237536668777466
        vf_loss: 0.2862515151500702
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.781697154045105
        entropy_coeff: 0.0017600000137463212
        kl: 0.006475845351815224
        model: {}
        policy_loss: -0.013856517150998116
        total_loss: -0.01392747275531292
        vf_explained_var: -0.006751552224159241
        vf_loss: 0.09665825963020325
    load_time_ms: 22757.848
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 109408.619
    update_time_ms: 15.62
  iterations_since_restore: 12
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.264646464646464
    ram_util_percent: 9.266666666666666
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 6.0
    agent-2: 18.0
    agent-3: 15.0
    agent-4: 11.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.29
    agent-1: 0.45
    agent-2: 3.64
    agent-3: 2.64
    agent-4: 3.37
    agent-5: 1.36
  policy_reward_min:
    agent-0: 0.0
    agent-1: -99.0
    agent-2: 0.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 26.097491031110604
    mean_inference_ms: 13.931925489106312
    mean_processing_ms: 65.71492568943097
  time_since_restore: 1795.350596666336
  time_this_iter_s: 138.56256484985352
  time_total_s: 24305.6338391304
  timestamp: 1637221932
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    172 |          24305.6 | 16512000 |    14.75 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.74
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.68
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.17
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 234
    cleaning_beam_agent-0_mean: 101.21
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 226.86
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 25.96
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 90.6
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 228
    cleaning_beam_agent-4_mean: 96.29
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 17.55
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-54-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 14.42
  episode_reward_min: -45.0
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11676.972
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.6096514463424683
        entropy_coeff: 0.0017600000137463212
        kl: 0.007741605397313833
        model: {}
        policy_loss: -0.018870186060667038
        total_loss: -0.018374977633357048
        vf_explained_var: 0.009983256459236145
        vf_loss: 0.19875898957252502
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00022965119569562376
        entropy: 0.5552775859832764
        entropy_coeff: 0.0017600000137463212
        kl: 0.008189486339688301
        model: {}
        policy_loss: -0.015156819485127926
        total_loss: -0.015711355954408646
        vf_explained_var: -0.02606433629989624
        vf_loss: 0.13280412554740906
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.548261284828186
        entropy_coeff: 0.0017600000137463212
        kl: 0.007780916057527065
        model: {}
        policy_loss: -0.013769257813692093
        total_loss: -0.01378532126545906
        vf_explained_var: 0.011335283517837524
        vf_loss: 1.7078685760498047
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.686861515045166
        entropy_coeff: 0.0017600000137463212
        kl: 0.006094946525990963
        model: {}
        policy_loss: -0.008239034563302994
        total_loss: -0.008554731495678425
        vf_explained_var: 0.004654526710510254
        vf_loss: 2.836848497390747
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.7075627446174622
        entropy_coeff: 0.0017600000137463212
        kl: 0.009550959803164005
        model: {}
        policy_loss: -0.01925532892346382
        total_loss: -0.019506869837641716
        vf_explained_var: 0.02170749008655548
        vf_loss: 0.38671302795410156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.7744879722595215
        entropy_coeff: 0.0017600000137463212
        kl: 0.006511374842375517
        model: {}
        policy_loss: -0.01389399729669094
        total_loss: -0.013943787664175034
        vf_explained_var: 0.011538073420524597
        vf_loss: 0.11034323275089264
    load_time_ms: 22592.013
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 108873.065
    update_time_ms: 15.655
  iterations_since_restore: 13
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.835051546391753
    ram_util_percent: 9.250515463917525
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 18.0
    agent-4: 17.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.59
    agent-1: 1.69
    agent-2: 3.44
    agent-3: 2.09
    agent-4: 3.16
    agent-5: 1.45
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: -46.0
    agent-4: -50.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 26.048286936949104
    mean_inference_ms: 13.908045103179274
    mean_processing_ms: 65.57431441717964
  time_since_restore: 1931.3515486717224
  time_this_iter_s: 136.00095200538635
  time_total_s: 24441.634791135788
  timestamp: 1637222068
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    173 |          24441.6 | 16608000 |    14.42 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.6
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.7
    apples_agent-3_min: 0
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 0.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 90.24
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 512
    cleaning_beam_agent-1_mean: 226.74
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 26.39
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 259
    cleaning_beam_agent-3_mean: 89.02
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 200
    cleaning_beam_agent-4_mean: 89.61
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 15.62
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-56-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 12.13
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11677.484
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.6043996810913086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045592146925628185
        model: {}
        policy_loss: -0.00990450382232666
        total_loss: -0.00990619882941246
        vf_explained_var: 0.003876611590385437
        vf_loss: 1.5020627975463867
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00022366079792845994
        entropy: 0.5506453514099121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0070068868808448315
        model: {}
        policy_loss: -0.013225947506725788
        total_loss: -0.013822639361023903
        vf_explained_var: -0.007475823163986206
        vf_loss: 0.2210482358932495
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.546538233757019
        entropy_coeff: 0.0017600000137463212
        kl: 0.008342544548213482
        model: {}
        policy_loss: -0.018839744850993156
        total_loss: -0.018936507403850555
        vf_explained_var: -0.0007006525993347168
        vf_loss: 0.308931440114975
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.670947790145874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0067390394397079945
        model: {}
        policy_loss: -0.011828698217868805
        total_loss: -0.012287525460124016
        vf_explained_var: -0.0021019428968429565
        vf_loss: 0.4813555181026459
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.7163265943527222
        entropy_coeff: 0.0017600000137463212
        kl: 0.006830907892435789
        model: {}
        policy_loss: -0.010027853772044182
        total_loss: -0.010458376258611679
        vf_explained_var: 0.0050476789474487305
        vf_loss: 1.4711897373199463
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.7708321213722229
        entropy_coeff: 0.0017600000137463212
        kl: 0.003939098212867975
        model: {}
        policy_loss: -0.0067131007090210915
        total_loss: -0.006882325280457735
        vf_explained_var: 0.011648893356323242
        vf_loss: 3.9962222576141357
    load_time_ms: 21671.182
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 108668.276
    update_time_ms: 15.395
  iterations_since_restore: 14
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.684693877551023
    ram_util_percent: 9.261224489795918
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 14.0
    agent-2: 12.0
    agent-3: 29.0
    agent-4: 11.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 1.44
    agent-1: 1.91
    agent-2: 3.58
    agent-3: 2.84
    agent-4: 2.55
    agent-5: -0.19
  policy_reward_min:
    agent-0: -49.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: -43.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 26.038015273871462
    mean_inference_ms: 13.894052337998414
    mean_processing_ms: 65.53356123778866
  time_since_restore: 2069.120582818985
  time_this_iter_s: 137.76903414726257
  time_total_s: 24579.40382528305
  timestamp: 1637222206
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    174 |          24579.4 | 16704000 |    12.13 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.61
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.84
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 1.92
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.07
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 0.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 165
    cleaning_beam_agent-0_mean: 90.72
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 216.55
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 21.27
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 193
    cleaning_beam_agent-3_mean: 88.18
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 225
    cleaning_beam_agent-4_mean: 93.81
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 14.4
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-59-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 12.49
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11671.547
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6061939001083374
        entropy_coeff: 0.0017600000137463212
        kl: 0.008447428233921528
        model: {}
        policy_loss: -0.018743544816970825
        total_loss: -0.01894582435488701
        vf_explained_var: 0.003440752625465393
        vf_loss: 0.19876837730407715
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00021767040016129613
        entropy: 0.5760285258293152
        entropy_coeff: 0.0017600000137463212
        kl: 0.010920645669102669
        model: {}
        policy_loss: -0.009379720315337181
        total_loss: -0.009574897587299347
        vf_explained_var: 0.005278334021568298
        vf_loss: 2.726008653640747
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5282198786735535
        entropy_coeff: 0.0017600000137463212
        kl: 0.00609426386654377
        model: {}
        policy_loss: -0.009555967524647713
        total_loss: -0.009705623611807823
        vf_explained_var: 0.0007102638483047485
        vf_loss: 1.7058557271957397
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6697103977203369
        entropy_coeff: 0.0017600000137463212
        kl: 0.007158744148910046
        model: {}
        policy_loss: -0.014668548479676247
        total_loss: -0.015104248188436031
        vf_explained_var: 0.011989966034889221
        vf_loss: 0.2711421549320221
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.7299495339393616
        entropy_coeff: 0.0017600000137463212
        kl: 0.009536303579807281
        model: {}
        policy_loss: -0.019012032076716423
        total_loss: -0.019312838092446327
        vf_explained_var: 0.01426762342453003
        vf_loss: 0.30275076627731323
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.7540388107299805
        entropy_coeff: 0.0017600000137463212
        kl: 0.007138072047382593
        model: {}
        policy_loss: -0.012541511096060276
        total_loss: -0.013143747113645077
        vf_explained_var: -0.04039916396141052
        vf_loss: 0.11065945029258728
    load_time_ms: 21572.901
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 108461.166
    update_time_ms: 15.374
  iterations_since_restore: 15
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.970408163265304
    ram_util_percent: 9.22091836734694
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.53
    agent-1: 0.51
    agent-2: 3.23
    agent-3: 2.92
    agent-4: 2.69
    agent-5: 0.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: -50.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 26.019559138192868
    mean_inference_ms: 13.881456177502399
    mean_processing_ms: 65.47625547646642
  time_since_restore: 2206.691655397415
  time_this_iter_s: 137.57107257843018
  time_total_s: 24716.97489786148
  timestamp: 1637222343
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    175 |            24717 | 16800000 |    12.49 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.77
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 1.88
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.73
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 225
    cleaning_beam_agent-0_mean: 86.28
    cleaning_beam_agent-0_min: 40
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 214.16
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 24.79
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 252
    cleaning_beam_agent-3_mean: 95.06
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 89.88
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 15.13
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-01-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 39.0
  episode_reward_mean: 13.06
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11681.845
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6112921833992004
        entropy_coeff: 0.0017600000137463212
        kl: 0.00842177215963602
        model: {}
        policy_loss: -0.018827829509973526
        total_loss: -0.019044896587729454
        vf_explained_var: 0.0026435256004333496
        vf_loss: 0.16631200909614563
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00021168000239413232
        entropy: 0.5615252256393433
        entropy_coeff: 0.0017600000137463212
        kl: 0.007886825129389763
        model: {}
        policy_loss: -0.013895822688937187
        total_loss: -0.014476742595434189
        vf_explained_var: 0.011777907609939575
        vf_loss: 0.1302356868982315
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5292572975158691
        entropy_coeff: 0.0017600000137463212
        kl: 0.007702394388616085
        model: {}
        policy_loss: -0.018101686611771584
        total_loss: -0.018231995403766632
        vf_explained_var: -0.005486160516738892
        vf_loss: 0.3094179034233093
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6679412126541138
        entropy_coeff: 0.0017600000137463212
        kl: 0.005636586342006922
        model: {}
        policy_loss: -0.008922586217522621
        total_loss: -0.009400185197591782
        vf_explained_var: -0.0029931962490081787
        vf_loss: 1.3431696891784668
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.7168081998825073
        entropy_coeff: 0.0017600000137463212
        kl: 0.009318601340055466
        model: {}
        policy_loss: -0.02064453437924385
        total_loss: -0.020949415862560272
        vf_explained_var: 0.018810704350471497
        vf_loss: 0.24840909242630005
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.7780623435974121
        entropy_coeff: 0.0017600000137463212
        kl: 0.00682654045522213
        model: {}
        policy_loss: -0.006580313201993704
        total_loss: -0.007134970743209124
        vf_explained_var: 0.0014794915914535522
        vf_loss: 1.3207981586456299
    load_time_ms: 20424.661
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 108198.475
    update_time_ms: 15.18
  iterations_since_restore: 16
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.312690355329945
    ram_util_percent: 9.281218274111675
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 7.0
    agent-2: 11.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 4.0
  policy_reward_mean:
    agent-0: 2.24
    agent-1: 1.52
    agent-2: 3.48
    agent-3: 2.15
    agent-4: 3.07
    agent-5: 0.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 25.975488583123944
    mean_inference_ms: 13.867668976569366
    mean_processing_ms: 65.41913537657612
  time_since_restore: 2344.566304206848
  time_this_iter_s: 137.87464880943298
  time_total_s: 24854.849546670914
  timestamp: 1637222481
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    176 |          24854.8 | 16896000 |    13.06 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.46
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 1.97
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 309
    cleaning_beam_agent-0_mean: 94.3
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 226.87
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 25.89
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 90.16
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 261
    cleaning_beam_agent-4_mean: 90.61
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 15.57
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-03-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 15.71
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11688.115
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6091094017028809
        entropy_coeff: 0.0017600000137463212
        kl: 0.011232241056859493
        model: {}
        policy_loss: -0.00672519113868475
        total_loss: -0.006517907604575157
        vf_explained_var: 0.010052502155303955
        vf_loss: 1.5608854293823242
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0002056896046269685
        entropy: 0.5605146288871765
        entropy_coeff: 0.0017600000137463212
        kl: 0.007071217522025108
        model: {}
        policy_loss: -0.013257782906293869
        total_loss: -0.013872301205992699
        vf_explained_var: 0.009115219116210938
        vf_loss: 0.18424953520298004
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5277162194252014
        entropy_coeff: 0.0017600000137463212
        kl: 0.007634919136762619
        model: {}
        policy_loss: -0.0171632282435894
        total_loss: -0.017283905297517776
        vf_explained_var: 0.009395211935043335
        vf_loss: 0.44610485434532166
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.668418824672699
        entropy_coeff: 0.0017600000137463212
        kl: 0.006983870640397072
        model: {}
        policy_loss: -0.013626612722873688
        total_loss: -0.014071900397539139
        vf_explained_var: 0.0034427791833877563
        vf_loss: 0.3274741768836975
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.717732310295105
        entropy_coeff: 0.0017600000137463212
        kl: 0.009587438777089119
        model: {}
        policy_loss: -0.020929895341396332
        total_loss: -0.02120283991098404
        vf_explained_var: 0.018550336360931396
        vf_loss: 0.3151628375053406
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.7475451231002808
        entropy_coeff: 0.0017600000137463212
        kl: 0.007825789973139763
        model: {}
        policy_loss: -0.012705635279417038
        total_loss: -0.013229483738541603
        vf_explained_var: 0.013332948088645935
        vf_loss: 0.0925028920173645
    load_time_ms: 19265.34
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 108076.316
    update_time_ms: 14.884
  iterations_since_restore: 17
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.78680203045685
    ram_util_percent: 9.295939086294414
  pid: 6435
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.41
    agent-1: 1.42
    agent-2: 4.03
    agent-3: 3.17
    agent-4: 3.43
    agent-5: 1.25
  policy_reward_min:
    agent-0: -48.0
    agent-1: -49.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.95999159814647
    mean_inference_ms: 13.855696353777592
    mean_processing_ms: 65.3850047054243
  time_since_restore: 2483.0662422180176
  time_this_iter_s: 138.49993801116943
  time_total_s: 24993.349484682083
  timestamp: 1637222620
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    177 |          24993.3 | 16992000 |    15.71 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.65
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.62
    apples_agent-2_min: 0
    apples_agent-3_max: 7
    apples_agent-3_mean: 2.18
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.63
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 0.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 304
    cleaning_beam_agent-0_mean: 88.8
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 372
    cleaning_beam_agent-1_mean: 222.15
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 25.33
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 96.58
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 261
    cleaning_beam_agent-4_mean: 92.86
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 16.29
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-06-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 14.77
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11706.883
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5912794470787048
        entropy_coeff: 0.0017600000137463212
        kl: 0.008479028940200806
        model: {}
        policy_loss: -0.017797579988837242
        total_loss: -0.017971765249967575
        vf_explained_var: -0.006764471530914307
        vf_loss: 0.18560701608657837
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001996992068598047
        entropy: 0.5671736001968384
        entropy_coeff: 0.0017600000137463212
        kl: 0.007639776915311813
        model: {}
        policy_loss: -0.013932272791862488
        total_loss: -0.014536280184984207
        vf_explained_var: 0.0020168423652648926
        vf_loss: 0.12229804694652557
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5286424160003662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0052728550508618355
        model: {}
        policy_loss: -0.008842210285365582
        total_loss: -0.009083455428481102
        vf_explained_var: 0.0011935830116271973
        vf_loss: 1.6187927722930908
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6565651893615723
        entropy_coeff: 0.0017600000137463212
        kl: 0.007054184563457966
        model: {}
        policy_loss: -0.014408230781555176
        total_loss: -0.014833114109933376
        vf_explained_var: -0.002976939082145691
        vf_loss: 0.2525262236595154
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.7045567631721497
        entropy_coeff: 0.0017600000137463212
        kl: 0.009031552821397781
        model: {}
        policy_loss: -0.020898673683404922
        total_loss: -0.021209917962551117
        vf_explained_var: 0.020018577575683594
        vf_loss: 0.25621676445007324
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.7580398917198181
        entropy_coeff: 0.0017600000137463212
        kl: 0.006771019659936428
        model: {}
        policy_loss: -0.012461431324481964
        total_loss: -0.013110236264765263
        vf_explained_var: -0.01362466812133789
        vf_loss: 0.0824313685297966
    load_time_ms: 18875.806
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 107790.294
    update_time_ms: 14.576
  iterations_since_restore: 18
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.17626262626263
    ram_util_percent: 9.251515151515152
  pid: 6435
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 9.0
    agent-5: 5.0
  policy_reward_mean:
    agent-0: 2.54
    agent-1: 1.64
    agent-2: 3.03
    agent-3: 3.13
    agent-4: 3.19
    agent-5: 1.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.948673395446022
    mean_inference_ms: 13.844462953238057
    mean_processing_ms: 65.35834790166633
  time_since_restore: 2622.3206317424774
  time_this_iter_s: 139.25438952445984
  time_total_s: 25132.603874206543
  timestamp: 1637222760
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    178 |          25132.6 | 17088000 |    14.77 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.35
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 158
    cleaning_beam_agent-0_mean: 85.41
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 221.65
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 24.69
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 96.54
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 228
    cleaning_beam_agent-4_mean: 81.69
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 16.03
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-08-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 14.96
  episode_reward_min: -96.0
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11701.235
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.589690089225769
        entropy_coeff: 0.0017600000137463212
        kl: 0.006086266599595547
        model: {}
        policy_loss: -0.011220481246709824
        total_loss: -0.011612002737820148
        vf_explained_var: 0.003420427441596985
        vf_loss: 0.3770768642425537
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00019370879454072565
        entropy: 0.5681884288787842
        entropy_coeff: 0.0017600000137463212
        kl: 0.007946019060909748
        model: {}
        policy_loss: -0.014129157178103924
        total_loss: -0.014719421043992043
        vf_explained_var: 0.006704941391944885
        vf_loss: 0.12443430721759796
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5212762355804443
        entropy_coeff: 0.0017600000137463212
        kl: 0.007712273858487606
        model: {}
        policy_loss: -0.016837995499372482
        total_loss: -0.01694563962519169
        vf_explained_var: 0.001756027340888977
        vf_loss: 0.3857448697090149
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6429742574691772
        entropy_coeff: 0.0017600000137463212
        kl: 0.006856460589915514
        model: {}
        policy_loss: -0.013901468366384506
        total_loss: -0.014319374226033688
        vf_explained_var: 0.004707783460617065
        vf_loss: 0.28084325790405273
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.7045561075210571
        entropy_coeff: 0.0017600000137463212
        kl: 0.007270541973412037
        model: {}
        policy_loss: -0.01473463885486126
        total_loss: -0.01518484577536583
        vf_explained_var: 0.020834535360336304
        vf_loss: 0.6275764107704163
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.7367312908172607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064679645001888275
        model: {}
        policy_loss: -0.012693917378783226
        total_loss: -0.013335388153791428
        vf_explained_var: 0.015016600489616394
        vf_loss: 0.08382861316204071
    load_time_ms: 18877.329
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 107470.76
    update_time_ms: 14.522
  iterations_since_restore: 19
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.988205128205127
    ram_util_percent: 9.266666666666667
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 1.97
    agent-1: 1.44
    agent-2: 3.89
    agent-3: 3.02
    agent-4: 3.32
    agent-5: 1.32
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.91957818340482
    mean_inference_ms: 13.830391810615176
    mean_processing_ms: 65.28105500799988
  time_since_restore: 2758.5814089775085
  time_this_iter_s: 136.26077723503113
  time_total_s: 25268.864651441574
  timestamp: 1637222896
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    179 |          25268.9 | 17184000 |    14.96 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.75
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.54
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.19
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 82.11
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 230.48
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 26.29
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 212
    cleaning_beam_agent-3_mean: 94.45
    cleaning_beam_agent-3_min: 45
    cleaning_beam_agent-4_max: 211
    cleaning_beam_agent-4_mean: 81.43
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 15.97
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-10-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 14.14
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11700.469
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.5861279964447021
        entropy_coeff: 0.0017600000137463212
        kl: 0.008121024817228317
        model: {}
        policy_loss: -0.01747218705713749
        total_loss: -0.017669454216957092
        vf_explained_var: 0.005506798624992371
        vf_loss: 0.22214104235172272
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018771839677356184
        entropy: 0.566124439239502
        entropy_coeff: 0.0017600000137463212
        kl: 0.007849824614822865
        model: {}
        policy_loss: -0.013702845200896263
        total_loss: -0.014296640641987324
        vf_explained_var: 0.02752549946308136
        vf_loss: 0.10094417631626129
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.5316684246063232
        entropy_coeff: 0.0017600000137463212
        kl: 0.007158615626394749
        model: {}
        policy_loss: -0.016018806025385857
        total_loss: -0.01620512455701828
        vf_explained_var: 0.006572291254997253
        vf_loss: 0.33556947112083435
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6683023571968079
        entropy_coeff: 0.0017600000137463212
        kl: 0.0065966444090008736
        model: {}
        policy_loss: -0.014188900589942932
        total_loss: -0.014682559296488762
        vf_explained_var: -0.0005105137825012207
        vf_loss: 0.22889858484268188
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.7045161724090576
        entropy_coeff: 0.0017600000137463212
        kl: 0.008190842345356941
        model: {}
        policy_loss: -0.018797678872942924
        total_loss: -0.019188622012734413
        vf_explained_var: 0.017466798424720764
        vf_loss: 0.29920822381973267
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.7218294143676758
        entropy_coeff: 0.0017600000137463212
        kl: 0.007577131036669016
        model: {}
        policy_loss: -0.012875836342573166
        total_loss: -0.0133833521977067
        vf_explained_var: 0.007494017481803894
        vf_loss: 0.05190503969788551
    load_time_ms: 18793.278
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 107462.357
    update_time_ms: 14.555
  iterations_since_restore: 20
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.82713567839196
    ram_util_percent: 9.287939698492462
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 4.0
  policy_reward_mean:
    agent-0: 2.55
    agent-1: 1.38
    agent-2: 2.97
    agent-3: 2.86
    agent-4: 3.43
    agent-5: 0.95
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.914365389859636
    mean_inference_ms: 13.826679189358252
    mean_processing_ms: 65.28820221338074
  time_since_restore: 2898.61896276474
  time_this_iter_s: 140.03755378723145
  time_total_s: 25408.902205228806
  timestamp: 1637223036
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    180 |          25408.9 | 17280000 |    14.14 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.47
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.14
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 0.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 74.83
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 210.4
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 21.47
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 234
    cleaning_beam_agent-3_mean: 104.23
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 239
    cleaning_beam_agent-4_mean: 85.73
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 14.8
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-12-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 12.11
  episode_reward_min: -89.0
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11716.421
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.582483172416687
        entropy_coeff: 0.0017600000137463212
        kl: 0.007777870632708073
        model: {}
        policy_loss: -0.01678578369319439
        total_loss: -0.01701780967414379
        vf_explained_var: 0.008263632655143738
        vf_loss: 0.1535608023405075
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018172799900639802
        entropy: 0.5628843307495117
        entropy_coeff: 0.0017600000137463212
        kl: 0.00669873645529151
        model: {}
        policy_loss: -0.012620843946933746
        total_loss: -0.013260246254503727
        vf_explained_var: 0.03399233520030975
        vf_loss: 0.16334162652492523
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.5252200365066528
        entropy_coeff: 0.0017600000137463212
        kl: 0.007555433548986912
        model: {}
        policy_loss: -0.016614753752946854
        total_loss: -0.016753964126110077
        vf_explained_var: 0.009530067443847656
        vf_loss: 0.2963320314884186
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.6470136642456055
        entropy_coeff: 0.0017600000137463212
        kl: 0.006794795393943787
        model: {}
        policy_loss: -0.014112445525825024
        total_loss: -0.014546673744916916
        vf_explained_var: -0.003327697515487671
        vf_loss: 0.2503681778907776
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.7174790501594543
        entropy_coeff: 0.0017600000137463212
        kl: 0.006179554387927055
        model: {}
        policy_loss: -0.010869530960917473
        total_loss: -0.01136982161551714
        vf_explained_var: 0.006581321358680725
        vf_loss: 1.4451537132263184
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.7496663331985474
        entropy_coeff: 0.0017600000137463212
        kl: 0.005318756215274334
        model: {}
        policy_loss: -0.006809699349105358
        total_loss: -0.0074677253141999245
        vf_explained_var: 0.0014929026365280151
        vf_loss: 1.2950832843780518
    load_time_ms: 18785.076
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 107374.877
    update_time_ms: 14.412
  iterations_since_restore: 21
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.720202020202017
    ram_util_percent: 9.136868686868688
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 10.0
    agent-2: 11.0
    agent-3: 16.0
    agent-4: 13.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.31
    agent-1: 1.21
    agent-2: 3.29
    agent-3: 2.82
    agent-4: 2.0
    agent-5: 0.48
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 25.898387634505823
    mean_inference_ms: 13.821604146944653
    mean_processing_ms: 65.27204931754328
  time_since_restore: 3036.6616384983063
  time_this_iter_s: 138.04267573356628
  time_total_s: 25546.944880962372
  timestamp: 1637223175
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    181 |          25546.9 | 17376000 |    12.11 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 1.7
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.74
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.55
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 1.99
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.09
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 72.56
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 471
    cleaning_beam_agent-1_mean: 221.25
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 186
    cleaning_beam_agent-2_mean: 23.01
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 192
    cleaning_beam_agent-3_mean: 95.26
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 206
    cleaning_beam_agent-4_mean: 89.75
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 18.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-15-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 34.0
  episode_reward_mean: 13.33
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11707.583
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.572451651096344
        entropy_coeff: 0.0017600000137463212
        kl: 0.007145551964640617
        model: {}
        policy_loss: -0.016901805996894836
        total_loss: -0.017180895432829857
        vf_explained_var: 0.009157076478004456
        vf_loss: 0.1386762112379074
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001757376012392342
        entropy: 0.5395117402076721
        entropy_coeff: 0.0017600000137463212
        kl: 0.00686641875654459
        model: {}
        policy_loss: -0.013464576564729214
        total_loss: -0.014057979919016361
        vf_explained_var: 0.01540808379650116
        vf_loss: 0.12815378606319427
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.5123587846755981
        entropy_coeff: 0.0017600000137463212
        kl: 0.006938644219189882
        model: {}
        policy_loss: -0.015853676944971085
        total_loss: -0.01602017879486084
        vf_explained_var: 0.008109644055366516
        vf_loss: 0.41386112570762634
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.6424154043197632
        entropy_coeff: 0.0017600000137463212
        kl: 0.005774619989097118
        model: {}
        policy_loss: -0.007110804785043001
        total_loss: -0.007510228082537651
        vf_explained_var: 0.005123227834701538
        vf_loss: 1.5376486778259277
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.717536211013794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064044492319226265
        model: {}
        policy_loss: -0.011169282719492912
        total_loss: -0.011632596142590046
        vf_explained_var: 0.010069385170936584
        vf_loss: 1.59105384349823
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.7510411143302917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0074178967624902725
        model: {}
        policy_loss: -0.012829604558646679
        total_loss: -0.013400602154433727
        vf_explained_var: 0.011565059423446655
        vf_loss: 0.09043879806995392
    load_time_ms: 18738.753
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 107282.732
    update_time_ms: 14.699
  iterations_since_restore: 22
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.719487179487174
    ram_util_percent: 9.28923076923077
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 8.0
    agent-4: 10.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.18
    agent-1: 1.7
    agent-2: 3.19
    agent-3: 2.37
    agent-4: 2.79
    agent-5: 1.1
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: -47.0
    agent-4: -39.0
    agent-5: -3.0
  sampler_perf:
    mean_env_wait_ms: 25.883619731180897
    mean_inference_ms: 13.812907727889126
    mean_processing_ms: 65.24037372133566
  time_since_restore: 3173.657886981964
  time_this_iter_s: 136.99624848365784
  time_total_s: 25683.94112944603
  timestamp: 1637223312
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    182 |          25683.9 | 17472000 |    13.33 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.42
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.62
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.02
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 158
    cleaning_beam_agent-0_mean: 76.92
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 430
    cleaning_beam_agent-1_mean: 221.93
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 19.99
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 99.97
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 231
    cleaning_beam_agent-4_mean: 86.35
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 14.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-17-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 16.21
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11715.193
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5679188966751099
        entropy_coeff: 0.0017600000137463212
        kl: 0.006837835535407066
        model: {}
        policy_loss: -0.015056721866130829
        total_loss: -0.01534952037036419
        vf_explained_var: 0.005876705050468445
        vf_loss: 0.22955156862735748
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.5464820861816406
        entropy_coeff: 0.0017600000137463212
        kl: 0.00663220789283514
        model: {}
        policy_loss: -0.012398270890116692
        total_loss: -0.013011312112212181
        vf_explained_var: 0.02409413456916809
        vf_loss: 0.1715731918811798
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5044316649436951
        entropy_coeff: 0.0017600000137463212
        kl: 0.007086178287863731
        model: {}
        policy_loss: -0.016116630285978317
        total_loss: -0.016258591786026955
        vf_explained_var: 0.0010593980550765991
        vf_loss: 0.37220555543899536
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.6348198652267456
        entropy_coeff: 0.0017600000137463212
        kl: 0.006821352057158947
        model: {}
        policy_loss: -0.014087101444602013
        total_loss: -0.014495160430669785
        vf_explained_var: 0.002478674054145813
        vf_loss: 0.2709004878997803
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.7234718799591064
        entropy_coeff: 0.0017600000137463212
        kl: 0.00809394009411335
        model: {}
        policy_loss: -0.020271118730306625
        total_loss: -0.02070474997162819
        vf_explained_var: 0.01087997853755951
        vf_loss: 0.30281394720077515
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.752885103225708
        entropy_coeff: 0.0017600000137463212
        kl: 0.005325770936906338
        model: {}
        policy_loss: -0.006181729957461357
        total_loss: -0.006836884189397097
        vf_explained_var: -0.0027877390384674072
        vf_loss: 1.3734612464904785
    load_time_ms: 18820.024
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 107123.246
    update_time_ms: 14.546
  iterations_since_restore: 23
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.137305699481864
    ram_util_percent: 9.29119170984456
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 11.0
    agent-4: 10.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.66
    agent-1: 1.94
    agent-2: 4.12
    agent-3: 3.11
    agent-4: 3.68
    agent-5: 0.7
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.82793499975734
    mean_inference_ms: 13.795807143239266
    mean_processing_ms: 65.10535652187414
  time_since_restore: 3308.953796863556
  time_this_iter_s: 135.2959098815918
  time_total_s: 25819.23703932762
  timestamp: 1637223447
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    183 |          25819.2 | 17568000 |    16.21 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.54
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.62
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.69
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.01
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.83
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 73.74
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 208.97
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 18.93
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 100.17
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 204
    cleaning_beam_agent-4_mean: 80.26
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 15.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-19-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 13.0
  episode_reward_min: -79.0
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11716.058
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.5568472743034363
        entropy_coeff: 0.0017600000137463212
        kl: 0.007104505784809589
        model: {}
        policy_loss: -0.01446373201906681
        total_loss: -0.014713066630065441
        vf_explained_var: 0.012962296605110168
        vf_loss: 0.2026793360710144
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.5215238332748413
        entropy_coeff: 0.0017600000137463212
        kl: 0.006456770934164524
        model: {}
        policy_loss: -0.012171290814876556
        total_loss: -0.012755335308611393
        vf_explained_var: 0.0044767409563064575
        vf_loss: 0.10999103635549545
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.5166468024253845
        entropy_coeff: 0.0017600000137463212
        kl: 0.005049367900937796
        model: {}
        policy_loss: -0.00783221609890461
        total_loss: -0.008082007989287376
        vf_explained_var: 0.007944673299789429
        vf_loss: 1.5456867218017578
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.6539738774299622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0051049706526100636
        model: {}
        policy_loss: -0.00809973943978548
        total_loss: -0.0085905147716403
        vf_explained_var: 0.0040527284145355225
        vf_loss: 1.4972097873687744
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.7203652262687683
        entropy_coeff: 0.0017600000137463212
        kl: 0.008033971302211285
        model: {}
        policy_loss: -0.018237799406051636
        total_loss: -0.018674056977033615
        vf_explained_var: 0.02955596148967743
        vf_loss: 0.28190934658050537
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.7578286528587341
        entropy_coeff: 0.0017600000137463212
        kl: 0.005720206536352634
        model: {}
        policy_loss: -0.007692883722484112
        total_loss: -0.00832400843501091
        vf_explained_var: 0.008945569396018982
        vf_loss: 1.306318998336792
    load_time_ms: 18801.435
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 106998.002
    update_time_ms: 14.49
  iterations_since_restore: 24
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.818041237113402
    ram_util_percent: 9.245876288659792
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 12.0
    agent-3: 10.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.48
    agent-1: 1.48
    agent-2: 2.84
    agent-3: 2.58
    agent-4: 3.1
    agent-5: 0.52
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -43.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 25.796971049334612
    mean_inference_ms: 13.787891391259302
    mean_processing_ms: 65.05760068060324
  time_since_restore: 3445.292993783951
  time_this_iter_s: 136.3391969203949
  time_total_s: 25955.576236248016
  timestamp: 1637223584
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    184 |          25955.6 | 17664000 |       13 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.8
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 2.35
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 1.92
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.72
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 70.24
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 506
    cleaning_beam_agent-1_mean: 216.06
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 19.69
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 224
    cleaning_beam_agent-3_mean: 96.51
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 279
    cleaning_beam_agent-4_mean: 87.08
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 15.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-22-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 15.7
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11718.039
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.567310094833374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064182281494140625
        model: {}
        policy_loss: -0.015029305592179298
        total_loss: -0.015360647812485695
        vf_explained_var: 0.004238530993461609
        vf_loss: 0.2530369460582733
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.5117796659469604
        entropy_coeff: 0.0017600000137463212
        kl: 0.006083093583583832
        model: {}
        policy_loss: -0.012751269154250622
        total_loss: -0.013337058946490288
        vf_explained_var: 0.02012917399406433
        vf_loss: 0.1078895628452301
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.5227282047271729
        entropy_coeff: 0.0017600000137463212
        kl: 0.006448620930314064
        model: {}
        policy_loss: -0.013865338638424873
        total_loss: -0.014100337401032448
        vf_explained_var: 0.00441606342792511
        vf_loss: 0.4014149010181427
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.662965714931488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062011051923036575
        model: {}
        policy_loss: -0.013637453317642212
        total_loss: -0.014160254038870335
        vf_explained_var: -0.002470076084136963
        vf_loss: 0.23908457159996033
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.7198210954666138
        entropy_coeff: 0.0017600000137463212
        kl: 0.00813075341284275
        model: {}
        policy_loss: -0.018239671364426613
        total_loss: -0.018660936504602432
        vf_explained_var: 0.03339366614818573
        vf_loss: 0.32543057203292847
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.7603464126586914
        entropy_coeff: 0.0017600000137463212
        kl: 0.006313531659543514
        model: {}
        policy_loss: -0.011861460283398628
        total_loss: -0.012560664676129818
        vf_explained_var: -0.019335448741912842
        vf_loss: 0.07652293890714645
    load_time_ms: 18786.744
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 106974.671
    update_time_ms: 14.483
  iterations_since_restore: 25
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.861734693877548
    ram_util_percent: 9.234183673469389
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 5.0
    agent-2: 22.0
    agent-3: 9.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.68
    agent-1: 1.71
    agent-2: 3.53
    agent-3: 2.99
    agent-4: 3.67
    agent-5: 1.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.78586602134833
    mean_inference_ms: 13.782420780992982
    mean_processing_ms: 65.03854654022739
  time_since_restore: 3582.498188018799
  time_this_iter_s: 137.20519423484802
  time_total_s: 26092.781430482864
  timestamp: 1637223721
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    185 |          26092.8 | 17760000 |     15.7 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.8
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.17
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.58
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 177
    cleaning_beam_agent-0_mean: 69.67
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 225.88
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 17.29
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 220
    cleaning_beam_agent-3_mean: 106.79
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 180
    cleaning_beam_agent-4_mean: 78.39
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 15.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-24-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 15.25
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11708.526
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.5634930729866028
        entropy_coeff: 0.0017600000137463212
        kl: 0.006489094812422991
        model: {}
        policy_loss: -0.014544869773089886
        total_loss: -0.014866773039102554
        vf_explained_var: 0.014184057712554932
        vf_loss: 0.209320530295372
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.5104965567588806
        entropy_coeff: 0.0017600000137463212
        kl: 0.005991722922772169
        model: {}
        policy_loss: -0.011626452207565308
        total_loss: -0.012215051800012589
        vf_explained_var: 0.02156725525856018
        vf_loss: 0.10285666584968567
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.5227636098861694
        entropy_coeff: 0.0017600000137463212
        kl: 0.006553116254508495
        model: {}
        policy_loss: -0.015495484694838524
        total_loss: -0.015721868723630905
        vf_explained_var: -0.0047692060470581055
        vf_loss: 0.38365066051483154
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.6533958911895752
        entropy_coeff: 0.0017600000137463212
        kl: 0.006220933049917221
        model: {}
        policy_loss: -0.012796706520020962
        total_loss: -0.013291106559336185
        vf_explained_var: -0.0014206618070602417
        vf_loss: 0.3348444104194641
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.7130176424980164
        entropy_coeff: 0.0017600000137463212
        kl: 0.00578355835750699
        model: {}
        policy_loss: -0.009492173790931702
        total_loss: -0.010069403797388077
        vf_explained_var: 0.01938605308532715
        vf_loss: 0.9932470917701721
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.7570908069610596
        entropy_coeff: 0.0017600000137463212
        kl: 0.006637080106884241
        model: {}
        policy_loss: -0.012217363342642784
        total_loss: -0.012876308523118496
        vf_explained_var: -0.008913278579711914
        vf_loss: 0.09830458462238312
    load_time_ms: 18806.452
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 107063.104
    update_time_ms: 14.21
  iterations_since_restore: 26
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.71212121212121
    ram_util_percent: 9.259090909090908
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 11.0
    agent-4: 11.0
    agent-5: 5.0
  policy_reward_mean:
    agent-0: 2.44
    agent-1: 1.53
    agent-2: 3.76
    agent-3: 3.23
    agent-4: 2.87
    agent-5: 1.42
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.783821293179173
    mean_inference_ms: 13.779885606180697
    mean_processing_ms: 65.04175805660613
  time_since_restore: 3721.2947788238525
  time_this_iter_s: 138.7965908050537
  time_total_s: 26231.578021287918
  timestamp: 1637223860
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    186 |          26231.6 | 17856000 |    15.25 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.66
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.32
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.83
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 0.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 77.07
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 418
    cleaning_beam_agent-1_mean: 223.93
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 22.76
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 225
    cleaning_beam_agent-3_mean: 95.8
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 218
    cleaning_beam_agent-4_mean: 78.59
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 13.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 12
    fire_beam_agent-5_mean: 0.23
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-26-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 12.13
  episode_reward_min: -242.0
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11706.848
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.5816283226013184
        entropy_coeff: 0.0017600000137463212
        kl: 0.00545222545042634
        model: {}
        policy_loss: -0.008292945101857185
        total_loss: -0.008607693947851658
        vf_explained_var: 0.0050099194049835205
        vf_loss: 1.6369271278381348
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.5143044590950012
        entropy_coeff: 0.0017600000137463212
        kl: 0.007075692992657423
        model: {}
        policy_loss: -0.007862570695579052
        total_loss: -0.008137939497828484
        vf_explained_var: 0.007980629801750183
        vf_loss: 2.76023530960083
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.5161401033401489
        entropy_coeff: 0.0017600000137463212
        kl: 0.005893736146390438
        model: {}
        policy_loss: -0.00702275475487113
        total_loss: -0.005915001034736633
        vf_explained_var: 0.0043033212423324585
        vf_loss: 14.26786994934082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.6779110431671143
        entropy_coeff: 0.0017600000137463212
        kl: 0.007249127142131329
        model: {}
        policy_loss: -0.0076883090659976006
        total_loss: -0.007475073449313641
        vf_explained_var: 0.007157757878303528
        vf_loss: 6.8144659996032715
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.7340458631515503
        entropy_coeff: 0.0017600000137463212
        kl: 0.006663578096777201
        model: {}
        policy_loss: -0.01664048619568348
        total_loss: -0.017220744863152504
        vf_explained_var: 0.026275634765625
        vf_loss: 0.45304495096206665
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.7504411339759827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0043849279172718525
        model: {}
        policy_loss: -0.007095976732671261
        total_loss: -0.0078896414488554
        vf_explained_var: 0.01420101523399353
        vf_loss: 0.8861916065216064
    load_time_ms: 18807.401
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 107030.077
    update_time_ms: 14.338
  iterations_since_restore: 27
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.32131979695431
    ram_util_percent: 9.146700507614213
  pid: 6435
  policy_reward_max:
    agent-0: 15.0
    agent-1: 6.0
    agent-2: 19.0
    agent-3: 17.0
    agent-4: 21.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.69
    agent-1: 0.88
    agent-2: 1.74
    agent-3: 2.05
    agent-4: 4.01
    agent-5: 0.76
  policy_reward_min:
    agent-0: -47.0
    agent-1: -50.0
    agent-2: -148.0
    agent-3: -95.0
    agent-4: 0.0
    agent-5: -51.0
  sampler_perf:
    mean_env_wait_ms: 25.786677827758062
    mean_inference_ms: 13.779557717307519
    mean_processing_ms: 65.04483242896653
  time_since_restore: 3859.4998779296875
  time_this_iter_s: 138.20509910583496
  time_total_s: 26369.783120393753
  timestamp: 1637223998
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    187 |          26369.8 | 17952000 |    12.13 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.26
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 2.12
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 289
    cleaning_beam_agent-0_mean: 77.17
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 517
    cleaning_beam_agent-1_mean: 237.56
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 19.26
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 234
    cleaning_beam_agent-3_mean: 104.34
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 223
    cleaning_beam_agent-4_mean: 86.66
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 13.63
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.1
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-28-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 40.0
  episode_reward_mean: 11.8
  episode_reward_min: -142.0
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11683.489
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.581449568271637
        entropy_coeff: 0.0017600000137463212
        kl: 0.006692246533930302
        model: {}
        policy_loss: -0.015919027850031853
        total_loss: -0.016251010820269585
        vf_explained_var: 0.002045154571533203
        vf_loss: 0.2214381992816925
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.5019503831863403
        entropy_coeff: 0.0017600000137463212
        kl: 0.005270523019134998
        model: {}
        policy_loss: -0.01103384979069233
        total_loss: -0.011639309115707874
        vf_explained_var: 0.02434656023979187
        vf_loss: 0.14448276162147522
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.5202149152755737
        entropy_coeff: 0.0017600000137463212
        kl: 0.006484402809292078
        model: {}
        policy_loss: -0.015453916043043137
        total_loss: -0.01568695157766342
        vf_explained_var: -0.004820704460144043
        vf_loss: 0.34103918075561523
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.6879909634590149
        entropy_coeff: 0.0017600000137463212
        kl: 0.005733531899750233
        model: {}
        policy_loss: -0.006952568888664246
        total_loss: -0.007431745529174805
        vf_explained_var: 0.0024331510066986084
        vf_loss: 1.5833497047424316
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.7332794666290283
        entropy_coeff: 0.0017600000137463212
        kl: 0.00458531966432929
        model: {}
        policy_loss: -0.007328720297664404
        total_loss: -0.007023507263511419
        vf_explained_var: 0.0034832805395126343
        vf_loss: 11.372513771057129
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.7524243593215942
        entropy_coeff: 0.0017600000137463212
        kl: 0.009757086634635925
        model: {}
        policy_loss: -0.006939485669136047
        total_loss: -0.007632337044924498
        vf_explained_var: 0.003920108079910278
        vf_loss: 1.4356093406677246
    load_time_ms: 18680.137
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 107120.467
    update_time_ms: 14.262
  iterations_since_restore: 28
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.8530303030303
    ram_util_percent: 9.25252525252525
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 10.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.74
    agent-1: 1.8
    agent-2: 2.98
    agent-3: 2.62
    agent-4: 1.92
    agent-5: -0.26
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: -48.0
    agent-4: -145.0
    agent-5: -51.0
  sampler_perf:
    mean_env_wait_ms: 25.793908287949954
    mean_inference_ms: 13.777855206162924
    mean_processing_ms: 65.05216337877256
  time_since_restore: 3998.1540699005127
  time_this_iter_s: 138.6541919708252
  time_total_s: 26508.43731236458
  timestamp: 1637224137
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    188 |          26508.4 | 18048000 |     11.8 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.65
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.03
    apples_agent-3_min: 0
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 0.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 151
    cleaning_beam_agent-0_mean: 74.06
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 237.18
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 18.08
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 190
    cleaning_beam_agent-3_mean: 87.67
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 328
    cleaning_beam_agent-4_mean: 87.85
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 16.55
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-31-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 37.0
  episode_reward_mean: 14.1
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11680.041
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.581341564655304
        entropy_coeff: 0.0017600000137463212
        kl: 0.006070246454328299
        model: {}
        policy_loss: -0.014948706142604351
        total_loss: -0.015337836928665638
        vf_explained_var: -0.0019143372774124146
        vf_loss: 0.2700667083263397
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.5260944366455078
        entropy_coeff: 0.0017600000137463212
        kl: 0.005741769913583994
        model: {}
        policy_loss: -0.007763447239995003
        total_loss: -0.008261546492576599
        vf_explained_var: 0.008164435625076294
        vf_loss: 1.407418966293335
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.5049406886100769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048001110553741455
        model: {}
        policy_loss: -0.00833153910934925
        total_loss: -0.008580368012189865
        vf_explained_var: 0.0010848939418792725
        vf_loss: 1.5985547304153442
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.6929225325584412
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050168465822935104
        model: {}
        policy_loss: -0.007665438111871481
        total_loss: -0.008230152539908886
        vf_explained_var: -0.0020025670528411865
        vf_loss: 1.5314244031906128
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.7306970357894897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0061493865214288235
        model: {}
        policy_loss: -0.010555065236985683
        total_loss: -0.01139510702341795
        vf_explained_var: -0.002695620059967041
        vf_loss: 1.3851187229156494
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.7835375666618347
        entropy_coeff: 0.0017600000137463212
        kl: 0.007054818794131279
        model: {}
        policy_loss: -0.011749237775802612
        total_loss: -0.012767421081662178
        vf_explained_var: 0.005019739270210266
        vf_loss: 0.0810089111328125
    load_time_ms: 18655.736
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 106977.983
    update_time_ms: 14.38
  iterations_since_restore: 29
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.35833333333333
    ram_util_percent: 9.288541666666667
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 11.0
    agent-3: 12.0
    agent-4: 12.0
    agent-5: 5.0
  policy_reward_mean:
    agent-0: 2.82
    agent-1: 1.07
    agent-2: 3.1
    agent-3: 2.65
    agent-4: 3.13
    agent-5: 1.33
  policy_reward_min:
    agent-0: 0.0
    agent-1: -46.0
    agent-2: -43.0
    agent-3: -46.0
    agent-4: -36.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.747890103397193
    mean_inference_ms: 13.765217935310716
    mean_processing_ms: 64.96554636901278
  time_since_restore: 4132.718255281448
  time_this_iter_s: 134.56418538093567
  time_total_s: 26643.001497745514
  timestamp: 1637224272
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    189 |            26643 | 18144000 |     14.1 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.31
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 2.34
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.65
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 69.7
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 231.96
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 76
    cleaning_beam_agent-2_mean: 19.03
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 190
    cleaning_beam_agent-3_mean: 92.49
    cleaning_beam_agent-3_min: 33
    cleaning_beam_agent-4_max: 328
    cleaning_beam_agent-4_mean: 78.66
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 14.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-33-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 12.06
  episode_reward_min: -190.0
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11685.006
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.5699234008789062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0059324949979782104
        model: {}
        policy_loss: -0.008638640865683556
        total_loss: -0.008765488862991333
        vf_explained_var: 0.005582630634307861
        vf_loss: 2.829681634902954
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.5263330936431885
        entropy_coeff: 0.0017600000137463212
        kl: 0.005602434277534485
        model: {}
        policy_loss: -0.011393141001462936
        total_loss: -0.012026568874716759
        vf_explained_var: 0.009653538465499878
        vf_loss: 0.12799672782421112
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.5214231610298157
        entropy_coeff: 0.0017600000137463212
        kl: 0.005611070431768894
        model: {}
        policy_loss: -0.007144567556679249
        total_loss: -0.007134247105568647
        vf_explained_var: 0.0023838430643081665
        vf_loss: 6.474721431732178
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.6800352334976196
        entropy_coeff: 0.0017600000137463212
        kl: 0.005671689286828041
        model: {}
        policy_loss: -0.012629838660359383
        total_loss: -0.01323356106877327
        vf_explained_var: -0.0005367100238800049
        vf_loss: 0.2596971094608307
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.7385862469673157
        entropy_coeff: 0.0017600000137463212
        kl: 0.00792002771049738
        model: {}
        policy_loss: -0.010056264698505402
        total_loss: -0.010538462549448013
        vf_explained_var: 0.0069442689418792725
        vf_loss: 4.21709680557251
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.8091781735420227
        entropy_coeff: 0.0017600000137463212
        kl: 0.007696711458265781
        model: {}
        policy_loss: -0.007104473654180765
        total_loss: -0.008002175949513912
        vf_explained_var: 0.005637511610984802
        vf_loss: 1.4161601066589355
    load_time_ms: 18645.848
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 106797.153
    update_time_ms: 14.437
  iterations_since_restore: 30
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.450761421319797
    ram_util_percent: 9.263451776649745
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 11.0
    agent-3: 11.0
    agent-4: 9.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 1.95
    agent-1: 1.62
    agent-2: 2.48
    agent-3: 3.27
    agent-4: 1.92
    agent-5: 0.82
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: -97.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.733704972759472
    mean_inference_ms: 13.759600172336556
    mean_processing_ms: 64.93858567589163
  time_since_restore: 4270.841574192047
  time_this_iter_s: 138.12331891059875
  time_total_s: 26781.124816656113
  timestamp: 1637224410
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    190 |          26781.1 | 18240000 |    12.06 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.13
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 151
    cleaning_beam_agent-0_mean: 72.4
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 231.8
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 21.64
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 223
    cleaning_beam_agent-3_mean: 92.12
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 229
    cleaning_beam_agent-4_mean: 74.03
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 15.38
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-35-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 15.57
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11681.149
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.5740013718605042
        entropy_coeff: 0.0017600000137463212
        kl: 0.006439203396439552
        model: {}
        policy_loss: -0.014964310452342033
        total_loss: -0.015309457667171955
        vf_explained_var: -0.004506811499595642
        vf_loss: 0.21173372864723206
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.5070250034332275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034003094770014286
        model: {}
        policy_loss: -0.0043619172647595406
        total_loss: -0.00493851350620389
        vf_explained_var: 0.003998413681983948
        vf_loss: 1.457507610321045
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.5204050540924072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064080748707056046
        model: {}
        policy_loss: -0.013985030353069305
        total_loss: -0.014548616483807564
        vf_explained_var: 0.00254608690738678
        vf_loss: 0.3192058205604553
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.6760475039482117
        entropy_coeff: 0.0017600000137463212
        kl: 0.005476573947817087
        model: {}
        policy_loss: -0.012538693845272064
        total_loss: -0.013158184476196766
        vf_explained_var: -0.006004840135574341
        vf_loss: 0.22694765031337738
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.7468128800392151
        entropy_coeff: 0.0017600000137463212
        kl: 0.005578445270657539
        model: {}
        policy_loss: -0.010159007273614407
        total_loss: -0.011032504960894585
        vf_explained_var: -0.00015281140804290771
        vf_loss: 1.6196950674057007
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.7981717586517334
        entropy_coeff: 0.0017600000137463212
        kl: 0.006635210011154413
        model: {}
        policy_loss: -0.010368180461227894
        total_loss: -0.011426434852182865
        vf_explained_var: -0.004341930150985718
        vf_loss: 0.14764031767845154
    load_time_ms: 18590.739
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 106888.219
    update_time_ms: 14.502
  iterations_since_restore: 31
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.93383838383838
    ram_util_percent: 9.115151515151513
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 11.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.76
    agent-1: 1.27
    agent-2: 3.74
    agent-3: 3.03
    agent-4: 3.16
    agent-5: 1.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.73115028852255
    mean_inference_ms: 13.758474659556423
    mean_processing_ms: 64.9382773489901
  time_since_restore: 4409.290512800217
  time_this_iter_s: 138.44893860816956
  time_total_s: 26919.573755264282
  timestamp: 1637224549
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    191 |          26919.6 | 18336000 |    15.57 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.78
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 1.78
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 72.31
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 217.45
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 18.46
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 88.12
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 173
    cleaning_beam_agent-4_mean: 79.7
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 17.76
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-38-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 12.67
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11688.786
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.5827966928482056
        entropy_coeff: 0.0017600000137463212
        kl: 0.00586257129907608
        model: {}
        policy_loss: -0.014719180762767792
        total_loss: -0.015134280547499657
        vf_explained_var: -0.0018382817506790161
        vf_loss: 0.24362093210220337
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00011583360173972324
        entropy: 0.5191307067871094
        entropy_coeff: 0.0017600000137463212
        kl: 0.005772060714662075
        model: {}
        policy_loss: -0.005098182708024979
        total_loss: -0.005600308533757925
        vf_explained_var: 0.0017331838607788086
        vf_loss: 2.6724228858947754
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.5258340835571289
        entropy_coeff: 0.0017600000137463212
        kl: 0.005784850567579269
        model: {}
        policy_loss: -0.008696028031408787
        total_loss: -0.009096837602555752
        vf_explained_var: -0.0019478201866149902
        vf_loss: 2.3541555404663086
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.6682931184768677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054304590448737144
        model: {}
        policy_loss: -0.012049662880599499
        total_loss: -0.012660281732678413
        vf_explained_var: -0.005876749753952026
        vf_loss: 0.22528520226478577
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.7635720372200012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062517765909433365
        model: {}
        policy_loss: -0.0119934668764472
        total_loss: -0.012976432219147682
        vf_explained_var: 0.009158521890640259
        vf_loss: 0.48330387473106384
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.8069540858268738
        entropy_coeff: 0.0017600000137463212
        kl: 0.008546676486730576
        model: {}
        policy_loss: -0.007352589629590511
        total_loss: -0.008207608014345169
        vf_explained_var: 0.0015057772397994995
        vf_loss: 1.3788723945617676
    load_time_ms: 18569.441
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 106986.282
    update_time_ms: 14.244
  iterations_since_restore: 32
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.24438775510204
    ram_util_percent: 9.30408163265306
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.11
    agent-1: 0.05
    agent-2: 2.55
    agent-3: 2.87
    agent-4: 3.27
    agent-5: 0.82
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: -41.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 25.724999442853914
    mean_inference_ms: 13.755881718969595
    mean_processing_ms: 64.93083764283303
  time_since_restore: 4547.127265691757
  time_this_iter_s: 137.83675289154053
  time_total_s: 27057.410508155823
  timestamp: 1637224687
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    192 |          27057.4 | 18432000 |    12.67 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 3.22
    apples_agent-2_min: 0
    apples_agent-3_max: 37
    apples_agent-3_mean: 2.61
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 4
    apples_agent-5_mean: 0.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 207
    cleaning_beam_agent-0_mean: 73.96
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 217.78
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 153
    cleaning_beam_agent-2_mean: 24.36
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 83.48
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 241
    cleaning_beam_agent-4_mean: 76.06
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 14.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-40-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 74.0
  episode_reward_mean: 15.59
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11680.184
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.5730725526809692
        entropy_coeff: 0.0017600000137463212
        kl: 0.005206005647778511
        model: {}
        policy_loss: -0.008572114631533623
        total_loss: -0.008905915543437004
        vf_explained_var: 0.005984082818031311
        vf_loss: 1.542061448097229
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00010984319669660181
        entropy: 0.5261589288711548
        entropy_coeff: 0.0017600000137463212
        kl: 0.005009530112147331
        model: {}
        policy_loss: -0.008974205702543259
        total_loss: -0.009755216538906097
        vf_explained_var: 0.0033310353755950928
        vf_loss: 0.19795674085617065
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.5153568983078003
        entropy_coeff: 0.0017600000137463212
        kl: 0.004895347636193037
        model: {}
        policy_loss: -0.006540507078170776
        total_loss: -0.007032991386950016
        vf_explained_var: -0.0008878111839294434
        vf_loss: 1.6977362632751465
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.6530631184577942
        entropy_coeff: 0.0017600000137463212
        kl: 0.004852352198213339
        model: {}
        policy_loss: -0.009864944033324718
        total_loss: -0.010490160435438156
        vf_explained_var: -0.0013299286365509033
        vf_loss: 0.3894287943840027
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.7634103298187256
        entropy_coeff: 0.0017600000137463212
        kl: 0.007224486209452152
        model: {}
        policy_loss: -0.016217660158872604
        total_loss: -0.017171408981084824
        vf_explained_var: 0.032099127769470215
        vf_loss: 0.2862567603588104
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.7847943305969238
        entropy_coeff: 0.0017600000137463212
        kl: 0.006045442074537277
        model: {}
        policy_loss: -0.010291844606399536
        total_loss: -0.011360889300704002
        vf_explained_var: -0.020716428756713867
        vf_loss: 0.0992138534784317
    load_time_ms: 18497.239
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 107222.011
    update_time_ms: 14.315
  iterations_since_restore: 33
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.338974358974355
    ram_util_percent: 9.208205128205128
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 14.0
    agent-2: 21.0
    agent-3: 19.0
    agent-4: 19.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.31
    agent-1: 1.74
    agent-2: 3.52
    agent-3: 3.48
    agent-4: 3.26
    agent-5: 1.28
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.713490803140648
    mean_inference_ms: 13.753404069154854
    mean_processing_ms: 64.92276705075834
  time_since_restore: 4683.9742176532745
  time_this_iter_s: 136.84695196151733
  time_total_s: 27194.25746011734
  timestamp: 1637224824
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    193 |          27194.3 | 18528000 |    15.59 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 2.4
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.62
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 135
    cleaning_beam_agent-0_mean: 73.05
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 220.58
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 19.84
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 76.82
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 209
    cleaning_beam_agent-4_mean: 86.4
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 17.36
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-42-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 15.81
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11684.12
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.5687995553016663
        entropy_coeff: 0.0017600000137463212
        kl: 0.005563461221754551
        model: {}
        policy_loss: -0.01337395329028368
        total_loss: -0.013791590929031372
        vf_explained_var: -0.00485701858997345
        vf_loss: 0.27102363109588623
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000103852798929438
        entropy: 0.5343550443649292
        entropy_coeff: 0.0017600000137463212
        kl: 0.005563674494624138
        model: {}
        policy_loss: -0.005725808907300234
        total_loss: -0.0059984587132930756
        vf_explained_var: 0.004502907395362854
        vf_loss: 5.2872633934021
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000103852798929438
        entropy: 0.4969820976257324
        entropy_coeff: 0.0017600000137463212
        kl: 0.005975908134132624
        model: {}
        policy_loss: -0.008270434103906155
        total_loss: -0.008821490220725536
        vf_explained_var: 0.006101861596107483
        vf_loss: 1.7423619031906128
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.6625245213508606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054361168295145035
        model: {}
        policy_loss: -0.0118844173848629
        total_loss: -0.012748561799526215
        vf_explained_var: -0.0010525137186050415
        vf_loss: 0.3009285032749176
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.7323534488677979
        entropy_coeff: 0.0017600000137463212
        kl: 0.006627473048865795
        model: {}
        policy_loss: -0.015856094658374786
        total_loss: -0.016778787598013878
        vf_explained_var: 0.01950646936893463
        vf_loss: 0.3487607538700104
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.8098004460334778
        entropy_coeff: 0.0017600000137463212
        kl: 0.006156259682029486
        model: {}
        policy_loss: -0.011280721053481102
        total_loss: -0.012386994436383247
        vf_explained_var: 0.01385912299156189
        vf_loss: 0.11165716499090195
    load_time_ms: 18472.375
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 107360.767
    update_time_ms: 14.352
  iterations_since_restore: 34
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.982653061224493
    ram_util_percent: 9.179591836734694
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 5.0
    agent-2: 14.0
    agent-3: 23.0
    agent-4: 10.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.84
    agent-1: 0.54
    agent-2: 3.48
    agent-3: 3.36
    agent-4: 4.01
    agent-5: 1.58
  policy_reward_min:
    agent-0: -47.0
    agent-1: -98.0
    agent-2: -46.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 25.711102765377788
    mean_inference_ms: 13.752642198930305
    mean_processing_ms: 64.92251607774939
  time_since_restore: 4821.500392436981
  time_this_iter_s: 137.52617478370667
  time_total_s: 27331.783634901047
  timestamp: 1637224962
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    194 |          27331.8 | 18624000 |    15.81 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.52
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 206
    cleaning_beam_agent-0_mean: 78.38
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 233.31
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 23.94
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 173
    cleaning_beam_agent-3_mean: 78.97
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 271
    cleaning_beam_agent-4_mean: 77.78
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 16.63
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-45-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 19.2
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11687.609
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.5672416090965271
        entropy_coeff: 0.0017600000137463212
        kl: 0.005435307044535875
        model: {}
        policy_loss: -0.013634837232530117
        total_loss: -0.014066429808735847
        vf_explained_var: 0.012291967868804932
        vf_loss: 0.23221158981323242
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.786240116227418e-05
        entropy: 0.5280239582061768
        entropy_coeff: 0.0017600000137463212
        kl: 0.005151644349098206
        model: {}
        policy_loss: -0.010578273795545101
        total_loss: -0.011364534497261047
        vf_explained_var: -0.008464068174362183
        vf_loss: 0.1426917314529419
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.786240116227418e-05
        entropy: 0.5281069874763489
        entropy_coeff: 0.0017600000137463212
        kl: 0.005702441558241844
        model: {}
        policy_loss: -0.013509437441825867
        total_loss: -0.014253819361329079
        vf_explained_var: 0.0026189833879470825
        vf_loss: 0.42522838711738586
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.6687978506088257
        entropy_coeff: 0.0017600000137463212
        kl: 0.005134145729243755
        model: {}
        policy_loss: -0.01061495766043663
        total_loss: -0.011504006572067738
        vf_explained_var: 0.005935952067375183
        vf_loss: 0.31331056356430054
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.7390335202217102
        entropy_coeff: 0.0017600000137463212
        kl: 0.007230271585285664
        model: {}
        policy_loss: -0.015013608150184155
        total_loss: -0.015915028750896454
        vf_explained_var: 0.0212552547454834
        vf_loss: 0.3776446580886841
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.7928344011306763
        entropy_coeff: 0.0017600000137463212
        kl: 0.005798179190605879
        model: {}
        policy_loss: -0.009410480037331581
        total_loss: -0.010500248521566391
        vf_explained_var: 0.006732285022735596
        vf_loss: 0.15711507201194763
    load_time_ms: 18512.794
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 107456.176
    update_time_ms: 14.353
  iterations_since_restore: 35
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.373737373737374
    ram_util_percent: 9.312121212121212
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.13
    agent-1: 1.81
    agent-2: 4.56
    agent-3: 3.66
    agent-4: 4.34
    agent-5: 1.7
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.70653043649281
    mean_inference_ms: 13.755665372305463
    mean_processing_ms: 64.92447818375989
  time_since_restore: 4960.106501579285
  time_this_iter_s: 138.60610914230347
  time_total_s: 27470.38974404335
  timestamp: 1637225101
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    195 |          27470.4 | 18720000 |     19.2 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.61
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.62
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 232
    cleaning_beam_agent-0_mean: 84.55
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 234.87
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 26.09
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 301
    cleaning_beam_agent-3_mean: 80.35
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 232
    cleaning_beam_agent-4_mean: 78.59
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 13.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-47-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 17.56
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11682.161
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 0.5779139399528503
        entropy_coeff: 0.0017600000137463212
        kl: 0.003618948394432664
        model: {}
        policy_loss: -0.007016204763203859
        total_loss: -0.0075127254240214825
        vf_explained_var: 0.007131755352020264
        vf_loss: 1.587106466293335
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.5244919061660767
        entropy_coeff: 0.0017600000137463212
        kl: 0.004726185463368893
        model: {}
        policy_loss: -0.009907431900501251
        total_loss: -0.01069914922118187
        vf_explained_var: 0.019097432494163513
        vf_loss: 0.1323353350162506
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.5380902886390686
        entropy_coeff: 0.0017600000137463212
        kl: 0.005763724911957979
        model: {}
        policy_loss: -0.012434592470526695
        total_loss: -0.013192440383136272
        vf_explained_var: -0.0010747909545898438
        vf_loss: 0.4510127305984497
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.6428989171981812
        entropy_coeff: 0.0017600000137463212
        kl: 0.004948691464960575
        model: {}
        policy_loss: -0.009232382290065289
        total_loss: -0.01006382517516613
        vf_explained_var: 0.0044999271631240845
        vf_loss: 0.5262163281440735
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.7351638078689575
        entropy_coeff: 0.0017600000137463212
        kl: 0.006181548815220594
        model: {}
        policy_loss: -0.009152895770967007
        total_loss: -0.009966913610696793
        vf_explained_var: 0.010516107082366943
        vf_loss: 1.7078967094421387
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.791685163974762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0051478310488164425
        model: {}
        policy_loss: -0.009258279576897621
        total_loss: -0.010378694161772728
        vf_explained_var: 0.0025016069412231445
        vf_loss: 0.1556340456008911
    load_time_ms: 18447.359
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 107519.143
    update_time_ms: 14.61
  iterations_since_restore: 36
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.4489898989899
    ram_util_percent: 9.171717171717171
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 25.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.67
    agent-1: 1.68
    agent-2: 3.82
    agent-3: 3.87
    agent-4: 3.67
    agent-5: 1.85
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -44.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.70895421597105
    mean_inference_ms: 13.755792004278023
    mean_processing_ms: 64.92711974538115
  time_since_restore: 5098.827734231949
  time_this_iter_s: 138.72123265266418
  time_total_s: 27609.110976696014
  timestamp: 1637225240
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    196 |          27609.1 | 18816000 |    17.56 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.65
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.21
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 186
    cleaning_beam_agent-0_mean: 76.54
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 562
    cleaning_beam_agent-1_mean: 242.38
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 93
    cleaning_beam_agent-2_mean: 28.07
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 80.17
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 337
    cleaning_beam_agent-4_mean: 84.03
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 13.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-49-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 15.63
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11687.676
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.5714591145515442
        entropy_coeff: 0.0017600000137463212
        kl: 0.005260700359940529
        model: {}
        policy_loss: -0.012476017698645592
        total_loss: -0.013196349143981934
        vf_explained_var: -0.0018669664859771729
        vf_loss: 0.22402715682983398
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 8.588159835198894e-05
        entropy: 0.5371990203857422
        entropy_coeff: 0.0017600000137463212
        kl: 0.004126332700252533
        model: {}
        policy_loss: -0.00438398402184248
        total_loss: -0.005135115701705217
        vf_explained_var: 0.0025871843099594116
        vf_loss: 1.4275898933410645
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.5497317314147949
        entropy_coeff: 0.0017600000137463212
        kl: 0.005886354949325323
        model: {}
        policy_loss: -0.012823618948459625
        total_loss: -0.013607527129352093
        vf_explained_var: 0.0023492425680160522
        vf_loss: 0.3646160364151001
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.6316115856170654
        entropy_coeff: 0.0017600000137463212
        kl: 0.004825770854949951
        model: {}
        policy_loss: -0.008176186122000217
        total_loss: -0.009124331176280975
        vf_explained_var: 0.005276620388031006
        vf_loss: 0.42844173312187195
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.7431324124336243
        entropy_coeff: 0.0017600000137463212
        kl: 0.005898297764360905
        model: {}
        policy_loss: -0.014031494036316872
        total_loss: -0.015008790418505669
        vf_explained_var: 0.021702855825424194
        vf_loss: 0.3569892644882202
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.8206005096435547
        entropy_coeff: 0.0017600000137463212
        kl: 0.006336605176329613
        model: {}
        policy_loss: -0.009876182302832603
        total_loss: -0.010989222675561905
        vf_explained_var: 0.014678627252578735
        vf_loss: 0.14386393129825592
    load_time_ms: 18297.837
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 107190.562
    update_time_ms: 14.572
  iterations_since_restore: 37
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.009473684210526
    ram_util_percent: 9.311052631578947
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.95
    agent-1: 1.17
    agent-2: 3.88
    agent-3: 2.71
    agent-4: 3.26
    agent-5: 1.66
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.686918168407473
    mean_inference_ms: 13.748366047210546
    mean_processing_ms: 64.86913384857469
  time_since_restore: 5232.278088092804
  time_this_iter_s: 133.4503538608551
  time_total_s: 27742.56133055687
  timestamp: 1637225373
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    197 |          27742.6 | 18912000 |    15.63 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.21
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 79.1
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 524
    cleaning_beam_agent-1_mean: 240.4
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 125
    cleaning_beam_agent-2_mean: 25.19
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 73.84
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 310
    cleaning_beam_agent-4_mean: 85.94
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 13.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-51-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 17.53
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11698.265
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.989120058482513e-05
        entropy: 0.5772812962532043
        entropy_coeff: 0.0017600000137463212
        kl: 0.005003759637475014
        model: {}
        policy_loss: -0.011194928549230099
        total_loss: -0.011931188404560089
        vf_explained_var: 0.011299893260002136
        vf_loss: 0.29568347334861755
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.989120058482513e-05
        entropy: 0.53525710105896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0035390034317970276
        model: {}
        policy_loss: -0.004789405968040228
        total_loss: -0.005561890080571175
        vf_explained_var: 0.017180174589157104
        vf_loss: 1.4745053052902222
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.54313725233078
        entropy_coeff: 0.0017600000137463212
        kl: 0.008394554257392883
        model: {}
        policy_loss: -0.009695264510810375
        total_loss: -0.010259488597512245
        vf_explained_var: 0.0015666782855987549
        vf_loss: 1.8183354139328003
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.989120058482513e-05
        entropy: 0.6280061602592468
        entropy_coeff: 0.0017600000137463212
        kl: 0.004840205889195204
        model: {}
        policy_loss: -0.008855348452925682
        total_loss: -0.0098528191447258
        vf_explained_var: 0.0014664530754089355
        vf_loss: 0.47318220138549805
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.989120058482513e-05
        entropy: 0.7389468550682068
        entropy_coeff: 0.0017600000137463212
        kl: 0.006196234840899706
        model: {}
        policy_loss: -0.013768035918474197
        total_loss: -0.014727132394909859
        vf_explained_var: 0.031619369983673096
        vf_loss: 0.3163832426071167
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.989120058482513e-05
        entropy: 0.8006547093391418
        entropy_coeff: 0.0017600000137463212
        kl: 0.004409932531416416
        model: {}
        policy_loss: -0.008208198472857475
        total_loss: -0.009382389485836029
        vf_explained_var: 0.005748286843299866
        vf_loss: 0.14465078711509705
    load_time_ms: 18289.352
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 107065.743
    update_time_ms: 14.725
  iterations_since_restore: 38
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.292346938775506
    ram_util_percent: 9.233673469387753
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 12.0
    agent-2: 21.0
    agent-3: 23.0
    agent-4: 11.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.13
    agent-1: 1.36
    agent-2: 3.88
    agent-3: 3.89
    agent-4: 3.49
    agent-5: 1.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -50.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.679676281279615
    mean_inference_ms: 13.743526787984747
    mean_processing_ms: 64.8525319674409
  time_since_restore: 5369.701452970505
  time_this_iter_s: 137.4233648777008
  time_total_s: 27879.98469543457
  timestamp: 1637225511
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    198 |            27880 | 19008000 |    17.53 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.67
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 55
    apples_agent-3_mean: 3.29
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 0.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 74.4
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 524
    cleaning_beam_agent-1_mean: 227.48
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 22.95
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 195
    cleaning_beam_agent-3_mean: 84.29
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 262
    cleaning_beam_agent-4_mean: 83.1
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 13.85
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-54-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 16.12
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11691.805
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.576367974281311
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045248642563819885
        model: {}
        policy_loss: -0.01041087880730629
        total_loss: -0.011163746938109398
        vf_explained_var: 0.0036508142948150635
        vf_loss: 0.35297897458076477
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 7.390080281766132e-05
        entropy: 0.5367887616157532
        entropy_coeff: 0.0017600000137463212
        kl: 0.006199005991220474
        model: {}
        policy_loss: -0.005855953320860863
        total_loss: -0.0065131476148962975
        vf_explained_var: 0.001025080680847168
        vf_loss: 2.6818408966064453
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.5397579669952393
        entropy_coeff: 0.0017600000137463212
        kl: 0.00523816142231226
        model: {}
        policy_loss: -0.011225031688809395
        total_loss: -0.012009471654891968
        vf_explained_var: 0.000748857855796814
        vf_loss: 0.34579968452453613
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.390080281766132e-05
        entropy: 0.6250120997428894
        entropy_coeff: 0.0017600000137463212
        kl: 0.005188336130231619
        model: {}
        policy_loss: -0.00995635986328125
        total_loss: -0.010993784293532372
        vf_explained_var: -0.0010947585105895996
        vf_loss: 0.3017027676105499
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.7325682640075684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0055612968280911446
        model: {}
        policy_loss: -0.013069890439510345
        total_loss: -0.014047643169760704
        vf_explained_var: 0.01811838150024414
        vf_loss: 0.33505794405937195
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.8219109773635864
        entropy_coeff: 0.0017600000137463212
        kl: 0.005830162204802036
        model: {}
        policy_loss: -0.005281725898385048
        total_loss: -0.0064458297565579414
        vf_explained_var: 0.0020441412925720215
        vf_loss: 1.3670648336410522
    load_time_ms: 18379.947
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 107352.63
    update_time_ms: 14.73
  iterations_since_restore: 39
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.69593908629442
    ram_util_percent: 9.258883248730964
  pid: 6435
  policy_reward_max:
    agent-0: 19.0
    agent-1: 6.0
    agent-2: 17.0
    agent-3: 13.0
    agent-4: 11.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.28
    agent-1: 0.66
    agent-2: 3.6
    agent-3: 3.67
    agent-4: 4.11
    agent-5: 0.8
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.67786793334842
    mean_inference_ms: 13.741432575114452
    mean_processing_ms: 64.84405187931108
  time_since_restore: 5508.016305208206
  time_this_iter_s: 138.31485223770142
  time_total_s: 28018.29954767227
  timestamp: 1637225649
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    199 |          28018.3 | 19104000 |    16.12 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.18
    apples_agent-3_min: 0
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 70.12
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 249.58
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 125
    cleaning_beam_agent-2_mean: 22.33
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 198
    cleaning_beam_agent-3_mean: 80.54
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 237
    cleaning_beam_agent-4_mean: 86.01
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 15.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-56-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 18.52
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11687.008
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.791039777453989e-05
        entropy: 0.5705325603485107
        entropy_coeff: 0.0017600000137463212
        kl: 0.004977978765964508
        model: {}
        policy_loss: -0.011259377002716064
        total_loss: -0.012115566059947014
        vf_explained_var: -0.004541993141174316
        vf_loss: 0.23497670888900757
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.791039777453989e-05
        entropy: 0.5673207640647888
        entropy_coeff: 0.0017600000137463212
        kl: 0.004662002436816692
        model: {}
        policy_loss: -0.009429403580725193
        total_loss: -0.010399390943348408
        vf_explained_var: 0.005299314856529236
        vf_loss: 0.1392982304096222
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.791039777453989e-05
        entropy: 0.5371967554092407
        entropy_coeff: 0.0017600000137463212
        kl: 0.005163457244634628
        model: {}
        policy_loss: -0.012115946039557457
        total_loss: -0.012886859476566315
        vf_explained_var: 0.004881441593170166
        vf_loss: 0.4546951949596405
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.791039777453989e-05
        entropy: 0.6203819513320923
        entropy_coeff: 0.0017600000137463212
        kl: 0.005021653603762388
        model: {}
        policy_loss: -0.008667198941111565
        total_loss: -0.009692700579762459
        vf_explained_var: -0.002447843551635742
        vf_loss: 0.3498839735984802
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.7408376932144165
        entropy_coeff: 0.0017600000137463212
        kl: 0.005487313959747553
        model: {}
        policy_loss: -0.012669000774621964
        total_loss: -0.013659270480275154
        vf_explained_var: 0.02810010313987732
        vf_loss: 0.39242807030677795
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.791039777453989e-05
        entropy: 0.8417720198631287
        entropy_coeff: 0.0017600000137463212
        kl: 0.004769844003021717
        model: {}
        policy_loss: -0.008490747772157192
        total_loss: -0.009837516583502293
        vf_explained_var: 0.0023410767316818237
        vf_loss: 0.15504944324493408
    load_time_ms: 18462.907
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 107310.349
    update_time_ms: 14.584
  iterations_since_restore: 40
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.95959595959596
    ram_util_percent: 9.25151515151515
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.02
    agent-1: 1.72
    agent-2: 4.27
    agent-3: 3.61
    agent-4: 4.04
    agent-5: 1.86
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.67188914908487
    mean_inference_ms: 13.736600135756522
    mean_processing_ms: 64.82664276824174
  time_since_restore: 5646.493205308914
  time_this_iter_s: 138.476900100708
  time_total_s: 28156.77644777298
  timestamp: 1637225788
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    200 |          28156.8 | 19200000 |    18.52 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.45
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.34
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 187
    cleaning_beam_agent-0_mean: 78.53
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 457
    cleaning_beam_agent-1_mean: 235.35
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 106
    cleaning_beam_agent-2_mean: 19.04
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 82.5
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 240
    cleaning_beam_agent-4_mean: 77.53
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 19.26
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-58-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 16.24
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11688.166
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 6.192000000737607e-05
        entropy: 0.5780805349349976
        entropy_coeff: 0.0017600000137463212
        kl: 0.005228511989116669
        model: {}
        policy_loss: -0.010942360386252403
        total_loss: -0.011869492009282112
        vf_explained_var: -0.006140530109405518
        vf_loss: 0.24933373928070068
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 6.192000000737607e-05
        entropy: 0.5531240701675415
        entropy_coeff: 0.0017600000137463212
        kl: 0.004012223798781633
        model: {}
        policy_loss: -0.00871354527771473
        total_loss: -0.00966951809823513
        vf_explained_var: 0.011267155408859253
        vf_loss: 0.11257036030292511
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.536095142364502
        entropy_coeff: 0.0017600000137463212
        kl: 0.004899037070572376
        model: {}
        policy_loss: -0.011088402941823006
        total_loss: -0.011872361414134502
        vf_explained_var: 0.0075685083866119385
        vf_loss: 0.37092605233192444
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.192000000737607e-05
        entropy: 0.623682975769043
        entropy_coeff: 0.0017600000137463212
        kl: 0.004649279173463583
        model: {}
        policy_loss: -0.005959399975836277
        total_loss: -0.0064087831415236
        vf_explained_var: 0.002973198890686035
        vf_loss: 6.192415237426758
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 0.751039981842041
        entropy_coeff: 0.0017600000137463212
        kl: 0.005393108353018761
        model: {}
        policy_loss: -0.012144616805016994
        total_loss: -0.01315966434776783
        vf_explained_var: 0.017433688044548035
        vf_loss: 0.37126779556274414
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 6.192000000737607e-05
        entropy: 0.8359245657920837
        entropy_coeff: 0.0017600000137463212
        kl: 0.005337285343557596
        model: {}
        policy_loss: -0.008048506453633308
        total_loss: -0.00944170169532299
        vf_explained_var: 0.00011837482452392578
        vf_loss: 0.11315184831619263
    load_time_ms: 18579.546
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 107068.069
    update_time_ms: 14.532
  iterations_since_restore: 41
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.631122448979596
    ram_util_percent: 9.317857142857143
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.17
    agent-1: 1.63
    agent-2: 4.0
    agent-3: 1.65
    agent-4: 4.12
    agent-5: 1.67
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -94.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.6629103089996
    mean_inference_ms: 13.733002192380306
    mean_processing_ms: 64.80660116257705
  time_since_restore: 5783.609593153
  time_this_iter_s: 137.1163878440857
  time_total_s: 28293.892835617065
  timestamp: 1637225925
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    201 |          28293.9 | 19296000 |    16.24 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.29
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.56
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 0.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 78.94
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 244.47
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 20.55
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 91.31
    cleaning_beam_agent-3_min: 37
    cleaning_beam_agent-4_max: 274
    cleaning_beam_agent-4_mean: 82.81
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 82
    cleaning_beam_agent-5_mean: 19.79
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-01-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 18.5
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11686.064
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5727370381355286
        entropy_coeff: 0.0017600000137463212
        kl: 0.004106628242880106
        model: {}
        policy_loss: -0.010025705210864544
        total_loss: -0.010960154235363007
        vf_explained_var: 0.0038249343633651733
        vf_loss: 0.22239205241203308
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5638679265975952
        entropy_coeff: 0.0017600000137463212
        kl: 0.003759157843887806
        model: {}
        policy_loss: -0.007206609472632408
        total_loss: -0.008178195916116238
        vf_explained_var: 0.018007665872573853
        vf_loss: 0.17885830998420715
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5178611874580383
        entropy_coeff: 0.0017600000137463212
        kl: 0.004323518369346857
        model: {}
        policy_loss: -0.010616406798362732
        total_loss: -0.011437252163887024
        vf_explained_var: 0.004680708050727844
        vf_loss: 0.36548981070518494
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 5.5929598602233455e-05
        entropy: 0.6186470985412598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0041091544553637505
        model: {}
        policy_loss: -0.007862571626901627
        total_loss: -0.008901193737983704
        vf_explained_var: -0.009221643209457397
        vf_loss: 0.3735707402229309
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 0.7366382479667664
        entropy_coeff: 0.0017600000137463212
        kl: 0.004467045422643423
        model: {}
        policy_loss: -0.011219054460525513
        total_loss: -0.012256370857357979
        vf_explained_var: 0.02268281579017639
        vf_loss: 0.3581550717353821
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.8455295562744141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045716227032244205
        model: {}
        policy_loss: -0.007520601153373718
        total_loss: -0.008939038030803204
        vf_explained_var: 0.008053585886955261
        vf_loss: 0.12548843026161194
    load_time_ms: 18647.489
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 107016.337
    update_time_ms: 14.601
  iterations_since_restore: 42
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.155837563451776
    ram_util_percent: 9.274111675126903
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 13.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.89
    agent-1: 1.83
    agent-2: 4.16
    agent-3: 3.71
    agent-4: 4.15
    agent-5: 1.76
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.658657008116833
    mean_inference_ms: 13.731396981358525
    mean_processing_ms: 64.78540570308341
  time_since_restore: 5921.639234304428
  time_this_iter_s: 138.02964115142822
  time_total_s: 28431.922476768494
  timestamp: 1637226064
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    202 |          28431.9 | 19392000 |     18.5 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 1.94
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 2.6
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.98
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 74.43
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 234.47
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 18.8
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 84.94
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 227
    cleaning_beam_agent-4_mean: 82.63
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 16.23
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-03-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 16.96
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11696.293
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.562580943107605
        entropy_coeff: 0.0017600000137463212
        kl: 0.004506873898208141
        model: {}
        policy_loss: -0.010016348212957382
        total_loss: -0.010952847078442574
        vf_explained_var: -0.00032107532024383545
        vf_loss: 0.254770964384079
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.993920083506964e-05
        entropy: 0.5549030303955078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037375648971647024
        model: {}
        policy_loss: -0.0075845057144761086
        total_loss: -0.0085475854575634
        vf_explained_var: -0.009998500347137451
        vf_loss: 0.120878666639328
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.5248656272888184
        entropy_coeff: 0.0017600000137463212
        kl: 0.004044953268021345
        model: {}
        policy_loss: -0.00917111337184906
        total_loss: -0.010016705840826035
        vf_explained_var: -0.007351696491241455
        vf_loss: 0.528907060623169
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 4.993920083506964e-05
        entropy: 0.6218653917312622
        entropy_coeff: 0.0017600000137463212
        kl: 0.004324711859226227
        model: {}
        policy_loss: -0.008093992248177528
        total_loss: -0.00915538426488638
        vf_explained_var: -0.008526593446731567
        vf_loss: 0.2633347511291504
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 0.7311893105506897
        entropy_coeff: 0.0017600000137463212
        kl: 0.004414723254740238
        model: {}
        policy_loss: -0.010119360871613026
        total_loss: -0.011260030791163445
        vf_explained_var: 0.020583555102348328
        vf_loss: 0.3585374355316162
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.8497060537338257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0047538792714476585
        model: {}
        policy_loss: -0.007220062427222729
        total_loss: -0.008676496334373951
        vf_explained_var: 0.010772332549095154
        vf_loss: 0.09337557852268219
    load_time_ms: 18691.111
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 107320.053
    update_time_ms: 14.545
  iterations_since_restore: 43
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.357
    ram_util_percent: 9.199000000000002
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 11.0
    agent-2: 25.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.02
    agent-1: 1.59
    agent-2: 4.34
    agent-3: 3.54
    agent-4: 3.13
    agent-5: 1.34
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.664203016940778
    mean_inference_ms: 13.732963775199405
    mean_processing_ms: 64.8074420157796
  time_since_restore: 6062.056604862213
  time_this_iter_s: 140.41737055778503
  time_total_s: 28572.33984732628
  timestamp: 1637226204
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    203 |          28572.3 | 19488000 |    16.96 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.12
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.68
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.54
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.89
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 144
    cleaning_beam_agent-0_mean: 75.32
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 534
    cleaning_beam_agent-1_mean: 235.08
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 88
    cleaning_beam_agent-2_mean: 21.49
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 76.3
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 234
    cleaning_beam_agent-4_mean: 81.39
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 17.88
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-05-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 18.42
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11703.7
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.547268271446228
        entropy_coeff: 0.0017600000137463212
        kl: 0.003965151961892843
        model: {}
        policy_loss: -0.008171970024704933
        total_loss: -0.009095695801079273
        vf_explained_var: 0.003406539559364319
        vf_loss: 0.27075397968292236
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 4.394879942992702e-05
        entropy: 0.5579816699028015
        entropy_coeff: 0.0017600000137463212
        kl: 0.003014251124113798
        model: {}
        policy_loss: -0.0069227926433086395
        total_loss: -0.007894105277955532
        vf_explained_var: 0.011182904243469238
        vf_loss: 0.10147887468338013
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.539263129234314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038942480459809303
        model: {}
        policy_loss: -0.008928468450903893
        total_loss: -0.00982543546706438
        vf_explained_var: 0.010662838816642761
        vf_loss: 0.3996726870536804
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.394879942992702e-05
        entropy: 0.6230881214141846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037172455340623856
        model: {}
        policy_loss: -0.007190327160060406
        total_loss: -0.008251097984611988
        vf_explained_var: 7.539987564086914e-05
        vf_loss: 0.3296298086643219
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.394879942992702e-05
        entropy: 0.7332829236984253
        entropy_coeff: 0.0017600000137463212
        kl: 0.004256201907992363
        model: {}
        policy_loss: -0.009694243781268597
        total_loss: -0.010887527838349342
        vf_explained_var: 0.027559131383895874
        vf_loss: 0.4408968985080719
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.8569021224975586
        entropy_coeff: 0.0017600000137463212
        kl: 0.003772755851969123
        model: {}
        policy_loss: -0.00610725674778223
        total_loss: -0.007588151376694441
        vf_explained_var: 0.0031701326370239258
        vf_loss: 0.1546214371919632
    load_time_ms: 18782.137
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 107284.469
    update_time_ms: 15.22
  iterations_since_restore: 44
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.67258883248731
    ram_util_percent: 9.298984771573604
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.19
    agent-1: 1.47
    agent-2: 4.14
    agent-3: 3.9
    agent-4: 4.58
    agent-5: 1.14
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.65697684959847
    mean_inference_ms: 13.730914772060448
    mean_processing_ms: 64.79344088749265
  time_since_restore: 6200.259881734848
  time_this_iter_s: 138.2032768726349
  time_total_s: 28710.543124198914
  timestamp: 1637226343
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    204 |          28710.5 | 19584000 |    18.42 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.88
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.75
    apples_agent-3_min: 0
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 78.74
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 225.73
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 19.7
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 253
    cleaning_beam_agent-3_mean: 81.91
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 181
    cleaning_beam_agent-4_mean: 81.86
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 83
    cleaning_beam_agent-5_mean: 18.5
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-08-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 18.18
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11698.206
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.5628831386566162
        entropy_coeff: 0.0017600000137463212
        kl: 0.003968852572143078
        model: {}
        policy_loss: -0.008098874241113663
        total_loss: -0.009062102064490318
        vf_explained_var: 0.0034401267766952515
        vf_loss: 0.21242591738700867
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.795840166276321e-05
        entropy: 0.5641288757324219
        entropy_coeff: 0.0017600000137463212
        kl: 0.003278049174696207
        model: {}
        policy_loss: -0.006228690966963768
        total_loss: -0.007206217385828495
        vf_explained_var: 0.03210330009460449
        vf_loss: 0.15023918449878693
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.522517204284668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036640181206166744
        model: {}
        policy_loss: -0.008559129200875759
        total_loss: -0.00942559540271759
        vf_explained_var: -0.0020151734352111816
        vf_loss: 0.4743908643722534
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 3.795840166276321e-05
        entropy: 0.6078084707260132
        entropy_coeff: 0.0017600000137463212
        kl: 0.003788426984101534
        model: {}
        policy_loss: -0.006609579548239708
        total_loss: -0.007639490999281406
        vf_explained_var: -0.0031960904598236084
        vf_loss: 0.38353535532951355
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.795840166276321e-05
        entropy: 0.7318723797798157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034740669652819633
        model: {}
        policy_loss: -0.005967849399894476
        total_loss: -0.007106536999344826
        vf_explained_var: 0.00986228883266449
        vf_loss: 1.2769577503204346
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.8841273188591003
        entropy_coeff: 0.0017600000137463212
        kl: 0.004918063525110483
        model: {}
        policy_loss: -0.006468336097896099
        total_loss: -0.00800299271941185
        vf_explained_var: -0.0017102062702178955
        vf_loss: 0.1372547298669815
    load_time_ms: 18773.982
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 107261.343
    update_time_ms: 15.268
  iterations_since_restore: 45
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.312690355329952
    ram_util_percent: 9.232487309644672
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.77
    agent-1: 1.82
    agent-2: 4.4
    agent-3: 3.87
    agent-4: 3.55
    agent-5: 1.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.65492948794919
    mean_inference_ms: 13.728793245705388
    mean_processing_ms: 64.78891227689004
  time_since_restore: 6338.484771728516
  time_this_iter_s: 138.2248899936676
  time_total_s: 28848.76801419258
  timestamp: 1637226481
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    205 |          28848.8 | 19680000 |    18.18 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 1.79
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.51
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 69
    apples_agent-5_mean: 1.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 138
    cleaning_beam_agent-0_mean: 74.26
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 224.78
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 17.08
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 72.5
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 269
    cleaning_beam_agent-4_mean: 85.55
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 17.2
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-10-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 16.17
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11697.176
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.5659384727478027
        entropy_coeff: 0.0017600000137463212
        kl: 0.003261659760028124
        model: {}
        policy_loss: -0.006995045579969883
        total_loss: -0.007962373085319996
        vf_explained_var: 0.008490890264511108
        vf_loss: 0.2618170380592346
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.5888722538948059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020668550860136747
        model: {}
        policy_loss: -0.0021719192154705524
        total_loss: -0.0029110745526850224
        vf_explained_var: 0.003712892532348633
        vf_loss: 2.971613883972168
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.5116258263587952
        entropy_coeff: 0.0017600000137463212
        kl: 0.002520978916436434
        model: {}
        policy_loss: -0.0042173778638243675
        total_loss: -0.00495067099109292
        vf_explained_var: 0.0021603405475616455
        vf_loss: 1.6520212888717651
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.196800025762059e-05
        entropy: 0.6024093627929688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026944500859826803
        model: {}
        policy_loss: -0.0056076617911458015
        total_loss: -0.006637588143348694
        vf_explained_var: 0.010120436549186707
        vf_loss: 0.2979090213775635
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.196800025762059e-05
        entropy: 0.7384783625602722
        entropy_coeff: 0.0017600000137463212
        kl: 0.003524277824908495
        model: {}
        policy_loss: -0.008077289909124374
        total_loss: -0.00933394581079483
        vf_explained_var: 0.014329716563224792
        vf_loss: 0.320531964302063
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.8613964319229126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033274381421506405
        model: {}
        policy_loss: -0.005522163584828377
        total_loss: -0.007022616919130087
        vf_explained_var: 0.011638522148132324
        vf_loss: 0.13005490601062775
    load_time_ms: 18840.759
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 107138.181
    update_time_ms: 15.246
  iterations_since_restore: 46
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.707614213197967
    ram_util_percent: 9.313705583756343
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.5
    agent-1: 0.11
    agent-2: 3.48
    agent-3: 3.38
    agent-4: 3.96
    agent-5: 1.74
  policy_reward_min:
    agent-0: 0.0
    agent-1: -99.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -5.0
  sampler_perf:
    mean_env_wait_ms: 25.65092017005077
    mean_inference_ms: 13.72765708175787
    mean_processing_ms: 64.78975468176823
  time_since_restore: 6476.696473836899
  time_this_iter_s: 138.21170210838318
  time_total_s: 28986.979716300964
  timestamp: 1637226619
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    206 |            28987 | 19776000 |    16.17 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.68
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.61
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.82
    apples_agent-3_min: 0
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.37
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 212
    cleaning_beam_agent-0_mean: 76.72
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 557
    cleaning_beam_agent-1_mean: 243.11
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 60
    cleaning_beam_agent-2_mean: 19.44
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 76.01
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 171
    cleaning_beam_agent-4_mean: 75.3
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 19.71
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-12-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 17.91
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11694.356
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.5620734691619873
        entropy_coeff: 0.0017600000137463212
        kl: 0.002586532849818468
        model: {}
        policy_loss: -0.0067450073547661304
        total_loss: -0.007707267068326473
        vf_explained_var: 0.0037466734647750854
        vf_loss: 0.2598184049129486
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.5888619422912598
        entropy_coeff: 0.0017600000137463212
        kl: 0.002089326735585928
        model: {}
        policy_loss: -0.00543802697211504
        total_loss: -0.006462299730628729
        vf_explained_var: 0.014048084616661072
        vf_loss: 0.12074480950832367
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.5324578881263733
        entropy_coeff: 0.0017600000137463212
        kl: 0.002809463767334819
        model: {}
        policy_loss: -0.006806711200624704
        total_loss: -0.00770209264010191
        vf_explained_var: -0.0018722712993621826
        vf_loss: 0.4064944386482239
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.5944715738296509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022635934874415398
        model: {}
        policy_loss: -0.005053523927927017
        total_loss: -0.006068801507353783
        vf_explained_var: -0.00511971116065979
        vf_loss: 0.3077557682991028
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 2.597760067146737e-05
        entropy: 0.7284729480743408
        entropy_coeff: 0.0017600000137463212
        kl: 0.002926948945969343
        model: {}
        policy_loss: -0.006954890210181475
        total_loss: -0.008197632618248463
        vf_explained_var: 0.01936236023902893
        vf_loss: 0.3479903042316437
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.8827447891235352
        entropy_coeff: 0.0017600000137463212
        kl: 0.003196905367076397
        model: {}
        policy_loss: -0.004953954368829727
        total_loss: -0.0064927078783512115
        vf_explained_var: 0.0036100149154663086
        vf_loss: 0.13627927005290985
    load_time_ms: 19004.863
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 107353.284
    update_time_ms: 15.316
  iterations_since_restore: 47
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.484183673469392
    ram_util_percent: 9.259693877551019
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 23.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.05
    agent-1: 1.59
    agent-2: 4.17
    agent-3: 3.61
    agent-4: 3.67
    agent-5: 1.82
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.64777795014923
    mean_inference_ms: 13.727331249252137
    mean_processing_ms: 64.78386302023935
  time_since_restore: 6613.897449970245
  time_this_iter_s: 137.20097613334656
  time_total_s: 29124.18069243431
  timestamp: 1637226757
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    207 |          29124.2 | 19872000 |    17.91 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.68
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 157
    cleaning_beam_agent-0_mean: 83.42
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 493
    cleaning_beam_agent-1_mean: 243.17
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 73
    cleaning_beam_agent-2_mean: 18.58
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 76.38
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 187
    cleaning_beam_agent-4_mean: 77.29
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 73
    cleaning_beam_agent-5_mean: 20.9
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-14-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 16.6
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11691.366
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.5746837854385376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022348426282405853
        model: {}
        policy_loss: -0.005198878236114979
        total_loss: -0.006184745579957962
        vf_explained_var: 0.0004048645496368408
        vf_loss: 0.25140640139579773
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.5764020085334778
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020620401483029127
        model: {}
        policy_loss: -0.005007123574614525
        total_loss: -0.006012950092554092
        vf_explained_var: 0.018660932779312134
        vf_loss: 0.08620283752679825
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.5122119784355164
        entropy_coeff: 0.0017600000137463212
        kl: 0.002277882769703865
        model: {}
        policy_loss: -0.0038110511377453804
        total_loss: -0.004525000229477882
        vf_explained_var: 0.0027009695768356323
        vf_loss: 1.8710463047027588
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.6061290502548218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024324983824044466
        model: {}
        policy_loss: -0.0029643988236784935
        total_loss: -0.0038723330944776535
        vf_explained_var: 0.0012829452753067017
        vf_loss: 1.5873732566833496
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.998719926632475e-05
        entropy: 0.745854914188385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019788374193012714
        model: {}
        policy_loss: -0.004034045618027449
        total_loss: -0.005184108391404152
        vf_explained_var: 0.0059967488050460815
        vf_loss: 1.6109659671783447
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.8795585036277771
        entropy_coeff: 0.0017600000137463212
        kl: 0.00249246577732265
        model: {}
        policy_loss: -0.004200364463031292
        total_loss: -0.005736669059842825
        vf_explained_var: 0.009899348020553589
        vf_loss: 0.11233169585466385
    load_time_ms: 19098.8
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 107324.805
    update_time_ms: 15.485
  iterations_since_restore: 48
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.80406091370558
    ram_util_percent: 9.313197969543147
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 6.0
    agent-2: 22.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.35
    agent-2: 4.04
    agent-3: 2.96
    agent-4: 3.7
    agent-5: 1.58
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: -48.0
    agent-4: -46.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.647133088280842
    mean_inference_ms: 13.726318621097986
    mean_processing_ms: 64.78495873024974
  time_since_restore: 6752.019908189774
  time_this_iter_s: 138.1224582195282
  time_total_s: 29262.30315065384
  timestamp: 1637226895
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    208 |          29262.3 | 19968000 |     16.6 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.16
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.78
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.74
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 75.93
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 247.41
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 84
    cleaning_beam_agent-2_mean: 19.52
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 79.85
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 188
    cleaning_beam_agent-4_mean: 78.93
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 20.9
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-17-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 17.49
  episode_reward_min: -96.0
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11694.218
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5623306035995483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018378837266936898
        model: {}
        policy_loss: -0.004493219777941704
        total_loss: -0.005459845997393131
        vf_explained_var: -0.003289937973022461
        vf_loss: 0.22903688251972198
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5829975008964539
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013181015383452177
        model: {}
        policy_loss: -0.0034931779373437166
        total_loss: -0.004505915567278862
        vf_explained_var: 0.021100088953971863
        vf_loss: 0.1332746148109436
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.49867039918899536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016660862602293491
        model: {}
        policy_loss: -0.004432118497788906
        total_loss: -0.005270088091492653
        vf_explained_var: 0.0003972947597503662
        vf_loss: 0.3952774107456207
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5969102382659912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014203457394614816
        model: {}
        policy_loss: -0.0033021140843629837
        total_loss: -0.004318897612392902
        vf_explained_var: -0.00768125057220459
        vf_loss: 0.33751124143600464
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.3996799680171534e-05
        entropy: 0.7439161539077759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019746359903365374
        model: {}
        policy_loss: -0.004842815920710564
        total_loss: -0.006113155744969845
        vf_explained_var: 0.02248057723045349
        vf_loss: 0.3818398118019104
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.8972021341323853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016841915203258395
        model: {}
        policy_loss: -0.0029553186614066362
        total_loss: -0.004517306108027697
        vf_explained_var: -0.0037450194358825684
        vf_loss: 0.16925129294395447
    load_time_ms: 19075.466
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 107312.457
    update_time_ms: 15.692
  iterations_since_restore: 49
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.43197969543147
    ram_util_percent: 9.269035532994923
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.8
    agent-2: 4.11
    agent-3: 3.26
    agent-4: 4.08
    agent-5: 1.27
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: -50.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.642625228532108
    mean_inference_ms: 13.724526125351304
    mean_processing_ms: 64.7676627624416
  time_since_restore: 6889.962311267853
  time_this_iter_s: 137.94240307807922
  time_total_s: 29400.24555373192
  timestamp: 1637227033
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    209 |          29400.2 | 20064000 |    17.49 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.46
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 2.73
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 78.8
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 418
    cleaning_beam_agent-1_mean: 241.24
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 59
    cleaning_beam_agent-2_mean: 18.87
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 79.41
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 71.07
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 21.14
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-19-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 15.61
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11690.08
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5752277970314026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014127868926152587
        model: {}
        policy_loss: -0.0038734846748411655
        total_loss: -0.004866315051913261
        vf_explained_var: 0.0007167160511016846
        vf_loss: 0.19507108628749847
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.582637369632721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016574900364503264
        model: {}
        policy_loss: -0.003671407699584961
        total_loss: -0.0046868836507201195
        vf_explained_var: 0.01755543053150177
        vf_loss: 0.09962666779756546
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.517444372177124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019494667649269104
        model: {}
        policy_loss: -0.004453503526747227
        total_loss: -0.005324791185557842
        vf_explained_var: -0.010245472192764282
        vf_loss: 0.3931969404220581
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6049317121505737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012265993282198906
        model: {}
        policy_loss: -0.0029133916832506657
        total_loss: -0.003949612844735384
        vf_explained_var: -0.005414336919784546
        vf_loss: 0.2844575345516205
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7374368906021118
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013103957753628492
        model: {}
        policy_loss: -0.0022822897881269455
        total_loss: -0.0034164926037192345
        vf_explained_var: 0.00843779742717743
        vf_loss: 1.6343134641647339
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9051024913787842
        entropy_coeff: 0.0017600000137463212
        kl: 0.001556498114950955
        model: {}
        policy_loss: -0.0023995903320610523
        total_loss: -0.003975698724389076
        vf_explained_var: 0.0034546852111816406
        vf_loss: 0.167982816696167
    load_time_ms: 19004.984
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 107294.515
    update_time_ms: 15.887
  iterations_since_restore: 50
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.796428571428574
    ram_util_percent: 9.263265306122449
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 11.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.63
    agent-1: 1.51
    agent-2: 3.88
    agent-3: 3.16
    agent-4: 3.25
    agent-5: 1.18
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 25.640126332666505
    mean_inference_ms: 13.72386715423164
    mean_processing_ms: 64.76099463460503
  time_since_restore: 7027.560467481613
  time_this_iter_s: 137.59815621376038
  time_total_s: 29537.84370994568
  timestamp: 1637227171
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    210 |          29537.8 | 20160000 |    15.61 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 2.32
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 77.9
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 245.6
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 14.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 78.21
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 163
    cleaning_beam_agent-4_mean: 65.89
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 96
    cleaning_beam_agent-5_mean: 18.79
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-21-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 41.0
  episode_reward_mean: 16.18
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11687.311
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.577684760093689
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014908494194969535
        model: {}
        policy_loss: -0.003942857496440411
        total_loss: -0.004938027821481228
        vf_explained_var: -0.008149504661560059
        vf_loss: 0.21521295607089996
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5776302218437195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014275878202170134
        model: {}
        policy_loss: -0.002890473697334528
        total_loss: -0.0038959926459938288
        vf_explained_var: 0.025180980563163757
        vf_loss: 0.11108498275279999
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5004734992980957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013294833479449153
        model: {}
        policy_loss: -0.0038632878568023443
        total_loss: -0.004703965038061142
        vf_explained_var: -0.008777916431427002
        vf_loss: 0.4012507498264313
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6063488721847534
        entropy_coeff: 0.0017600000137463212
        kl: 0.001215361407957971
        model: {}
        policy_loss: -0.0029801020864397287
        total_loss: -0.004019560758024454
        vf_explained_var: 0.005652934312820435
        vf_loss: 0.2771037817001343
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7329686880111694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020419906359165907
        model: {}
        policy_loss: -0.004345336928963661
        total_loss: -0.00560694420710206
        vf_explained_var: 0.013301849365234375
        vf_loss: 0.2822280824184418
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8879637718200684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015011685900390148
        model: {}
        policy_loss: -0.0029737732838839293
        total_loss: -0.004523983225226402
        vf_explained_var: 0.0056517720222473145
        vf_loss: 0.12573552131652832
    load_time_ms: 18987.018
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 107550.71
    update_time_ms: 16.049
  iterations_since_restore: 51
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.250500000000002
    ram_util_percent: 9.248999999999999
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.6
    agent-1: 1.52
    agent-2: 3.68
    agent-3: 3.24
    agent-4: 3.45
    agent-5: 1.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.639494661383264
    mean_inference_ms: 13.722848476161136
    mean_processing_ms: 64.76523897302017
  time_since_restore: 7167.03143954277
  time_this_iter_s: 139.47097206115723
  time_total_s: 29677.314682006836
  timestamp: 1637227311
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    211 |          29677.3 | 20256000 |    16.18 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.7
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 2.43
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 2.76
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 74.63
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 488
    cleaning_beam_agent-1_mean: 238.54
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 16.01
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 220
    cleaning_beam_agent-3_mean: 79.04
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 173
    cleaning_beam_agent-4_mean: 69.07
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 20.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-24-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 15.61
  episode_reward_min: -192.0
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11675.314
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5621321797370911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014317528111860156
        model: {}
        policy_loss: -0.003456552978605032
        total_loss: -0.0044176699593663216
        vf_explained_var: 0.0016489028930664062
        vf_loss: 0.2822057604789734
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5739933848381042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012616118183359504
        model: {}
        policy_loss: -0.003129085525870323
        total_loss: -0.0038525755517184734
        vf_explained_var: 0.012734577059745789
        vf_loss: 2.867394208908081
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4977072477340698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016905118245631456
        model: {}
        policy_loss: -0.002116934396326542
        total_loss: -0.0026481831446290016
        vf_explained_var: 0.002803802490234375
        vf_loss: 3.4469337463378906
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5956157445907593
        entropy_coeff: 0.0017600000137463212
        kl: 0.001186578767374158
        model: {}
        policy_loss: -0.0027305136900395155
        total_loss: -0.0037460217718034983
        vf_explained_var: 0.0017096549272537231
        vf_loss: 0.3277156352996826
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7481223940849304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022645548451691866
        model: {}
        policy_loss: -0.00448459479957819
        total_loss: -0.005766273010522127
        vf_explained_var: 0.01698531210422516
        vf_loss: 0.34908464550971985
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8845435380935669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011083935387432575
        model: {}
        policy_loss: -0.002568933181464672
        total_loss: -0.004111857153475285
        vf_explained_var: 0.0032796859741210938
        vf_loss: 0.1386362910270691
    load_time_ms: 18924.654
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 107523.516
    update_time_ms: 16.041
  iterations_since_restore: 52
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.833333333333332
    ram_util_percent: 9.257948717948716
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 21.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.86
    agent-1: 0.55
    agent-2: 3.28
    agent-3: 3.31
    agent-4: 3.93
    agent-5: 1.68
  policy_reward_min:
    agent-0: 0.0
    agent-1: -99.0
    agent-2: -100.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.635841668975964
    mean_inference_ms: 13.720212403968551
    mean_processing_ms: 64.76204054213983
  time_since_restore: 7304.043174266815
  time_this_iter_s: 137.0117347240448
  time_total_s: 29814.32641673088
  timestamp: 1637227448
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    212 |          29814.3 | 20352000 |    15.61 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 2.47
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 180
    cleaning_beam_agent-0_mean: 80.27
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 623
    cleaning_beam_agent-1_mean: 230.77
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 17.47
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 239
    cleaning_beam_agent-3_mean: 76.21
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 193
    cleaning_beam_agent-4_mean: 71.36
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 72
    cleaning_beam_agent-5_mean: 20.77
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-26-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 16.41
  episode_reward_min: -100.0
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11660.997
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5731421709060669
        entropy_coeff: 0.0017600000137463212
        kl: 0.001191636547446251
        model: {}
        policy_loss: -0.0027485203463584185
        total_loss: -0.0036001575645059347
        vf_explained_var: -0.0005078166723251343
        vf_loss: 1.5708860158920288
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5729837417602539
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010515510803088546
        model: {}
        policy_loss: -0.003280793549492955
        total_loss: -0.004274807404726744
        vf_explained_var: 0.020449131727218628
        vf_loss: 0.1443566083908081
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5054641366004944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013277147663757205
        model: {}
        policy_loss: -0.00351955508813262
        total_loss: -0.0043619354255497456
        vf_explained_var: 0.007116332650184631
        vf_loss: 0.47230374813079834
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6016043424606323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016295943642035127
        model: {}
        policy_loss: -0.0031723547726869583
        total_loss: -0.004189683124423027
        vf_explained_var: -0.006705284118652344
        vf_loss: 0.4149472415447235
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.735680103302002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014750254340469837
        model: {}
        policy_loss: -0.002585931681096554
        total_loss: -0.0037155416794121265
        vf_explained_var: 0.006929516792297363
        vf_loss: 1.6515172719955444
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9054538011550903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017119657713919878
        model: {}
        policy_loss: -0.002992411144077778
        total_loss: -0.004571424797177315
        vf_explained_var: -0.0021979212760925293
        vf_loss: 0.14576973021030426
    load_time_ms: 19090.886
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 107377.013
    update_time_ms: 16.161
  iterations_since_restore: 53
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.163
    ram_util_percent: 9.283499999999998
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 21.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.47
    agent-1: 1.77
    agent-2: 3.64
    agent-3: 3.46
    agent-4: 3.19
    agent-5: 1.88
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: -42.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.632096840422097
    mean_inference_ms: 13.719314860323312
    mean_processing_ms: 64.76035834466319
  time_since_restore: 7444.515527009964
  time_this_iter_s: 140.4723527431488
  time_total_s: 29954.79876947403
  timestamp: 1637227589
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    213 |          29954.8 | 20448000 |    16.41 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 1.8
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 1.84
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.07
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 74.74
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 230.27
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 16.08
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 232
    cleaning_beam_agent-3_mean: 85.38
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 203
    cleaning_beam_agent-4_mean: 73.27
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 18.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-28-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 13.84
  episode_reward_min: -100.0
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11646.271
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.57778400182724
        entropy_coeff: 0.0017600000137463212
        kl: 0.001857279916293919
        model: {}
        policy_loss: -0.003914338536560535
        total_loss: -0.0049087535589933395
        vf_explained_var: 0.0012683570384979248
        vf_loss: 0.2247854471206665
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5741156339645386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011704035568982363
        model: {}
        policy_loss: -0.0033726519905030727
        total_loss: -0.004370850510895252
        vf_explained_var: 0.028986915946006775
        vf_loss: 0.12243245542049408
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5079880952835083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010401710169389844
        model: {}
        policy_loss: -0.0024799304082989693
        total_loss: -0.003243185579776764
        vf_explained_var: 0.001957640051841736
        vf_loss: 1.307992696762085
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6054689884185791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011984583688899875
        model: {}
        policy_loss: -0.0024965174961835146
        total_loss: -0.0035350038670003414
        vf_explained_var: -0.0034355521202087402
        vf_loss: 0.2713894248008728
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7290276288986206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015912747476249933
        model: {}
        policy_loss: -0.004198397975414991
        total_loss: -0.005443146917968988
        vf_explained_var: 0.02332940697669983
        vf_loss: 0.38328343629837036
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.910219669342041
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017978486139327288
        model: {}
        policy_loss: -0.002911949995905161
        total_loss: -0.004500345792621374
        vf_explained_var: 0.00502362847328186
        vf_loss: 0.13588158786296844
    load_time_ms: 19118.817
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 107300.301
    update_time_ms: 15.649
  iterations_since_restore: 54
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.60969387755102
    ram_util_percent: 9.259693877551019
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.19
    agent-1: 1.6
    agent-2: 3.28
    agent-3: 2.46
    agent-4: 3.34
    agent-5: 0.97
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: -50.0
    agent-4: -50.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 25.626046358834227
    mean_inference_ms: 13.71653967698003
    mean_processing_ms: 64.74695719172375
  time_since_restore: 7582.115297079086
  time_this_iter_s: 137.59977006912231
  time_total_s: 30092.398539543152
  timestamp: 1637227726
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    214 |          30092.4 | 20544000 |    13.84 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.86
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.13
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.22
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 180
    cleaning_beam_agent-0_mean: 72.66
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 396
    cleaning_beam_agent-1_mean: 230.09
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 61
    cleaning_beam_agent-2_mean: 17.42
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 74.74
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 169
    cleaning_beam_agent-4_mean: 70.64
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 18.88
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-31-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 14.04
  episode_reward_min: -73.0
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11640.154
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5521013736724854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010553553001955152
        model: {}
        policy_loss: -0.0024375077337026596
        total_loss: -0.0031575849279761314
        vf_explained_var: -0.0009516477584838867
        vf_loss: 2.516204833984375
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5707083940505981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008269788813777268
        model: {}
        policy_loss: -0.0026396140456199646
        total_loss: -0.0035032564774155617
        vf_explained_var: 0.005375966429710388
        vf_loss: 1.4080146551132202
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5127677917480469
        entropy_coeff: 0.0017600000137463212
        kl: 0.001970297656953335
        model: {}
        policy_loss: -0.00262092100456357
        total_loss: -0.0033852681517601013
        vf_explained_var: -0.0019941627979278564
        vf_loss: 1.3812446594238281
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6029695272445679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010933021549135447
        model: {}
        policy_loss: -0.0028902729973196983
        total_loss: -0.003926996141672134
        vf_explained_var: 0.003234878182411194
        vf_loss: 0.24501879513263702
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.743972659111023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012962142936885357
        model: {}
        policy_loss: -0.0035793553106486797
        total_loss: -0.004858741536736488
        vf_explained_var: 0.014203280210494995
        vf_loss: 0.30003249645233154
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8932517766952515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012403854634612799
        model: {}
        policy_loss: -0.0028528799302875996
        total_loss: -0.004408552311360836
        vf_explained_var: 0.003742918372154236
        vf_loss: 0.16447357833385468
    load_time_ms: 19169.832
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 107284.937
    update_time_ms: 15.58
  iterations_since_restore: 55
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.76969696969697
    ram_util_percent: 9.315151515151513
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 10.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 1.77
    agent-1: 1.1
    agent-2: 2.48
    agent-3: 3.21
    agent-4: 3.65
    agent-5: 1.83
  policy_reward_min:
    agent-0: -47.0
    agent-1: -45.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -1.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.620967294433914
    mean_inference_ms: 13.71536818714534
    mean_processing_ms: 64.73959628647896
  time_since_restore: 7720.638065099716
  time_this_iter_s: 138.52276802062988
  time_total_s: 30230.92130756378
  timestamp: 1637227865
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    215 |          30230.9 | 20640000 |    14.04 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.05
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 161
    cleaning_beam_agent-0_mean: 74.25
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 236.3
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 85
    cleaning_beam_agent-2_mean: 18.36
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 75.29
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 212
    cleaning_beam_agent-4_mean: 79.77
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 20.05
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-33-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 18.33
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11654.422
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5738597512245178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017240457236766815
        model: {}
        policy_loss: -0.003870258806273341
        total_loss: -0.004853312857449055
        vf_explained_var: 0.001017346978187561
        vf_loss: 0.2694139778614044
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5737057328224182
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012885305332019925
        model: {}
        policy_loss: -0.002545733004808426
        total_loss: -0.003540369449183345
        vf_explained_var: 0.01362822949886322
        vf_loss: 0.15085244178771973
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5171518325805664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018171360716223717
        model: {}
        policy_loss: -0.0038704583421349525
        total_loss: -0.004736770875751972
        vf_explained_var: -0.001371428370475769
        vf_loss: 0.4387492537498474
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6119765639305115
        entropy_coeff: 0.0017600000137463212
        kl: 0.001222329679876566
        model: {}
        policy_loss: -0.0030468888580799103
        total_loss: -0.0040886253118515015
        vf_explained_var: -0.004793822765350342
        vf_loss: 0.35344770550727844
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7396867871284485
        entropy_coeff: 0.0017600000137463212
        kl: 0.001622502226382494
        model: {}
        policy_loss: -0.003943128976970911
        total_loss: -0.005210105329751968
        vf_explained_var: 0.0253048837184906
        vf_loss: 0.3486786186695099
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9112881422042847
        entropy_coeff: 0.0017600000137463212
        kl: 0.001529838191345334
        model: {}
        policy_loss: -0.0030644063372164965
        total_loss: -0.004649681504815817
        vf_explained_var: 0.0034017413854599
        vf_loss: 0.1859079897403717
    load_time_ms: 19179.256
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 107158.106
    update_time_ms: 15.566
  iterations_since_restore: 56
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.82717948717949
    ram_util_percent: 9.264615384615384
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 12.0
    agent-4: 12.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.16
    agent-1: 1.28
    agent-2: 4.25
    agent-3: 3.67
    agent-4: 3.83
    agent-5: 2.14
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.61916252534542
    mean_inference_ms: 13.714623803685006
    mean_processing_ms: 64.73300008495234
  time_since_restore: 7857.805848121643
  time_this_iter_s: 137.16778302192688
  time_total_s: 30368.08909058571
  timestamp: 1637228002
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    216 |          30368.1 | 20736000 |    18.33 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.8
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.25
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.32
    apples_agent-4_min: 0
    apples_agent-5_max: 33
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 77.06
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 235.55
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 85
    cleaning_beam_agent-2_mean: 18.78
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 230
    cleaning_beam_agent-3_mean: 75.15
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 239
    cleaning_beam_agent-4_mean: 74.34
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 20.52
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-35-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 17.78
  episode_reward_min: -45.0
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11671.526
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5876427888870239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015583114000037313
        model: {}
        policy_loss: -0.0029510303866118193
        total_loss: -0.0038318128790706396
        vf_explained_var: 0.0006816685199737549
        vf_loss: 1.5346767902374268
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5777860879898071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009974080603569746
        model: {}
        policy_loss: -0.0030188490636646748
        total_loss: -0.0040184613317251205
        vf_explained_var: 0.007700607180595398
        vf_loss: 0.17288784682750702
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5225634574890137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013110481668263674
        model: {}
        policy_loss: -0.00369159784168005
        total_loss: -0.004558084532618523
        vf_explained_var: 0.0016973018646240234
        vf_loss: 0.532230019569397
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6118318438529968
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122040044516325
        model: {}
        policy_loss: -0.002790330443531275
        total_loss: -0.003829726716503501
        vf_explained_var: -0.005338698625564575
        vf_loss: 0.3742654323577881
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7473424673080444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016134653706103563
        model: {}
        policy_loss: -0.004043292254209518
        total_loss: -0.005322255659848452
        vf_explained_var: 0.014867112040519714
        vf_loss: 0.3635551631450653
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.924394428730011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031550610437989235
        model: {}
        policy_loss: -0.0027655907906591892
        total_loss: -0.004120662808418274
        vf_explained_var: 0.0012877881526947021
        vf_loss: 2.718590259552002
    load_time_ms: 19165.545
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 107211.37
    update_time_ms: 15.527
  iterations_since_restore: 57
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.36700507614213
    ram_util_percent: 9.28527918781726
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 12.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.42
    agent-1: 1.77
    agent-2: 4.75
    agent-3: 3.82
    agent-4: 4.16
    agent-5: 0.86
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.613684734711338
    mean_inference_ms: 13.71260174088329
    mean_processing_ms: 64.72467239442359
  time_since_restore: 7995.578540802002
  time_this_iter_s: 137.7726926803589
  time_total_s: 30505.861783266068
  timestamp: 1637228140
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    217 |          30505.9 | 20832000 |    17.78 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.08
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.19
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 136
    cleaning_beam_agent-0_mean: 75.42
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 233.34
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 16.82
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 71.89
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 239
    cleaning_beam_agent-4_mean: 77.48
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 17.81
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-38-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 16.81
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11677.149
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5744620561599731
        entropy_coeff: 0.0017600000137463212
        kl: 0.001081497292034328
        model: {}
        policy_loss: -0.003716487903147936
        total_loss: -0.004706882406026125
        vf_explained_var: -0.002183765172958374
        vf_loss: 0.20659668743610382
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5802839398384094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013865407090634108
        model: {}
        policy_loss: -0.0029711686074733734
        total_loss: -0.003982787020504475
        vf_explained_var: 0.003528803586959839
        vf_loss: 0.09675019979476929
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4932781457901001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015598853351548314
        model: {}
        policy_loss: -0.0037056864239275455
        total_loss: -0.004531113896518946
        vf_explained_var: -0.0009224563837051392
        vf_loss: 0.42743682861328125
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6054370999336243
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010585265699774027
        model: {}
        policy_loss: -0.0027947458438575268
        total_loss: -0.003830862697213888
        vf_explained_var: -0.0007771700620651245
        vf_loss: 0.29453378915786743
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7430410385131836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014477933291345835
        model: {}
        policy_loss: -0.003938822075724602
        total_loss: -0.005212343297898769
        vf_explained_var: 0.016804993152618408
        vf_loss: 0.3422909379005432
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9372937083244324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013937150361016393
        model: {}
        policy_loss: -0.002952972427010536
        total_loss: -0.004588994197547436
        vf_explained_var: -0.00013786554336547852
        vf_loss: 0.1361616551876068
    load_time_ms: 19129.927
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 107473.735
    update_time_ms: 15.533
  iterations_since_restore: 58
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.246499999999997
    ram_util_percent: 9.293000000000001
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 5.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 1.47
    agent-2: 4.49
    agent-3: 3.04
    agent-4: 3.16
    agent-5: 1.8
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: -44.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.61370943839593
    mean_inference_ms: 13.71561520997697
    mean_processing_ms: 64.74066408884597
  time_since_restore: 8135.9758932590485
  time_this_iter_s: 140.3973524570465
  time_total_s: 30646.259135723114
  timestamp: 1637228281
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    218 |          30646.3 | 20928000 |    16.81 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.59
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 142
    cleaning_beam_agent-0_mean: 75.85
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 236.06
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 46
    cleaning_beam_agent-2_mean: 16.96
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 72.72
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 156
    cleaning_beam_agent-4_mean: 77.63
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 78
    cleaning_beam_agent-5_mean: 19.41
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-40-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 16.28
  episode_reward_min: -89.0
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11680.162
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.568638801574707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010575652122497559
        model: {}
        policy_loss: -0.0017689038068056107
        total_loss: -0.0026149125769734383
        vf_explained_var: 0.0006623268127441406
        vf_loss: 1.547943353652954
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5804587602615356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011511120246723294
        model: {}
        policy_loss: -0.0028916175942867994
        total_loss: -0.003900055307894945
        vf_explained_var: 0.02996937930583954
        vf_loss: 0.13169166445732117
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4989543855190277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015530611854046583
        model: {}
        policy_loss: -0.0035829227417707443
        total_loss: -0.004421275109052658
        vf_explained_var: -0.00905337929725647
        vf_loss: 0.39805588126182556
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5991986393928528
        entropy_coeff: 0.0017600000137463212
        kl: 0.001309521496295929
        model: {}
        policy_loss: -0.002261041197925806
        total_loss: -0.0031557672191411257
        vf_explained_var: -0.0007821321487426758
        vf_loss: 1.598636507987976
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7275956869125366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015374080976471305
        model: {}
        policy_loss: -0.0038242824375629425
        total_loss: -0.005066931713372469
        vf_explained_var: 0.02077867090702057
        vf_loss: 0.3791905343532562
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9523862600326538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023383991792798042
        model: {}
        policy_loss: -0.0033124862238764763
        total_loss: -0.004973697476089001
        vf_explained_var: 0.0006295293569564819
        vf_loss: 0.14986863732337952
    load_time_ms: 19108.301
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 107519.126
    update_time_ms: 15.313
  iterations_since_restore: 59
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.691878172588833
    ram_util_percent: 9.28984771573604
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 1.97
    agent-1: 1.66
    agent-2: 3.84
    agent-3: 3.09
    agent-4: 3.92
    agent-5: 1.8
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 25.609271116866722
    mean_inference_ms: 13.713930900726991
    mean_processing_ms: 64.73504469873784
  time_since_restore: 8274.183752298355
  time_this_iter_s: 138.20785903930664
  time_total_s: 30784.46699476242
  timestamp: 1637228419
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    219 |          30784.5 | 21024000 |    16.28 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.88
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 36
    apples_agent-3_mean: 2.93
    apples_agent-3_min: 0
    apples_agent-4_max: 101
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 74.55
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 225.94
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 60
    cleaning_beam_agent-2_mean: 15.82
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 69.78
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 227
    cleaning_beam_agent-4_mean: 76.93
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 19.6
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-42-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 17.71
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11682.598
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5747979879379272
        entropy_coeff: 0.0017600000137463212
        kl: 0.001655040425248444
        model: {}
        policy_loss: -0.004292019177228212
        total_loss: -0.005281676538288593
        vf_explained_var: 0.006913095712661743
        vf_loss: 0.21988148987293243
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5682427883148193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012575786095112562
        model: {}
        policy_loss: -0.0031028129160404205
        total_loss: -0.004090974107384682
        vf_explained_var: 0.02439776062965393
        vf_loss: 0.11943402886390686
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49922242760658264
        entropy_coeff: 0.0017600000137463212
        kl: 0.001654681283980608
        model: {}
        policy_loss: -0.004066759720444679
        total_loss: -0.004904640838503838
        vf_explained_var: 0.003100275993347168
        vf_loss: 0.40751343965530396
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6027355194091797
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013235665392130613
        model: {}
        policy_loss: -0.002745069097727537
        total_loss: -0.0037747458554804325
        vf_explained_var: 0.0004640817642211914
        vf_loss: 0.3113686442375183
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7335121631622314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014441738603636622
        model: {}
        policy_loss: -0.0038740714080631733
        total_loss: -0.005127022974193096
        vf_explained_var: 0.02406148612499237
        vf_loss: 0.38028815388679504
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9271920919418335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024084988981485367
        model: {}
        policy_loss: -0.0035668176133185625
        total_loss: -0.005189213901758194
        vf_explained_var: -0.006925821304321289
        vf_loss: 0.0946153923869133
    load_time_ms: 19136.267
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 107547.835
    update_time_ms: 15.23
  iterations_since_restore: 60
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.67766497461929
    ram_util_percent: 9.251269035532992
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 17.0
    agent-4: 12.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.87
    agent-1: 1.7
    agent-2: 4.11
    agent-3: 3.48
    agent-4: 3.94
    agent-5: 1.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.605951269904764
    mean_inference_ms: 13.713622225242519
    mean_processing_ms: 64.73433326324931
  time_since_restore: 8412.36143708229
  time_this_iter_s: 138.17768478393555
  time_total_s: 30922.644679546356
  timestamp: 1637228557
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    220 |          30922.6 | 21120000 |    17.71 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.99
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.75
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 77.09
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 226.16
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 14.66
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 69.0
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 176
    cleaning_beam_agent-4_mean: 75.11
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 17.57
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-44-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 17.71
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11679.489
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5708807706832886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010296792024746537
        model: {}
        policy_loss: -0.002499464899301529
        total_loss: -0.003353937529027462
        vf_explained_var: -0.00120621919631958
        vf_loss: 1.502816915512085
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5643606185913086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008518130052834749
        model: {}
        policy_loss: -0.0029165735468268394
        total_loss: -0.0038967449218034744
        vf_explained_var: 0.012868911027908325
        vf_loss: 0.1310315877199173
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4807654917240143
        entropy_coeff: 0.0017600000137463212
        kl: 0.001141183776780963
        model: {}
        policy_loss: -0.0022722631692886353
        total_loss: -0.002826537936925888
        vf_explained_var: 0.001773938536643982
        vf_loss: 2.9187304973602295
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5997770428657532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016851788386702538
        model: {}
        policy_loss: -0.0025218252558261156
        total_loss: -0.0034176232293248177
        vf_explained_var: -0.00011430680751800537
        vf_loss: 1.5980877876281738
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.748216986656189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020313882268965244
        model: {}
        policy_loss: -0.004293970763683319
        total_loss: -0.005569377448409796
        vf_explained_var: 0.019342482089996338
        vf_loss: 0.41453325748443604
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9523916244506836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016624610871076584
        model: {}
        policy_loss: -0.0028496808372437954
        total_loss: -0.004507220350205898
        vf_explained_var: 0.007403925061225891
        vf_loss: 0.18668481707572937
    load_time_ms: 19053.798
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 107557.784
    update_time_ms: 15.148
  iterations_since_restore: 61
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.422222222222224
    ram_util_percent: 9.209595959595958
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 15.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.59
    agent-1: 1.65
    agent-2: 3.39
    agent-3: 3.26
    agent-4: 4.47
    agent-5: 2.35
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: -40.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.607965729933195
    mean_inference_ms: 13.713393903304278
    mean_processing_ms: 64.74569025072556
  time_since_restore: 8551.074866056442
  time_this_iter_s: 138.7134289741516
  time_total_s: 31061.358108520508
  timestamp: 1637228697
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    221 |          31061.4 | 21216000 |    17.71 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.17
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.11
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.92
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 170
    cleaning_beam_agent-0_mean: 77.02
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 460
    cleaning_beam_agent-1_mean: 214.78
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 14.82
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 241
    cleaning_beam_agent-3_mean: 67.71
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 185
    cleaning_beam_agent-4_mean: 75.31
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 18.94
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-47-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 41.0
  episode_reward_mean: 15.43
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11690.267
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5832019448280334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015020582359284163
        model: {}
        policy_loss: -0.003927820362150669
        total_loss: -0.004933319985866547
        vf_explained_var: -0.0030471980571746826
        vf_loss: 0.209337055683136
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5622238516807556
        entropy_coeff: 0.0017600000137463212
        kl: 0.001151739270426333
        model: {}
        policy_loss: -0.0027505983598530293
        total_loss: -0.0037262786645442247
        vf_explained_var: 0.03419774770736694
        vf_loss: 0.1383151113986969
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4955810010433197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011289623798802495
        model: {}
        policy_loss: -0.0022047474049031734
        total_loss: -0.0029114801436662674
        vf_explained_var: 0.000488772988319397
        vf_loss: 1.654922604560852
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5935201644897461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011602008016780019
        model: {}
        policy_loss: -0.002868466079235077
        total_loss: -0.0038848649710416794
        vf_explained_var: 0.0020797550678253174
        vf_loss: 0.28198087215423584
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7463171482086182
        entropy_coeff: 0.0017600000137463212
        kl: 0.001566453604027629
        model: {}
        policy_loss: -0.0042434874922037125
        total_loss: -0.0055260625667870045
        vf_explained_var: 0.009290963411331177
        vf_loss: 0.3094678819179535
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9556651711463928
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017993551446124911
        model: {}
        policy_loss: -0.0027446220628917217
        total_loss: -0.004415380768477917
        vf_explained_var: 0.0016594082117080688
        vf_loss: 0.1121169924736023
    load_time_ms: 19101.24
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 107729.13
    update_time_ms: 15.213
  iterations_since_restore: 62
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.48391959798995
    ram_util_percent: 9.319095477386934
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 13.0
    agent-2: 15.0
    agent-3: 11.0
    agent-4: 11.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.6
    agent-1: 1.65
    agent-2: 2.96
    agent-3: 3.32
    agent-4: 3.46
    agent-5: 1.44
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.608739549619877
    mean_inference_ms: 13.713597960158786
    mean_processing_ms: 64.75287723335835
  time_since_restore: 8690.39071393013
  time_this_iter_s: 139.31584787368774
  time_total_s: 31200.673956394196
  timestamp: 1637228836
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    222 |          31200.7 | 21312000 |    15.43 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.67
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.62
    apples_agent-2_min: 0
    apples_agent-3_max: 41
    apples_agent-3_mean: 2.9
    apples_agent-3_min: 0
    apples_agent-4_max: 107
    apples_agent-4_mean: 2.24
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 73.24
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 306
    cleaning_beam_agent-1_mean: 212.75
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 15.65
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 67.05
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 175
    cleaning_beam_agent-4_mean: 80.58
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 18.72
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-49-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 17.06
  episode_reward_min: -93.0
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11703.792
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5712919235229492
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009884744649752975
        model: {}
        policy_loss: -0.0029087276197969913
        total_loss: -0.0038640277925878763
        vf_explained_var: 0.0011347681283950806
        vf_loss: 0.5017319321632385
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.559177577495575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009579595644026995
        model: {}
        policy_loss: -0.0029695150442421436
        total_loss: -0.003943049348890781
        vf_explained_var: 0.021795496344566345
        vf_loss: 0.10620148479938507
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48944270610809326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013464599614962935
        model: {}
        policy_loss: -0.0026043574325740337
        total_loss: -0.0032793632708489895
        vf_explained_var: 0.0022127628326416016
        vf_loss: 1.864134430885315
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6070312857627869
        entropy_coeff: 0.0017600000137463212
        kl: 0.000809726829174906
        model: {}
        policy_loss: -0.0016702422872185707
        total_loss: -0.002566627226769924
        vf_explained_var: -0.0011724531650543213
        vf_loss: 1.719877004623413
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7365064024925232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017531171906739473
        model: {}
        policy_loss: -0.0041824448853731155
        total_loss: -0.005452260375022888
        vf_explained_var: 0.01233816146850586
        vf_loss: 0.26436468958854675
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9496843814849854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017545856535434723
        model: {}
        policy_loss: -0.003017161041498184
        total_loss: -0.004542531445622444
        vf_explained_var: 0.002149030566215515
        vf_loss: 1.4607501029968262
    load_time_ms: 18889.305
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 107728.255
    update_time_ms: 15.211
  iterations_since_restore: 63
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.65482233502538
    ram_util_percent: 9.281218274111676
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 6.0
    agent-2: 21.0
    agent-3: 17.0
    agent-4: 10.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.76
    agent-1: 1.61
    agent-2: 4.29
    agent-3: 3.26
    agent-4: 3.46
    agent-5: 1.68
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.606229383482177
    mean_inference_ms: 13.71224221887382
    mean_processing_ms: 64.74974536070346
  time_since_restore: 8828.876954078674
  time_this_iter_s: 138.4862401485443
  time_total_s: 31339.16019654274
  timestamp: 1637228975
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    223 |          31339.2 | 21408000 |    17.06 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.69
    apples_agent-2_min: 0
    apples_agent-3_max: 62
    apples_agent-3_mean: 3.73
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 73.82
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 534
    cleaning_beam_agent-1_mean: 221.57
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 53
    cleaning_beam_agent-2_mean: 14.85
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 73.55
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 231
    cleaning_beam_agent-4_mean: 85.5
    cleaning_beam_agent-4_min: 27
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 18.61
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-51-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 16.73
  episode_reward_min: -150.0
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11700.594
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5705334544181824
        entropy_coeff: 0.0017600000137463212
        kl: 0.001511050621047616
        model: {}
        policy_loss: -0.003872278146445751
        total_loss: -0.0048492285422980785
        vf_explained_var: -0.003044694662094116
        vf_loss: 0.27188828587532043
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5616192817687988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013047700049355626
        model: {}
        policy_loss: -0.0020818375051021576
        total_loss: -0.0029259962029755116
        vf_explained_var: 0.0114116370677948
        vf_loss: 1.4429130554199219
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48961642384529114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018001958960667253
        model: {}
        policy_loss: -0.004034132231026888
        total_loss: -0.004853200167417526
        vf_explained_var: -0.00205036997795105
        vf_loss: 0.4265976548194885
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.588085412979126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011018035002052784
        model: {}
        policy_loss: -0.0028786719776690006
        total_loss: -0.003880386706441641
        vf_explained_var: 0.0007182508707046509
        vf_loss: 0.3331460952758789
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7538885474205017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018328421283513308
        model: {}
        policy_loss: -0.0024574613198637962
        total_loss: -0.0032224422320723534
        vf_explained_var: 0.002654820680618286
        vf_loss: 5.618625164031982
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9577112197875977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016791375819593668
        model: {}
        policy_loss: -0.0030998019501566887
        total_loss: -0.004768607206642628
        vf_explained_var: 0.004703700542449951
        vf_loss: 0.16767896711826324
    load_time_ms: 18864.829
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 107805.231
    update_time_ms: 15.216
  iterations_since_restore: 64
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.336548223350256
    ram_util_percent: 9.269035532994923
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.81
    agent-1: 1.25
    agent-2: 4.26
    agent-3: 3.53
    agent-4: 2.9
    agent-5: 1.98
  policy_reward_min:
    agent-0: -1.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -2.0
    agent-4: -100.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.60585046381855
    mean_inference_ms: 13.71099424070334
    mean_processing_ms: 64.74787826247479
  time_since_restore: 8966.913443803787
  time_this_iter_s: 138.03648972511292
  time_total_s: 31477.196686267853
  timestamp: 1637229113
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    224 |          31477.2 | 21504000 |    16.73 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.82
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.43
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 162
    cleaning_beam_agent-0_mean: 69.23
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 228.44
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 17.28
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 71.32
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 82.21
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 21.23
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-54-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 15.75
  episode_reward_min: -150.0
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11705.195
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5585917830467224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013898768229410052
        model: {}
        policy_loss: -0.0032849013805389404
        total_loss: -0.0042416127398610115
        vf_explained_var: -0.003348022699356079
        vf_loss: 0.26409995555877686
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5679262280464172
        entropy_coeff: 0.0017600000137463212
        kl: 0.001297990558668971
        model: {}
        policy_loss: -0.0028156060725450516
        total_loss: -0.0038026487454771996
        vf_explained_var: 0.022126376628875732
        vf_loss: 0.1250753104686737
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5079631805419922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013564814580604434
        model: {}
        policy_loss: -0.0038397894240915775
        total_loss: -0.004691588692367077
        vf_explained_var: -0.0026701539754867554
        vf_loss: 0.4221787452697754
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5922983884811401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011600225698202848
        model: {}
        policy_loss: -0.0028109950944781303
        total_loss: -0.003824342042207718
        vf_explained_var: -0.002011612057685852
        vf_loss: 0.2909984290599823
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7590283155441284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016333654057234526
        model: {}
        policy_loss: -0.00415097875520587
        total_loss: -0.005450188182294369
        vf_explained_var: 0.017920836806297302
        vf_loss: 0.3667967915534973
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9555684328079224
        entropy_coeff: 0.0017600000137463212
        kl: 0.001102279289625585
        model: {}
        policy_loss: -0.0018806677544489503
        total_loss: -0.0034152204170823097
        vf_explained_var: 0.002995237708091736
        vf_loss: 1.4724591970443726
    load_time_ms: 18770.359
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 107853.569
    update_time_ms: 15.154
  iterations_since_restore: 65
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.67969543147208
    ram_util_percent: 9.287309644670051
  pid: 6435
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.63
    agent-1: 1.25
    agent-2: 4.03
    agent-3: 3.4
    agent-4: 2.86
    agent-5: 1.58
  policy_reward_min:
    agent-0: -1.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -2.0
    agent-4: -100.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 25.605568863004056
    mean_inference_ms: 13.709733506583431
    mean_processing_ms: 64.74330604820125
  time_since_restore: 9105.019599676132
  time_this_iter_s: 138.10615587234497
  time_total_s: 31615.302842140198
  timestamp: 1637229251
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    225 |          31615.3 | 21600000 |    15.75 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.83
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 200
    cleaning_beam_agent-0_mean: 70.57
    cleaning_beam_agent-0_min: 5
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 234.82
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 59
    cleaning_beam_agent-2_mean: 16.63
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 67.8
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 201
    cleaning_beam_agent-4_mean: 90.09
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 20.0
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-56-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 16.87
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11681.743
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5641217827796936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013581595849245787
        model: {}
        policy_loss: -0.0019955886527895927
        total_loss: -0.0028330222703516483
        vf_explained_var: -0.0014367103576660156
        vf_loss: 1.5542044639587402
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5709550380706787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013188081793487072
        model: {}
        policy_loss: -0.0025968109257519245
        total_loss: -0.0035910666920244694
        vf_explained_var: 0.01771959662437439
        vf_loss: 0.10624136030673981
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5011278390884399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014677607687190175
        model: {}
        policy_loss: -0.0041004265658557415
        total_loss: -0.004946758970618248
        vf_explained_var: 0.0004904568195343018
        vf_loss: 0.3565588593482971
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6152051687240601
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012760175159201026
        model: {}
        policy_loss: -0.0026940428651869297
        total_loss: -0.0037523359060287476
        vf_explained_var: 0.00026795268058776855
        vf_loss: 0.24467456340789795
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7609570026397705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019058049656450748
        model: {}
        policy_loss: -0.004222013987600803
        total_loss: -0.005526908673346043
        vf_explained_var: 0.011171340942382812
        vf_loss: 0.34388023614883423
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9531733989715576
        entropy_coeff: 0.0017600000137463212
        kl: 0.001707868417724967
        model: {}
        policy_loss: -0.003035494592040777
        total_loss: -0.004698262549936771
        vf_explained_var: 0.006821721792221069
        vf_loss: 0.14815406501293182
    load_time_ms: 18914.017
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 107898.121
    update_time_ms: 15.042
  iterations_since_restore: 66
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.481218274111676
    ram_util_percent: 9.288832487309643
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 6.0
    agent-2: 12.0
    agent-3: 9.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.5
    agent-1: 1.58
    agent-2: 3.93
    agent-3: 2.97
    agent-4: 3.82
    agent-5: 2.07
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.602962096215887
    mean_inference_ms: 13.70886412100132
    mean_processing_ms: 64.73512173298093
  time_since_restore: 9243.822215557098
  time_this_iter_s: 138.8026158809662
  time_total_s: 31754.105458021164
  timestamp: 1637229390
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    226 |          31754.1 | 21696000 |    16.87 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.69
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 2.51
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.57
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 70.65
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 568
    cleaning_beam_agent-1_mean: 241.59
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 16.53
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 64.97
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 172
    cleaning_beam_agent-4_mean: 75.36
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 18.56
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-58-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 18.53
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11660.076
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5667060613632202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018906292971223593
        model: {}
        policy_loss: -0.0026786974631249905
        total_loss: -0.0035207183100283146
        vf_explained_var: -0.0005733370780944824
        vf_loss: 1.553827166557312
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5698056221008301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011115531669929624
        model: {}
        policy_loss: -0.0025045624934136868
        total_loss: -0.0034954529255628586
        vf_explained_var: 0.014118850231170654
        vf_loss: 0.11966867744922638
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49333634972572327
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212532864883542
        model: {}
        policy_loss: -0.0030920018907636404
        total_loss: -0.003908802755177021
        vf_explained_var: 0.0073122382164001465
        vf_loss: 0.5147204399108887
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5939405560493469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008266032673418522
        model: {}
        policy_loss: -0.0023965430445969105
        total_loss: -0.003397000953555107
        vf_explained_var: 0.003720119595527649
        vf_loss: 0.44875508546829224
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7475977540016174
        entropy_coeff: 0.0017600000137463212
        kl: 0.001683174865320325
        model: {}
        policy_loss: -0.003635044675320387
        total_loss: -0.00490022636950016
        vf_explained_var: 0.02079176902770996
        vf_loss: 0.5059221982955933
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9669640064239502
        entropy_coeff: 0.0017600000137463212
        kl: 0.002073697978630662
        model: {}
        policy_loss: -0.003215574659407139
        total_loss: -0.004900800995528698
        vf_explained_var: 0.008130699396133423
        vf_loss: 0.16628992557525635
    load_time_ms: 18926.219
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 107935.243
    update_time_ms: 15.074
  iterations_since_restore: 67
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.216751269035534
    ram_util_percent: 9.313197969543149
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 20.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.67
    agent-1: 1.47
    agent-2: 4.84
    agent-3: 2.99
    agent-4: 4.55
    agent-5: 2.01
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.600098188074124
    mean_inference_ms: 13.707227406271599
    mean_processing_ms: 64.7295924779252
  time_since_restore: 9381.870893239975
  time_this_iter_s: 138.0486776828766
  time_total_s: 31892.15413570404
  timestamp: 1637229528
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    227 |          31892.2 | 21792000 |    18.53 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.29
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.47
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 74.79
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 781
    cleaning_beam_agent-1_mean: 237.7
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 16.37
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 64.84
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 211
    cleaning_beam_agent-4_mean: 79.15
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 17.44
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-01-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 12.56
  episode_reward_min: -294.0
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11662.634
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5683598518371582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016077737091109157
        model: {}
        policy_loss: -0.004187669605016708
        total_loss: -0.005169226787984371
        vf_explained_var: 0.0034576058387756348
        vf_loss: 0.18760038912296295
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.574485182762146
        entropy_coeff: 0.0017600000137463212
        kl: 0.000723195553291589
        model: {}
        policy_loss: -0.002585333539173007
        total_loss: -0.003582513192668557
        vf_explained_var: 0.010121852159500122
        vf_loss: 0.13910746574401855
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49902456998825073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016855588182806969
        model: {}
        policy_loss: -0.0032337333541363478
        total_loss: -0.0020491022150963545
        vf_explained_var: 0.0007442981004714966
        vf_loss: 20.629106521606445
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6050896644592285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017052324255928397
        model: {}
        policy_loss: -0.0019926982931792736
        total_loss: -0.00249909027479589
        vf_explained_var: -0.0004885196685791016
        vf_loss: 5.585648536682129
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7612724900245667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015215182211250067
        model: {}
        policy_loss: -0.002493423642590642
        total_loss: -0.003661941736936569
        vf_explained_var: 0.003937870264053345
        vf_loss: 1.7132195234298706
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.968127965927124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022564276587218046
        model: {}
        policy_loss: -0.003000435885041952
        total_loss: -0.004690554458647966
        vf_explained_var: 0.008194416761398315
        vf_loss: 0.13784968852996826
    load_time_ms: 18957.941
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 107786.809
    update_time_ms: 14.995
  iterations_since_restore: 68
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.172361809045228
    ram_util_percent: 9.302010050251257
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 1.86
    agent-1: 1.55
    agent-2: 2.33
    agent-3: 1.78
    agent-4: 3.16
    agent-5: 1.88
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: -198.0
    agent-3: -98.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.598873099188417
    mean_inference_ms: 13.70675736464225
    mean_processing_ms: 64.73252064548818
  time_since_restore: 9521.21792292595
  time_this_iter_s: 139.34702968597412
  time_total_s: 32031.501165390015
  timestamp: 1637229668
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    228 |          32031.5 | 21888000 |    12.56 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.34
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 79
    apples_agent-5_mean: 1.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 171
    cleaning_beam_agent-0_mean: 70.54
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 226.78
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 16.4
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 59.41
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 176
    cleaning_beam_agent-4_mean: 75.25
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 18.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-03-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 16.39
  episode_reward_min: -77.0
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11661.878
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.568434476852417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014549641637131572
        model: {}
        policy_loss: -0.002897447906434536
        total_loss: -0.0036111734807491302
        vf_explained_var: -0.0007264018058776855
        vf_loss: 2.8671786785125732
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5765368342399597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015487786149606109
        model: {}
        policy_loss: -0.003315734677016735
        total_loss: -0.0043204063549637794
        vf_explained_var: 0.020980998873710632
        vf_loss: 0.10034695267677307
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4899333119392395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011318644974380732
        model: {}
        policy_loss: -0.0037588882260024548
        total_loss: -0.004582806024700403
        vf_explained_var: -0.0017609894275665283
        vf_loss: 0.3836379945278168
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5969693660736084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009394055232405663
        model: {}
        policy_loss: -0.002788169775158167
        total_loss: -0.003806316526606679
        vf_explained_var: -0.00024056434631347656
        vf_loss: 0.3252153992652893
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7565996646881104
        entropy_coeff: 0.0017600000137463212
        kl: 0.001615164801478386
        model: {}
        policy_loss: -0.0035327416844666004
        total_loss: -0.004692763090133667
        vf_explained_var: 0.005639240145683289
        vf_loss: 1.7159473896026611
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.952998161315918
        entropy_coeff: 0.0017600000137463212
        kl: 0.001523659797385335
        model: {}
        policy_loss: -0.0030997858848422766
        total_loss: -0.0047624013386666775
        vf_explained_var: 0.011009663343429565
        vf_loss: 0.146611750125885
    load_time_ms: 18958.13
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 107835.357
    update_time_ms: 15.063
  iterations_since_restore: 69
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.216243654822335
    ram_util_percent: 9.270558375634517
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 6.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.05
    agent-1: 1.59
    agent-2: 4.09
    agent-3: 3.5
    agent-4: 3.33
    agent-5: 1.83
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.59480034221368
    mean_inference_ms: 13.70642817799078
    mean_processing_ms: 64.72609247846577
  time_since_restore: 9659.91198682785
  time_this_iter_s: 138.69406390190125
  time_total_s: 32170.195229291916
  timestamp: 1637229807
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    229 |          32170.2 | 21984000 |    16.39 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.7
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.62
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 245
    cleaning_beam_agent-0_mean: 74.38
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 224.76
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 17.02
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 55.62
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 284
    cleaning_beam_agent-4_mean: 73.63
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 16.76
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-05-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 16.53
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11677.484
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5600669384002686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021518273279070854
        model: {}
        policy_loss: -0.003907450940459967
        total_loss: -0.004868175368756056
        vf_explained_var: 0.0030416399240493774
        vf_loss: 0.24995210766792297
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5720705986022949
        entropy_coeff: 0.0017600000137463212
        kl: 0.001296616392210126
        model: {}
        policy_loss: -0.002909640083089471
        total_loss: -0.003905404359102249
        vf_explained_var: 0.01982894539833069
        vf_loss: 0.11081259697675705
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5128072500228882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018800005782395601
        model: {}
        policy_loss: -0.0034343916922807693
        total_loss: -0.004292176105082035
        vf_explained_var: 0.003971606492996216
        vf_loss: 0.4475669264793396
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.601482629776001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010724856983870268
        model: {}
        policy_loss: -0.002017832361161709
        total_loss: -0.00290579441934824
        vf_explained_var: 0.0011059492826461792
        vf_loss: 1.7064721584320068
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7582463622093201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018164431676268578
        model: {}
        policy_loss: -0.003644048934802413
        total_loss: -0.004947247914969921
        vf_explained_var: 0.011655360460281372
        vf_loss: 0.313186377286911
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9681882858276367
        entropy_coeff: 0.0017600000137463212
        kl: 0.003379049012437463
        model: {}
        policy_loss: -0.0017642163438722491
        total_loss: -0.0033211547415703535
        vf_explained_var: 0.0015144497156143188
        vf_loss: 1.4707298278808594
    load_time_ms: 19026.462
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 107729.358
    update_time_ms: 14.802
  iterations_since_restore: 70
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.980203045685275
    ram_util_percent: 9.320812182741117
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 21.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.7
    agent-1: 1.56
    agent-2: 3.92
    agent-3: 3.15
    agent-4: 3.69
    agent-5: 1.51
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 25.590671706688195
    mean_inference_ms: 13.70515974090985
    mean_processing_ms: 64.72154501213993
  time_since_restore: 9797.899230718613
  time_this_iter_s: 137.98724389076233
  time_total_s: 32308.18247318268
  timestamp: 1637229945
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    230 |          32308.2 | 22080000 |    16.53 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.91
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.3
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 158
    cleaning_beam_agent-0_mean: 69.57
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 218.35
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 17.63
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 55.01
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 205
    cleaning_beam_agent-4_mean: 75.91
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 16.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-08-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 17.42
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11680.512
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5628880262374878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009353734203614295
        model: {}
        policy_loss: -0.0018781796097755432
        total_loss: -0.0025890287943184376
        vf_explained_var: -0.0003410428762435913
        vf_loss: 2.798304796218872
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5585815906524658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010133794276043773
        model: {}
        policy_loss: -0.002733422676101327
        total_loss: -0.0037024724297225475
        vf_explained_var: 0.012929573655128479
        vf_loss: 0.1405266672372818
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48879894614219666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013712081126868725
        model: {}
        policy_loss: -0.003692630212754011
        total_loss: -0.0045082829892635345
        vf_explained_var: -0.007804214954376221
        vf_loss: 0.4463362395763397
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6032153964042664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013708818005397916
        model: {}
        policy_loss: -0.0026490718591958284
        total_loss: -0.003683991963043809
        vf_explained_var: -0.0034950673580169678
        vf_loss: 0.26740697026252747
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7579173445701599
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016756942495703697
        model: {}
        policy_loss: -0.0039154160767793655
        total_loss: -0.005205616820603609
        vf_explained_var: 0.018452033400535583
        vf_loss: 0.4373207688331604
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9487049579620361
        entropy_coeff: 0.0017600000137463212
        kl: 0.002046193229034543
        model: {}
        policy_loss: -0.002827452262863517
        total_loss: -0.004483762197196484
        vf_explained_var: -0.006043225526809692
        vf_loss: 0.13408073782920837
    load_time_ms: 19118.967
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 107671.095
    update_time_ms: 14.882
  iterations_since_restore: 71
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.008542713567838
    ram_util_percent: 9.262814070351757
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 24.0
    agent-3: 10.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 1.94
    agent-1: 1.65
    agent-2: 4.25
    agent-3: 3.22
    agent-4: 4.47
    agent-5: 1.89
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.590544211629012
    mean_inference_ms: 13.704468268175031
    mean_processing_ms: 64.723493720568
  time_since_restore: 9936.984134674072
  time_this_iter_s: 139.0849039554596
  time_total_s: 32447.267377138138
  timestamp: 1637230085
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    231 |          32447.3 | 22176000 |    17.42 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 7
    apples_agent-3_mean: 2.14
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.46
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 1.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 111
    cleaning_beam_agent-0_mean: 68.47
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 219.17
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 19.86
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 59.14
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 268
    cleaning_beam_agent-4_mean: 78.02
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 19.91
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.08
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-10-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 40.0
  episode_reward_mean: 14.3
  episode_reward_min: -283.0
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11683.639
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.560444712638855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012641589855775237
        model: {}
        policy_loss: -0.0025552783627063036
        total_loss: -0.0034135128371417522
        vf_explained_var: -0.0026322752237319946
        vf_loss: 1.2814934253692627
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5624154806137085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016061835922300816
        model: {}
        policy_loss: -0.003020427655428648
        total_loss: -0.0040013231337070465
        vf_explained_var: 0.01894344389438629
        vf_loss: 0.08953012526035309
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5120071172714233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014549943152815104
        model: {}
        policy_loss: -0.0037028498481959105
        total_loss: -0.004564085975289345
        vf_explained_var: 0.012360244989395142
        vf_loss: 0.3989766836166382
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6029083728790283
        entropy_coeff: 0.0017600000137463212
        kl: 0.001415713457390666
        model: {}
        policy_loss: -0.0029992545023560524
        total_loss: -0.0040400829166173935
        vf_explained_var: -0.009009480476379395
        vf_loss: 0.2029460072517395
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.742007851600647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025704456493258476
        model: {}
        policy_loss: -0.002738889306783676
        total_loss: -0.00169714679941535
        vf_explained_var: 0.0011245012283325195
        vf_loss: 23.476749420166016
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9758723378181458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012855171225965023
        model: {}
        policy_loss: -0.003033693414181471
        total_loss: -0.0047380090691149235
        vf_explained_var: 0.008580133318901062
        vf_loss: 0.1322154402732849
    load_time_ms: 19191.018
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 107467.988
    update_time_ms: 14.933
  iterations_since_restore: 72
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.27309644670051
    ram_util_percent: 9.29238578680203
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 4.0
    agent-2: 12.0
    agent-3: 10.0
    agent-4: 18.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.43
    agent-1: 1.5
    agent-2: 4.29
    agent-3: 2.87
    agent-4: 1.49
    agent-5: 1.72
  policy_reward_min:
    agent-0: -47.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -248.0
    agent-5: -4.0
  sampler_perf:
    mean_env_wait_ms: 25.586290528873448
    mean_inference_ms: 13.703354063600653
    mean_processing_ms: 64.72087614918476
  time_since_restore: 10075.057299613953
  time_this_iter_s: 138.07316493988037
  time_total_s: 32585.34054207802
  timestamp: 1637230223
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    232 |          32585.3 | 22272000 |     14.3 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 3.16
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.63
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 100
    apples_agent-4_mean: 2.02
    apples_agent-4_min: 0
    apples_agent-5_max: 62
    apples_agent-5_mean: 2.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 138
    cleaning_beam_agent-0_mean: 72.25
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 237.03
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 17.96
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 59.08
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 201
    cleaning_beam_agent-4_mean: 79.73
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 19.32
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-12-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 13.98
  episode_reward_min: -283.0
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11674.328
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5612027645111084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015658893389627337
        model: {}
        policy_loss: -0.003663234878331423
        total_loss: -0.0046215662732720375
        vf_explained_var: 0.0003007054328918457
        vf_loss: 0.2938154339790344
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5876248478889465
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007326241466216743
        model: {}
        policy_loss: -0.0025638840161263943
        total_loss: -0.0035818303003907204
        vf_explained_var: 0.015071526169776917
        vf_loss: 0.16274258494377136
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5074368119239807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016586626879870892
        model: {}
        policy_loss: -0.0036729390267282724
        total_loss: -0.004524315241724253
        vf_explained_var: 0.005592167377471924
        vf_loss: 0.41715380549430847
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5955322980880737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009530064999125898
        model: {}
        policy_loss: -0.002304578199982643
        total_loss: -0.0031772861257195473
        vf_explained_var: -0.0008212476968765259
        vf_loss: 1.75429105758667
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7472985982894897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014431263552978635
        model: {}
        policy_loss: -0.003796714823693037
        total_loss: -0.00507689593359828
        vf_explained_var: 0.02457432448863983
        vf_loss: 0.3506159782409668
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9735363125801086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018768341979011893
        model: {}
        policy_loss: -0.0030558952130377293
        total_loss: -0.00475494796410203
        vf_explained_var: 2.2172927856445312e-05
        vf_loss: 0.143703430891037
    load_time_ms: 19232.149
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 107303.018
    update_time_ms: 15.038
  iterations_since_restore: 73
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.93179487179487
    ram_util_percent: 9.286153846153844
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 16.0
    agent-3: 22.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.27
    agent-1: 1.02
    agent-2: 4.11
    agent-3: 3.22
    agent-4: 1.52
    agent-5: 1.84
  policy_reward_min:
    agent-0: -47.0
    agent-1: -47.0
    agent-2: 0.0
    agent-3: -47.0
    agent-4: -248.0
    agent-5: -4.0
  sampler_perf:
    mean_env_wait_ms: 25.58464551051592
    mean_inference_ms: 13.702832298432247
    mean_processing_ms: 64.71679054950901
  time_since_restore: 10212.212644338608
  time_this_iter_s: 137.15534472465515
  time_total_s: 32722.495886802673
  timestamp: 1637230360
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    233 |          32722.5 | 22368000 |    13.98 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.32
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.63
    apples_agent-3_min: 0
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.25
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 130
    cleaning_beam_agent-0_mean: 70.65
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 232.12
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 19.8
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 55.12
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 178
    cleaning_beam_agent-4_mean: 77.8
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 18.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-14-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 17.43
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11682.406
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5498942136764526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017171415966004133
        model: {}
        policy_loss: -0.0038629479240626097
        total_loss: -0.004807622637599707
        vf_explained_var: 0.0014834851026535034
        vf_loss: 0.2313837707042694
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5751000642776489
        entropy_coeff: 0.0017600000137463212
        kl: 0.000952470931224525
        model: {}
        policy_loss: -0.0026779742911458015
        total_loss: -0.0036734193563461304
        vf_explained_var: 0.014718860387802124
        vf_loss: 0.1673019379377365
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5174954533576965
        entropy_coeff: 0.0017600000137463212
        kl: 0.001377711072564125
        model: {}
        policy_loss: -0.003525936510413885
        total_loss: -0.004399034194648266
        vf_explained_var: 0.006631642580032349
        vf_loss: 0.376925528049469
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6005148887634277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018590633990243077
        model: {}
        policy_loss: -0.00331515702418983
        total_loss: -0.0043351855129003525
        vf_explained_var: 0.009200245141983032
        vf_loss: 0.36878377199172974
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7423611879348755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014222570462152362
        model: {}
        policy_loss: -0.003640851704403758
        total_loss: -0.004905169829726219
        vf_explained_var: 0.01730082929134369
        vf_loss: 0.42237117886543274
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.972205638885498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017239588778465986
        model: {}
        policy_loss: -0.001960025168955326
        total_loss: -0.0035268061328679323
        vf_explained_var: 0.00015535950660705566
        vf_loss: 1.4430209398269653
    load_time_ms: 19249.899
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 107299.676
    update_time_ms: 15.152
  iterations_since_restore: 74
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.63401015228426
    ram_util_percent: 9.264974619289339
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.04
    agent-1: 1.73
    agent-2: 3.87
    agent-3: 3.74
    agent-4: 3.7
    agent-5: 1.35
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 25.581378603785343
    mean_inference_ms: 13.701833330115292
    mean_processing_ms: 64.71110409770259
  time_since_restore: 10350.488719463348
  time_this_iter_s: 138.2760751247406
  time_total_s: 32860.771961927414
  timestamp: 1637230499
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    234 |          32860.8 | 22464000 |    17.43 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.71
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.82
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 129
    cleaning_beam_agent-0_mean: 68.13
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 666
    cleaning_beam_agent-1_mean: 226.76
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 110
    cleaning_beam_agent-2_mean: 22.38
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 57.58
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 360
    cleaning_beam_agent-4_mean: 81.18
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 17.22
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-17-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 19.13
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11674.023
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5568081140518188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014010340673848987
        model: {}
        policy_loss: -0.003815750125795603
        total_loss: -0.004769369959831238
        vf_explained_var: 0.003642663359642029
        vf_loss: 0.2636367380619049
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.575994610786438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008717601886019111
        model: {}
        policy_loss: -0.0027744413819164038
        total_loss: -0.003770321374759078
        vf_explained_var: 0.02098594605922699
        vf_loss: 0.1787084937095642
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5241421461105347
        entropy_coeff: 0.0017600000137463212
        kl: 0.001298802555538714
        model: {}
        policy_loss: -0.003920579329133034
        total_loss: -0.004803390242159367
        vf_explained_var: 0.009388044476509094
        vf_loss: 0.3967767357826233
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5919411182403564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016179676167666912
        model: {}
        policy_loss: -0.0030486038886010647
        total_loss: -0.00405355729162693
        vf_explained_var: -0.00019307434558868408
        vf_loss: 0.3686105012893677
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7338595390319824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013867395464330912
        model: {}
        policy_loss: -0.004045117646455765
        total_loss: -0.005305158905684948
        vf_explained_var: 0.018501058220863342
        vf_loss: 0.31549468636512756
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9920108914375305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017572749638929963
        model: {}
        policy_loss: -0.0030452541541308165
        total_loss: -0.004772872664034367
        vf_explained_var: -0.0038340240716934204
        vf_loss: 0.18321539461612701
    load_time_ms: 19294.713
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 107331.995
    update_time_ms: 15.174
  iterations_since_restore: 75
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.429797979797982
    ram_util_percent: 9.266161616161614
  pid: 6435
  policy_reward_max:
    agent-0: 19.0
    agent-1: 13.0
    agent-2: 17.0
    agent-3: 16.0
    agent-4: 11.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.93
    agent-1: 2.1
    agent-2: 4.21
    agent-3: 3.76
    agent-4: 3.9
    agent-5: 2.23
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.578722459543613
    mean_inference_ms: 13.70167683818199
    mean_processing_ms: 64.7101771251489
  time_since_restore: 10489.28280043602
  time_this_iter_s: 138.7940809726715
  time_total_s: 32999.566042900085
  timestamp: 1637230638
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    235 |          32999.6 | 22560000 |    19.13 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.64
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 1.98
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 34
    apples_agent-5_mean: 1.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 66.26
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 224.07
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 148
    cleaning_beam_agent-2_mean: 21.6
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 49.53
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 157
    cleaning_beam_agent-4_mean: 73.8
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 16.4
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-19-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 38.0
  episode_reward_mean: 16.45
  episode_reward_min: -49.0
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11682.317
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5585161447525024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017772975843399763
        model: {}
        policy_loss: -0.0036904909648001194
        total_loss: -0.0046462928876280785
        vf_explained_var: -0.00301457941532135
        vf_loss: 0.27188968658447266
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5701748132705688
        entropy_coeff: 0.0017600000137463212
        kl: 0.00105575961060822
        model: {}
        policy_loss: -0.0026421735528856516
        total_loss: -0.0036320760846138
        vf_explained_var: 0.016552969813346863
        vf_loss: 0.13607865571975708
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5297918319702148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013927194522693753
        model: {}
        policy_loss: -0.00333238672465086
        total_loss: -0.004099398385733366
        vf_explained_var: 0.0009863227605819702
        vf_loss: 1.6542141437530518
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5915465354919434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013061235658824444
        model: {}
        policy_loss: -0.0027918131090700626
        total_loss: -0.003803623840212822
        vf_explained_var: -0.002628415822982788
        vf_loss: 0.2930874526500702
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7350913286209106
        entropy_coeff: 0.0017600000137463212
        kl: 0.001066706608980894
        model: {}
        policy_loss: -0.0028811898082494736
        total_loss: -0.0041252560913562775
        vf_explained_var: 0.015351518988609314
        vf_loss: 0.49693983793258667
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9893982410430908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018020269926637411
        model: {}
        policy_loss: -0.0027364064007997513
        total_loss: -0.00446197297424078
        vf_explained_var: 0.0020045191049575806
        vf_loss: 0.15774774551391602
    load_time_ms: 19154.473
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 107352.165
    update_time_ms: 15.184
  iterations_since_restore: 76
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.682233502538068
    ram_util_percent: 9.306598984771574
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.61
    agent-1: 1.81
    agent-2: 3.66
    agent-3: 3.4
    agent-4: 3.59
    agent-5: 1.38
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.57479091172364
    mean_inference_ms: 13.700621063574117
    mean_processing_ms: 64.7077039205609
  time_since_restore: 10626.983817577362
  time_this_iter_s: 137.70101714134216
  time_total_s: 33137.26706004143
  timestamp: 1637230775
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    236 |          33137.3 | 22656000 |    16.45 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 63
    apples_agent-3_mean: 3.54
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 149
    cleaning_beam_agent-0_mean: 71.17
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 319
    cleaning_beam_agent-1_mean: 210.69
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 19.47
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 55.07
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 188
    cleaning_beam_agent-4_mean: 75.72
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 16.89
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-21-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 16.71
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11687.336
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5685299038887024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012283232063055038
        model: {}
        policy_loss: -0.00273522874340415
        total_loss: -0.003458216320723295
        vf_explained_var: 0.001016363501548767
        vf_loss: 2.7762537002563477
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5606346726417542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013618929078802466
        model: {}
        policy_loss: -0.0030704261735081673
        total_loss: -0.004046247806400061
        vf_explained_var: 0.025716125965118408
        vf_loss: 0.10898423194885254
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5142824053764343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018026434117928147
        model: {}
        policy_loss: -0.004226934164762497
        total_loss: -0.005083961877971888
        vf_explained_var: -0.005628317594528198
        vf_loss: 0.48110437393188477
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5909139513969421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009610385168343782
        model: {}
        policy_loss: -0.0027951179072260857
        total_loss: -0.0038101072423160076
        vf_explained_var: 0.003514394164085388
        vf_loss: 0.250188946723938
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7521592974662781
        entropy_coeff: 0.0017600000137463212
        kl: 0.001488526351749897
        model: {}
        policy_loss: -0.003035616595298052
        total_loss: -0.00425436906516552
        vf_explained_var: 0.010988563299179077
        vf_loss: 1.050459384918213
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9849989414215088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023366580717265606
        model: {}
        policy_loss: -0.0023496190551668406
        total_loss: -0.003935595974326134
        vf_explained_var: 0.000432819128036499
        vf_loss: 1.4761923551559448
    load_time_ms: 19197.85
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 107272.138
    update_time_ms: 15.469
  iterations_since_restore: 77
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.171938775510203
    ram_util_percent: 9.320408163265308
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 17.0
    agent-3: 10.0
    agent-4: 23.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 1.67
    agent-1: 1.74
    agent-2: 4.48
    agent-3: 3.29
    agent-4: 3.91
    agent-5: 1.62
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.568952405273944
    mean_inference_ms: 13.698635703132025
    mean_processing_ms: 64.69314560690384
  time_since_restore: 10764.720975399017
  time_this_iter_s: 137.73715782165527
  time_total_s: 33275.00421786308
  timestamp: 1637230913
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    237 |            33275 | 22752000 |    16.71 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.71
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.73
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.08
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 72.14
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 215.02
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 70
    cleaning_beam_agent-2_mean: 19.53
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 53.45
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 165
    cleaning_beam_agent-4_mean: 80.39
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 17.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-24-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 17.83
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11673.381
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5748012661933899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015425360761582851
        model: {}
        policy_loss: -0.003923432435840368
        total_loss: -0.004913235083222389
        vf_explained_var: 0.006673738360404968
        vf_loss: 0.21849370002746582
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5692917108535767
        entropy_coeff: 0.0017600000137463212
        kl: 0.001327855745330453
        model: {}
        policy_loss: -0.0028509418480098248
        total_loss: -0.003842724487185478
        vf_explained_var: 0.022533804178237915
        vf_loss: 0.10167599469423294
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5335972905158997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016058818437159061
        model: {}
        policy_loss: -0.00252347718924284
        total_loss: -0.0032903170213103294
        vf_explained_var: -0.0038372278213500977
        vf_loss: 1.722893238067627
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6110911965370178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012544742785394192
        model: {}
        policy_loss: -0.0027293877210468054
        total_loss: -0.003780762664973736
        vf_explained_var: -0.0012312829494476318
        vf_loss: 0.24143055081367493
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7401852011680603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016396311111748219
        model: {}
        policy_loss: -0.0039061433635652065
        total_loss: -0.005169453099370003
        vf_explained_var: 0.014965668320655823
        vf_loss: 0.3941938579082489
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9904937744140625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016141379019245505
        model: {}
        policy_loss: -0.002849436132237315
        total_loss: -0.0045738061890006065
        vf_explained_var: 0.0060304999351501465
        vf_loss: 0.1890137791633606
    load_time_ms: 19141.522
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 107188.507
    update_time_ms: 15.398
  iterations_since_restore: 78
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.779591836734696
    ram_util_percent: 9.212755102040818
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 19.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.86
    agent-1: 1.56
    agent-2: 3.83
    agent-3: 3.18
    agent-4: 4.16
    agent-5: 2.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.565164554556098
    mean_inference_ms: 13.698373730791765
    mean_processing_ms: 64.69199331064492
  time_since_restore: 10902.4536318779
  time_this_iter_s: 137.73265647888184
  time_total_s: 33412.736874341965
  timestamp: 1637231051
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    238 |          33412.7 | 22848000 |    17.83 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.62
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 2.48
    apples_agent-3_min: 0
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 133
    cleaning_beam_agent-0_mean: 68.19
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 376
    cleaning_beam_agent-1_mean: 217.1
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 22.59
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 55.64
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 193
    cleaning_beam_agent-4_mean: 80.9
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 17.61
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-26-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 16.41
  episode_reward_min: -47.0
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11673.568
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5724890828132629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017946022562682629
        model: {}
        policy_loss: -0.004234720021486282
        total_loss: -0.0052218372002244
        vf_explained_var: -0.0067469775676727295
        vf_loss: 0.20466962456703186
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5668179392814636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007343919714912772
        model: {}
        policy_loss: -0.0007630884647369385
        total_loss: -0.001619807444512844
        vf_explained_var: 0.008180245757102966
        vf_loss: 1.4088106155395508
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.549220621585846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016153472242876887
        model: {}
        policy_loss: -0.003704599803313613
        total_loss: -0.0046321190893650055
        vf_explained_var: 0.0010515600442886353
        vf_loss: 0.39107751846313477
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6016779541969299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012162188068032265
        model: {}
        policy_loss: -0.002440605778247118
        total_loss: -0.003470276016741991
        vf_explained_var: -0.0009935051202774048
        vf_loss: 0.2928417921066284
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7483471632003784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015250155702233315
        model: {}
        policy_loss: -0.0040077930316329
        total_loss: -0.005293712019920349
        vf_explained_var: 0.005133002996444702
        vf_loss: 0.31171098351478577
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.991584062576294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016153339529410005
        model: {}
        policy_loss: -0.0030582998879253864
        total_loss: -0.004785939119756222
        vf_explained_var: 0.00878053903579712
        vf_loss: 0.17549435794353485
    load_time_ms: 19246.269
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 107103.524
    update_time_ms: 15.333
  iterations_since_restore: 79
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.3469696969697
    ram_util_percent: 9.303535353535352
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 0.9
    agent-2: 3.7
    agent-3: 3.21
    agent-4: 3.51
    agent-5: 2.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: -1.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.561717310188033
    mean_inference_ms: 13.697693193753656
    mean_processing_ms: 64.68892057133003
  time_since_restore: 11041.340932130814
  time_this_iter_s: 138.88730025291443
  time_total_s: 33551.62417459488
  timestamp: 1637231190
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    239 |          33551.6 | 22944000 |    16.41 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.69
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.71
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.28
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.27
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 67.71
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 215.74
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 139
    cleaning_beam_agent-2_mean: 24.84
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 55.49
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 71.02
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 18.76
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-28-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 17.23
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11653.359
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5713357925415039
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015263555105775595
        model: {}
        policy_loss: -0.0038026284892112017
        total_loss: -0.0047895824536681175
        vf_explained_var: -0.0014927983283996582
        vf_loss: 0.18595287203788757
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5653977394104004
        entropy_coeff: 0.0017600000137463212
        kl: 0.000951614580117166
        model: {}
        policy_loss: -0.002791902981698513
        total_loss: -0.0037773342337459326
        vf_explained_var: 0.026224151253700256
        vf_loss: 0.09668013453483582
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.541105329990387
        entropy_coeff: 0.0017600000137463212
        kl: 0.001187985180877149
        model: {}
        policy_loss: -0.0035640059504657984
        total_loss: -0.004473773762583733
        vf_explained_var: -0.0009576082229614258
        vf_loss: 0.4257946014404297
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6121244430541992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013171825557947159
        model: {}
        policy_loss: -0.0031367805786430836
        total_loss: -0.004184907302260399
        vf_explained_var: 0.002187967300415039
        vf_loss: 0.2921202480792999
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7460904121398926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014867288991808891
        model: {}
        policy_loss: -0.003826944623142481
        total_loss: -0.005093107465654612
        vf_explained_var: 0.008818641304969788
        vf_loss: 0.4695749878883362
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9900602102279663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016413942212238908
        model: {}
        policy_loss: -0.0026791011914610863
        total_loss: -0.004401208367198706
        vf_explained_var: 0.00044707953929901123
        vf_loss: 0.20398733019828796
    load_time_ms: 19135.458
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 106995.935
    update_time_ms: 15.498
  iterations_since_restore: 80
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.835751295336788
    ram_util_percent: 9.23056994818653
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 2.37
    agent-1: 1.46
    agent-2: 3.84
    agent-3: 3.17
    agent-4: 4.3
    agent-5: 2.09
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.55613573142747
    mean_inference_ms: 13.696149966127793
    mean_processing_ms: 64.67935788513381
  time_since_restore: 11176.912738800049
  time_this_iter_s: 135.57180666923523
  time_total_s: 33687.195981264114
  timestamp: 1637231326
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    240 |          33687.2 | 23040000 |    17.23 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 1.58
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 2.83
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 67.92
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 220.94
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 20.09
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 56.48
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 172
    cleaning_beam_agent-4_mean: 78.35
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 17.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-31-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 18.59
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11652.085
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5589380264282227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013081340584903955
        model: {}
        policy_loss: -0.003515808144584298
        total_loss: -0.004478275775909424
        vf_explained_var: -0.0015099197626113892
        vf_loss: 0.21265164017677307
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5710040330886841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010289413621649146
        model: {}
        policy_loss: -0.0027037393301725388
        total_loss: -0.0036985306069254875
        vf_explained_var: 0.014665886759757996
        vf_loss: 0.10175105929374695
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5431650280952454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015229845885187387
        model: {}
        policy_loss: -0.00391651364043355
        total_loss: -0.004841493908315897
        vf_explained_var: 0.007352560758590698
        vf_loss: 0.3098585605621338
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6029787659645081
        entropy_coeff: 0.0017600000137463212
        kl: 0.001368981902487576
        model: {}
        policy_loss: -0.0027443612925708294
        total_loss: -0.003774387529119849
        vf_explained_var: 0.002582073211669922
        vf_loss: 0.31215700507164
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7418363094329834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021295221522450447
        model: {}
        policy_loss: -0.0038552400656044483
        total_loss: -0.005123441107571125
        vf_explained_var: 0.014018580317497253
        vf_loss: 0.37434232234954834
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9912481307983398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018982692854478955
        model: {}
        policy_loss: -0.002945589367300272
        total_loss: -0.004675753880292177
        vf_explained_var: 0.0006465613842010498
        vf_loss: 0.14432275295257568
    load_time_ms: 19085.659
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 106862.707
    update_time_ms: 15.416
  iterations_since_restore: 81
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.718274111675125
    ram_util_percent: 9.303553299492384
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 1.66
    agent-2: 3.89
    agent-3: 3.63
    agent-4: 4.52
    agent-5: 2.04
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.54928387080939
    mean_inference_ms: 13.694560722430978
    mean_processing_ms: 64.67094812723776
  time_since_restore: 11314.154242753983
  time_this_iter_s: 137.24150395393372
  time_total_s: 33824.43748521805
  timestamp: 1637231464
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    241 |          33824.4 | 23136000 |    18.59 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.46
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.53
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 42
    apples_agent-5_mean: 1.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 123
    cleaning_beam_agent-0_mean: 67.07
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 223.69
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 78
    cleaning_beam_agent-2_mean: 21.66
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 63.79
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 188
    cleaning_beam_agent-4_mean: 82.56
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 111
    cleaning_beam_agent-5_mean: 15.96
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-33-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 18.34
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11639.114
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5666308403015137
        entropy_coeff: 0.0017600000137463212
        kl: 0.001303981407545507
        model: {}
        policy_loss: -0.0037628242280334234
        total_loss: -0.004735634662210941
        vf_explained_var: 0.00483277440071106
        vf_loss: 0.24461688101291656
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5800290107727051
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010007394012063742
        model: {}
        policy_loss: -0.0029724163468927145
        total_loss: -0.003978322260081768
        vf_explained_var: 0.03172723948955536
        vf_loss: 0.1494603455066681
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5324715375900269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013275188393890858
        model: {}
        policy_loss: -0.0036421886179596186
        total_loss: -0.004534598905593157
        vf_explained_var: 0.0045614093542099
        vf_loss: 0.4473724365234375
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5987542271614075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010269113117828965
        model: {}
        policy_loss: -0.002813798375427723
        total_loss: -0.003836303483694792
        vf_explained_var: -0.002406775951385498
        vf_loss: 0.3130183219909668
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7365095615386963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014655636623501778
        model: {}
        policy_loss: -0.004094662144780159
        total_loss: -0.0053557176142930984
        vf_explained_var: 0.018111899495124817
        vf_loss: 0.3519783020019531
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9823163747787476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016466609667986631
        model: {}
        policy_loss: -0.0028703156858682632
        total_loss: -0.004582568071782589
        vf_explained_var: 0.01768706738948822
        vf_loss: 0.1662445366382599
    load_time_ms: 19020.172
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 106929.894
    update_time_ms: 15.316
  iterations_since_restore: 82
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.74438775510204
    ram_util_percent: 9.217857142857143
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.92
    agent-1: 1.98
    agent-2: 3.84
    agent-3: 3.48
    agent-4: 4.04
    agent-5: 2.08
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.546059816130995
    mean_inference_ms: 13.693793324912521
    mean_processing_ms: 64.6687676604611
  time_since_restore: 11452.083108186722
  time_this_iter_s: 137.92886543273926
  time_total_s: 33962.36635065079
  timestamp: 1637231602
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    242 |          33962.4 | 23232000 |    18.34 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.46
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 2.6
    apples_agent-3_min: 0
    apples_agent-4_max: 31
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 127
    cleaning_beam_agent-0_mean: 67.97
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 211.45
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 102
    cleaning_beam_agent-2_mean: 20.21
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 57.1
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 365
    cleaning_beam_agent-4_mean: 84.56
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 17.61
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-35-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 17.52
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11657.201
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5767244100570679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018373900093138218
        model: {}
        policy_loss: -0.003682300914078951
        total_loss: -0.004669811110943556
        vf_explained_var: 0.000838935375213623
        vf_loss: 0.2752663791179657
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5650245547294617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015332292532548308
        model: {}
        policy_loss: -0.0017271838150918484
        total_loss: -0.0025808466598391533
        vf_explained_var: 0.007855474948883057
        vf_loss: 1.4077963829040527
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5273582935333252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018807647284120321
        model: {}
        policy_loss: -0.003811520989984274
        total_loss: -0.004701710771769285
        vf_explained_var: 0.006601706147193909
        vf_loss: 0.37960779666900635
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.590613842010498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010361444437876344
        model: {}
        policy_loss: -0.002177769085392356
        total_loss: -0.0030688862316310406
        vf_explained_var: -0.002481400966644287
        vf_loss: 1.4836452007293701
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7539321184158325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016489096451550722
        model: {}
        policy_loss: -0.00394944055005908
        total_loss: -0.005244546569883823
        vf_explained_var: 0.009425804018974304
        vf_loss: 0.31816017627716064
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9930484890937805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012833247892558575
        model: {}
        policy_loss: -0.0027569688390940428
        total_loss: -0.0044916048645973206
        vf_explained_var: 0.010000959038734436
        vf_loss: 0.13126057386398315
    load_time_ms: 19049.633
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 107007.511
    update_time_ms: 15.074
  iterations_since_restore: 83
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.710606060606064
    ram_util_percent: 9.298989898989898
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.17
    agent-1: 1.16
    agent-2: 4.12
    agent-3: 3.05
    agent-4: 4.11
    agent-5: 1.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: -44.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.543476030355073
    mean_inference_ms: 13.692889315703347
    mean_processing_ms: 64.66277701177722
  time_since_restore: 11590.487682580948
  time_this_iter_s: 138.40457439422607
  time_total_s: 34100.77092504501
  timestamp: 1637231740
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    243 |          34100.8 | 23328000 |    17.52 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.98
    apples_agent-4_min: 0
    apples_agent-5_max: 33
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 63.62
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 212.5
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 116
    cleaning_beam_agent-2_mean: 19.19
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 59.57
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 187
    cleaning_beam_agent-4_mean: 84.68
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 18.11
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-37-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 18.95
  episode_reward_min: -5.0
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11656.951
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5599823594093323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015067782951518893
        model: {}
        policy_loss: -0.003563724923878908
        total_loss: -0.004523347597569227
        vf_explained_var: 0.0042355358600616455
        vf_loss: 0.2594442367553711
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.571090817451477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012644141679629683
        model: {}
        policy_loss: -0.002893992466852069
        total_loss: -0.003883443307131529
        vf_explained_var: 0.016012459993362427
        vf_loss: 0.1566857397556305
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5283886194229126
        entropy_coeff: 0.0017600000137463212
        kl: 0.001168981078080833
        model: {}
        policy_loss: -0.0034845350310206413
        total_loss: -0.004368189722299576
        vf_explained_var: -0.00666046142578125
        vf_loss: 0.4630804657936096
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5970772504806519
        entropy_coeff: 0.0017600000137463212
        kl: 0.000991641660220921
        model: {}
        policy_loss: -0.002612303476780653
        total_loss: -0.0036241530906409025
        vf_explained_var: 0.004224881529808044
        vf_loss: 0.390064537525177
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7487305402755737
        entropy_coeff: 0.0017600000137463212
        kl: 0.001570589025504887
        model: {}
        policy_loss: -0.0039036672096699476
        total_loss: -0.005188500974327326
        vf_explained_var: 0.026225343346595764
        vf_loss: 0.3292984664440155
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9767476916313171
        entropy_coeff: 0.0017600000137463212
        kl: 0.002367735607549548
        model: {}
        policy_loss: -0.0022263070568442345
        total_loss: -0.0038000736385583878
        vf_explained_var: -0.0019193589687347412
        vf_loss: 1.453055500984192
    load_time_ms: 19033.904
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 106927.667
    update_time_ms: 14.818
  iterations_since_restore: 84
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.357435897435895
    ram_util_percent: 9.26974358974359
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 3.15
    agent-1: 1.87
    agent-2: 4.31
    agent-3: 4.16
    agent-4: 3.87
    agent-5: 1.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 25.5400555589948
    mean_inference_ms: 13.692005348634797
    mean_processing_ms: 64.65984853529255
  time_since_restore: 11727.802005767822
  time_this_iter_s: 137.3143231868744
  time_total_s: 34238.08524823189
  timestamp: 1637231878
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    244 |          34238.1 | 23424000 |    18.95 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 1.62
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.26
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 124
    cleaning_beam_agent-0_mean: 68.03
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 207.47
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 238
    cleaning_beam_agent-2_mean: 20.61
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 65.97
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 173
    cleaning_beam_agent-4_mean: 81.9
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 16.7
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-40-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 35.0
  episode_reward_mean: 15.63
  episode_reward_min: -69.0
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11660.968
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704421997070312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006216114852577448
        model: {}
        policy_loss: -0.0015789964236319065
        total_loss: -0.0024690115824341774
        vf_explained_var: -0.00024512410163879395
        vf_loss: 1.1396442651748657
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5738893747329712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009016575640998781
        model: {}
        policy_loss: -0.00305553968064487
        total_loss: -0.004055686295032501
        vf_explained_var: 0.025212213397026062
        vf_loss: 0.09900228679180145
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5271918773651123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009559699101373553
        model: {}
        policy_loss: -0.003411895129829645
        total_loss: -0.004307475406676531
        vf_explained_var: -0.0021946877241134644
        vf_loss: 0.3227488696575165
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5853685736656189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015370147302746773
        model: {}
        policy_loss: -0.0025909901596605778
        total_loss: -0.003593047149479389
        vf_explained_var: -0.0024928897619247437
        vf_loss: 0.2818946838378906
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7550251483917236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012493288377299905
        model: {}
        policy_loss: -0.0037517272867262363
        total_loss: -0.005048785358667374
        vf_explained_var: 0.020821228623390198
        vf_loss: 0.3178923428058624
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0083726644515991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023244409821927547
        model: {}
        policy_loss: -0.002881883643567562
        total_loss: -0.004544920288026333
        vf_explained_var: 0.0017623156309127808
        vf_loss: 1.116969347000122
    load_time_ms: 19081.604
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 106883.277
    update_time_ms: 14.933
  iterations_since_restore: 85
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.07323232323232
    ram_util_percent: 9.294949494949496
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 6.0
    agent-2: 13.0
    agent-3: 10.0
    agent-4: 13.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.15
    agent-1: 1.51
    agent-2: 3.37
    agent-3: 3.44
    agent-4: 3.56
    agent-5: 1.6
  policy_reward_min:
    agent-0: -40.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 25.536271477984943
    mean_inference_ms: 13.690598702122646
    mean_processing_ms: 64.65293912295346
  time_since_restore: 11866.66954445839
  time_this_iter_s: 138.86753869056702
  time_total_s: 34376.952786922455
  timestamp: 1637232017
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    245 |            34377 | 23520000 |    15.63 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.75
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 149
    cleaning_beam_agent-0_mean: 64.87
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 213.62
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 23.43
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 68.77
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 198
    cleaning_beam_agent-4_mean: 85.63
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 14.21
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-42-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 17.33
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11671.225
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5715276598930359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010952434968203306
        model: {}
        policy_loss: -0.0035445773974061012
        total_loss: -0.00453104916960001
        vf_explained_var: 0.00046353042125701904
        vf_loss: 0.19417038559913635
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704610347747803
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009299752418883145
        model: {}
        policy_loss: -0.0015315376222133636
        total_loss: -0.0023861369118094444
        vf_explained_var: 0.0001620948314666748
        vf_loss: 1.494119644165039
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5422692894935608
        entropy_coeff: 0.0017600000137463212
        kl: 0.002206485252827406
        model: {}
        policy_loss: -0.002520219422876835
        total_loss: -0.0032898266799747944
        vf_explained_var: 0.0023253560066223145
        vf_loss: 1.8478692770004272
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6103642582893372
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013799131847918034
        model: {}
        policy_loss: -0.003050202503800392
        total_loss: -0.004093203693628311
        vf_explained_var: 0.00037294626235961914
        vf_loss: 0.312400758266449
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7272375822067261
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015553426928818226
        model: {}
        policy_loss: -0.003927459008991718
        total_loss: -0.005165221635252237
        vf_explained_var: 0.024940624833106995
        vf_loss: 0.421750009059906
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9967825412750244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028329319320619106
        model: {}
        policy_loss: -0.0024547928478568792
        total_loss: -0.004009706899523735
        vf_explained_var: 0.0015919357538223267
        vf_loss: 1.9942309856414795
    load_time_ms: 19155.625
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 106915.001
    update_time_ms: 14.934
  iterations_since_restore: 86
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.613636363636363
    ram_util_percent: 9.30808080808081
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 5.0
    agent-2: 20.0
    agent-3: 10.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 0.78
    agent-2: 3.94
    agent-3: 3.86
    agent-4: 4.49
    agent-5: 1.41
  policy_reward_min:
    agent-0: -1.0
    agent-1: -50.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.53416857541056
    mean_inference_ms: 13.690114792234372
    mean_processing_ms: 64.65394045119791
  time_since_restore: 12005.54631114006
  time_this_iter_s: 138.87676668167114
  time_total_s: 34515.829553604126
  timestamp: 1637232156
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    246 |          34515.8 | 23616000 |    17.33 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 1.57
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.9
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 175
    cleaning_beam_agent-0_mean: 68.81
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 215.7
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 88
    cleaning_beam_agent-2_mean: 20.69
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 165
    cleaning_beam_agent-3_mean: 72.76
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 86.82
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 16.4
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-44-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 16.43
  episode_reward_min: -128.0
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11676.495
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5767565369606018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014592414954677224
        model: {}
        policy_loss: -0.0036607012152671814
        total_loss: -0.0046445392072200775
        vf_explained_var: 0.0016483813524246216
        vf_loss: 0.31256574392318726
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5865559577941895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010413627605885267
        model: {}
        policy_loss: -0.0015337178483605385
        total_loss: -0.0024207155220210552
        vf_explained_var: 5.938112735748291e-05
        vf_loss: 1.4534059762954712
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5317385792732239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015525366179645061
        model: {}
        policy_loss: -0.0029161665588617325
        total_loss: -0.003685818752273917
        vf_explained_var: -9.714066982269287e-05
        vf_loss: 1.6620922088623047
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6197620630264282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012421493884176016
        model: {}
        policy_loss: -0.0027702837251126766
        total_loss: -0.0038255355320870876
        vf_explained_var: 0.002647712826728821
        vf_loss: 0.3552716374397278
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7375558614730835
        entropy_coeff: 0.0017600000137463212
        kl: 0.00116072129458189
        model: {}
        policy_loss: -0.0028390930965542793
        total_loss: -0.0038460418581962585
        vf_explained_var: 0.0035781115293502808
        vf_loss: 2.9115216732025146
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.003840446472168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014793586451560259
        model: {}
        policy_loss: -0.0019459282048046589
        total_loss: -0.00355979660525918
        vf_explained_var: 0.000731930136680603
        vf_loss: 1.528909683227539
    load_time_ms: 19103.958
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 106938.963
    update_time_ms: 14.689
  iterations_since_restore: 87
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.70561224489796
    ram_util_percent: 9.323979591836734
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 22.0
    agent-3: 20.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.13
    agent-2: 3.31
    agent-3: 3.75
    agent-4: 2.98
    agent-5: 2.01
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: -41.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 25.53211744001853
    mean_inference_ms: 13.689507991010332
    mean_processing_ms: 64.64552928815105
  time_since_restore: 12143.050881624222
  time_this_iter_s: 137.50457048416138
  time_total_s: 34653.33412408829
  timestamp: 1637232294
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    247 |          34653.3 | 23712000 |    16.43 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.65
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.95
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 162
    cleaning_beam_agent-0_mean: 70.45
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 217.79
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 128
    cleaning_beam_agent-2_mean: 21.96
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 65.9
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 86.44
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 16.76
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-47-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 17.57
  episode_reward_min: -89.0
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11678.845
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5782464742660522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015357037773355842
        model: {}
        policy_loss: -0.004159913863986731
        total_loss: -0.005151835735887289
        vf_explained_var: 0.002076968550682068
        vf_loss: 0.2579370141029358
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.563767671585083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012408769689500332
        model: {}
        policy_loss: -0.002075046766549349
        total_loss: -0.0030408811289817095
        vf_explained_var: 0.012452811002731323
        vf_loss: 0.26395994424819946
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5277278423309326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011552273062989116
        model: {}
        policy_loss: -0.0023805159144103527
        total_loss: -0.0032315808348357677
        vf_explained_var: 0.005733057856559753
        vf_loss: 0.7773290276527405
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6048202514648438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012581096962094307
        model: {}
        policy_loss: -0.0026229058858007193
        total_loss: -0.0036532115191221237
        vf_explained_var: 0.00029362738132476807
        vf_loss: 0.341785192489624
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7449489235877991
        entropy_coeff: 0.0017600000137463212
        kl: 0.001932794344611466
        model: {}
        policy_loss: -0.002801134716719389
        total_loss: -0.00391099788248539
        vf_explained_var: 0.009250164031982422
        vf_loss: 2.0124692916870117
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0125577449798584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020690353121608496
        model: {}
        policy_loss: -0.0022301294375211
        total_loss: -0.0038506421260535717
        vf_explained_var: -0.002034783363342285
        vf_loss: 1.6158884763717651
    load_time_ms: 19144.498
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 106968.574
    update_time_ms: 14.831
  iterations_since_restore: 88
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.45757575757576
    ram_util_percent: 9.276262626262625
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 23.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.39
    agent-1: 1.33
    agent-2: 4.09
    agent-3: 4.29
    agent-4: 3.33
    agent-5: 1.14
  policy_reward_min:
    agent-0: 0.0
    agent-1: -46.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.53211976626756
    mean_inference_ms: 13.689476866649265
    mean_processing_ms: 64.64837120638538
  time_since_restore: 12281.562462568283
  time_this_iter_s: 138.51158094406128
  time_total_s: 34791.84570503235
  timestamp: 1637232432
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    248 |          34791.8 | 23808000 |    17.57 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 1.55
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.68
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 75.38
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 212.09
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 77
    cleaning_beam_agent-2_mean: 19.31
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 69.11
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 85.84
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 15.97
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-49-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 18.36
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11678.293
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5833189487457275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018518988508731127
        model: {}
        policy_loss: -0.004101291298866272
        total_loss: -0.005107153207063675
        vf_explained_var: 0.000755608081817627
        vf_loss: 0.20775490999221802
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5663459300994873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011949336621910334
        model: {}
        policy_loss: -0.0029933135956525803
        total_loss: -0.003976403269916773
        vf_explained_var: 0.03038233518600464
        vf_loss: 0.13682357966899872
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5337064862251282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011937879025936127
        model: {}
        policy_loss: -0.003706469666212797
        total_loss: -0.004614618141204119
        vf_explained_var: 0.002686053514480591
        vf_loss: 0.3117898106575012
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6106809377670288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013201548717916012
        model: {}
        policy_loss: -0.0027629444375634193
        total_loss: -0.0038073575124144554
        vf_explained_var: -0.0010891854763031006
        vf_loss: 0.303829163312912
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7511453032493591
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018282445380464196
        model: {}
        policy_loss: -0.004276217892765999
        total_loss: -0.005563347600400448
        vf_explained_var: 0.006863117218017578
        vf_loss: 0.3488735556602478
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0116300582885742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019299176055938005
        model: {}
        policy_loss: -0.0030853240750730038
        total_loss: -0.004851911682635546
        vf_explained_var: 0.0010237544775009155
        vf_loss: 0.13881699740886688
    load_time_ms: 18982.747
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 107219.05
    update_time_ms: 14.978
  iterations_since_restore: 89
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.629145728643216
    ram_util_percent: 9.25678391959799
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 12.0
    agent-3: 12.0
    agent-4: 11.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.11
    agent-1: 1.91
    agent-2: 3.6
    agent-3: 3.89
    agent-4: 3.78
    agent-5: 2.07
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.532448339130628
    mean_inference_ms: 13.68986280586637
    mean_processing_ms: 64.65124730902086
  time_since_restore: 12421.330777406693
  time_this_iter_s: 139.76831483840942
  time_total_s: 34931.61401987076
  timestamp: 1637232572
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    249 |          34931.6 | 23904000 |    18.36 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.06
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 78.31
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 306
    cleaning_beam_agent-1_mean: 204.49
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 19.37
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 69.21
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 165
    cleaning_beam_agent-4_mean: 79.92
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 19.55
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-51-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 16.69
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11687.312
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5802900195121765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013329563662409782
        model: {}
        policy_loss: -0.0035116826184093952
        total_loss: -0.004508888814598322
        vf_explained_var: 0.0025997310876846313
        vf_loss: 0.2410360723733902
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5745194554328918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004866327508352697
        model: {}
        policy_loss: -0.0013353575486689806
        total_loss: -0.002035093493759632
        vf_explained_var: 0.012336179614067078
        vf_loss: 3.114180564880371
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5228598117828369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015268407296389341
        model: {}
        policy_loss: -0.003870832733809948
        total_loss: -0.0047550685703754425
        vf_explained_var: -0.0012610554695129395
        vf_loss: 0.3599908649921417
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6041856408119202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011594914831221104
        model: {}
        policy_loss: -0.002692596986889839
        total_loss: -0.003731176257133484
        vf_explained_var: 0.003550320863723755
        vf_loss: 0.24790313839912415
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7480819225311279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016138883074745536
        model: {}
        policy_loss: -0.0036866087466478348
        total_loss: -0.004967356566339731
        vf_explained_var: 0.0142136812210083
        vf_loss: 0.3587746024131775
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0164391994476318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014123141299933195
        model: {}
        policy_loss: -0.0029461588710546494
        total_loss: -0.004721533507108688
        vf_explained_var: -0.004557490348815918
        vf_loss: 0.13558360934257507
    load_time_ms: 19052.39
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 107499.313
    update_time_ms: 14.827
  iterations_since_restore: 90
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.425757575757576
    ram_util_percent: 9.237373737373737
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.0
    agent-1: 0.65
    agent-2: 3.99
    agent-3: 3.17
    agent-4: 4.03
    agent-5: 1.85
  policy_reward_min:
    agent-0: -1.0
    agent-1: -100.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.532659509013794
    mean_inference_ms: 13.689082390073514
    mean_processing_ms: 64.65282333428556
  time_since_restore: 12560.50938129425
  time_this_iter_s: 139.17860388755798
  time_total_s: 35070.792623758316
  timestamp: 1637232711
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    250 |          35070.8 | 24000000 |    16.69 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.63
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 2.95
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.81
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 75.42
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 204.18
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 81
    cleaning_beam_agent-2_mean: 21.42
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 62.99
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 181
    cleaning_beam_agent-4_mean: 76.72
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 78
    cleaning_beam_agent-5_mean: 16.49
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-54-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 19.31
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11676.56
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5877508521080017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014236201532185078
        model: {}
        policy_loss: -0.004081259481608868
        total_loss: -0.005087297409772873
        vf_explained_var: 0.0011316239833831787
        vf_loss: 0.283998042345047
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5541833639144897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008333349251188338
        model: {}
        policy_loss: -0.0025680288672447205
        total_loss: -0.0035301372408866882
        vf_explained_var: 0.021763086318969727
        vf_loss: 0.13252483308315277
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.53924560546875
        entropy_coeff: 0.0017600000137463212
        kl: 0.001517607714049518
        model: {}
        policy_loss: -0.0030009367037564516
        total_loss: -0.0038062932435423136
        vf_explained_var: 0.0034943968057632446
        vf_loss: 1.4371567964553833
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6051324605941772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010060935746878386
        model: {}
        policy_loss: -0.0025183544494211674
        total_loss: -0.0035468973219394684
        vf_explained_var: -0.0004827827215194702
        vf_loss: 0.36492210626602173
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7325631380081177
        entropy_coeff: 0.0017600000137463212
        kl: 0.001374220009893179
        model: {}
        policy_loss: -0.00406438997015357
        total_loss: -0.005317483097314835
        vf_explained_var: 0.026737987995147705
        vf_loss: 0.3621978461742401
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0040638446807861
        entropy_coeff: 0.0017600000137463212
        kl: 0.002046463079750538
        model: {}
        policy_loss: -0.0028065908700227737
        total_loss: -0.004556158557534218
        vf_explained_var: 0.00583706796169281
        vf_loss: 0.17587465047836304
    load_time_ms: 19046.251
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 107676.622
    update_time_ms: 15.101
  iterations_since_restore: 91
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.52060301507538
    ram_util_percent: 9.246733668341708
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.18
    agent-1: 1.73
    agent-2: 3.75
    agent-3: 4.0
    agent-4: 4.26
    agent-5: 2.39
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.53361829456091
    mean_inference_ms: 13.689733058971116
    mean_processing_ms: 64.65983005396711
  time_since_restore: 12699.361181020737
  time_this_iter_s: 138.8517997264862
  time_total_s: 35209.6444234848
  timestamp: 1637232851
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    251 |          35209.6 | 24096000 |    19.31 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.27
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 2.81
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.9
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 156
    cleaning_beam_agent-0_mean: 81.53
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 207.5
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 19.76
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 65.91
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 171
    cleaning_beam_agent-4_mean: 73.65
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 19.78
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-56-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 17.35
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11681.635
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.596085786819458
        entropy_coeff: 0.0017600000137463212
        kl: 0.001450483687222004
        model: {}
        policy_loss: -0.0038837995380163193
        total_loss: -0.004904658533632755
        vf_explained_var: 0.005061805248260498
        vf_loss: 0.28253626823425293
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5751265287399292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011841515079140663
        model: {}
        policy_loss: -0.001883674063719809
        total_loss: -0.002751152263954282
        vf_explained_var: 0.0033950507640838623
        vf_loss: 1.4474610090255737
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5310527682304382
        entropy_coeff: 0.0017600000137463212
        kl: 0.001281330594792962
        model: {}
        policy_loss: -0.003857832169160247
        total_loss: -0.00474502332508564
        vf_explained_var: -0.0014455169439315796
        vf_loss: 0.4745923578739166
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6065183281898499
        entropy_coeff: 0.0017600000137463212
        kl: 0.001310507534071803
        model: {}
        policy_loss: -0.002566304989159107
        total_loss: -0.003599305870011449
        vf_explained_var: -0.0034163296222686768
        vf_loss: 0.3447214961051941
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7462828755378723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012875376269221306
        model: {}
        policy_loss: -0.003140259999781847
        total_loss: -0.0042815133929252625
        vf_explained_var: 0.006987899541854858
        vf_loss: 1.7220544815063477
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0135715007781982
        entropy_coeff: 0.0017600000137463212
        kl: 0.002962179481983185
        model: {}
        policy_loss: -0.00192298274487257
        total_loss: -0.0035609158221632242
        vf_explained_var: 0.0039019137620925903
        vf_loss: 1.4595530033111572
    load_time_ms: 19247.168
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 107645.648
    update_time_ms: 15.194
  iterations_since_restore: 92
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.248743718592966
    ram_util_percent: 9.246733668341708
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 6.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.15
    agent-1: 1.24
    agent-2: 4.03
    agent-3: 3.4
    agent-4: 3.97
    agent-5: 1.56
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.52993623713943
    mean_inference_ms: 13.68873479860545
    mean_processing_ms: 64.65702429597891
  time_since_restore: 12839.107275485992
  time_this_iter_s: 139.74609446525574
  time_total_s: 35349.39051795006
  timestamp: 1637232991
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    252 |          35349.4 | 24192000 |    17.35 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.34
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.48
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 2.53
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 81.82
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 208.0
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 21.85
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 68.07
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 169
    cleaning_beam_agent-4_mean: 74.83
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 18.87
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-58-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 18.53
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11662.643
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5836890339851379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014920609537512064
        model: {}
        policy_loss: -0.0039054653607308865
        total_loss: -0.004909008275717497
        vf_explained_var: 0.0020110756158828735
        vf_loss: 0.2375178337097168
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.565241813659668
        entropy_coeff: 0.0017600000137463212
        kl: 0.000860285887029022
        model: {}
        policy_loss: -0.002659209305420518
        total_loss: -0.0036412272602319717
        vf_explained_var: 0.01835392415523529
        vf_loss: 0.12805664539337158
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5321100354194641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018004282610490918
        model: {}
        policy_loss: -0.003812110051512718
        total_loss: -0.004707410000264645
        vf_explained_var: 0.005638539791107178
        vf_loss: 0.4121066927909851
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6054503321647644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011306445812806487
        model: {}
        policy_loss: -0.0026962277479469776
        total_loss: -0.0037338065449148417
        vf_explained_var: -0.005447566509246826
        vf_loss: 0.2801355719566345
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7419140338897705
        entropy_coeff: 0.0017600000137463212
        kl: 0.00167274777777493
        model: {}
        policy_loss: -0.003899922128766775
        total_loss: -0.005173982586711645
        vf_explained_var: 0.013750165700912476
        vf_loss: 0.3170984983444214
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9990772604942322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021537705324590206
        model: {}
        policy_loss: -0.0029496378265321255
        total_loss: -0.004690173547714949
        vf_explained_var: 0.006330356001853943
        vf_loss: 0.1784212291240692
    load_time_ms: 19196.089
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 107738.845
    update_time_ms: 15.434
  iterations_since_restore: 93
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.651010101010105
    ram_util_percent: 9.25151515151515
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.88
    agent-1: 1.67
    agent-2: 3.99
    agent-3: 3.38
    agent-4: 4.21
    agent-5: 2.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.527701983056527
    mean_inference_ms: 13.688454206274159
    mean_processing_ms: 64.65924517202544
  time_since_restore: 12977.742051839828
  time_this_iter_s: 138.63477635383606
  time_total_s: 35488.025294303894
  timestamp: 1637233130
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    253 |            35488 | 24288000 |    18.53 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.63
    apples_agent-2_min: 0
    apples_agent-3_max: 51
    apples_agent-3_mean: 3.25
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 80.94
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 219.26
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 20.14
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 63.39
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 207
    cleaning_beam_agent-4_mean: 71.92
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 78
    cleaning_beam_agent-5_mean: 19.99
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.07
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-01-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 41.0
  episode_reward_mean: 15.09
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11657.772
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5831267237663269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019947567488998175
        model: {}
        policy_loss: -0.002393658272922039
        total_loss: -0.003229367546737194
        vf_explained_var: 0.0007140040397644043
        vf_loss: 1.9059288501739502
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5816013813018799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010600137757137418
        model: {}
        policy_loss: -0.0028075214941054583
        total_loss: -0.003819654230028391
        vf_explained_var: 0.014068126678466797
        vf_loss: 0.11484883725643158
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5380899310112
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015067008789628744
        model: {}
        policy_loss: -0.002597428858280182
        total_loss: -0.003254678100347519
        vf_explained_var: -0.002732217311859131
        vf_loss: 2.8979320526123047
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5992112755775452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009527542861178517
        model: {}
        policy_loss: -0.002608349546790123
        total_loss: -0.0036359287332743406
        vf_explained_var: -0.003790527582168579
        vf_loss: 0.2703104615211487
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7463390827178955
        entropy_coeff: 0.0017600000137463212
        kl: 0.001593987224623561
        model: {}
        policy_loss: -0.0029051881283521652
        total_loss: -0.004049914889037609
        vf_explained_var: 0.006939142942428589
        vf_loss: 1.6882874965667725
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0095696449279785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023037961218506098
        model: {}
        policy_loss: -0.003299817442893982
        total_loss: -0.005064291413873434
        vf_explained_var: -0.0029366761445999146
        vf_loss: 0.12370319664478302
    load_time_ms: 19117.504
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 107849.387
    update_time_ms: 15.52
  iterations_since_restore: 94
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.303061224489795
    ram_util_percent: 9.278571428571428
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 1.85
    agent-1: 1.62
    agent-2: 2.42
    agent-3: 3.55
    agent-4: 3.79
    agent-5: 1.86
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: -6.0
  sampler_perf:
    mean_env_wait_ms: 25.525491246807697
    mean_inference_ms: 13.688618671723505
    mean_processing_ms: 64.65915653543787
  time_since_restore: 13115.327944517136
  time_this_iter_s: 137.58589267730713
  time_total_s: 35625.6111869812
  timestamp: 1637233267
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    254 |          35625.6 | 24384000 |    15.09 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.76
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 82.11
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 212.32
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 22.43
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 72.11
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 70.03
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 20.51
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-03-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 41.0
  episode_reward_mean: 18.29
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11656.208
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5818500518798828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016515902243554592
        model: {}
        policy_loss: -0.003651591017842293
        total_loss: -0.004651026334613562
        vf_explained_var: 0.002257615327835083
        vf_loss: 0.24620935320854187
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5664633512496948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011275897268205881
        model: {}
        policy_loss: -0.0027527320198714733
        total_loss: -0.00373562378808856
        vf_explained_var: 0.022852003574371338
        vf_loss: 0.14082440733909607
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.527220606803894
        entropy_coeff: 0.0017600000137463212
        kl: 0.001787596382200718
        model: {}
        policy_loss: -0.003963969647884369
        total_loss: -0.00485244020819664
        vf_explained_var: 0.004001796245574951
        vf_loss: 0.3943696916103363
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6181237697601318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011922635603696108
        model: {}
        policy_loss: -0.0028027836233377457
        total_loss: -0.0038612051866948605
        vf_explained_var: 0.0028355419635772705
        vf_loss: 0.29473811388015747
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7418027520179749
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016994180623441935
        model: {}
        policy_loss: -0.0037279631942510605
        total_loss: -0.004996453411877155
        vf_explained_var: 0.011668950319290161
        vf_loss: 0.37081408500671387
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.987285852432251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016361481975764036
        model: {}
        policy_loss: -0.0031889285892248154
        total_loss: -0.004907694645226002
        vf_explained_var: 0.0011325329542160034
        vf_loss: 0.18858416378498077
    load_time_ms: 19061.12
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 107848.469
    update_time_ms: 15.353
  iterations_since_restore: 95
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.697969543147213
    ram_util_percent: 9.33045685279188
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 10.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.33
    agent-1: 1.75
    agent-2: 4.24
    agent-3: 3.72
    agent-4: 3.91
    agent-5: 2.34
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: -6.0
  sampler_perf:
    mean_env_wait_ms: 25.524510456997085
    mean_inference_ms: 13.687966593300603
    mean_processing_ms: 64.6586865482628
  time_since_restore: 13253.605415821075
  time_this_iter_s: 138.27747130393982
  time_total_s: 35763.88865828514
  timestamp: 1637233406
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    255 |          35763.9 | 24480000 |    18.29 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 42
    apples_agent-4_mean: 2.23
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 80.48
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 297
    cleaning_beam_agent-1_mean: 201.81
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 19.86
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 66.54
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 397
    cleaning_beam_agent-4_mean: 68.36
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 17.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-05-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 20.01
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11662.614
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.580669641494751
        entropy_coeff: 0.0017600000137463212
        kl: 0.001631009392440319
        model: {}
        policy_loss: -0.0037525170482695103
        total_loss: -0.0047467853873968124
        vf_explained_var: 0.003588259220123291
        vf_loss: 0.27710217237472534
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5486796498298645
        entropy_coeff: 0.0017600000137463212
        kl: 0.00104562530759722
        model: {}
        policy_loss: -0.002772532869130373
        total_loss: -0.0037240181118249893
        vf_explained_var: 0.02366006374359131
        vf_loss: 0.14193421602249146
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5267698764801025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014956248924136162
        model: {}
        policy_loss: -0.00396867236122489
        total_loss: -0.004854556173086166
        vf_explained_var: -0.005244225263595581
        vf_loss: 0.4123179316520691
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6026768684387207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009753440972417593
        model: {}
        policy_loss: -0.002562461420893669
        total_loss: -0.00358575489372015
        vf_explained_var: -0.0007725059986114502
        vf_loss: 0.37418872117996216
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7540704607963562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012222437653690577
        model: {}
        policy_loss: -0.00402095727622509
        total_loss: -0.005310346372425556
        vf_explained_var: 0.017076566815376282
        vf_loss: 0.37774431705474854
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9855252504348755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015462522860616446
        model: {}
        policy_loss: -0.0027758381329476833
        total_loss: -0.004489589016884565
        vf_explained_var: 0.00648874044418335
        vf_loss: 0.20772181451320648
    load_time_ms: 19026.009
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 107691.36
    update_time_ms: 15.367
  iterations_since_restore: 96
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.771428571428576
    ram_util_percent: 9.346938775510203
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.31
    agent-1: 1.77
    agent-2: 4.23
    agent-3: 4.14
    agent-4: 4.08
    agent-5: 2.48
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.5190726145698
    mean_inference_ms: 13.68617605890895
    mean_processing_ms: 64.64702884170948
  time_since_restore: 13390.656245946884
  time_this_iter_s: 137.05083012580872
  time_total_s: 35900.93948841095
  timestamp: 1637233543
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    256 |          35900.9 | 24576000 |    20.01 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.68
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.69
    apples_agent-3_min: 0
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 88.26
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 215.81
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 20.67
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 59.14
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 177
    cleaning_beam_agent-4_mean: 67.48
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 14.73
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-08-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 19.89
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11658.625
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.586774468421936
        entropy_coeff: 0.0017600000137463212
        kl: 0.001395840896293521
        model: {}
        policy_loss: -0.0037665637210011482
        total_loss: -0.004771247506141663
        vf_explained_var: -0.0013005733489990234
        vf_loss: 0.2803901731967926
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5670349597930908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009574998985044658
        model: {}
        policy_loss: -0.0026298114098608494
        total_loss: -0.0036173867993056774
        vf_explained_var: 0.026666387915611267
        vf_loss: 0.1040731891989708
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5199031829833984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016484635416418314
        model: {}
        policy_loss: -0.003927083685994148
        total_loss: -0.0048043392598629
        vf_explained_var: 0.003051549196243286
        vf_loss: 0.3777564764022827
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5971435308456421
        entropy_coeff: 0.0017600000137463212
        kl: 0.001039138063788414
        model: {}
        policy_loss: -0.0026479559019207954
        total_loss: -0.0036654584109783173
        vf_explained_var: 0.0018947720527648926
        vf_loss: 0.33473506569862366
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7418670654296875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015908668283373117
        model: {}
        policy_loss: -0.0037723234854638577
        total_loss: -0.005037681199610233
        vf_explained_var: 0.02058151364326477
        vf_loss: 0.40327298641204834
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9974088668823242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024931798689067364
        model: {}
        policy_loss: -0.003491387004032731
        total_loss: -0.005227445624768734
        vf_explained_var: -0.0009856075048446655
        vf_loss: 0.19380781054496765
    load_time_ms: 19055.324
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 107735.662
    update_time_ms: 15.251
  iterations_since_restore: 97
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.25685279187817
    ram_util_percent: 9.343147208121827
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.13
    agent-1: 1.54
    agent-2: 4.32
    agent-3: 3.88
    agent-4: 4.61
    agent-5: 2.41
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.517729899118184
    mean_inference_ms: 13.686401295772807
    mean_processing_ms: 64.64463726276269
  time_since_restore: 13528.855050086975
  time_this_iter_s: 138.19880414009094
  time_total_s: 36039.13829255104
  timestamp: 1637233681
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    257 |          36039.1 | 24672000 |    19.89 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.88
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.53
    apples_agent-2_min: 0
    apples_agent-3_max: 7
    apples_agent-3_mean: 2.41
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 80.81
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 219.38
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 70
    cleaning_beam_agent-2_mean: 19.95
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 59.8
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 173
    cleaning_beam_agent-4_mean: 69.55
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 18.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-10-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 17.47
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11673.619
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5859324336051941
        entropy_coeff: 0.0017600000137463212
        kl: 0.001457097940146923
        model: {}
        policy_loss: -0.0023239925503730774
        total_loss: -0.0032074821647256613
        vf_explained_var: -0.0006789863109588623
        vf_loss: 1.4774787425994873
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5628237128257751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010031862184405327
        model: {}
        policy_loss: -0.0024558440782129765
        total_loss: -0.003433497156947851
        vf_explained_var: 0.02071651816368103
        vf_loss: 0.12914970517158508
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.513260006904602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016500061610713601
        model: {}
        policy_loss: -0.0037995032034814358
        total_loss: -0.0046725086867809296
        vf_explained_var: -0.00019660592079162598
        vf_loss: 0.3033117651939392
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5814327001571655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008842844399623573
        model: {}
        policy_loss: -0.001615617424249649
        total_loss: -0.0024886219762265682
        vf_explained_var: -0.0011470615863800049
        vf_loss: 1.5031489133834839
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7596474289894104
        entropy_coeff: 0.0017600000137463212
        kl: 0.002513882704079151
        model: {}
        policy_loss: -0.004222295247018337
        total_loss: -0.0055183847434818745
        vf_explained_var: 0.016329780220985413
        vf_loss: 0.40889614820480347
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.003259539604187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016362089663743973
        model: {}
        policy_loss: -0.0030613946728408337
        total_loss: -0.004809758625924587
        vf_explained_var: 0.008220180869102478
        vf_loss: 0.17369577288627625
    load_time_ms: 19038.441
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 107754.324
    update_time_ms: 15.089
  iterations_since_restore: 98
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.299492385786802
    ram_util_percent: 9.279187817258883
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 11.0
    agent-2: 12.0
    agent-3: 12.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.08
    agent-1: 1.53
    agent-2: 3.92
    agent-3: 3.18
    agent-4: 4.38
    agent-5: 2.38
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.51562912634615
    mean_inference_ms: 13.685784891461845
    mean_processing_ms: 64.64243560213427
  time_since_restore: 13667.503219127655
  time_this_iter_s: 138.64816904067993
  time_total_s: 36177.78646159172
  timestamp: 1637233820
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    258 |          36177.8 | 24768000 |    17.47 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.71
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.36
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.39
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 79.78
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 327
    cleaning_beam_agent-1_mean: 210.26
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 73
    cleaning_beam_agent-2_mean: 19.82
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 63.02
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 156
    cleaning_beam_agent-4_mean: 72.42
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 18.17
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-12-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 17.28
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11677.471
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.583360493183136
        entropy_coeff: 0.0017600000137463212
        kl: 0.00117873540148139
        model: {}
        policy_loss: -0.0025556599721312523
        total_loss: -0.0034275976940989494
        vf_explained_var: 0.00048692524433135986
        vf_loss: 1.5477328300476074
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5627830624580383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011813894379884005
        model: {}
        policy_loss: -0.002702374942600727
        total_loss: -0.0036826771683990955
        vf_explained_var: 0.018553197383880615
        vf_loss: 0.10196007788181305
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.501280665397644
        entropy_coeff: 0.0017600000137463212
        kl: 0.001149432617239654
        model: {}
        policy_loss: -0.003495857585221529
        total_loss: -0.004344988148659468
        vf_explained_var: -0.0009586513042449951
        vf_loss: 0.33126431703567505
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5842700004577637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009902040474116802
        model: {}
        policy_loss: -0.0027518863789737225
        total_loss: -0.003752459306269884
        vf_explained_var: -0.0007195621728897095
        vf_loss: 0.2774357497692108
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7480902075767517
        entropy_coeff: 0.0017600000137463212
        kl: 0.001665725838392973
        model: {}
        policy_loss: -0.0037015597335994244
        total_loss: -0.004981005564332008
        vf_explained_var: 0.01568007469177246
        vf_loss: 0.3719520568847656
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0065144300460815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015289554139599204
        model: {}
        policy_loss: -0.002856162376701832
        total_loss: -0.0046099890023469925
        vf_explained_var: 0.0027618855237960815
        vf_loss: 0.17636054754257202
    load_time_ms: 19091.461
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 107576.886
    update_time_ms: 15.211
  iterations_since_restore: 99
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.736363636363638
    ram_util_percent: 9.21767676767677
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 10.0
    agent-3: 11.0
    agent-4: 14.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 2.15
    agent-1: 1.48
    agent-2: 3.84
    agent-3: 3.33
    agent-4: 4.24
    agent-5: 2.24
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.516548889578818
    mean_inference_ms: 13.686140843811277
    mean_processing_ms: 64.6458890448405
  time_since_restore: 13806.06634235382
  time_this_iter_s: 138.56312322616577
  time_total_s: 36316.349584817886
  timestamp: 1637233959
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    259 |          36316.3 | 24864000 |    17.28 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.93
    apples_agent-0_min: 0
    apples_agent-1_max: 37
    apples_agent-1_mean: 1.25
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 2.0
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.38
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.98
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 128
    cleaning_beam_agent-0_mean: 78.72
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 210.34
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 98
    cleaning_beam_agent-2_mean: 22.16
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 184
    cleaning_beam_agent-3_mean: 63.15
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 180
    cleaning_beam_agent-4_mean: 72.62
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 17.97
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-14-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 17.9
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11679.894
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5815792679786682
        entropy_coeff: 0.0017600000137463212
        kl: 0.001305930083617568
        model: {}
        policy_loss: -0.0033905236050486565
        total_loss: -0.004387795925140381
        vf_explained_var: 0.004368215799331665
        vf_loss: 0.2630506753921509
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.556191086769104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008409139700233936
        model: {}
        policy_loss: -0.0026451563462615013
        total_loss: -0.003612743690609932
        vf_explained_var: 0.030578777194023132
        vf_loss: 0.11309118568897247
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.510754406452179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014034728519618511
        model: {}
        policy_loss: -0.003423064248636365
        total_loss: -0.004277320578694344
        vf_explained_var: 0.007290676236152649
        vf_loss: 0.4467007517814636
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5961010456085205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010733293602243066
        model: {}
        policy_loss: -0.0026505980640649796
        total_loss: -0.0036697201430797577
        vf_explained_var: -0.0008988678455352783
        vf_loss: 0.30020731687545776
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7472769021987915
        entropy_coeff: 0.0017600000137463212
        kl: 0.002030409639701247
        model: {}
        policy_loss: -0.003910430707037449
        total_loss: -0.005184744484722614
        vf_explained_var: 0.024874955415725708
        vf_loss: 0.40894389152526855
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0037013292312622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020622853189706802
        model: {}
        policy_loss: -0.003020527772605419
        total_loss: -0.0047734687104821205
        vf_explained_var: 0.0039058327674865723
        vf_loss: 0.13571754097938538
    load_time_ms: 19085.018
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 107507.931
    update_time_ms: 15.56
  iterations_since_restore: 100
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.70203045685279
    ram_util_percent: 9.318781725888323
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 11.0
    agent-2: 21.0
    agent-3: 13.0
    agent-4: 18.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.55
    agent-1: 1.71
    agent-2: 3.68
    agent-3: 3.39
    agent-4: 4.49
    agent-5: 2.08
  policy_reward_min:
    agent-0: -43.0
    agent-1: 0.0
    agent-2: -41.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.514804995024168
    mean_inference_ms: 13.685233813325505
    mean_processing_ms: 64.64353128760928
  time_since_restore: 13944.578545093536
  time_this_iter_s: 138.51220273971558
  time_total_s: 36454.8617875576
  timestamp: 1637234097
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    260 |          36454.9 | 24960000 |     17.9 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.13
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.74
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 2.7
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 79.11
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 214.31
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 21.26
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 64.74
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 261
    cleaning_beam_agent-4_mean: 68.35
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 20.29
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-17-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 18.58
  episode_reward_min: -21.0
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11711.281
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5858696699142456
        entropy_coeff: 0.0017600000137463212
        kl: 0.001236560638062656
        model: {}
        policy_loss: -0.0035928913857787848
        total_loss: -0.004599365405738354
        vf_explained_var: -0.002538621425628662
        vf_loss: 0.24656736850738525
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5666548013687134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009653243469074368
        model: {}
        policy_loss: -0.0027584789786487818
        total_loss: -0.003743659006431699
        vf_explained_var: 0.02802734076976776
        vf_loss: 0.1213318407535553
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5069519281387329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014504550490528345
        model: {}
        policy_loss: -0.0038070683367550373
        total_loss: -0.004653112031519413
        vf_explained_var: -0.0010518431663513184
        vf_loss: 0.4619165062904358
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5899401307106018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010847346857190132
        model: {}
        policy_loss: -0.002509427024051547
        total_loss: -0.003511121729388833
        vf_explained_var: 0.0029159337282180786
        vf_loss: 0.36596325039863586
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7503707408905029
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013058142503723502
        model: {}
        policy_loss: -0.0037197042256593704
        total_loss: -0.005002275574952364
        vf_explained_var: 0.022822871804237366
        vf_loss: 0.3807872235774994
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0227251052856445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017744579818099737
        model: {}
        policy_loss: -0.002821622183546424
        total_loss: -0.004603336565196514
        vf_explained_var: 0.010854333639144897
        vf_loss: 0.18279623985290527
    load_time_ms: 19148.429
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 107525.998
    update_time_ms: 15.437
  iterations_since_restore: 101
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.2425
    ram_util_percent: 9.285499999999999
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 20.0
    agent-4: 17.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.91
    agent-1: 1.58
    agent-2: 4.41
    agent-3: 3.75
    agent-4: 4.16
    agent-5: 1.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -45.0
  sampler_perf:
    mean_env_wait_ms: 25.515570496621564
    mean_inference_ms: 13.685161249498174
    mean_processing_ms: 64.64753346667588
  time_since_restore: 14084.562678575516
  time_this_iter_s: 139.98413348197937
  time_total_s: 36594.84592103958
  timestamp: 1637234238
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    261 |          36594.8 | 25056000 |    18.58 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.52
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.06
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 78.23
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 208.93
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 125
    cleaning_beam_agent-2_mean: 18.61
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 63.41
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 146
    cleaning_beam_agent-4_mean: 61.61
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 17.12
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-19-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 18.14
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11708.195
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5752472877502441
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010949058923870325
        model: {}
        policy_loss: -0.003412767080590129
        total_loss: -0.004400397185236216
        vf_explained_var: 0.0015650391578674316
        vf_loss: 0.2480674684047699
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5556738376617432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008386353147216141
        model: {}
        policy_loss: -0.0029123187996447086
        total_loss: -0.003878755494952202
        vf_explained_var: 0.019757971167564392
        vf_loss: 0.11549338698387146
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5082292556762695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015315969940274954
        model: {}
        policy_loss: -0.0037367416080087423
        total_loss: -0.004595975391566753
        vf_explained_var: 0.0068259090185165405
        vf_loss: 0.3524702191352844
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5960135459899902
        entropy_coeff: 0.0017600000137463212
        kl: 0.001252866699360311
        model: {}
        policy_loss: -0.002645455300807953
        total_loss: -0.003667019307613373
        vf_explained_var: 0.004545122385025024
        vf_loss: 0.2741847634315491
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.758955180644989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011658233124762774
        model: {}
        policy_loss: -0.003674547653645277
        total_loss: -0.004970770329236984
        vf_explained_var: 0.02833297848701477
        vf_loss: 0.39535656571388245
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0061557292938232
        entropy_coeff: 0.0017600000137463212
        kl: 0.001713777193799615
        model: {}
        policy_loss: -0.002956557087600231
        total_loss: -0.00471388828009367
        vf_explained_var: 0.003201499581336975
        vf_loss: 0.13499657809734344
    load_time_ms: 18971.867
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 107486.174
    update_time_ms: 15.436
  iterations_since_restore: 102
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.657142857142855
    ram_util_percent: 9.304591836734694
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 12.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.42
    agent-1: 1.54
    agent-2: 3.94
    agent-3: 3.43
    agent-4: 4.68
    agent-5: 2.13
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.513099436282996
    mean_inference_ms: 13.68474327624577
    mean_processing_ms: 64.64194339650437
  time_since_restore: 14222.026991128922
  time_this_iter_s: 137.46431255340576
  time_total_s: 36732.31023359299
  timestamp: 1637234376
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    262 |          36732.3 | 25152000 |    18.14 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.71
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.84
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.49
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 1.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 73.46
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 396
    cleaning_beam_agent-1_mean: 222.25
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 98
    cleaning_beam_agent-2_mean: 19.59
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 60.26
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 163
    cleaning_beam_agent-4_mean: 60.43
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 17.22
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-21-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 18.81
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11724.119
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5769813060760498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013902393402531743
        model: {}
        policy_loss: -0.0034931879490613937
        total_loss: -0.004481985233724117
        vf_explained_var: 0.006214842200279236
        vf_loss: 0.26690125465393066
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5675444006919861
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014207169879227877
        model: {}
        policy_loss: -0.0025811917148530483
        total_loss: -0.00356976012699306
        vf_explained_var: 0.02205730974674225
        vf_loss: 0.10309411585330963
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5019946694374084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012135226279497147
        model: {}
        policy_loss: -0.003633253276348114
        total_loss: -0.004479476250708103
        vf_explained_var: -0.002072557806968689
        vf_loss: 0.37288156151771545
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5860212445259094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012227578554302454
        model: {}
        policy_loss: -0.00271180784329772
        total_loss: -0.0037112864665687084
        vf_explained_var: -0.0012165755033493042
        vf_loss: 0.31916216015815735
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7559281587600708
        entropy_coeff: 0.0017600000137463212
        kl: 0.001411644509062171
        model: {}
        policy_loss: -0.0038382336497306824
        total_loss: -0.005136130843311548
        vf_explained_var: 0.01914404332637787
        vf_loss: 0.325380802154541
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9878218173980713
        entropy_coeff: 0.0017600000137463212
        kl: 0.002064927015453577
        model: {}
        policy_loss: -0.002775669563561678
        total_loss: -0.004501796327531338
        vf_explained_var: -0.0022833943367004395
        vf_loss: 0.12438998371362686
    load_time_ms: 18975.546
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 107575.661
    update_time_ms: 15.29
  iterations_since_restore: 103
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.57437185929648
    ram_util_percent: 9.265829145728643
  pid: 6435
  policy_reward_max:
    agent-0: 18.0
    agent-1: 7.0
    agent-2: 12.0
    agent-3: 17.0
    agent-4: 12.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.49
    agent-2: 4.22
    agent-3: 3.8
    agent-4: 4.31
    agent-5: 2.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.512552115631642
    mean_inference_ms: 13.684661802042237
    mean_processing_ms: 64.64403020304398
  time_since_restore: 14361.752095937729
  time_this_iter_s: 139.72510480880737
  time_total_s: 36872.035338401794
  timestamp: 1637234516
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    263 |            36872 | 25248000 |    18.81 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.75
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.31
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.57
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 206
    cleaning_beam_agent-0_mean: 72.78
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 235.41
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 19.18
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 61.15
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 181
    cleaning_beam_agent-4_mean: 66.54
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 17.52
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-24-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 19.43
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11723.793
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5840703248977661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014206923078745604
        model: {}
        policy_loss: -0.0036307252012193203
        total_loss: -0.004637994337826967
        vf_explained_var: -0.006429702043533325
        vf_loss: 0.20692843198776245
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5739720463752747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009464852628298104
        model: {}
        policy_loss: -0.0029503651894629
        total_loss: -0.003949764184653759
        vf_explained_var: 0.03362688422203064
        vf_loss: 0.10793383419513702
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5104876756668091
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013503269292414188
        model: {}
        policy_loss: -0.00330340419895947
        total_loss: -0.004153286572545767
        vf_explained_var: 0.008586719632148743
        vf_loss: 0.48577386140823364
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5882799625396729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012473366223275661
        model: {}
        policy_loss: -0.0027611469849944115
        total_loss: -0.0037636887282133102
        vf_explained_var: -0.002599969506263733
        vf_loss: 0.32833802700042725
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7461597919464111
        entropy_coeff: 0.0017600000137463212
        kl: 0.002015463076531887
        model: {}
        policy_loss: -0.004063278436660767
        total_loss: -0.00533040426671505
        vf_explained_var: 0.02544945478439331
        vf_loss: 0.46116188168525696
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.010585069656372
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024397920351475477
        model: {}
        policy_loss: -0.003204690758138895
        total_loss: -0.004964773543179035
        vf_explained_var: -0.003322035074234009
        vf_loss: 0.18544596433639526
    load_time_ms: 19074.461
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 107596.178
    update_time_ms: 15.351
  iterations_since_restore: 104
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.300505050505052
    ram_util_percent: 9.324242424242426
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.69
    agent-1: 1.67
    agent-2: 4.35
    agent-3: 3.66
    agent-4: 4.58
    agent-5: 2.48
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.513315337596385
    mean_inference_ms: 13.684417142284548
    mean_processing_ms: 64.64657479556395
  time_since_restore: 14500.52945113182
  time_this_iter_s: 138.7773551940918
  time_total_s: 37010.812693595886
  timestamp: 1637234655
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    264 |          37010.8 | 25344000 |    19.43 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.33
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.04
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 29
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 133
    cleaning_beam_agent-0_mean: 70.26
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 365
    cleaning_beam_agent-1_mean: 229.36
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 17.2
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 61.4
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 131
    cleaning_beam_agent-4_mean: 63.89
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 16.2
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-26-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 19.73
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11720.974
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5743969678878784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018797023221850395
        model: {}
        policy_loss: -0.0036394817288964987
        total_loss: -0.004627387039363384
        vf_explained_var: 0.0018936097621917725
        vf_loss: 0.23035810887813568
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.561089038848877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009948551887646317
        model: {}
        policy_loss: -0.0025278646498918533
        total_loss: -0.003502780571579933
        vf_explained_var: 0.020015329122543335
        vf_loss: 0.12602996826171875
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4949831962585449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018305890262126923
        model: {}
        policy_loss: -0.003465233137831092
        total_loss: -0.004295359831303358
        vf_explained_var: -0.0008434802293777466
        vf_loss: 0.410439133644104
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5858083963394165
        entropy_coeff: 0.0017600000137463212
        kl: 0.001557780196890235
        model: {}
        policy_loss: -0.002754386980086565
        total_loss: -0.0037494483403861523
        vf_explained_var: 0.0020380914211273193
        vf_loss: 0.35963165760040283
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7500149011611938
        entropy_coeff: 0.0017600000137463212
        kl: 0.002322898479178548
        model: {}
        policy_loss: -0.0038234374951571226
        total_loss: -0.005102370399981737
        vf_explained_var: 0.01135621964931488
        vf_loss: 0.41095107793807983
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9889765381813049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017057653749361634
        model: {}
        policy_loss: -0.0026201587170362473
        total_loss: -0.004344837740063667
        vf_explained_var: 0.0023989975452423096
        vf_loss: 0.1591676026582718
    load_time_ms: 19070.509
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 107682.351
    update_time_ms: 15.304
  iterations_since_restore: 105
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.61767676767677
    ram_util_percent: 9.226262626262628
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 17.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.78
    agent-2: 4.36
    agent-3: 3.95
    agent-4: 4.6
    agent-5: 2.07
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.51350122076699
    mean_inference_ms: 13.684613190839091
    mean_processing_ms: 64.65002043463178
  time_since_restore: 14639.599443435669
  time_this_iter_s: 139.06999230384827
  time_total_s: 37149.882685899734
  timestamp: 1637234794
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    265 |          37149.9 | 25440000 |    19.73 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 2.95
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.74
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 1.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 140
    cleaning_beam_agent-0_mean: 71.12
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 220.96
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 16.88
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 57.8
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 150
    cleaning_beam_agent-4_mean: 69.18
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 16.44
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-28-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 43.0
  episode_reward_mean: 18.65
  episode_reward_min: -27.0
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11707.601
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5723733305931091
        entropy_coeff: 0.0017600000137463212
        kl: 0.001796223921701312
        model: {}
        policy_loss: -0.0037346126046031713
        total_loss: -0.004586383700370789
        vf_explained_var: 0.0012025237083435059
        vf_loss: 1.556037187576294
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5629520416259766
        entropy_coeff: 0.0017600000137463212
        kl: 0.001192565425299108
        model: {}
        policy_loss: -0.0028047829400748014
        total_loss: -0.003782480489462614
        vf_explained_var: 0.01473604142665863
        vf_loss: 0.1309536099433899
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5008342266082764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012427775654941797
        model: {}
        policy_loss: -0.003913743421435356
        total_loss: -0.004761440679430962
        vf_explained_var: -0.009381502866744995
        vf_loss: 0.3376860022544861
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5851982831954956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010920463828369975
        model: {}
        policy_loss: -0.002876534126698971
        total_loss: -0.0038725598715245724
        vf_explained_var: -0.00043010711669921875
        vf_loss: 0.3392210900783539
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.765117883682251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015321120154112577
        model: {}
        policy_loss: -0.003754790872335434
        total_loss: -0.005057061556726694
        vf_explained_var: 0.013468533754348755
        vf_loss: 0.4433486759662628
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9986114501953125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015005517052486539
        model: {}
        policy_loss: -0.002873266115784645
        total_loss: -0.004615869838744402
        vf_explained_var: 0.0008198171854019165
        vf_loss: 0.1495063453912735
    load_time_ms: 18954.442
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 107762.355
    update_time_ms: 16.017
  iterations_since_restore: 106
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.732307692307693
    ram_util_percent: 9.332820512820513
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.28
    agent-1: 1.67
    agent-2: 4.13
    agent-3: 3.65
    agent-4: 4.65
    agent-5: 2.27
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.50974537455885
    mean_inference_ms: 13.683937081600034
    mean_processing_ms: 64.64533512474735
  time_since_restore: 14776.095835208893
  time_this_iter_s: 136.49639177322388
  time_total_s: 37286.37907767296
  timestamp: 1637234930
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    266 |          37286.4 | 25536000 |    18.65 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 3.79
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.81
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 116
    cleaning_beam_agent-0_mean: 64.39
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 221.66
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 20.52
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 56.14
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 68.21
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 19.23
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-31-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 17.69
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11705.487
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5666297078132629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009396737441420555
        model: {}
        policy_loss: -0.003470507450401783
        total_loss: -0.004439058713614941
        vf_explained_var: 0.002554580569267273
        vf_loss: 0.2871990203857422
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5638388991355896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010331979719921947
        model: {}
        policy_loss: -0.002576430793851614
        total_loss: -0.003557799616828561
        vf_explained_var: 0.027166858315467834
        vf_loss: 0.10988794267177582
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5124534368515015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014973990619182587
        model: {}
        policy_loss: -0.0028420211747288704
        total_loss: -0.00358413252979517
        vf_explained_var: -0.001379936933517456
        vf_loss: 1.5980873107910156
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5880581140518188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007307440391741693
        model: {}
        policy_loss: -0.001646862830966711
        total_loss: -0.0025089867413043976
        vf_explained_var: 0.0006451606750488281
        vf_loss: 1.7285754680633545
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7445402145385742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014880613889545202
        model: {}
        policy_loss: -0.0036884150467813015
        total_loss: -0.004965833388268948
        vf_explained_var: 0.023751437664031982
        vf_loss: 0.3297538757324219
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0134985446929932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020332280546426773
        model: {}
        policy_loss: -0.0027020731940865517
        total_loss: -0.004470435902476311
        vf_explained_var: 0.0014833807945251465
        vf_loss: 0.15395718812942505
    load_time_ms: 18900.744
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 107788.884
    update_time_ms: 16.168
  iterations_since_restore: 107
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.353061224489796
    ram_util_percent: 9.300510204081633
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 12.0
    agent-3: 16.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.23
    agent-1: 1.68
    agent-2: 3.22
    agent-3: 3.3
    agent-4: 4.08
    agent-5: 2.18
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: -46.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.507174605152212
    mean_inference_ms: 13.683305324552226
    mean_processing_ms: 64.64875629586287
  time_since_restore: 14914.00731420517
  time_this_iter_s: 137.91147899627686
  time_total_s: 37424.290556669235
  timestamp: 1637235068
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    267 |          37424.3 | 25632000 |    17.69 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.83
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.72
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.6
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 128
    cleaning_beam_agent-0_mean: 67.99
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 208.17
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 112
    cleaning_beam_agent-2_mean: 20.02
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 61.37
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 66.09
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 75
    cleaning_beam_agent-5_mean: 18.43
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-33-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 18.68
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11691.759
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5640010237693787
        entropy_coeff: 0.0017600000137463212
        kl: 0.001506470376625657
        model: {}
        policy_loss: -0.003248997963964939
        total_loss: -0.00421641580760479
        vf_explained_var: -0.005960121750831604
        vf_loss: 0.25225830078125
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5554640293121338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012017209082841873
        model: {}
        policy_loss: -0.0025492964778095484
        total_loss: -0.003517730627208948
        vf_explained_var: 0.03723640739917755
        vf_loss: 0.09180459380149841
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5037206411361694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012945455964654684
        model: {}
        policy_loss: -0.0037066994700580835
        total_loss: -0.004553597886115313
        vf_explained_var: 0.0024973005056381226
        vf_loss: 0.3964821398258209
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5979077816009521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009592526475898921
        model: {}
        policy_loss: -0.002651061862707138
        total_loss: -0.0036768829450011253
        vf_explained_var: -0.0009205639362335205
        vf_loss: 0.2649804651737213
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7496246099472046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013661093544214964
        model: {}
        policy_loss: -0.0036866699811071157
        total_loss: -0.0049682492390275
        vf_explained_var: 0.017776012420654297
        vf_loss: 0.37762650847435
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9945575594902039
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017903856933116913
        model: {}
        policy_loss: -0.0027035612147301435
        total_loss: -0.004433325957506895
        vf_explained_var: 0.007667466998100281
        vf_loss: 0.20656809210777283
    load_time_ms: 18902.851
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 107624.964
    update_time_ms: 16.421
  iterations_since_restore: 108
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.777435897435893
    ram_util_percent: 9.349743589743591
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.0
    agent-1: 1.51
    agent-2: 4.09
    agent-3: 3.43
    agent-4: 4.43
    agent-5: 2.22
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.505279403300257
    mean_inference_ms: 13.68287836787151
    mean_processing_ms: 64.64704572801433
  time_since_restore: 15050.900807619095
  time_this_iter_s: 136.89349341392517
  time_total_s: 37561.18405008316
  timestamp: 1637235205
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    268 |          37561.2 | 25728000 |    18.68 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.41
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.04
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 151
    cleaning_beam_agent-0_mean: 73.18
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 208.71
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 22.59
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 57.88
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 144
    cleaning_beam_agent-4_mean: 62.47
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 18.32
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-35-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 18.99
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11698.243
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5711067914962769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014912525657564402
        model: {}
        policy_loss: -0.003703606780618429
        total_loss: -0.004685591906309128
        vf_explained_var: 0.007349535822868347
        vf_loss: 0.23162296414375305
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5363951325416565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010250122286379337
        model: {}
        policy_loss: -0.002604434732347727
        total_loss: -0.003537288401275873
        vf_explained_var: 0.01919342577457428
        vf_loss: 0.11198681592941284
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5016018152236938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015693189343437552
        model: {}
        policy_loss: -0.003478744300082326
        total_loss: -0.004327959846705198
        vf_explained_var: 0.006252765655517578
        vf_loss: 0.3360440135002136
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5933943390846252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015528469812124968
        model: {}
        policy_loss: -0.002912072464823723
        total_loss: -0.003919271752238274
        vf_explained_var: -0.0007766783237457275
        vf_loss: 0.37177205085754395
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7421524524688721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014120133128017187
        model: {}
        policy_loss: -0.0037617688067257404
        total_loss: -0.005027370061725378
        vf_explained_var: 0.015515565872192383
        vf_loss: 0.40585845708847046
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9994194507598877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014035364147275686
        model: {}
        policy_loss: -0.002757410053163767
        total_loss: -0.004501016810536385
        vf_explained_var: 0.010882273316383362
        vf_loss: 0.15370012819766998
    load_time_ms: 18903.593
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 107581.079
    update_time_ms: 16.22
  iterations_since_restore: 109
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.781725888324875
    ram_util_percent: 9.341624365482234
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 12.0
    agent-3: 17.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.02
    agent-1: 1.6
    agent-2: 3.92
    agent-3: 3.94
    agent-4: 4.2
    agent-5: 2.31
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.504334562214584
    mean_inference_ms: 13.683293295799276
    mean_processing_ms: 64.64840914956707
  time_since_restore: 15189.099667072296
  time_this_iter_s: 138.1988594532013
  time_total_s: 37699.38290953636
  timestamp: 1637235344
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    269 |          37699.4 | 25824000 |    18.99 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.93
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.61
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.94
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 39
    apples_agent-5_mean: 1.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 70.78
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 219.86
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 20.53
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 55.42
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 69.5
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 19.48
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-38-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 19.4
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11699.968
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5634568929672241
        entropy_coeff: 0.0017600000137463212
        kl: 0.001457972452044487
        model: {}
        policy_loss: -0.0028753913938999176
        total_loss: -0.003756673075258732
        vf_explained_var: 0.0009895563125610352
        vf_loss: 1.1040037870407104
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5365666151046753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014265318168327212
        model: {}
        policy_loss: -0.0031012804247438908
        total_loss: -0.004033252596855164
        vf_explained_var: 0.02577972412109375
        vf_loss: 0.12383072078227997
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5086089372634888
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011889244196936488
        model: {}
        policy_loss: -0.003273681038990617
        total_loss: -0.004129817709326744
        vf_explained_var: 0.00981108844280243
        vf_loss: 0.390137255191803
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.571112871170044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011489125899970531
        model: {}
        policy_loss: -0.0023875823244452477
        total_loss: -0.0033521829172968864
        vf_explained_var: -3.297626972198486e-05
        vf_loss: 0.40557968616485596
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7364764213562012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017869395669549704
        model: {}
        policy_loss: -0.00361868878826499
        total_loss: -0.004875468090176582
        vf_explained_var: 0.02270647883415222
        vf_loss: 0.3942021131515503
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9955208897590637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017305829096585512
        model: {}
        policy_loss: -0.0025896308943629265
        total_loss: -0.004324652254581451
        vf_explained_var: 0.007168501615524292
        vf_loss: 0.1709606796503067
    load_time_ms: 18852.407
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 107510.734
    update_time_ms: 15.932
  iterations_since_restore: 110
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.68622448979592
    ram_util_percent: 9.267857142857142
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.69
    agent-1: 1.77
    agent-2: 4.44
    agent-3: 4.36
    agent-4: 4.61
    agent-5: 1.53
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.501526902171268
    mean_inference_ms: 13.682408403144928
    mean_processing_ms: 64.64500802994162
  time_since_restore: 15326.340878725052
  time_this_iter_s: 137.24121165275574
  time_total_s: 37836.62412118912
  timestamp: 1637235481
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    270 |          37836.6 | 25920000 |     19.4 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.66
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 1.89
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.64
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.92
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 1.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 71.66
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 217.79
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 20.45
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 54.41
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 168
    cleaning_beam_agent-4_mean: 65.33
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 19.29
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-40-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 17.69
  episode_reward_min: -86.0
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11666.193
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5661118030548096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017710383981466293
        model: {}
        policy_loss: -0.0023603220470249653
        total_loss: -0.0031986027024686337
        vf_explained_var: 0.0009049326181411743
        vf_loss: 1.5807644128799438
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5515251159667969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008096856763586402
        model: {}
        policy_loss: -0.0029039159417152405
        total_loss: -0.0038655325770378113
        vf_explained_var: 0.014316335320472717
        vf_loss: 0.09071812033653259
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5082902312278748
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013362881727516651
        model: {}
        policy_loss: -0.0030871895141899586
        total_loss: -0.003945645410567522
        vf_explained_var: -0.004081279039382935
        vf_loss: 0.36136695742607117
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5767924785614014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012177546741440892
        model: {}
        policy_loss: -0.002547312993556261
        total_loss: -0.0035348311066627502
        vf_explained_var: -0.002083301544189453
        vf_loss: 0.2763691544532776
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7431199550628662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010281653376296163
        model: {}
        policy_loss: -0.0026746541261672974
        total_loss: -0.0038077831268310547
        vf_explained_var: 0.012756600975990295
        vf_loss: 1.7476376295089722
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0137261152267456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015494704712182283
        model: {}
        policy_loss: -0.002751451451331377
        total_loss: -0.004518507048487663
        vf_explained_var: 0.008617997169494629
        vf_loss: 0.1709759682416916
    load_time_ms: 18811.091
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 107409.629
    update_time_ms: 15.88
  iterations_since_restore: 111
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.588383838383837
    ram_util_percent: 9.341414141414143
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 6.0
    agent-2: 23.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.71
    agent-1: 1.42
    agent-2: 3.96
    agent-3: 3.4
    agent-4: 3.91
    agent-5: 2.29
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.49932180144828
    mean_inference_ms: 13.682080278465037
    mean_processing_ms: 64.64456494361455
  time_since_restore: 15464.552666187286
  time_this_iter_s: 138.2117874622345
  time_total_s: 37974.83590865135
  timestamp: 1637235620
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    271 |          37974.8 | 26016000 |    17.69 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.21
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 1.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 165
    cleaning_beam_agent-0_mean: 72.28
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 218.62
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 20.09
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 55.19
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 152
    cleaning_beam_agent-4_mean: 64.17
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 18.72
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-42-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 21.73
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11674.225
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5710683465003967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013715450186282396
        model: {}
        policy_loss: -0.0034239073283970356
        total_loss: -0.004400944337248802
        vf_explained_var: 0.0051231831312179565
        vf_loss: 0.28045275807380676
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5338941812515259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012181710917502642
        model: {}
        policy_loss: -0.0026298891752958298
        total_loss: -0.0035536265932023525
        vf_explained_var: 0.020874500274658203
        vf_loss: 0.1591637283563614
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5103265047073364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017046858556568623
        model: {}
        policy_loss: -0.003579752054065466
        total_loss: -0.004433797672390938
        vf_explained_var: 0.0023774653673171997
        vf_loss: 0.4412914514541626
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5759597420692444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008940405095927417
        model: {}
        policy_loss: -0.0024845246225595474
        total_loss: -0.003459871979430318
        vf_explained_var: 0.0010892599821090698
        vf_loss: 0.3834156394004822
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7423273324966431
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017730210674926639
        model: {}
        policy_loss: -0.0041967276483774185
        total_loss: -0.005444107577204704
        vf_explained_var: 0.015230178833007812
        vf_loss: 0.5911566019058228
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0066972970962524
        entropy_coeff: 0.0017600000137463212
        kl: 0.002766239922493696
        model: {}
        policy_loss: -0.0023067942820489407
        total_loss: -0.003954961895942688
        vf_explained_var: 0.0012880116701126099
        vf_loss: 1.2362323999404907
    load_time_ms: 18809.551
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 107392.653
    update_time_ms: 15.736
  iterations_since_restore: 112
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.744387755102043
    ram_util_percent: 9.356632653061226
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 19.0
    agent-3: 17.0
    agent-4: 20.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.41
    agent-1: 1.92
    agent-2: 4.56
    agent-3: 4.26
    agent-4: 5.71
    agent-5: 1.87
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.496701321257852
    mean_inference_ms: 13.68121123281025
    mean_processing_ms: 64.64211637663043
  time_since_restore: 15601.911886453629
  time_this_iter_s: 137.35922026634216
  time_total_s: 38112.195128917694
  timestamp: 1637235758
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    272 |          38112.2 | 26112000 |    21.73 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.8
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 69.47
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 220.22
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 56
    cleaning_beam_agent-2_mean: 16.74
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 54.95
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 133
    cleaning_beam_agent-4_mean: 61.25
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 19.68
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-44-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 20.4
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11658.001
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.566072404384613
        entropy_coeff: 0.0017600000137463212
        kl: 0.001516312244348228
        model: {}
        policy_loss: -0.003470947965979576
        total_loss: -0.004441702738404274
        vf_explained_var: -0.0015954077243804932
        vf_loss: 0.25532469153404236
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5305948853492737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013069193810224533
        model: {}
        policy_loss: -0.0028919419273734093
        total_loss: -0.0038147694431245327
        vf_explained_var: 0.030290409922599792
        vf_loss: 0.11019312590360641
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5041530132293701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011720105540007353
        model: {}
        policy_loss: -0.0032550012692809105
        total_loss: -0.004091072827577591
        vf_explained_var: 0.0039017200469970703
        vf_loss: 0.5123787522315979
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5796436667442322
        entropy_coeff: 0.0017600000137463212
        kl: 0.00074381084414199
        model: {}
        policy_loss: -0.002273289952427149
        total_loss: -0.003260598052293062
        vf_explained_var: 0.0009199231863021851
        vf_loss: 0.32867664098739624
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7383989691734314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016385758062824607
        model: {}
        policy_loss: -0.003763394895941019
        total_loss: -0.005021385848522186
        vf_explained_var: 0.014230534434318542
        vf_loss: 0.4159313440322876
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0079926252365112
        entropy_coeff: 0.0017600000137463212
        kl: 0.002495593624189496
        model: {}
        policy_loss: -0.0029762438498437405
        total_loss: -0.004729118663817644
        vf_explained_var: 0.008386671543121338
        vf_loss: 0.21191711723804474
    load_time_ms: 18806.042
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 107156.891
    update_time_ms: 15.883
  iterations_since_restore: 113
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.821938775510205
    ram_util_percent: 9.204081632653063
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.08
    agent-1: 1.59
    agent-2: 4.78
    agent-3: 3.97
    agent-4: 4.7
    agent-5: 2.28
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.495185653062194
    mean_inference_ms: 13.68088203329736
    mean_processing_ms: 64.64122388591925
  time_since_restore: 15739.08117890358
  time_this_iter_s: 137.16929244995117
  time_total_s: 38249.364421367645
  timestamp: 1637235895
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    273 |          38249.4 | 26208000 |     20.4 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 1.99
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.05
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 152
    cleaning_beam_agent-0_mean: 74.77
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 213.59
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 18.42
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 54.72
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 140
    cleaning_beam_agent-4_mean: 67.1
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 18.67
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-47-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 22.01
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11670.51
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5626078844070435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017823048401623964
        model: {}
        policy_loss: -0.003907263278961182
        total_loss: -0.004865613766014576
        vf_explained_var: 0.004585593938827515
        vf_loss: 0.3183956742286682
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5323137640953064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008826901903375983
        model: {}
        policy_loss: -0.002331106225028634
        total_loss: -0.003254181006923318
        vf_explained_var: 0.025095269083976746
        vf_loss: 0.13796862959861755
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4895229935646057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015666850376874208
        model: {}
        policy_loss: -0.0037724764551967382
        total_loss: -0.004579877480864525
        vf_explained_var: 0.0016283541917800903
        vf_loss: 0.541588306427002
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.578373372554779
        entropy_coeff: 0.0017600000137463212
        kl: 0.001032947446219623
        model: {}
        policy_loss: -0.0024321479722857475
        total_loss: -0.0034201210364699364
        vf_explained_var: 0.0005374401807785034
        vf_loss: 0.29963418841362
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.737439751625061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019047186942771077
        model: {}
        policy_loss: -0.003979151137173176
        total_loss: -0.00523130688816309
        vf_explained_var: 0.01102668046951294
        vf_loss: 0.45738485455513
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9996763467788696
        entropy_coeff: 0.0017600000137463212
        kl: 0.002637379104271531
        model: {}
        policy_loss: -0.0030540027655661106
        total_loss: -0.004792015068233013
        vf_explained_var: 0.004959657788276672
        vf_loss: 0.21416035294532776
    load_time_ms: 18674.232
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 107207.367
    update_time_ms: 15.842
  iterations_since_restore: 114
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.58469387755102
    ram_util_percent: 9.30561224489796
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 10.0
    agent-4: 19.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.9
    agent-1: 1.19
    agent-2: 4.78
    agent-3: 4.15
    agent-4: 5.16
    agent-5: 2.83
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.493947374551194
    mean_inference_ms: 13.680938466519063
    mean_processing_ms: 64.64529288125176
  time_since_restore: 15877.130899429321
  time_this_iter_s: 138.04972052574158
  time_total_s: 38387.41414189339
  timestamp: 1637236033
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    274 |          38387.4 | 26304000 |    22.01 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.12
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 2.33
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.81
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.05
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 175
    cleaning_beam_agent-0_mean: 72.56
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 221.4
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 19.5
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 51.68
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 155
    cleaning_beam_agent-4_mean: 66.65
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 16.51
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-49-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 20.73
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11678.832
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5587739944458008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013286267640069127
        model: {}
        policy_loss: -0.003481518477201462
        total_loss: -0.004434608854353428
        vf_explained_var: 0.007360875606536865
        vf_loss: 0.3035082519054413
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5340246558189392
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009150341502390802
        model: {}
        policy_loss: -0.002631276845932007
        total_loss: -0.003559325821697712
        vf_explained_var: 0.030227109789848328
        vf_loss: 0.11836168169975281
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49441105127334595
        entropy_coeff: 0.0017600000137463212
        kl: 0.001143657136708498
        model: {}
        policy_loss: -0.0035145769361406565
        total_loss: -0.0043471138924360275
        vf_explained_var: 0.005893439054489136
        vf_loss: 0.3762480616569519
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5739518404006958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010134275071322918
        model: {}
        policy_loss: -0.0024472910445183516
        total_loss: -0.003424940397962928
        vf_explained_var: 0.0019476711750030518
        vf_loss: 0.3250707983970642
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7376164793968201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015254509635269642
        model: {}
        policy_loss: -0.003669521538540721
        total_loss: -0.004931402392685413
        vf_explained_var: 0.030928701162338257
        vf_loss: 0.363236665725708
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.007108211517334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014755291631445289
        model: {}
        policy_loss: -0.0026587259490042925
        total_loss: -0.0044113146141171455
        vf_explained_var: 0.0036422908306121826
        vf_loss: 0.19924841821193695
    load_time_ms: 18636.623
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 107051.95
    update_time_ms: 15.996
  iterations_since_restore: 115
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.658163265306122
    ram_util_percent: 9.344897959183676
  pid: 6435
  policy_reward_max:
    agent-0: 17.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.72
    agent-2: 4.42
    agent-3: 4.13
    agent-4: 4.61
    agent-5: 2.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.491209972484736
    mean_inference_ms: 13.681033288625622
    mean_processing_ms: 64.6405088405031
  time_since_restore: 16014.398082733154
  time_this_iter_s: 137.267183303833
  time_total_s: 38524.68132519722
  timestamp: 1637236171
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    275 |          38524.7 | 26400000 |    20.73 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.3
    apples_agent-2_min: 0
    apples_agent-3_max: 53
    apples_agent-3_mean: 3.87
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 72.2
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 228.49
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 82
    cleaning_beam_agent-2_mean: 18.51
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 49.07
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 68.05
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 17.54
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-51-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 21.29
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11698.581
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5670739412307739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017703160410746932
        model: {}
        policy_loss: -0.0038014953024685383
        total_loss: -0.004772192798554897
        vf_explained_var: 0.0004704296588897705
        vf_loss: 0.2735089063644409
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5386438965797424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007211593328975141
        model: {}
        policy_loss: -0.0022380570881068707
        total_loss: -0.003173163626343012
        vf_explained_var: 0.027553066611289978
        vf_loss: 0.12910118699073792
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48790493607521057
        entropy_coeff: 0.0017600000137463212
        kl: 0.001491499482654035
        model: {}
        policy_loss: -0.003622521413490176
        total_loss: -0.004424722399562597
        vf_explained_var: 0.00393812358379364
        vf_loss: 0.5651192665100098
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5660549402236938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007699998095631599
        model: {}
        policy_loss: -0.0024896198883652687
        total_loss: -0.003449277952313423
        vf_explained_var: -0.004657089710235596
        vf_loss: 0.36600247025489807
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7391548156738281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013808785006403923
        model: {}
        policy_loss: -0.00391731271520257
        total_loss: -0.005179289728403091
        vf_explained_var: 0.018359139561653137
        vf_loss: 0.3893679976463318
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9946581721305847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010140279773622751
        model: {}
        policy_loss: -0.002177522052079439
        total_loss: -0.0039062201976776123
        vf_explained_var: 0.010803908109664917
        vf_loss: 0.21899351477622986
    load_time_ms: 18659.53
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 107221.732
    update_time_ms: 15.469
  iterations_since_restore: 116
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.6741116751269
    ram_util_percent: 9.302030456852792
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.22
    agent-1: 1.75
    agent-2: 4.95
    agent-3: 4.45
    agent-4: 4.88
    agent-5: 2.04
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 25.489219151627697
    mean_inference_ms: 13.681239906618577
    mean_processing_ms: 64.63822384921197
  time_since_restore: 16152.975378036499
  time_this_iter_s: 138.57729530334473
  time_total_s: 38663.258620500565
  timestamp: 1637236309
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    276 |          38663.3 | 26496000 |    21.29 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 41
    apples_agent-1_mean: 1.23
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.04
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.89
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 74.44
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 442
    cleaning_beam_agent-1_mean: 231.53
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 19.19
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 50.9
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 168
    cleaning_beam_agent-4_mean: 63.72
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 86
    cleaning_beam_agent-5_mean: 17.53
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-54-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 19.76
  episode_reward_min: -78.0
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11705.082
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5541491508483887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014326736563816667
        model: {}
        policy_loss: -0.003592368680983782
        total_loss: -0.004537341650575399
        vf_explained_var: 0.005195260047912598
        vf_loss: 0.3032825291156769
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5429686903953552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009475182741880417
        model: {}
        policy_loss: -0.0025638574734330177
        total_loss: -0.003510815091431141
        vf_explained_var: 0.014352858066558838
        vf_loss: 0.08666960895061493
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48647886514663696
        entropy_coeff: 0.0017600000137463212
        kl: 0.001723884604871273
        model: {}
        policy_loss: -0.002960058394819498
        total_loss: -0.003682512789964676
        vf_explained_var: 0.0008944272994995117
        vf_loss: 1.337496280670166
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5805441737174988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008985702879726887
        model: {}
        policy_loss: -0.002111459383741021
        total_loss: -0.0030958924908190966
        vf_explained_var: 0.00025707483291625977
        vf_loss: 0.373264342546463
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7335454821586609
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006593415746465325
        model: {}
        policy_loss: -0.002517059678211808
        total_loss: -0.003669473109766841
        vf_explained_var: 0.0062896013259887695
        vf_loss: 1.3862539529800415
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9830039143562317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016621174290776253
        model: {}
        policy_loss: -0.0023367200046777725
        total_loss: -0.004043989814817905
        vf_explained_var: 0.0008387714624404907
        vf_loss: 0.2281813621520996
    load_time_ms: 18666.423
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 107183.976
    update_time_ms: 15.513
  iterations_since_restore: 117
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.885714285714286
    ram_util_percent: 9.312755102040816
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 5.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.43
    agent-1: 1.36
    agent-2: 4.18
    agent-3: 3.53
    agent-4: 4.66
    agent-5: 2.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -40.0
    agent-3: -41.0
    agent-4: -45.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.488542445205958
    mean_inference_ms: 13.681460516003094
    mean_processing_ms: 64.64133732907666
  time_since_restore: 16290.683599710464
  time_this_iter_s: 137.70822167396545
  time_total_s: 38800.96684217453
  timestamp: 1637236447
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    277 |            38801 | 26592000 |    19.76 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.42
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 2.93
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 72.36
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 227.95
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 17.44
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 50.9
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 72.49
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 14.05
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-56-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 18.98
  episode_reward_min: -72.0
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11722.747
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5553074479103088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012478240532800555
        model: {}
        policy_loss: -0.003185044741258025
        total_loss: -0.004130753688514233
        vf_explained_var: 0.005089655518531799
        vf_loss: 0.31630292534828186
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5392757654190063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007221686537377536
        model: {}
        policy_loss: -0.0017613386735320091
        total_loss: -0.0025675338692963123
        vf_explained_var: 0.012503370642662048
        vf_loss: 1.4293055534362793
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4859582185745239
        entropy_coeff: 0.0017600000137463212
        kl: 0.001244099112227559
        model: {}
        policy_loss: -0.0035627372562885284
        total_loss: -0.004378474783152342
        vf_explained_var: -0.006220340728759766
        vf_loss: 0.3954831659793854
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5671705603599548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012366095324978232
        model: {}
        policy_loss: -0.00262759393081069
        total_loss: -0.0035980194807052612
        vf_explained_var: -0.0008139610290527344
        vf_loss: 0.2779087424278259
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7229487895965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013596019707620144
        model: {}
        policy_loss: -0.002780324313789606
        total_loss: -0.003491759765893221
        vf_explained_var: 0.0019872188568115234
        vf_loss: 5.6095404624938965
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9720693826675415
        entropy_coeff: 0.0017600000137463212
        kl: 0.001605649129487574
        model: {}
        policy_loss: -0.002906169043853879
        total_loss: -0.004598247818648815
        vf_explained_var: 0.0010605454444885254
        vf_loss: 0.1876325160264969
    load_time_ms: 18782.6
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 107426.523
    update_time_ms: 15.511
  iterations_since_restore: 118
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.225870646766168
    ram_util_percent: 9.35820895522388
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.43
    agent-1: 1.11
    agent-2: 4.52
    agent-3: 3.93
    agent-4: 3.42
    agent-5: 2.57
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -94.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.487378915890126
    mean_inference_ms: 13.681482123677469
    mean_processing_ms: 64.64234362994999
  time_since_restore: 16431.279720544815
  time_this_iter_s: 140.5961208343506
  time_total_s: 38941.56296300888
  timestamp: 1637236588
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    278 |          38941.6 | 26688000 |    18.98 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.41
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.09
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 71.97
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 219.83
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 21.27
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 51.58
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 185
    cleaning_beam_agent-4_mean: 71.65
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 15.86
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-58-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 22.06
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11710.362
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704650282859802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016242703422904015
        model: {}
        policy_loss: -0.003910982981324196
        total_loss: -0.004881430417299271
        vf_explained_var: 7.434189319610596e-05
        vf_loss: 0.3357073664665222
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5306681394577026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011960533447563648
        model: {}
        policy_loss: -0.0026385870296508074
        total_loss: -0.003560977755114436
        vf_explained_var: 0.02775578200817108
        vf_loss: 0.11586934328079224
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5070161819458008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014155777171254158
        model: {}
        policy_loss: -0.0038055768236517906
        total_loss: -0.004649139475077391
        vf_explained_var: 0.005857586860656738
        vf_loss: 0.48786461353302
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5799628496170044
        entropy_coeff: 0.0017600000137463212
        kl: 0.001124383881688118
        model: {}
        policy_loss: -0.0025398158468306065
        total_loss: -0.0035240016877651215
        vf_explained_var: -0.001789972186088562
        vf_loss: 0.3654947578907013
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7272588014602661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016893635038286448
        model: {}
        policy_loss: -0.0038370718248188496
        total_loss: -0.005070417188107967
        vf_explained_var: 0.017824754118919373
        vf_loss: 0.46630096435546875
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9857690930366516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018176550511270761
        model: {}
        policy_loss: -0.0027809953317046165
        total_loss: -0.004490545950829983
        vf_explained_var: 0.0012284517288208008
        vf_loss: 0.2540460228919983
    load_time_ms: 18779.914
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 107284.987
    update_time_ms: 15.615
  iterations_since_restore: 119
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.73282051282051
    ram_util_percent: 9.328717948717948
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 12.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.69
    agent-1: 1.81
    agent-2: 4.91
    agent-3: 4.21
    agent-4: 4.78
    agent-5: 2.66
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.48574332817479
    mean_inference_ms: 13.680678303349996
    mean_processing_ms: 64.63756628935317
  time_since_restore: 16567.92584848404
  time_this_iter_s: 136.64612793922424
  time_total_s: 39078.209090948105
  timestamp: 1637236725
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    279 |          39078.2 | 26784000 |    22.06 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.69
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.75
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.53
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.12
    apples_agent-3_min: 0
    apples_agent-4_max: 112
    apples_agent-4_mean: 2.08
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 129
    cleaning_beam_agent-0_mean: 67.2
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 220.16
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 17.82
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 49.87
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 179
    cleaning_beam_agent-4_mean: 69.81
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 15.45
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-01-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 19.74
  episode_reward_min: -67.0
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11708.953
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5598280429840088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014283847995102406
        model: {}
        policy_loss: -0.0027141347527503967
        total_loss: -0.0035383834037929773
        vf_explained_var: 0.003335297107696533
        vf_loss: 1.6104857921600342
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5310149192810059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009092154214158654
        model: {}
        policy_loss: -0.00205813255161047
        total_loss: -0.002850631019100547
        vf_explained_var: 0.006219744682312012
        vf_loss: 1.420897126197815
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.492642343044281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010436630109325051
        model: {}
        policy_loss: -0.0028960497584193945
        total_loss: -0.003583258017897606
        vf_explained_var: 0.0009639114141464233
        vf_loss: 1.7984260320663452
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5762142539024353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012473323149606586
        model: {}
        policy_loss: -0.002165064215660095
        total_loss: -0.00313413143157959
        vf_explained_var: -0.003799945116043091
        vf_loss: 0.45067256689071655
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7394498586654663
        entropy_coeff: 0.0017600000137463212
        kl: 0.001748409355059266
        model: {}
        policy_loss: -0.003965912852436304
        total_loss: -0.005230416543781757
        vf_explained_var: 0.013024210929870605
        vf_loss: 0.36927399039268494
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.984233558177948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026691469829529524
        model: {}
        policy_loss: -0.0029274257831275463
        total_loss: -0.004639762919396162
        vf_explained_var: -0.0012801885604858398
        vf_loss: 0.1991431713104248
    load_time_ms: 18753.757
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 107601.882
    update_time_ms: 15.828
  iterations_since_restore: 120
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.1715
    ram_util_percent: 9.3675
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.79
    agent-1: 1.35
    agent-2: 4.61
    agent-3: 3.7
    agent-4: 4.66
    agent-5: 2.63
  policy_reward_min:
    agent-0: -49.0
    agent-1: -46.0
    agent-2: -44.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.48450868596206
    mean_inference_ms: 13.68171180064592
    mean_processing_ms: 64.63974634195351
  time_since_restore: 16708.022327184677
  time_this_iter_s: 140.09647870063782
  time_total_s: 39218.30556964874
  timestamp: 1637236865
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    280 |          39218.3 | 26880000 |    19.74 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.84
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.92
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.09
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.0
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 70.97
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 219.56
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 116
    cleaning_beam_agent-2_mean: 18.9
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 51.58
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 264
    cleaning_beam_agent-4_mean: 71.32
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 15.85
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-03-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 22.42
  episode_reward_min: -26.0
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11720.323
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5696491003036499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014146908652037382
        model: {}
        policy_loss: -0.0036004669964313507
        total_loss: -0.00457545043900609
        vf_explained_var: 0.0013436675071716309
        vf_loss: 0.27601003646850586
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.516179084777832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010347508359700441
        model: {}
        policy_loss: -0.0024198712781071663
        total_loss: -0.003314194269478321
        vf_explained_var: 0.008889436721801758
        vf_loss: 0.14150553941726685
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4837420582771301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015117691364139318
        model: {}
        policy_loss: -0.0035262657329440117
        total_loss: -0.004328559152781963
        vf_explained_var: 0.002646222710609436
        vf_loss: 0.49092966318130493
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5698043704032898
        entropy_coeff: 0.0017600000137463212
        kl: 0.001246495870873332
        model: {}
        policy_loss: -0.002671839203685522
        total_loss: -0.0036339224316179752
        vf_explained_var: -0.0021514296531677246
        vf_loss: 0.40772712230682373
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7496762871742249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017827757401391864
        model: {}
        policy_loss: -0.0036574192345142365
        total_loss: -0.004925772547721863
        vf_explained_var: 0.02565203607082367
        vf_loss: 0.5107927918434143
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0040204524993896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018312011379748583
        model: {}
        policy_loss: -0.0025796908885240555
        total_loss: -0.004325767047703266
        vf_explained_var: 0.0031484663486480713
        vf_loss: 0.2099609673023224
    load_time_ms: 18794.506
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 107514.468
    update_time_ms: 15.804
  iterations_since_restore: 121
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.799492385786802
    ram_util_percent: 9.313705583756347
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.38
    agent-1: 1.75
    agent-2: 4.73
    agent-3: 4.29
    agent-4: 5.44
    agent-5: 2.83
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.483870853404305
    mean_inference_ms: 13.682175257106714
    mean_processing_ms: 64.64175047244154
  time_since_restore: 16845.925775289536
  time_this_iter_s: 137.9034481048584
  time_total_s: 39356.2090177536
  timestamp: 1637237004
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    281 |          39356.2 | 26976000 |    22.42 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 2.56
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.05
    apples_agent-3_min: 0
    apples_agent-4_max: 99
    apples_agent-4_mean: 2.45
    apples_agent-4_min: 0
    apples_agent-5_max: 69
    apples_agent-5_mean: 2.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 120
    cleaning_beam_agent-0_mean: 73.04
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 218.79
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 73
    cleaning_beam_agent-2_mean: 19.09
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 53.35
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 159
    cleaning_beam_agent-4_mean: 64.37
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 15.79
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-05-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 21.24
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11719.361
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5679722428321838
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014979727566242218
        model: {}
        policy_loss: -0.0034268833696842194
        total_loss: -0.004403481259942055
        vf_explained_var: -0.0016797780990600586
        vf_loss: 0.23034629225730896
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.522399365901947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008100642007775605
        model: {}
        policy_loss: -0.0025158999487757683
        total_loss: -0.0034250388853251934
        vf_explained_var: 0.022287517786026
        vf_loss: 0.10286207497119904
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48311305046081543
        entropy_coeff: 0.0017600000137463212
        kl: 0.001330489874817431
        model: {}
        policy_loss: -0.002235334599390626
        total_loss: -0.0029169106855988503
        vf_explained_var: -0.0012792348861694336
        vf_loss: 1.6870561838150024
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5834606885910034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009603300131857395
        model: {}
        policy_loss: -0.0024737720377743244
        total_loss: -0.003461644286289811
        vf_explained_var: -0.00032132863998413086
        vf_loss: 0.3901516795158386
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.750135600566864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016834568232297897
        model: {}
        policy_loss: -0.0038163012359291315
        total_loss: -0.00509173097088933
        vf_explained_var: 0.02851910889148712
        vf_loss: 0.44806280732154846
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.979562520980835
        entropy_coeff: 0.0017600000137463212
        kl: 0.001636193017475307
        model: {}
        policy_loss: -0.002894687233492732
        total_loss: -0.0045970408245921135
        vf_explained_var: 0.004324689507484436
        vf_loss: 0.21671096980571747
    load_time_ms: 18770.182
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 107759.376
    update_time_ms: 16.157
  iterations_since_restore: 122
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.223618090452263
    ram_util_percent: 9.311557788944725
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 22.0
    agent-3: 19.0
    agent-4: 19.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.05
    agent-1: 1.81
    agent-2: 4.4
    agent-3: 4.19
    agent-4: 4.96
    agent-5: 2.83
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.483394004956345
    mean_inference_ms: 13.681799560732388
    mean_processing_ms: 64.64558643849527
  time_since_restore: 16985.441341638565
  time_this_iter_s: 139.51556634902954
  time_total_s: 39495.72458410263
  timestamp: 1637237143
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    282 |          39495.7 | 27072000 |    21.24 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.17
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.63
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.81
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 144
    cleaning_beam_agent-0_mean: 71.62
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 216.31
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 170
    cleaning_beam_agent-2_mean: 17.48
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 55.89
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 186
    cleaning_beam_agent-4_mean: 57.8
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 74
    cleaning_beam_agent-5_mean: 16.01
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-08-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 19.27
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11739.664
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5647661089897156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012104168999940157
        model: {}
        policy_loss: -0.003308931365609169
        total_loss: -0.004272568039596081
        vf_explained_var: -0.0005596429109573364
        vf_loss: 0.30352282524108887
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5352203845977783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014611424412578344
        model: {}
        policy_loss: -0.0027659002225846052
        total_loss: -0.0036987382918596268
        vf_explained_var: 0.017556846141815186
        vf_loss: 0.09147573262453079
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4603812098503113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008125404128804803
        model: {}
        policy_loss: -0.0015051555819809437
        total_loss: -0.0021498072892427444
        vf_explained_var: 0.0006617158651351929
        vf_loss: 1.6561973094940186
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5733373761177063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013901484198868275
        model: {}
        policy_loss: -0.0027049826458096504
        total_loss: -0.003676647087559104
        vf_explained_var: 0.00403963029384613
        vf_loss: 0.37407171726226807
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7346060276031494
        entropy_coeff: 0.0017600000137463212
        kl: 0.001407684525474906
        model: {}
        policy_loss: -0.003643225645646453
        total_loss: -0.004888300318270922
        vf_explained_var: 0.021188080310821533
        vf_loss: 0.47831904888153076
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9922583103179932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021091578528285027
        model: {}
        policy_loss: -0.0029326947405934334
        total_loss: -0.004662817809730768
        vf_explained_var: 0.00512106716632843
        vf_loss: 0.1625327616930008
    load_time_ms: 18774.943
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 107835.736
    update_time_ms: 15.911
  iterations_since_restore: 123
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.551269035532997
    ram_util_percent: 9.319796954314722
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.72
    agent-1: 1.52
    agent-2: 4.44
    agent-3: 3.41
    agent-4: 4.87
    agent-5: 2.31
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: -25.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.484292843870275
    mean_inference_ms: 13.682721848224316
    mean_processing_ms: 64.65271370876266
  time_since_restore: 17123.634657144547
  time_this_iter_s: 138.19331550598145
  time_total_s: 39633.91789960861
  timestamp: 1637237282
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    283 |          39633.9 | 27168000 |    19.27 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.26
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 64.54
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 218.91
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 17.14
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 52.21
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 161
    cleaning_beam_agent-4_mean: 60.47
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 17.44
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-10-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 20.49
  episode_reward_min: -72.0
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11742.32
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.552081286907196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016534891910851002
        model: {}
        policy_loss: -0.003557089250534773
        total_loss: -0.004499921575188637
        vf_explained_var: -0.003710225224494934
        vf_loss: 0.2883182764053345
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.515831708908081
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015664196107536554
        model: {}
        policy_loss: -0.002258041873574257
        total_loss: -0.003021515905857086
        vf_explained_var: 0.0002367645502090454
        vf_loss: 1.443927526473999
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4779883921146393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012911483645439148
        model: {}
        policy_loss: -0.0024289824068546295
        total_loss: -0.003114813007414341
        vf_explained_var: -0.0010009557008743286
        vf_loss: 1.554282307624817
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5825297832489014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015127225778996944
        model: {}
        policy_loss: -0.003063788404688239
        total_loss: -0.004053826909512281
        vf_explained_var: -0.004334449768066406
        vf_loss: 0.3521862030029297
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7385011911392212
        entropy_coeff: 0.0017600000137463212
        kl: 0.001964755356311798
        model: {}
        policy_loss: -0.002995249815285206
        total_loss: -0.004131901077926159
        vf_explained_var: 0.008555158972740173
        vf_loss: 1.6310975551605225
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.999118447303772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016877215821295977
        model: {}
        policy_loss: -0.002762364922091365
        total_loss: -0.004501239396631718
        vf_explained_var: -0.001488521695137024
        vf_loss: 0.19574463367462158
    load_time_ms: 18818.353
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 107663.918
    update_time_ms: 15.951
  iterations_since_restore: 124
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.910769230769233
    ram_util_percent: 9.324102564102565
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.45
    agent-1: 1.14
    agent-2: 3.98
    agent-3: 4.18
    agent-4: 4.93
    agent-5: 2.81
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: -43.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.481107262783492
    mean_inference_ms: 13.68245725524267
    mean_processing_ms: 64.64973064812764
  time_since_restore: 17260.42575955391
  time_this_iter_s: 136.7911024093628
  time_total_s: 39770.709002017975
  timestamp: 1637237419
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    284 |          39770.7 | 27264000 |    20.49 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.8
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.26
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.23
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 118
    cleaning_beam_agent-0_mean: 67.41
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 335
    cleaning_beam_agent-1_mean: 208.04
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 106
    cleaning_beam_agent-2_mean: 19.32
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 45.16
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 151
    cleaning_beam_agent-4_mean: 57.46
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 16.37
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-12-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 20.36
  episode_reward_min: -176.0
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11747.373
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5588852167129517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013845977373421192
        model: {}
        policy_loss: -0.0033982626628130674
        total_loss: -0.004352661781013012
        vf_explained_var: -0.0032539069652557373
        vf_loss: 0.2923813462257385
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5160180926322937
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008483611163683236
        model: {}
        policy_loss: -0.002455515554174781
        total_loss: -0.0033527512568980455
        vf_explained_var: 0.025636807084083557
        vf_loss: 0.10954862087965012
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47416120767593384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010905904928222299
        model: {}
        policy_loss: -0.00292328093200922
        total_loss: -0.003701179288327694
        vf_explained_var: 0.007696360349655151
        vf_loss: 0.5662676095962524
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5784478783607483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020043635740876198
        model: {}
        policy_loss: -0.002780635841190815
        total_loss: -0.0037613818421959877
        vf_explained_var: 0.00012636184692382812
        vf_loss: 0.37319403886795044
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7329401969909668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011170005891472101
        model: {}
        policy_loss: -0.0021018246188759804
        total_loss: -0.00330708222463727
        vf_explained_var: 0.010849729180335999
        vf_loss: 0.8471558690071106
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9989532232284546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018763889092952013
        model: {}
        policy_loss: -0.002093746792525053
        total_loss: -0.0038123163394629955
        vf_explained_var: -0.003632664680480957
        vf_loss: 0.39586466550827026
    load_time_ms: 18866.035
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 107608.243
    update_time_ms: 15.761
  iterations_since_restore: 125
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.139487179487183
    ram_util_percent: 9.293846153846156
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.5
    agent-1: 1.6
    agent-2: 4.31
    agent-3: 4.5
    agent-4: 3.76
    agent-5: 2.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: -97.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 25.478797737127824
    mean_inference_ms: 13.682265085068895
    mean_processing_ms: 64.64772597189715
  time_since_restore: 17397.62270975113
  time_this_iter_s: 137.19695019721985
  time_total_s: 39907.905952215195
  timestamp: 1637237556
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    285 |          39907.9 | 27360000 |    20.36 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.84
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 134
    cleaning_beam_agent-0_mean: 70.99
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 201.63
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 15.82
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 44.61
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 136
    cleaning_beam_agent-4_mean: 61.03
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 17.59
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-14-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 17.98
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11733.092
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5683541893959045
        entropy_coeff: 0.0017600000137463212
        kl: 0.001387734548188746
        model: {}
        policy_loss: -0.003589059691876173
        total_loss: -0.0045645711943507195
        vf_explained_var: 0.0019439458847045898
        vf_loss: 0.2479584962129593
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49387261271476746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009773020865395665
        model: {}
        policy_loss: -0.0015839152038097382
        total_loss: -0.0023177172988653183
        vf_explained_var: 0.006518110632896423
        vf_loss: 1.3541498184204102
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4759344458580017
        entropy_coeff: 0.0017600000137463212
        kl: 0.00137354398611933
        model: {}
        policy_loss: -0.0036829467862844467
        total_loss: -0.004471952095627785
        vf_explained_var: 0.006447851657867432
        vf_loss: 0.4863724112510681
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5700494050979614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008833302417770028
        model: {}
        policy_loss: -0.0015096268616616726
        total_loss: -0.0023387959226965904
        vf_explained_var: 0.002423390746116638
        vf_loss: 1.7411702871322632
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.737601101398468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014520261902362108
        model: {}
        policy_loss: -0.0036862194538116455
        total_loss: -0.004942629486322403
        vf_explained_var: 0.017835870385169983
        vf_loss: 0.4177084267139435
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9985650777816772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011398200877010822
        model: {}
        policy_loss: -0.0020558289252221584
        total_loss: -0.0036737052723765373
        vf_explained_var: 0.00029543042182922363
        vf_loss: 1.3959674835205078
    load_time_ms: 18879.343
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 107430.802
    update_time_ms: 15.678
  iterations_since_restore: 126
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.848717948717947
    ram_util_percent: 9.26974358974359
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.19
    agent-1: 1.3
    agent-2: 4.68
    agent-3: 3.0
    agent-4: 4.43
    agent-5: 1.38
  policy_reward_min:
    agent-0: -1.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 25.475435691260536
    mean_inference_ms: 13.681716921737682
    mean_processing_ms: 64.6463791682083
  time_since_restore: 17534.46653676033
  time_this_iter_s: 136.84382700920105
  time_total_s: 40044.749779224396
  timestamp: 1637237693
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    286 |          40044.7 | 27456000 |    17.98 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.69
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.21
    apples_agent-3_min: 0
    apples_agent-4_max: 47
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 140
    cleaning_beam_agent-0_mean: 71.75
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 202.18
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 71
    cleaning_beam_agent-2_mean: 17.73
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 45.24
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 145
    cleaning_beam_agent-4_mean: 67.49
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 19.02
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-17-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 22.36
  episode_reward_min: -19.0
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11723.226
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5690621733665466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012345393188297749
        model: {}
        policy_loss: -0.0035397480241954327
        total_loss: -0.004505669232457876
        vf_explained_var: 0.005253493785858154
        vf_loss: 0.35631701350212097
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4998641014099121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009074233239516616
        model: {}
        policy_loss: -0.002585768699645996
        total_loss: -0.0034522854257375
        vf_explained_var: 0.015958786010742188
        vf_loss: 0.13243256509304047
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47320011258125305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009271977469325066
        model: {}
        policy_loss: -0.0035385186783969402
        total_loss: -0.00431971438229084
        vf_explained_var: 0.004640758037567139
        vf_loss: 0.5163873434066772
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5781850218772888
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016273793298751116
        model: {}
        policy_loss: -0.002736794762313366
        total_loss: -0.0037188101559877396
        vf_explained_var: -0.001436769962310791
        vf_loss: 0.35588890314102173
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7334182262420654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016250766348093748
        model: {}
        policy_loss: -0.0038562826812267303
        total_loss: -0.005103678908199072
        vf_explained_var: 0.017769619822502136
        vf_loss: 0.43421587347984314
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0177056789398193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016876849113032222
        model: {}
        policy_loss: -0.0024436330422759056
        total_loss: -0.004210618324577808
        vf_explained_var: 0.006644099950790405
        vf_loss: 0.2417680025100708
    load_time_ms: 18901.125
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 107568.717
    update_time_ms: 15.492
  iterations_since_restore: 127
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.468844221105527
    ram_util_percent: 9.329145728643216
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 10.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.5
    agent-1: 1.86
    agent-2: 5.27
    agent-3: 4.46
    agent-4: 4.92
    agent-5: 2.35
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -1.0
    agent-5: -45.0
  sampler_perf:
    mean_env_wait_ms: 25.47318064151986
    mean_inference_ms: 13.681781075451024
    mean_processing_ms: 64.65002261554847
  time_since_restore: 17673.62908554077
  time_this_iter_s: 139.16254878044128
  time_total_s: 40183.91232800484
  timestamp: 1637237832
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    287 |          40183.9 | 27552000 |    22.36 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 42
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.69
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 1.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 71.9
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 352
    cleaning_beam_agent-1_mean: 207.43
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 65
    cleaning_beam_agent-2_mean: 16.93
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 49.61
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 64.94
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 18.56
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-19-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 22.63
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11707.101
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5636823177337646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006247283890843391
        model: {}
        policy_loss: -0.0011785002425312996
        total_loss: -0.0020095682702958584
        vf_explained_var: 0.000621497631072998
        vf_loss: 1.6101528406143188
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.508992612361908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012471206719055772
        model: {}
        policy_loss: -0.0026889119762927294
        total_loss: -0.003573529189452529
        vf_explained_var: 0.013526394963264465
        vf_loss: 0.11211584508419037
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4771842658519745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012388144386932254
        model: {}
        policy_loss: -0.0034727624151855707
        total_loss: -0.004269905388355255
        vf_explained_var: -0.003274396061897278
        vf_loss: 0.4270187020301819
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5804946422576904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011853021569550037
        model: {}
        policy_loss: -0.002550678327679634
        total_loss: -0.003529052250087261
        vf_explained_var: -0.0006176382303237915
        vf_loss: 0.4329788088798523
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7282811403274536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017066029831767082
        model: {}
        policy_loss: -0.0037393299862742424
        total_loss: -0.004971442744135857
        vf_explained_var: 0.022700250148773193
        vf_loss: 0.49661681056022644
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0130536556243896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016764975152909756
        model: {}
        policy_loss: -0.0024624522775411606
        total_loss: -0.0042225150391459465
        vf_explained_var: 0.002474263310432434
        vf_loss: 0.22911013662815094
    load_time_ms: 18761.217
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 107351.359
    update_time_ms: 15.281
  iterations_since_restore: 128
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.8851282051282
    ram_util_percent: 9.341538461538462
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 12.0
    agent-3: 19.0
    agent-4: 18.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.1
    agent-1: 1.58
    agent-2: 5.15
    agent-3: 4.63
    agent-4: 5.31
    agent-5: 2.86
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.472021413354433
    mean_inference_ms: 13.681803587756752
    mean_processing_ms: 64.6502331810451
  time_since_restore: 17810.495284318924
  time_this_iter_s: 136.86619877815247
  time_total_s: 40320.77852678299
  timestamp: 1637237969
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    288 |          40320.8 | 27648000 |    22.63 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 38
    apples_agent-2_mean: 2.42
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 4.14
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 72.42
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 202.2
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 135
    cleaning_beam_agent-2_mean: 19.82
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 42.48
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 137
    cleaning_beam_agent-4_mean: 65.61
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 67
    cleaning_beam_agent-5_mean: 18.07
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-21-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 24.22
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11718.317
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5722023844718933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010056233732029796
        model: {}
        policy_loss: -0.00336680980399251
        total_loss: -0.004350338131189346
        vf_explained_var: 0.0034709423780441284
        vf_loss: 0.23544040322303772
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5001742243766785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012222279328852892
        model: {}
        policy_loss: -0.0025472494307905436
        total_loss: -0.0034126017708331347
        vf_explained_var: 0.012091755867004395
        vf_loss: 0.14952246844768524
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4845787286758423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013835718855261803
        model: {}
        policy_loss: -0.0034225676208734512
        total_loss: -0.004221665672957897
        vf_explained_var: 0.0011518150568008423
        vf_loss: 0.537619411945343
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5701124668121338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011841349769383669
        model: {}
        policy_loss: -0.002509769983589649
        total_loss: -0.0034588510170578957
        vf_explained_var: -0.003449559211730957
        vf_loss: 0.543142557144165
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7294595241546631
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018940503941848874
        model: {}
        policy_loss: -0.004087853245437145
        total_loss: -0.005319463089108467
        vf_explained_var: 0.020776137709617615
        vf_loss: 0.5223941802978516
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.998101532459259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014666339848190546
        model: {}
        policy_loss: -0.002579816384240985
        total_loss: -0.0043123504146933556
        vf_explained_var: 0.001963376998901367
        vf_loss: 0.2412315458059311
    load_time_ms: 18764.257
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 107303.158
    update_time_ms: 14.968
  iterations_since_restore: 129
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.78666666666667
    ram_util_percent: 9.299487179487178
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 19.0
    agent-4: 21.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.3
    agent-1: 1.83
    agent-2: 5.33
    agent-3: 5.25
    agent-4: 5.58
    agent-5: 2.93
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.46713100293792
    mean_inference_ms: 13.680737075111075
    mean_processing_ms: 64.6432018060643
  time_since_restore: 17946.91725873947
  time_this_iter_s: 136.42197442054749
  time_total_s: 40457.20050120354
  timestamp: 1637238106
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    289 |          40457.2 | 27744000 |    24.22 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.62
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 75.2
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 209.81
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 81
    cleaning_beam_agent-2_mean: 18.86
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 38.5
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 139
    cleaning_beam_agent-4_mean: 57.84
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 17.22
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-24-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 20.75
  episode_reward_min: -47.0
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11716.191
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5630351901054382
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011189124779775739
        model: {}
        policy_loss: -0.0034644645638763905
        total_loss: -0.004431428387761116
        vf_explained_var: -0.0013744831085205078
        vf_loss: 0.23977014422416687
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5180891752243042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008332220604643226
        model: {}
        policy_loss: -0.002404042985290289
        total_loss: -0.0033035469241440296
        vf_explained_var: 0.01181437075138092
        vf_loss: 0.12337939441204071
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46754223108291626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013754988322034478
        model: {}
        policy_loss: -0.002584205474704504
        total_loss: -0.0032395566813647747
        vf_explained_var: 0.001176685094833374
        vf_loss: 1.6752214431762695
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5722357630729675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012345645809546113
        model: {}
        policy_loss: -0.0025620772503316402
        total_loss: -0.003541742218658328
        vf_explained_var: -0.003099203109741211
        vf_loss: 0.2747056484222412
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7337211966514587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022158175706863403
        model: {}
        policy_loss: -0.003188984002918005
        total_loss: -0.004302521701902151
        vf_explained_var: 0.0036534667015075684
        vf_loss: 1.7780958414077759
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0005087852478027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014591313665732741
        model: {}
        policy_loss: -0.002545855939388275
        total_loss: -0.004280851222574711
        vf_explained_var: 0.004537925124168396
        vf_loss: 0.25904184579849243
    load_time_ms: 18785.185
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 107099.445
    update_time_ms: 14.88
  iterations_since_restore: 130
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.202040816326527
    ram_util_percent: 9.312244897959186
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 12.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.98
    agent-1: 1.63
    agent-2: 4.19
    agent-3: 4.05
    agent-4: 4.85
    agent-5: 3.05
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.46308160236422
    mean_inference_ms: 13.680493913654523
    mean_processing_ms: 64.64143587879606
  time_since_restore: 18085.15978884697
  time_this_iter_s: 138.24253010749817
  time_total_s: 40595.443031311035
  timestamp: 1637238244
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    290 |          40595.4 | 27840000 |    20.75 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.89
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.65
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 155
    cleaning_beam_agent-0_mean: 75.74
    cleaning_beam_agent-0_min: 42
    cleaning_beam_agent-1_max: 339
    cleaning_beam_agent-1_mean: 215.08
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 66
    cleaning_beam_agent-2_mean: 20.73
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 40.18
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 166
    cleaning_beam_agent-4_mean: 60.77
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 15.19
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-26-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 19.21
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11722.723
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5573945045471191
        entropy_coeff: 0.0017600000137463212
        kl: 0.001591215142980218
        model: {}
        policy_loss: -0.0033954863902181387
        total_loss: -0.00434520049020648
        vf_explained_var: -0.0026655197143554688
        vf_loss: 0.3130054771900177
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5296779870986938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010023134527727962
        model: {}
        policy_loss: -0.0022553829476237297
        total_loss: -0.0031740935519337654
        vf_explained_var: 0.023978576064109802
        vf_loss: 0.1352197676897049
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48380184173583984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014976505190134048
        model: {}
        policy_loss: -0.003618780057877302
        total_loss: -0.004433310590684414
        vf_explained_var: -0.0049484968185424805
        vf_loss: 0.3696075677871704
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5752679705619812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011527568567544222
        model: {}
        policy_loss: -0.002618857193738222
        total_loss: -0.0036015817895531654
        vf_explained_var: 0.00453561544418335
        vf_loss: 0.2974695861339569
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7264580726623535
        entropy_coeff: 0.0017600000137463212
        kl: 0.001839888864196837
        model: {}
        policy_loss: -0.003702179528772831
        total_loss: -0.004945525899529457
        vf_explained_var: 0.0043624043464660645
        vf_loss: 0.3521910309791565
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.985860288143158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018051635706797242
        model: {}
        policy_loss: -0.0028229604940861464
        total_loss: -0.004537284839898348
        vf_explained_var: 0.000322490930557251
        vf_loss: 0.20793059468269348
    load_time_ms: 18916.946
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 107158.527
    update_time_ms: 14.65
  iterations_since_restore: 131
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.09651741293532
    ram_util_percent: 9.33681592039801
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 11.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.73
    agent-1: 1.03
    agent-2: 4.19
    agent-3: 3.78
    agent-4: 4.84
    agent-5: 2.64
  policy_reward_min:
    agent-0: -49.0
    agent-1: -47.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.461190336123412
    mean_inference_ms: 13.680332240886557
    mean_processing_ms: 64.64049425629308
  time_since_restore: 18225.05270934105
  time_this_iter_s: 139.8929204940796
  time_total_s: 40735.335951805115
  timestamp: 1637238385
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    291 |          40735.3 | 27936000 |    19.21 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.18
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 2.81
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 2.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 232
    cleaning_beam_agent-0_mean: 73.83
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 220.84
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 21.22
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 36.99
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 124
    cleaning_beam_agent-4_mean: 61.45
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 15.83
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-28-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 20.03
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11719.406
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5661620497703552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014502593548968434
        model: {}
        policy_loss: -0.003687107004225254
        total_loss: -0.004661976359784603
        vf_explained_var: 0.0007390975952148438
        vf_loss: 0.21575358510017395
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5232875347137451
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009042172459885478
        model: {}
        policy_loss: -0.0026114291977137327
        total_loss: -0.0035210037603974342
        vf_explained_var: 0.02843843400478363
        vf_loss: 0.1140991598367691
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4815155565738678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014047345612198114
        model: {}
        policy_loss: -0.0023696322459727526
        total_loss: -0.0030431742779910564
        vf_explained_var: -0.00023989379405975342
        vf_loss: 1.739281415939331
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5643633604049683
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010795557172968984
        model: {}
        policy_loss: -0.00201100273989141
        total_loss: -0.0028378195129334927
        vf_explained_var: 0.0022448450326919556
        vf_loss: 1.6646370887756348
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7295693755149841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013839553575962782
        model: {}
        policy_loss: -0.003946421667933464
        total_loss: -0.005190934985876083
        vf_explained_var: 0.006239548325538635
        vf_loss: 0.3953070640563965
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9895489811897278
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019814029801636934
        model: {}
        policy_loss: -0.0026304093189537525
        total_loss: -0.004346722736954689
        vf_explained_var: -0.0008918344974517822
        vf_loss: 0.25291508436203003
    load_time_ms: 18840.559
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 106946.215
    update_time_ms: 14.498
  iterations_since_restore: 132
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.790206185567005
    ram_util_percent: 9.328865979381444
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 11.0
    agent-4: 13.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.05
    agent-1: 1.71
    agent-2: 3.75
    agent-3: 3.42
    agent-4: 4.86
    agent-5: 3.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -49.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.458957615265582
    mean_inference_ms: 13.679485395939993
    mean_processing_ms: 64.63763920233116
  time_since_restore: 18361.642466545105
  time_this_iter_s: 136.5897572040558
  time_total_s: 40871.92570900917
  timestamp: 1637238522
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    292 |          40871.9 | 28032000 |    20.03 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.94
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.49
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 207
    cleaning_beam_agent-0_mean: 72.6
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 219.47
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 23.17
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 36.46
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 161
    cleaning_beam_agent-4_mean: 63.21
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 18.94
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-30-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 21.38
  episode_reward_min: -20.0
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11711.923
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5585862398147583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015917839482426643
        model: {}
        policy_loss: -0.0024582389742136
        total_loss: -0.0031512780115008354
        vf_explained_var: -9.091198444366455e-05
        vf_loss: 2.900686264038086
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5199925899505615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007584156701341271
        model: {}
        policy_loss: -0.0025855903513729572
        total_loss: -0.0034893453121185303
        vf_explained_var: 0.030939355492591858
        vf_loss: 0.11436533182859421
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48365071415901184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012879838468506932
        model: {}
        policy_loss: -0.003723185509443283
        total_loss: -0.004519980400800705
        vf_explained_var: -0.00509113073348999
        vf_loss: 0.544297456741333
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5628596544265747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015497924759984016
        model: {}
        policy_loss: -0.002712145447731018
        total_loss: -0.0036657098680734634
        vf_explained_var: -0.0020596981048583984
        vf_loss: 0.3707130253314972
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7313555479049683
        entropy_coeff: 0.0017600000137463212
        kl: 0.001794458949007094
        model: {}
        policy_loss: -0.0038767289370298386
        total_loss: -0.005118675529956818
        vf_explained_var: 0.009957432746887207
        vf_loss: 0.4523935914039612
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9953502416610718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020881351083517075
        model: {}
        policy_loss: -0.002842087298631668
        total_loss: -0.004570895340293646
        vf_explained_var: -0.0001516491174697876
        vf_loss: 0.23009350895881653
    load_time_ms: 18880.441
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 106839.578
    update_time_ms: 14.506
  iterations_since_restore: 133
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.3530612244898
    ram_util_percent: 9.34234693877551
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 1.96
    agent-1: 1.8
    agent-2: 5.13
    agent-3: 4.39
    agent-4: 5.05
    agent-5: 3.05
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.457271620139178
    mean_inference_ms: 13.679092267844325
    mean_processing_ms: 64.63780079632444
  time_since_restore: 18499.08477807045
  time_this_iter_s: 137.44231152534485
  time_total_s: 41009.368020534515
  timestamp: 1637238659
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    293 |          41009.4 | 28128000 |    21.38 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.72
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.72
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.78
    apples_agent-3_min: 0
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.37
    apples_agent-4_min: 0
    apples_agent-5_max: 31
    apples_agent-5_mean: 1.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 70.95
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 323
    cleaning_beam_agent-1_mean: 219.95
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 18.2
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 35.65
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 56.23
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 18.24
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-33-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 19.92
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11707.199
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5565118789672852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010174696799367666
        model: {}
        policy_loss: -0.0025852536782622337
        total_loss: -0.0034065754152834415
        vf_explained_var: 0.0010774284601211548
        vf_loss: 1.581416130065918
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5256453156471252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008794955792836845
        model: {}
        policy_loss: -0.0018579661846160889
        total_loss: -0.002641409868374467
        vf_explained_var: 0.0035613328218460083
        vf_loss: 1.416904330253601
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4736890494823456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010899174958467484
        model: {}
        policy_loss: -0.0031598738860338926
        total_loss: -0.00394822284579277
        vf_explained_var: -0.0017714351415634155
        vf_loss: 0.45348426699638367
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.573943018913269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009828091133385897
        model: {}
        policy_loss: -0.002238635439425707
        total_loss: -0.0032202843576669693
        vf_explained_var: 0.0011707842350006104
        vf_loss: 0.2849090099334717
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7296934127807617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013703032163903117
        model: {}
        policy_loss: -0.00394856184720993
        total_loss: -0.005191759672015905
        vf_explained_var: 0.015632405877113342
        vf_loss: 0.41064003109931946
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.988584041595459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018675082828849554
        model: {}
        policy_loss: -0.002634720876812935
        total_loss: -0.0043561942875385284
        vf_explained_var: 0.007855996489524841
        vf_loss: 0.18435996770858765
    load_time_ms: 18895.397
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 106864.184
    update_time_ms: 14.395
  iterations_since_restore: 134
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.774489795918367
    ram_util_percent: 9.332653061224491
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.6
    agent-1: 1.02
    agent-2: 4.68
    agent-3: 3.78
    agent-4: 5.12
    agent-5: 2.72
  policy_reward_min:
    agent-0: -48.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.454840058141112
    mean_inference_ms: 13.678858076568586
    mean_processing_ms: 64.63620067819376
  time_since_restore: 18636.264852762222
  time_this_iter_s: 137.18007469177246
  time_total_s: 41146.54809522629
  timestamp: 1637238796
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    294 |          41146.5 | 28224000 |    19.92 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.04
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 3.58
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 157
    cleaning_beam_agent-0_mean: 67.25
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 216.6
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 19.98
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 38.13
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 206
    cleaning_beam_agent-4_mean: 59.36
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 18.14
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-35-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 21.38
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11709.901
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5512198209762573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014924223069101572
        model: {}
        policy_loss: -0.0034296982921659946
        total_loss: -0.004375284072011709
        vf_explained_var: 0.0034954845905303955
        vf_loss: 0.24561795592308044
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5179484486579895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016788648208603263
        model: {}
        policy_loss: -0.0030223620124161243
        total_loss: -0.003922805655747652
        vf_explained_var: 0.01702691614627838
        vf_loss: 0.11144252121448517
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4767289161682129
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015303802210837603
        model: {}
        policy_loss: -0.0033622710034251213
        total_loss: -0.00415741465985775
        vf_explained_var: 0.002239033579826355
        vf_loss: 0.4390069842338562
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5380474328994751
        entropy_coeff: 0.0017600000137463212
        kl: 0.000874045304954052
        model: {}
        policy_loss: -0.0021301538217812777
        total_loss: -0.0030446164309978485
        vf_explained_var: 0.0025103241205215454
        vf_loss: 0.32501551508903503
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7310909628868103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014587590703740716
        model: {}
        policy_loss: -0.003731765551492572
        total_loss: -0.0049779461696743965
        vf_explained_var: 0.008936673402786255
        vf_loss: 0.40538641810417175
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9971599578857422
        entropy_coeff: 0.0017600000137463212
        kl: 0.002081072423607111
        model: {}
        policy_loss: -0.002917625941336155
        total_loss: -0.004651305265724659
        vf_explained_var: -0.006336033344268799
        vf_loss: 0.21322935819625854
    load_time_ms: 18887.963
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 106948.974
    update_time_ms: 14.532
  iterations_since_restore: 135
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.765816326530615
    ram_util_percent: 9.268877551020408
  pid: 6435
  policy_reward_max:
    agent-0: 9.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 10.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.08
    agent-1: 1.66
    agent-2: 4.88
    agent-3: 3.81
    agent-4: 4.93
    agent-5: 3.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.451579243356576
    mean_inference_ms: 13.678311772744104
    mean_processing_ms: 64.63409741285872
  time_since_restore: 18774.26015686989
  time_this_iter_s: 137.99530410766602
  time_total_s: 41284.543399333954
  timestamp: 1637238935
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    295 |          41284.5 | 28320000 |    21.38 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.21
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.85
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 2.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 70.4
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 212.83
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 63
    cleaning_beam_agent-2_mean: 17.36
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 34.58
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 124
    cleaning_beam_agent-4_mean: 52.71
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 14.71
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-37-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 19.89
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11706.051
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5634341835975647
        entropy_coeff: 0.0017600000137463212
        kl: 0.001554848742671311
        model: {}
        policy_loss: -0.002708585001528263
        total_loss: -0.0035436421167105436
        vf_explained_var: -0.00048007071018218994
        vf_loss: 1.56585693359375
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5261859893798828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007545797852799296
        model: {}
        policy_loss: -0.002154732821509242
        total_loss: -0.003063569311052561
        vf_explained_var: 0.01976191997528076
        vf_loss: 0.17252564430236816
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4684469699859619
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014183847233653069
        model: {}
        policy_loss: -0.003246316220611334
        total_loss: -0.004023770336061716
        vf_explained_var: 0.003639325499534607
        vf_loss: 0.47009289264678955
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5449533462524414
        entropy_coeff: 0.0017600000137463212
        kl: 0.001215698546729982
        model: {}
        policy_loss: -0.0024644413497298956
        total_loss: -0.003388732671737671
        vf_explained_var: 0.004335388541221619
        vf_loss: 0.34829968214035034
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.722992479801178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015683795791119337
        model: {}
        policy_loss: -0.0037925615906715393
        total_loss: -0.0050178454257547855
        vf_explained_var: 0.02096475660800934
        vf_loss: 0.47183141112327576
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9678512215614319
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020850838627666235
        model: {}
        policy_loss: -0.0025091515854001045
        total_loss: -0.004183844197541475
        vf_explained_var: 0.0015190839767456055
        vf_loss: 0.2872772514820099
    load_time_ms: 18849.846
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 107124.224
    update_time_ms: 14.527
  iterations_since_restore: 136
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.5253807106599
    ram_util_percent: 9.326395939086295
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 17.0
    agent-4: 26.0
    agent-5: 19.0
  policy_reward_mean:
    agent-0: 1.86
    agent-1: 1.3
    agent-2: 4.73
    agent-3: 4.26
    agent-4: 4.71
    agent-5: 3.03
  policy_reward_min:
    agent-0: -50.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.45056094582321
    mean_inference_ms: 13.678445227347888
    mean_processing_ms: 64.63969202644321
  time_since_restore: 18912.430756807327
  time_this_iter_s: 138.17059993743896
  time_total_s: 41422.71399927139
  timestamp: 1637239073
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    296 |          41422.7 | 28416000 |    19.89 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.21
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.72
    apples_agent-1_min: 0
    apples_agent-2_max: 27
    apples_agent-2_mean: 1.8
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 69.74
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 293
    cleaning_beam_agent-1_mean: 207.3
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 57
    cleaning_beam_agent-2_mean: 18.01
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 32.74
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 161
    cleaning_beam_agent-4_mean: 59.86
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 13.59
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-40-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 41.0
  episode_reward_mean: 20.1
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11707.885
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.560568630695343
        entropy_coeff: 0.0017600000137463212
        kl: 0.001356292050331831
        model: {}
        policy_loss: -0.003405096475034952
        total_loss: -0.00437082489952445
        vf_explained_var: 0.0049867182970047
        vf_loss: 0.20875242352485657
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5170469880104065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008872923208400607
        model: {}
        policy_loss: -0.0028044451028108597
        total_loss: -0.0037035243585705757
        vf_explained_var: 0.02282172441482544
        vf_loss: 0.10924932360649109
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4818495512008667
        entropy_coeff: 0.0017600000137463212
        kl: 0.001251020934432745
        model: {}
        policy_loss: -0.0016216908115893602
        total_loss: -0.0022994382306933403
        vf_explained_var: -7.34180212020874e-05
        vf_loss: 1.7031102180480957
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5459311604499817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016004540957510471
        model: {}
        policy_loss: -0.002788264537230134
        total_loss: -0.0037192318122833967
        vf_explained_var: -0.0012483000755310059
        vf_loss: 0.2987019717693329
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7275302410125732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012852312065660954
        model: {}
        policy_loss: -0.003648062702268362
        total_loss: -0.004884988069534302
        vf_explained_var: 0.007902830839157104
        vf_loss: 0.4352930188179016
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9847506284713745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013483620714396238
        model: {}
        policy_loss: -0.0025944351218640804
        total_loss: -0.004309004172682762
        vf_explained_var: 0.0022354423999786377
        vf_loss: 0.18591082096099854
    load_time_ms: 18767.169
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 106968.515
    update_time_ms: 14.856
  iterations_since_restore: 137
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.92974358974359
    ram_util_percent: 9.214871794871796
  pid: 6435
  policy_reward_max:
    agent-0: 8.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 17.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.97
    agent-1: 1.54
    agent-2: 3.6
    agent-3: 4.28
    agent-4: 4.99
    agent-5: 2.72
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.44867162025788
    mean_inference_ms: 13.678540345203267
    mean_processing_ms: 64.64126700655048
  time_since_restore: 19049.22833967209
  time_this_iter_s: 136.79758286476135
  time_total_s: 41559.511582136154
  timestamp: 1637239210
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    297 |          41559.5 | 28512000 |     20.1 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 1.86
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.77
    apples_agent-1_min: 0
    apples_agent-2_max: 5
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.72
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.95
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 67.22
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 216.18
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 19.59
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 37.43
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 137
    cleaning_beam_agent-4_mean: 57.82
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 14.62
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-42-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 19.35
  episode_reward_min: -77.0
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11708.77
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5541573762893677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014171748189255595
        model: {}
        policy_loss: -0.0031387964263558388
        total_loss: -0.003968716133385897
        vf_explained_var: 0.001935333013534546
        vf_loss: 1.453999400138855
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5175988674163818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005765727837570012
        model: {}
        policy_loss: -0.0022143875248730183
        total_loss: -0.003111407393589616
        vf_explained_var: 0.008740663528442383
        vf_loss: 0.13955356180667877
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4724336266517639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015065387124195695
        model: {}
        policy_loss: -0.003289113286882639
        total_loss: -0.004085524007678032
        vf_explained_var: 0.0037149935960769653
        vf_loss: 0.3507281541824341
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.560106635093689
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008268123492598534
        model: {}
        policy_loss: -0.0019035034347325563
        total_loss: -0.002725384198129177
        vf_explained_var: 0.001295626163482666
        vf_loss: 1.6390706300735474
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7217020988464355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015950427623465657
        model: {}
        policy_loss: -0.0038505014963448048
        total_loss: -0.005082760471850634
        vf_explained_var: 0.017653167247772217
        vf_loss: 0.3793509006500244
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9819896817207336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015369602479040623
        model: {}
        policy_loss: -0.002500905655324459
        total_loss: -0.0042060548439621925
        vf_explained_var: 0.0015172511339187622
        vf_loss: 0.23150862753391266
    load_time_ms: 18747.699
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 106959.437
    update_time_ms: 14.788
  iterations_since_restore: 138
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.723589743589745
    ram_util_percent: 9.328717948717948
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 14.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.44
    agent-1: 1.77
    agent-2: 4.58
    agent-3: 3.32
    agent-4: 4.49
    agent-5: 2.75
  policy_reward_min:
    agent-0: -44.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.44569561078062
    mean_inference_ms: 13.67876846253597
    mean_processing_ms: 64.63762966291081
  time_since_restore: 19185.811214208603
  time_this_iter_s: 136.58287453651428
  time_total_s: 41696.09445667267
  timestamp: 1637239346
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    298 |          41696.1 | 28608000 |    19.35 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 73.94
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 429
    cleaning_beam_agent-1_mean: 212.82
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 67
    cleaning_beam_agent-2_mean: 17.46
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 34.74
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 164
    cleaning_beam_agent-4_mean: 62.77
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 14.36
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 16
    fire_beam_agent-5_mean: 0.17
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-44-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 20.74
  episode_reward_min: -5.0
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11709.455
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5642725825309753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013354324037209153
        model: {}
        policy_loss: -0.0034793152008205652
        total_loss: -0.004449557978659868
        vf_explained_var: 0.00197756290435791
        vf_loss: 0.22878098487854004
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5140770077705383
        entropy_coeff: 0.0017600000137463212
        kl: 0.001438237028196454
        model: {}
        policy_loss: -0.0024754232726991177
        total_loss: -0.0033695220481604338
        vf_explained_var: 0.029279515147209167
        vf_loss: 0.10682474821805954
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4544799327850342
        entropy_coeff: 0.0017600000137463212
        kl: 0.001705320319160819
        model: {}
        policy_loss: -0.003646553959697485
        total_loss: -0.004406913183629513
        vf_explained_var: -0.001650497317314148
        vf_loss: 0.3952252268791199
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5494918823242188
        entropy_coeff: 0.0017600000137463212
        kl: 0.001109433127567172
        model: {}
        policy_loss: -0.002736393827944994
        total_loss: -0.0036667047534137964
        vf_explained_var: -0.004117637872695923
        vf_loss: 0.3679552972316742
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7299535870552063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019265477312728763
        model: {}
        policy_loss: -0.003921456169337034
        total_loss: -0.005168210249394178
        vf_explained_var: 0.009090974926948547
        vf_loss: 0.37962454557418823
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9739336967468262
        entropy_coeff: 0.0017600000137463212
        kl: 0.001225497922860086
        model: {}
        policy_loss: -0.002364866901189089
        total_loss: -0.004047195892781019
        vf_explained_var: 0.011352941393852234
        vf_loss: 0.317932665348053
    load_time_ms: 18641.352
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 107024.089
    update_time_ms: 14.907
  iterations_since_restore: 139
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.882474226804124
    ram_util_percent: 9.280927835051546
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 10.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.19
    agent-1: 1.67
    agent-2: 4.5
    agent-3: 4.04
    agent-4: 4.57
    agent-5: 2.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -16.0
  sampler_perf:
    mean_env_wait_ms: 25.442710077357578
    mean_inference_ms: 13.678148032530526
    mean_processing_ms: 64.63746034395054
  time_since_restore: 19321.69814324379
  time_this_iter_s: 135.88692903518677
  time_total_s: 41831.981385707855
  timestamp: 1637239483
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 16.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    299 |            41832 | 28704000 |    20.74 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 0.85
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 2.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 67.75
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 306
    cleaning_beam_agent-1_mean: 199.52
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 18.37
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 36.29
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 128
    cleaning_beam_agent-4_mean: 58.04
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 13.52
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-47-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 81.0
  episode_reward_mean: 22.52
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11695.135
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5569870471954346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016037767054513097
        model: {}
        policy_loss: -0.0033746035769581795
        total_loss: -0.004329789895564318
        vf_explained_var: 0.0011866092681884766
        vf_loss: 0.2510823607444763
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5022880434989929
        entropy_coeff: 0.0017600000137463212
        kl: 0.00155154790263623
        model: {}
        policy_loss: -0.002510577207431197
        total_loss: -0.0033798927906900644
        vf_explained_var: 0.019959107041358948
        vf_loss: 0.14709606766700745
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46701207756996155
        entropy_coeff: 0.0017600000137463212
        kl: 0.000954410235863179
        model: {}
        policy_loss: -0.0031633281614631414
        total_loss: -0.0039338963106274605
        vf_explained_var: -0.0038416683673858643
        vf_loss: 0.5137330889701843
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5542261600494385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009355113143101335
        model: {}
        policy_loss: -0.0023465934209525585
        total_loss: -0.0032843369990587234
        vf_explained_var: -0.0017086565494537354
        vf_loss: 0.3769587278366089
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7300477027893066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022123975213617086
        model: {}
        policy_loss: -0.004191762767732143
        total_loss: -0.005433044862002134
        vf_explained_var: 0.009019047021865845
        vf_loss: 0.4360139071941376
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9682136178016663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017480086535215378
        model: {}
        policy_loss: -0.002580274362117052
        total_loss: -0.004259246867150068
        vf_explained_var: -0.004145443439483643
        vf_loss: 0.2508222162723541
    load_time_ms: 18580.603
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 106987.321
    update_time_ms: 14.843
  iterations_since_restore: 140
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.50923076923077
    ram_util_percent: 9.319487179487181
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 12.0
    agent-2: 25.0
    agent-3: 18.0
    agent-4: 14.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.35
    agent-1: 1.74
    agent-2: 5.08
    agent-3: 4.19
    agent-4: 4.95
    agent-5: 3.21
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.44033072157275
    mean_inference_ms: 13.677400785445341
    mean_processing_ms: 64.63524575058524
  time_since_restore: 19458.820878505707
  time_this_iter_s: 137.12273526191711
  time_total_s: 41969.10412096977
  timestamp: 1637239620
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 16.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    300 |          41969.1 | 28800000 |    22.52 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.21
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.14
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 133
    cleaning_beam_agent-0_mean: 66.93
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 201.57
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 16.38
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 30.63
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 148
    cleaning_beam_agent-4_mean: 57.5
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 12.91
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-49-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 22.34
  episode_reward_min: -26.0
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11693.189
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5655620098114014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012395993107929826
        model: {}
        policy_loss: -0.003240462625399232
        total_loss: -0.0042077768594026566
        vf_explained_var: 0.004888415336608887
        vf_loss: 0.28076693415641785
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.493185818195343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008543374133296311
        model: {}
        policy_loss: -0.0023382483050227165
        total_loss: -0.003190543968230486
        vf_explained_var: 0.007989868521690369
        vf_loss: 0.1571042835712433
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45968085527420044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012213289737701416
        model: {}
        policy_loss: -0.003157853614538908
        total_loss: -0.0039183758199214935
        vf_explained_var: 0.003150254487991333
        vf_loss: 0.48514947295188904
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5608313083648682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007855907315388322
        model: {}
        policy_loss: -0.0017544662114232779
        total_loss: -0.0025861235335469246
        vf_explained_var: 0.00019603967666625977
        vf_loss: 1.554044246673584
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7119780778884888
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013288110494613647
        model: {}
        policy_loss: -0.0037152436561882496
        total_loss: -0.004917541053146124
        vf_explained_var: 0.019507944583892822
        vf_loss: 0.5078085660934448
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9572785496711731
        entropy_coeff: 0.0017600000137463212
        kl: 0.001649559591896832
        model: {}
        policy_loss: -0.002898847684264183
        total_loss: -0.004559685476124287
        vf_explained_var: 0.006953150033950806
        vf_loss: 0.23971697688102722
    load_time_ms: 18431.001
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 106868.873
    update_time_ms: 15.011
  iterations_since_restore: 141
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.975126903553303
    ram_util_percent: 9.333502538071068
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.31
    agent-1: 1.95
    agent-2: 5.17
    agent-3: 3.55
    agent-4: 5.3
    agent-5: 3.06
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.43758129546885
    mean_inference_ms: 13.677469096931855
    mean_processing_ms: 64.63269949074186
  time_since_restore: 19596.028158187866
  time_this_iter_s: 137.20727968215942
  time_total_s: 42106.31140065193
  timestamp: 1637239758
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    301 |          42106.3 | 28896000 |    22.34 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.44
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 3.16
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 2.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 65.96
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 288
    cleaning_beam_agent-1_mean: 192.19
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 18.37
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 30.68
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 144
    cleaning_beam_agent-4_mean: 55.44
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 11.82
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-51-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 22.56
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11697.873
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5605127215385437
        entropy_coeff: 0.0017600000137463212
        kl: 0.001298324903473258
        model: {}
        policy_loss: -0.0032487649004906416
        total_loss: -0.004201171454042196
        vf_explained_var: 0.0002825111150741577
        vf_loss: 0.3409717381000519
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49163880944252014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009375588269904256
        model: {}
        policy_loss: -0.0024723238311707973
        total_loss: -0.0033249834086745977
        vf_explained_var: 0.016436412930488586
        vf_loss: 0.12623156607151031
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.470887690782547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014419968938454986
        model: {}
        policy_loss: -0.003499210812151432
        total_loss: -0.004279373679310083
        vf_explained_var: 0.006097733974456787
        vf_loss: 0.4859764873981476
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5707923769950867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009225857793353498
        model: {}
        policy_loss: -0.0022413944825530052
        total_loss: -0.0032078507356345654
        vf_explained_var: -0.000413745641708374
        vf_loss: 0.38136130571365356
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7242087125778198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024550845846533775
        model: {}
        policy_loss: -0.0038278065621852875
        total_loss: -0.005058346316218376
        vf_explained_var: 0.017102763056755066
        vf_loss: 0.44068413972854614
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.942765474319458
        entropy_coeff: 0.0017600000137463212
        kl: 0.002044191351160407
        model: {}
        policy_loss: -0.002978978678584099
        total_loss: -0.004616227932274342
        vf_explained_var: 0.000379905104637146
        vf_loss: 0.22015419602394104
    load_time_ms: 18457.104
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 107101.887
    update_time_ms: 15.086
  iterations_since_restore: 142
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 36.594472361809046
    ram_util_percent: 15.287437185929647
  pid: 6435
  policy_reward_max:
    agent-0: 20.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.85
    agent-1: 1.67
    agent-2: 5.08
    agent-3: 4.44
    agent-4: 5.31
    agent-5: 3.21
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.436803790680372
    mean_inference_ms: 13.678471595229615
    mean_processing_ms: 64.64242483390713
  time_since_restore: 19735.2594268322
  time_this_iter_s: 139.23126864433289
  time_total_s: 42245.542669296265
  timestamp: 1637239897
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    302 |          42245.5 | 28992000 |    22.56 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.58
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 2.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 67.63
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 323
    cleaning_beam_agent-1_mean: 206.25
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 89
    cleaning_beam_agent-2_mean: 15.06
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 33.42
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 52.46
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 13.2
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-53-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 23.33
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11691.015
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5571961998939514
        entropy_coeff: 0.0017600000137463212
        kl: 0.001418194267898798
        model: {}
        policy_loss: -0.0033827736042439938
        total_loss: -0.004331529140472412
        vf_explained_var: 0.0020598769187927246
        vf_loss: 0.3191058933734894
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4958709478378296
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010192443151026964
        model: {}
        policy_loss: -0.0026002381928265095
        total_loss: -0.0034595332108438015
        vf_explained_var: 0.016157209873199463
        vf_loss: 0.1344064176082611
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4593326151371002
        entropy_coeff: 0.0017600000137463212
        kl: 0.001481743180193007
        model: {}
        policy_loss: -0.0035082935355603695
        total_loss: -0.004273759201169014
        vf_explained_var: -0.002847433090209961
        vf_loss: 0.4296189248561859
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5553762316703796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008050122996792197
        model: {}
        policy_loss: -0.002552144229412079
        total_loss: -0.0034887250512838364
        vf_explained_var: 0.0009875744581222534
        vf_loss: 0.40878361463546753
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7134483456611633
        entropy_coeff: 0.0017600000137463212
        kl: 0.001554227201268077
        model: {}
        policy_loss: -0.0025440664030611515
        total_loss: -0.0036230222322046757
        vf_explained_var: 0.011127308011054993
        vf_loss: 1.767129898071289
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9374603033065796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020464241970330477
        model: {}
        policy_loss: -0.002911367453634739
        total_loss: -0.004534417763352394
        vf_explained_var: 0.003999039530754089
        vf_loss: 0.26882392168045044
    load_time_ms: 18494.223
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 107379.645
    update_time_ms: 15.89
  iterations_since_restore: 143
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.96200000000001
    ram_util_percent: 17.469
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.3
    agent-1: 1.91
    agent-2: 5.19
    agent-3: 4.6
    agent-4: 4.66
    agent-5: 3.67
  policy_reward_min:
    agent-0: -46.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 25.43724141414142
    mean_inference_ms: 13.679548284447437
    mean_processing_ms: 64.65361998930418
  time_since_restore: 19875.847059726715
  time_this_iter_s: 140.587632894516
  time_total_s: 42386.13030219078
  timestamp: 1637240038
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    303 |          42386.1 | 29088000 |    23.33 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.74
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.35
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.07
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 2.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 65.72
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 292
    cleaning_beam_agent-1_mean: 203.56
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 142
    cleaning_beam_agent-2_mean: 16.72
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 32.49
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 53.04
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 14.2
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-56-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 23.82
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11692.833
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5576236248016357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011017295764759183
        model: {}
        policy_loss: -0.003471804317086935
        total_loss: -0.004428306594491005
        vf_explained_var: -0.0011382997035980225
        vf_loss: 0.24915438890457153
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4820878505706787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011308734538033605
        model: {}
        policy_loss: -0.002826268784701824
        total_loss: -0.003662086557596922
        vf_explained_var: 0.021520540118217468
        vf_loss: 0.12656190991401672
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45759469270706177
        entropy_coeff: 0.0017600000137463212
        kl: 0.001101647736504674
        model: {}
        policy_loss: -0.0030612442642450333
        total_loss: -0.003818616271018982
        vf_explained_var: 0.0052254050970077515
        vf_loss: 0.47994959354400635
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5428029894828796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016328950878232718
        model: {}
        policy_loss: -0.002828848548233509
        total_loss: -0.003741813125088811
        vf_explained_var: -0.0006459951400756836
        vf_loss: 0.4236672520637512
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7339780926704407
        entropy_coeff: 0.0017600000137463212
        kl: 0.001917051151394844
        model: {}
        policy_loss: -0.0041238670237362385
        total_loss: -0.005375380162149668
        vf_explained_var: 0.009690284729003906
        vf_loss: 0.4028690755367279
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9549449682235718
        entropy_coeff: 0.0017600000137463212
        kl: 0.002292192541062832
        model: {}
        policy_loss: -0.003000544384121895
        total_loss: -0.004653986543416977
        vf_explained_var: 0.0039633214473724365
        vf_loss: 0.2726389169692993
    load_time_ms: 18631.3
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 107567.955
    update_time_ms: 16.071
  iterations_since_restore: 144
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.51
    ram_util_percent: 17.721500000000002
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 3.36
    agent-1: 1.87
    agent-2: 5.24
    agent-3: 4.87
    agent-4: 5.1
    agent-5: 3.38
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.438533941192674
    mean_inference_ms: 13.680684880803177
    mean_processing_ms: 64.66464495607886
  time_since_restore: 20016.258252859116
  time_this_iter_s: 140.4111931324005
  time_total_s: 42526.54149532318
  timestamp: 1637240178
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    304 |          42526.5 | 29184000 |    23.82 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 2.49
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.74
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 69.41
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 205.73
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 16.81
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 29.76
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 114
    cleaning_beam_agent-4_mean: 52.03
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 15.45
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-58-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 21.8
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11686.407
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5664815306663513
        entropy_coeff: 0.0017600000137463212
        kl: 0.001334509113803506
        model: {}
        policy_loss: -0.0036810673773288727
        total_loss: -0.004655950702726841
        vf_explained_var: 0.0030044615268707275
        vf_loss: 0.22125574946403503
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4949178993701935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012428814079612494
        model: {}
        policy_loss: -0.002706377301365137
        total_loss: -0.0035680769942700863
        vf_explained_var: 0.01792144775390625
        vf_loss: 0.09357842803001404
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4686795473098755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013189890887588263
        model: {}
        policy_loss: -0.0033897990360856056
        total_loss: -0.004170697182416916
        vf_explained_var: 0.004762589931488037
        vf_loss: 0.43977415561676025
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.54291832447052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009152957936748862
        model: {}
        policy_loss: -0.002497929148375988
        total_loss: -0.0034226099960505962
        vf_explained_var: -0.00146445631980896
        vf_loss: 0.30852818489074707
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7314969897270203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019873592536896467
        model: {}
        policy_loss: -0.003934542648494244
        total_loss: -0.005180585663765669
        vf_explained_var: 0.008223623037338257
        vf_loss: 0.4139205515384674
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9733188152313232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012381988344714046
        model: {}
        policy_loss: -0.002586460905149579
        total_loss: -0.004280258901417255
        vf_explained_var: -0.003983914852142334
        vf_loss: 0.19245103001594543
    load_time_ms: 18585.033
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 107746.03
    update_time_ms: 17.374
  iterations_since_restore: 145
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.76616161616162
    ram_util_percent: 17.97525252525253
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.12
    agent-1: 1.77
    agent-2: 4.86
    agent-3: 4.1
    agent-4: 5.13
    agent-5: 2.82
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.439385002931303
    mean_inference_ms: 13.681797995072094
    mean_processing_ms: 64.6744811180076
  time_since_restore: 20155.52522110939
  time_this_iter_s: 139.26696825027466
  time_total_s: 42665.808463573456
  timestamp: 1637240318
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 33.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    305 |          42665.8 | 29280000 |     21.8 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.01
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.08
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.55
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.6
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 2.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 166
    cleaning_beam_agent-0_mean: 64.28
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 307
    cleaning_beam_agent-1_mean: 208.91
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 16.82
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 33.12
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 145
    cleaning_beam_agent-4_mean: 54.91
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 12.79
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-00-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 22.31
  episode_reward_min: -79.0
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11688.576
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5508373975753784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014454940101131797
        model: {}
        policy_loss: -0.002577804494649172
        total_loss: -0.0034112632274627686
        vf_explained_var: -0.0004916489124298096
        vf_loss: 1.360148310661316
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4905250072479248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010040744673460722
        model: {}
        policy_loss: -0.0012842239812016487
        total_loss: -0.0020060641691088676
        vf_explained_var: 0.0023379921913146973
        vf_loss: 1.4148211479187012
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46646496653556824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010081077925860882
        model: {}
        policy_loss: -0.0028554522432386875
        total_loss: -0.003620675764977932
        vf_explained_var: 0.0050345659255981445
        vf_loss: 0.5575576424598694
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5671871900558472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010964880930259824
        model: {}
        policy_loss: -0.0023330007679760456
        total_loss: -0.003291425760835409
        vf_explained_var: 0.006665229797363281
        vf_loss: 0.39825156331062317
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7265580892562866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017787427641451359
        model: {}
        policy_loss: -0.003912378568202257
        total_loss: -0.005145910661667585
        vf_explained_var: 0.020326167345046997
        vf_loss: 0.4520875811576843
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9470721483230591
        entropy_coeff: 0.0017600000137463212
        kl: 0.001762621570378542
        model: {}
        policy_loss: -0.0030304337851703167
        total_loss: -0.004668900277465582
        vf_explained_var: -0.0022933781147003174
        vf_loss: 0.2838096022605896
    load_time_ms: 18612.085
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 107891.265
    update_time_ms: 17.187
  iterations_since_restore: 146
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.163000000000004
    ram_util_percent: 18.271
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 22.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.33
    agent-1: 1.26
    agent-2: 5.11
    agent-3: 4.66
    agent-4: 5.65
    agent-5: 3.3
  policy_reward_min:
    agent-0: -48.0
    agent-1: -50.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.4408493364995
    mean_inference_ms: 13.683129540156521
    mean_processing_ms: 64.68665952133047
  time_since_restore: 20295.389716625214
  time_this_iter_s: 139.86449551582336
  time_total_s: 42805.67295908928
  timestamp: 1637240458
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 33.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    306 |          42805.7 | 29376000 |    22.31 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.38
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 130
    cleaning_beam_agent-0_mean: 58.43
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 207.62
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 62
    cleaning_beam_agent-2_mean: 17.63
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 33.84
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 138
    cleaning_beam_agent-4_mean: 48.95
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 12.21
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-03-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 22.14
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11695.12
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5459034442901611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013148417929187417
        model: {}
        policy_loss: -0.0035837425384670496
        total_loss: -0.004517577588558197
        vf_explained_var: 0.005500048398971558
        vf_loss: 0.26956507563591003
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4783652722835541
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011961809359490871
        model: {}
        policy_loss: -0.0024552084505558014
        total_loss: -0.003283620346337557
        vf_explained_var: 0.025550976395606995
        vf_loss: 0.13511809706687927
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4658372700214386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017024895641952753
        model: {}
        policy_loss: -0.002376333810389042
        total_loss: -0.003081955946981907
        vf_explained_var: 0.003829285502433777
        vf_loss: 1.1425025463104248
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5318744778633118
        entropy_coeff: 0.0017600000137463212
        kl: 0.000950557878240943
        model: {}
        policy_loss: -0.0024002022109925747
        total_loss: -0.003297247691079974
        vf_explained_var: -0.0013664662837982178
        vf_loss: 0.39052319526672363
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7233960032463074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012573194690048695
        model: {}
        policy_loss: -0.0037605734542012215
        total_loss: -0.004993134178221226
        vf_explained_var: 0.012752830982208252
        vf_loss: 0.40616193413734436
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9513343572616577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012457225238904357
        model: {}
        policy_loss: -0.002603472676128149
        total_loss: -0.004256227053701878
        vf_explained_var: 0.002750292420387268
        vf_loss: 0.2159353643655777
    load_time_ms: 18636.741
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 108035.809
    update_time_ms: 16.84
  iterations_since_restore: 147
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.12588832487308
    ram_util_percent: 18.53553299492386
  pid: 6435
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 13.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.63
    agent-1: 1.93
    agent-2: 3.75
    agent-3: 4.64
    agent-4: 5.08
    agent-5: 3.11
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -43.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.441413585678006
    mean_inference_ms: 13.684003490972545
    mean_processing_ms: 64.69706120183794
  time_since_restore: 20433.972591400146
  time_this_iter_s: 138.58287477493286
  time_total_s: 42944.25583386421
  timestamp: 1637240597
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    307 |          42944.3 | 29472000 |    22.14 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 2.54
    apples_agent-2_min: 0
    apples_agent-3_max: 42
    apples_agent-3_mean: 3.58
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 43
    apples_agent-5_mean: 2.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 64.32
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 320
    cleaning_beam_agent-1_mean: 208.64
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 16.25
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 33.48
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 108
    cleaning_beam_agent-4_mean: 47.23
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 11.78
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-05-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 22.3
  episode_reward_min: -26.0
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11711.12
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5473220944404602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015348027227446437
        model: {}
        policy_loss: -0.0031551909632980824
        total_loss: -0.004086542408913374
        vf_explained_var: 0.0030135363340377808
        vf_loss: 0.3193650543689728
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4902186095714569
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007473195437341928
        model: {}
        policy_loss: -0.002499855123460293
        total_loss: -0.0033505363389849663
        vf_explained_var: 0.02120228111743927
        vf_loss: 0.1210680902004242
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47157201170921326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010499826166778803
        model: {}
        policy_loss: -0.003053201362490654
        total_loss: -0.003836674615740776
        vf_explained_var: -0.00013750791549682617
        vf_loss: 0.4649336338043213
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5485885143280029
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008233949774876237
        model: {}
        policy_loss: -0.00213622092269361
        total_loss: -0.0030684347730129957
        vf_explained_var: -0.0021245330572128296
        vf_loss: 0.33298197388648987
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7155966758728027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014344756491482258
        model: {}
        policy_loss: -0.003769560717046261
        total_loss: -0.004986678250133991
        vf_explained_var: 0.021114051342010498
        vf_loss: 0.42336714267730713
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9460018873214722
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012542877811938524
        model: {}
        policy_loss: -0.0022059069015085697
        total_loss: -0.0037148594856262207
        vf_explained_var: 0.0020507127046585083
        vf_loss: 1.560079574584961
    load_time_ms: 18590.484
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 108317.428
    update_time_ms: 16.906
  iterations_since_restore: 148
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.96381909547739
    ram_util_percent: 18.713065326633163
  pid: 6435
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.26
    agent-1: 1.93
    agent-2: 4.71
    agent-3: 4.28
    agent-4: 5.26
    agent-5: 2.86
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 25.440589352835165
    mean_inference_ms: 13.684873312980228
    mean_processing_ms: 64.70397430817214
  time_since_restore: 20573.06657600403
  time_this_iter_s: 139.09398460388184
  time_total_s: 43083.349818468094
  timestamp: 1637240736
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 34.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    308 |          43083.3 | 29568000 |     22.3 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 1.27
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.55
    apples_agent-2_min: 0
    apples_agent-3_max: 44
    apples_agent-3_mean: 3.58
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 229
    cleaning_beam_agent-0_mean: 62.91
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 211.66
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 17.03
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 35.62
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 48.34
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 12.02
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-07-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 22.99
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11704.292
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5476776361465454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013612564653158188
        model: {}
        policy_loss: -0.0034388815984129906
        total_loss: -0.004376807250082493
        vf_explained_var: 0.0038897544145584106
        vf_loss: 0.25985944271087646
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47724202275276184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006292976904660463
        model: {}
        policy_loss: -0.002337805461138487
        total_loss: -0.0031623137183487415
        vf_explained_var: 0.014740511775016785
        vf_loss: 0.15438896417617798
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4703945815563202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014456831850111485
        model: {}
        policy_loss: -0.00308249332010746
        total_loss: -0.0038587404415011406
        vf_explained_var: 0.001942947506904602
        vf_loss: 0.5164870023727417
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5458173751831055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010911194840446115
        model: {}
        policy_loss: -0.002477148547768593
        total_loss: -0.003402179107069969
        vf_explained_var: 0.000199928879737854
        vf_loss: 0.35606327652931213
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7134844064712524
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009856019169092178
        model: {}
        policy_loss: -0.0020887863356620073
        total_loss: -0.0032015463802963495
        vf_explained_var: 0.009756997227668762
        vf_loss: 1.4297187328338623
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9606207609176636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023911616299301386
        model: {}
        policy_loss: -0.003012660890817642
        total_loss: -0.00468097161501646
        vf_explained_var: 0.0003486126661300659
        vf_loss: 0.22382165491580963
    load_time_ms: 18666.764
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 108573.231
    update_time_ms: 17.016
  iterations_since_restore: 149
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.7010101010101
    ram_util_percent: 18.980808080808078
  pid: 6435
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 15.0
    agent-4: 12.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.44
    agent-1: 2.09
    agent-2: 5.41
    agent-3: 4.5
    agent-4: 4.61
    agent-5: 2.94
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -43.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.44208106728204
    mean_inference_ms: 13.686119065140874
    mean_processing_ms: 64.71525539255455
  time_since_restore: 20712.305943727493
  time_this_iter_s: 139.23936772346497
  time_total_s: 43222.58918619156
  timestamp: 1637240875
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    309 |          43222.6 | 29664000 |    22.99 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.83
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.21
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 2.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 103
    cleaning_beam_agent-0_mean: 58.27
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 320
    cleaning_beam_agent-1_mean: 203.9
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 14.64
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 34.65
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 143
    cleaning_beam_agent-4_mean: 46.52
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 12.21
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-10-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 24.4
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11717.285
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.538652777671814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013117676135152578
        model: {}
        policy_loss: -0.0033346377313137054
        total_loss: -0.004255059640854597
        vf_explained_var: -0.0017828047275543213
        vf_loss: 0.2760557532310486
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47074928879737854
        entropy_coeff: 0.0017600000137463212
        kl: 0.001139600994065404
        model: {}
        policy_loss: -0.002436388283967972
        total_loss: -0.003248568158596754
        vf_explained_var: 0.01513063907623291
        vf_loss: 0.16341006755828857
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4611794352531433
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016529983840882778
        model: {}
        policy_loss: -0.003247342072427273
        total_loss: -0.00401470810174942
        vf_explained_var: -0.00017401576042175293
        vf_loss: 0.44308555126190186
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5423189401626587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010703864973038435
        model: {}
        policy_loss: -0.00241920817643404
        total_loss: -0.003336744150146842
        vf_explained_var: -0.0006023943424224854
        vf_loss: 0.369451105594635
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7109683156013489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019454210996627808
        model: {}
        policy_loss: -0.0037332233041524887
        total_loss: -0.004919043742120266
        vf_explained_var: 0.01953822374343872
        vf_loss: 0.6548579335212708
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9670147895812988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017346404492855072
        model: {}
        policy_loss: -0.0027951602824032307
        total_loss: -0.004470831714570522
        vf_explained_var: -0.0024752020835876465
        vf_loss: 0.26275089383125305
    load_time_ms: 18691.853
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 108891.172
    update_time_ms: 17.096
  iterations_since_restore: 150
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.426865671641785
    ram_util_percent: 19.119402985074625
  pid: 6435
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 14.0
    agent-3: 17.0
    agent-4: 22.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.59
    agent-1: 2.06
    agent-2: 5.29
    agent-3: 4.2
    agent-4: 5.99
    agent-5: 3.27
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.443552818680732
    mean_inference_ms: 13.688170964880968
    mean_processing_ms: 64.7284204390732
  time_since_restore: 20852.991595745087
  time_this_iter_s: 140.68565201759338
  time_total_s: 43363.27483820915
  timestamp: 1637241016
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 35.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    310 |          43363.3 | 29760000 |     24.4 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.78
    apples_agent-1_min: 0
    apples_agent-2_max: 30
    apples_agent-2_mean: 2.3
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.24
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 2.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 118
    cleaning_beam_agent-0_mean: 59.32
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 200.29
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 15.73
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 40.38
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 100
    cleaning_beam_agent-4_mean: 46.77
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 13.41
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-12-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 24.12
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11720.347
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5391697883605957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013922553043812513
        model: {}
        policy_loss: -0.0032528371084481478
        total_loss: -0.00417195912450552
        vf_explained_var: -0.0006972402334213257
        vf_loss: 0.2981642186641693
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4824467599391937
        entropy_coeff: 0.0017600000137463212
        kl: 0.001453233533538878
        model: {}
        policy_loss: -0.0028016953729093075
        total_loss: -0.0036392617039382458
        vf_explained_var: 0.019179493188858032
        vf_loss: 0.11539191752672195
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46415168046951294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011366932885721326
        model: {}
        policy_loss: -0.0032471101731061935
        total_loss: -0.0040158461779356
        vf_explained_var: -0.00416332483291626
        vf_loss: 0.4817042350769043
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5699732899665833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012708381982520223
        model: {}
        policy_loss: -0.0022718864493072033
        total_loss: -0.003234309144318104
        vf_explained_var: 0.0018117725849151611
        vf_loss: 0.4073086977005005
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7098496556282043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015231042634695768
        model: {}
        policy_loss: -0.0037094734143465757
        total_loss: -0.004914509132504463
        vf_explained_var: 0.007023453712463379
        vf_loss: 0.44300270080566406
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9579606652259827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018950375961139798
        model: {}
        policy_loss: -0.003026185557246208
        total_loss: -0.0046846214681863785
        vf_explained_var: 0.000389784574508667
        vf_loss: 0.2757647931575775
    load_time_ms: 18671.387
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 109136.598
    update_time_ms: 17.245
  iterations_since_restore: 151
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 47.048
    ram_util_percent: 19.2355
  pid: 6435
  policy_reward_max:
    agent-0: 12.0
    agent-1: 6.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.77
    agent-1: 1.82
    agent-2: 5.45
    agent-3: 4.48
    agent-4: 5.13
    agent-5: 3.47
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.443801662311206
    mean_inference_ms: 13.689692472425229
    mean_processing_ms: 64.73875845042534
  time_since_restore: 20992.49489402771
  time_this_iter_s: 139.5032982826233
  time_total_s: 43502.778136491776
  timestamp: 1637241156
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    311 |          43502.8 | 29856000 |    24.12 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.84
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.18
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 122
    cleaning_beam_agent-0_mean: 57.58
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 428
    cleaning_beam_agent-1_mean: 211.57
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 14.9
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 39.05
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 121
    cleaning_beam_agent-4_mean: 44.17
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 13.87
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-15-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 22.17
  episode_reward_min: -78.0
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11718.974
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5436297655105591
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011996662942692637
        model: {}
        policy_loss: -0.00332575011998415
        total_loss: -0.00425057765096426
        vf_explained_var: 0.00677800178527832
        vf_loss: 0.319607675075531
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46987029910087585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008822624804452062
        model: {}
        policy_loss: -0.0024404218420386314
        total_loss: -0.003254768205806613
        vf_explained_var: 0.013928055763244629
        vf_loss: 0.12626026570796967
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46025508642196655
        entropy_coeff: 0.0017600000137463212
        kl: 0.001710500568151474
        model: {}
        policy_loss: -0.003401021007448435
        total_loss: -0.004159476142376661
        vf_explained_var: 0.0046520233154296875
        vf_loss: 0.5159367918968201
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5511415600776672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013474421575665474
        model: {}
        policy_loss: -0.002004837617278099
        total_loss: -0.002329731360077858
        vf_explained_var: -0.0020064115524291992
        vf_loss: 6.4511542320251465
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.705869197845459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011890598107129335
        model: {}
        policy_loss: -0.0025122654624283314
        total_loss: -0.0036032325588166714
        vf_explained_var: 0.008753776550292969
        vf_loss: 1.5136477947235107
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9492003917694092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019571050070226192
        model: {}
        policy_loss: -0.003029654733836651
        total_loss: -0.004676162730902433
        vf_explained_var: 0.0063680559396743774
        vf_loss: 0.24086853861808777
    load_time_ms: 20509.474
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 109223.154
    update_time_ms: 17.341
  iterations_since_restore: 152
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 44.987111111111105
    ram_util_percent: 19.768444444444444
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 20.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.31
    agent-1: 1.92
    agent-2: 5.67
    agent-3: 2.46
    agent-4: 5.41
    agent-5: 3.4
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -95.0
    agent-4: -45.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.444166721499595
    mean_inference_ms: 13.690671068234167
    mean_processing_ms: 64.7475445467275
  time_since_restore: 21151.033648490906
  time_this_iter_s: 158.5387544631958
  time_total_s: 43661.31689095497
  timestamp: 1637241315
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.26 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc             |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-----------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.3:6435 |    312 |          43661.3 | 29952000 |    22.17 |
+--------------------------------------+----------+-----------------+--------+------------------+----------+----------+


[2m[36m(pid=6435)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fe5d94de5f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 3.5
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 2.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 100
    cleaning_beam_agent-0_mean: 53.44
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 311
    cleaning_beam_agent-1_mean: 204.03
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 14.11
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 37.15
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 43.46
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 12.7
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_08-17-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 24.07
  episode_reward_min: -27.0
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu003
  info:
    grad_time_ms: 11719.402
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5263205170631409
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015227519907057285
        model: {}
        policy_loss: -0.0036494163796305656
        total_loss: -0.004542001988738775
        vf_explained_var: -0.0002019256353378296
        vf_loss: 0.33737388253211975
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48712044954299927
        entropy_coeff: 0.0017600000137463212
        kl: 0.001242476748302579
        model: {}
        policy_loss: -0.0023903183173388243
        total_loss: -0.00323601090349257
        vf_explained_var: 0.01163695752620697
        vf_loss: 0.11639255285263062
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.456434428691864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008002158137969673
        model: {}
        policy_loss: -0.0016211859183385968
        total_loss: -0.00216544303111732
        vf_explained_var: 0.0042028725147247314
        vf_loss: 2.5906620025634766
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5582543611526489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012602314818650484
        model: {}
        policy_loss: -0.0027562230825424194
        total_loss: -0.0037035690620541573
        vf_explained_var: 0.0009438246488571167
        vf_loss: 0.35186371207237244
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.714112401008606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019442785996943712
        model: {}
        policy_loss: -0.004357101395726204
        total_loss: -0.005573311820626259
        vf_explained_var: 0.011994019150733948
        vf_loss: 0.4062950015068054
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9466789960861206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011687013320624828
        model: {}
        policy_loss: -0.002541779074817896
        total_loss: -0.004180754069238901
        vf_explained_var: -0.00019793212413787842
        vf_loss: 0.27180975675582886
    load_time_ms: 21019.446
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 109313.797
    update_time_ms: 16.509
  iterations_since_restore: 153
  node_ip: 172.17.8.3
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 46.90813397129186
    ram_util_percent: 19.798564593301435
  pid: 6435
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.2
    agent-1: 1.82
    agent-2: 4.63
    agent-3: 4.65
    agent-4: 5.03
    agent-5: 3.74
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 25.445586096480678
    mean_inference_ms: 13.69194610038714
    mean_processing_ms: 64.76096320733946
  time_since_restore: 21297.65430831909
  time_this_iter_s: 146.62065982818604
  time_total_s: 43807.93755078316
  timestamp: 1637241462
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  